# Comparing `tmp/torchrec_nightly-2023.5.1-py39-none-any.whl.zip` & `tmp/torchrec_nightly-2023.5.3-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,141 +1,141 @@
-Zip file size: 339218 bytes, number of entries: 139
--rw-r--r--  2.0 unx      811 b- defN 23-May-01 11:18 torchrec/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 23-May-01 11:18 torchrec/streamable.py
--rw-r--r--  2.0 unx      854 b- defN 23-May-01 11:18 torchrec/types.py
--rw-r--r--  2.0 unx     1153 b- defN 23-May-01 11:18 torchrec/datasets/__init__.py
--rw-r--r--  2.0 unx    41469 b- defN 23-May-01 11:18 torchrec/datasets/criteo.py
--rw-r--r--  2.0 unx     4548 b- defN 23-May-01 11:18 torchrec/datasets/movielens.py
--rw-r--r--  2.0 unx     6539 b- defN 23-May-01 11:18 torchrec/datasets/random.py
--rw-r--r--  2.0 unx    10909 b- defN 23-May-01 11:18 torchrec/datasets/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-01 11:18 torchrec/datasets/scripts/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 23-May-01 11:18 torchrec/datasets/scripts/contiguous_preproc_criteo.py
--rw-r--r--  2.0 unx     2847 b- defN 23-May-01 11:18 torchrec/datasets/scripts/npy_preproc_criteo.py
--rw-r--r--  2.0 unx     3077 b- defN 23-May-01 11:18 torchrec/datasets/scripts/shuffle_preproc_criteo.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-01 11:18 torchrec/datasets/test_utils/__init__.py
--rw-r--r--  2.0 unx     5308 b- defN 23-May-01 11:18 torchrec/datasets/test_utils/criteo_test_utils.py
--rw-r--r--  2.0 unx     1912 b- defN 23-May-01 11:18 torchrec/distributed/__init__.py
--rw-r--r--  2.0 unx    37254 b- defN 23-May-01 11:18 torchrec/distributed/batched_embedding_kernel.py
--rw-r--r--  2.0 unx     2069 b- defN 23-May-01 11:18 torchrec/distributed/collective_utils.py
--rw-r--r--  2.0 unx     4925 b- defN 23-May-01 11:18 torchrec/distributed/comm.py
--rw-r--r--  2.0 unx    55820 b- defN 23-May-01 11:18 torchrec/distributed/comm_ops.py
--rw-r--r--  2.0 unx    35588 b- defN 23-May-01 11:18 torchrec/distributed/dist_data.py
--rw-r--r--  2.0 unx    29817 b- defN 23-May-01 11:18 torchrec/distributed/embedding.py
--rw-r--r--  2.0 unx     4443 b- defN 23-May-01 11:18 torchrec/distributed/embedding_kernel.py
--rw-r--r--  2.0 unx    27279 b- defN 23-May-01 11:18 torchrec/distributed/embedding_lookup.py
--rw-r--r--  2.0 unx    14963 b- defN 23-May-01 11:18 torchrec/distributed/embedding_sharding.py
--rw-r--r--  2.0 unx    37089 b- defN 23-May-01 11:18 torchrec/distributed/embedding_tower_sharding.py
--rw-r--r--  2.0 unx    15021 b- defN 23-May-01 11:18 torchrec/distributed/embedding_types.py
--rw-r--r--  2.0 unx    34339 b- defN 23-May-01 11:18 torchrec/distributed/embeddingbag.py
--rw-r--r--  2.0 unx     7373 b- defN 23-May-01 11:18 torchrec/distributed/fbgemm_qcomm_codec.py
--rw-r--r--  2.0 unx     5273 b- defN 23-May-01 11:18 torchrec/distributed/fused_embedding.py
--rw-r--r--  2.0 unx     5110 b- defN 23-May-01 11:18 torchrec/distributed/fused_embeddingbag.py
--rw-r--r--  2.0 unx     1699 b- defN 23-May-01 11:18 torchrec/distributed/fused_params.py
--rw-r--r--  2.0 unx     3807 b- defN 23-May-01 11:18 torchrec/distributed/grouped_position_weighted.py
--rw-r--r--  2.0 unx    19528 b- defN 23-May-01 11:18 torchrec/distributed/model_parallel.py
--rw-r--r--  2.0 unx    13657 b- defN 23-May-01 11:18 torchrec/distributed/quant_embedding.py
--rw-r--r--  2.0 unx    14006 b- defN 23-May-01 11:18 torchrec/distributed/quant_embedding_kernel.py
--rw-r--r--  2.0 unx    10915 b- defN 23-May-01 11:18 torchrec/distributed/quant_embeddingbag.py
--rw-r--r--  2.0 unx     9261 b- defN 23-May-01 11:18 torchrec/distributed/shard.py
--rw-r--r--  2.0 unx    19218 b- defN 23-May-01 11:18 torchrec/distributed/sharding_plan.py
--rw-r--r--  2.0 unx    22330 b- defN 23-May-01 11:18 torchrec/distributed/train_pipeline.py
--rw-r--r--  2.0 unx    24927 b- defN 23-May-01 11:18 torchrec/distributed/types.py
--rw-r--r--  2.0 unx    11373 b- defN 23-May-01 11:18 torchrec/distributed/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-01 11:18 torchrec/distributed/composable/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 23-May-01 11:18 torchrec/distributed/composable/table_batched_embedding_slice.py
--rw-r--r--  2.0 unx     1025 b- defN 23-May-01 11:18 torchrec/distributed/planner/__init__.py
--rw-r--r--  2.0 unx     3135 b- defN 23-May-01 11:18 torchrec/distributed/planner/constants.py
--rw-r--r--  2.0 unx    10318 b- defN 23-May-01 11:18 torchrec/distributed/planner/enumerators.py
--rw-r--r--  2.0 unx    12103 b- defN 23-May-01 11:18 torchrec/distributed/planner/partitioners.py
--rw-r--r--  2.0 unx      824 b- defN 23-May-01 11:18 torchrec/distributed/planner/perf_models.py
--rw-r--r--  2.0 unx    11869 b- defN 23-May-01 11:18 torchrec/distributed/planner/planners.py
--rw-r--r--  2.0 unx    11094 b- defN 23-May-01 11:18 torchrec/distributed/planner/proposers.py
--rw-r--r--  2.0 unx    40173 b- defN 23-May-01 11:18 torchrec/distributed/planner/shard_estimators.py
--rw-r--r--  2.0 unx    21410 b- defN 23-May-01 11:18 torchrec/distributed/planner/stats.py
--rw-r--r--  2.0 unx     9125 b- defN 23-May-01 11:18 torchrec/distributed/planner/storage_reservations.py
--rw-r--r--  2.0 unx    12611 b- defN 23-May-01 11:18 torchrec/distributed/planner/types.py
--rw-r--r--  2.0 unx     1119 b- defN 23-May-01 11:18 torchrec/distributed/planner/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-01 11:18 torchrec/distributed/sharding/__init__.py
--rw-r--r--  2.0 unx     2539 b- defN 23-May-01 11:18 torchrec/distributed/sharding/cw_sequence_sharding.py
--rw-r--r--  2.0 unx     9519 b- defN 23-May-01 11:18 torchrec/distributed/sharding/cw_sharding.py
--rw-r--r--  2.0 unx     2802 b- defN 23-May-01 11:18 torchrec/distributed/sharding/dp_sequence_sharding.py
--rw-r--r--  2.0 unx     7452 b- defN 23-May-01 11:18 torchrec/distributed/sharding/dp_sharding.py
--rw-r--r--  2.0 unx     5041 b- defN 23-May-01 11:18 torchrec/distributed/sharding/rw_sequence_sharding.py
--rw-r--r--  2.0 unx    12850 b- defN 23-May-01 11:18 torchrec/distributed/sharding/rw_sharding.py
--rw-r--r--  2.0 unx     3114 b- defN 23-May-01 11:18 torchrec/distributed/sharding/sequence_sharding.py
--rw-r--r--  2.0 unx     7620 b- defN 23-May-01 11:18 torchrec/distributed/sharding/tw_sequence_sharding.py
--rw-r--r--  2.0 unx    16102 b- defN 23-May-01 11:18 torchrec/distributed/sharding/tw_sharding.py
--rw-r--r--  2.0 unx     1284 b- defN 23-May-01 11:18 torchrec/distributed/sharding/twcw_sharding.py
--rw-r--r--  2.0 unx    19840 b- defN 23-May-01 11:18 torchrec/distributed/sharding/twrw_sharding.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-01 11:18 torchrec/distributed/test_utils/__init__.py
--rw-r--r--  2.0 unx    10125 b- defN 23-May-01 11:18 torchrec/distributed/test_utils/infer_utils.py
--rw-r--r--  2.0 unx     4868 b- defN 23-May-01 11:18 torchrec/distributed/test_utils/multi_process.py
--rw-r--r--  2.0 unx    34114 b- defN 23-May-01 11:18 torchrec/distributed/test_utils/test_model.py
--rw-r--r--  2.0 unx    11193 b- defN 23-May-01 11:18 torchrec/distributed/test_utils/test_model_parallel.py
--rw-r--r--  2.0 unx    25075 b- defN 23-May-01 11:18 torchrec/distributed/test_utils/test_model_parallel_base.py
--rw-r--r--  2.0 unx    15367 b- defN 23-May-01 11:18 torchrec/distributed/test_utils/test_sharding.py
--rw-r--r--  2.0 unx      422 b- defN 23-May-01 11:18 torchrec/fx/__init__.py
--rw-r--r--  2.0 unx     6477 b- defN 23-May-01 11:18 torchrec/fx/tracer.py
--rw-r--r--  2.0 unx     4401 b- defN 23-May-01 11:18 torchrec/fx/utils.py
--rw-r--r--  2.0 unx     1223 b- defN 23-May-01 11:18 torchrec/inference/__init__.py
--rw-r--r--  2.0 unx     3614 b- defN 23-May-01 11:18 torchrec/inference/client.py
--rw-r--r--  2.0 unx     3957 b- defN 23-May-01 11:18 torchrec/inference/model_packager.py
--rw-r--r--  2.0 unx     7834 b- defN 23-May-01 11:18 torchrec/inference/modules.py
--rw-r--r--  2.0 unx     3797 b- defN 23-May-01 11:18 torchrec/inference/state_dict_transform.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-01 11:18 torchrec/metrics/__init__.py
--rw-r--r--  2.0 unx    12728 b- defN 23-May-01 11:18 torchrec/metrics/auc.py
--rw-r--r--  2.0 unx     3703 b- defN 23-May-01 11:18 torchrec/metrics/calibration.py
--rw-r--r--  2.0 unx     3465 b- defN 23-May-01 11:18 torchrec/metrics/ctr.py
--rw-r--r--  2.0 unx     3836 b- defN 23-May-01 11:18 torchrec/metrics/mae.py
--rw-r--r--  2.0 unx    17573 b- defN 23-May-01 11:18 torchrec/metrics/metric_module.py
--rw-r--r--  2.0 unx     6615 b- defN 23-May-01 11:18 torchrec/metrics/metrics_config.py
--rw-r--r--  2.0 unx     3643 b- defN 23-May-01 11:18 torchrec/metrics/metrics_namespace.py
--rw-r--r--  2.0 unx     3904 b- defN 23-May-01 11:18 torchrec/metrics/model_utils.py
--rw-r--r--  2.0 unx     4631 b- defN 23-May-01 11:18 torchrec/metrics/mse.py
--rw-r--r--  2.0 unx     5605 b- defN 23-May-01 11:18 torchrec/metrics/multiclass_recall.py
--rw-r--r--  2.0 unx     6811 b- defN 23-May-01 11:18 torchrec/metrics/ne.py
--rw-r--r--  2.0 unx    30635 b- defN 23-May-01 11:18 torchrec/metrics/rec_metric.py
--rw-r--r--  2.0 unx    10490 b- defN 23-May-01 11:18 torchrec/metrics/recall_session.py
--rw-r--r--  2.0 unx     6057 b- defN 23-May-01 11:18 torchrec/metrics/throughput.py
--rw-r--r--  2.0 unx    10622 b- defN 23-May-01 11:18 torchrec/metrics/tower_qps.py
--rw-r--r--  2.0 unx     2867 b- defN 23-May-01 11:18 torchrec/metrics/weighted_avg.py
--rw-r--r--  2.0 unx    16441 b- defN 23-May-01 11:18 torchrec/metrics/test_utils/__init__.py
--rw-r--r--  2.0 unx      913 b- defN 23-May-01 11:18 torchrec/models/__init__.py
--rw-r--r--  2.0 unx    11410 b- defN 23-May-01 11:18 torchrec/models/deepfm.py
--rw-r--r--  2.0 unx    30000 b- defN 23-May-01 11:18 torchrec/models/dlrm.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-01 11:18 torchrec/models/experimental/__init__.py
--rw-r--r--  2.0 unx     9823 b- defN 23-May-01 11:18 torchrec/models/experimental/test_transformerdlrm.py
--rw-r--r--  2.0 unx     7434 b- defN 23-May-01 11:18 torchrec/models/experimental/transformerdlrm.py
--rw-r--r--  2.0 unx     1179 b- defN 23-May-01 11:18 torchrec/modules/__init__.py
--rw-r--r--  2.0 unx     1456 b- defN 23-May-01 11:18 torchrec/modules/activation.py
--rw-r--r--  2.0 unx    15163 b- defN 23-May-01 11:18 torchrec/modules/crossnet.py
--rw-r--r--  2.0 unx     8415 b- defN 23-May-01 11:18 torchrec/modules/deepfm.py
--rw-r--r--  2.0 unx     5131 b- defN 23-May-01 11:18 torchrec/modules/embedding_configs.py
--rw-r--r--  2.0 unx    12822 b- defN 23-May-01 11:18 torchrec/modules/embedding_modules.py
--rw-r--r--  2.0 unx     4858 b- defN 23-May-01 11:18 torchrec/modules/embedding_tower.py
--rw-r--r--  2.0 unx    12360 b- defN 23-May-01 11:18 torchrec/modules/feature_processor.py
--rw-r--r--  2.0 unx    31184 b- defN 23-May-01 11:18 torchrec/modules/fused_embedding_modules.py
--rw-r--r--  2.0 unx    10696 b- defN 23-May-01 11:18 torchrec/modules/lazy_extension.py
--rw-r--r--  2.0 unx     6309 b- defN 23-May-01 11:18 torchrec/modules/mlp.py
--rw-r--r--  2.0 unx     4022 b- defN 23-May-01 11:18 torchrec/modules/utils.py
--rw-r--r--  2.0 unx     1639 b- defN 23-May-01 11:18 torchrec/optim/__init__.py
--rw-r--r--  2.0 unx     2012 b- defN 23-May-01 11:18 torchrec/optim/apply_optimizer_in_backward.py
--rw-r--r--  2.0 unx     1569 b- defN 23-May-01 11:18 torchrec/optim/clipping.py
--rw-r--r--  2.0 unx     1353 b- defN 23-May-01 11:18 torchrec/optim/fused.py
--rw-r--r--  2.0 unx    16069 b- defN 23-May-01 11:18 torchrec/optim/keyed.py
--rw-r--r--  2.0 unx     4420 b- defN 23-May-01 11:18 torchrec/optim/optimizers.py
--rw-r--r--  2.0 unx     7405 b- defN 23-May-01 11:18 torchrec/optim/rowwise_adagrad.py
--rw-r--r--  2.0 unx     4865 b- defN 23-May-01 11:18 torchrec/optim/warmup.py
--rw-r--r--  2.0 unx      560 b- defN 23-May-01 11:18 torchrec/optim/test_utils/__init__.py
--rw-r--r--  2.0 unx     1140 b- defN 23-May-01 11:18 torchrec/quant/__init__.py
--rw-r--r--  2.0 unx    23109 b- defN 23-May-01 11:18 torchrec/quant/embedding_modules.py
--rw-r--r--  2.0 unx     3691 b- defN 23-May-01 11:18 torchrec/quant/utils.py
--rw-r--r--  2.0 unx     1163 b- defN 23-May-01 11:18 torchrec/sparse/__init__.py
--rw-r--r--  2.0 unx    52763 b- defN 23-May-01 11:18 torchrec/sparse/jagged_tensor.py
--rw-r--r--  2.0 unx     1430 b- defN 23-May-01 11:18 torchrec/sparse/test_utils/__init__.py
--rw-r--r--  2.0 unx     5661 b- defN 23-May-01 11:18 torchrec/test_utils/__init__.py
--rw-r--r--  2.0 unx     1530 b- defN 23-May-01 11:22 torchrec_nightly-2023.5.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     5011 b- defN 23-May-01 11:22 torchrec_nightly-2023.5.1.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-May-01 11:22 torchrec_nightly-2023.5.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-May-01 11:22 torchrec_nightly-2023.5.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    12974 b- defN 23-May-01 11:22 torchrec_nightly-2023.5.1.dist-info/RECORD
-139 files, 1380507 bytes uncompressed, 318458 bytes compressed:  76.9%
+Zip file size: 339500 bytes, number of entries: 139
+-rw-r--r--  2.0 unx      811 b- defN 23-May-03 11:23 torchrec/__init__.py
+-rw-r--r--  2.0 unx     1638 b- defN 23-May-03 11:23 torchrec/streamable.py
+-rw-r--r--  2.0 unx      854 b- defN 23-May-03 11:23 torchrec/types.py
+-rw-r--r--  2.0 unx     1153 b- defN 23-May-03 11:23 torchrec/datasets/__init__.py
+-rw-r--r--  2.0 unx    41469 b- defN 23-May-03 11:23 torchrec/datasets/criteo.py
+-rw-r--r--  2.0 unx     4548 b- defN 23-May-03 11:23 torchrec/datasets/movielens.py
+-rw-r--r--  2.0 unx     6539 b- defN 23-May-03 11:23 torchrec/datasets/random.py
+-rw-r--r--  2.0 unx    10909 b- defN 23-May-03 11:23 torchrec/datasets/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/datasets/scripts/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 23-May-03 11:23 torchrec/datasets/scripts/contiguous_preproc_criteo.py
+-rw-r--r--  2.0 unx     2847 b- defN 23-May-03 11:23 torchrec/datasets/scripts/npy_preproc_criteo.py
+-rw-r--r--  2.0 unx     3077 b- defN 23-May-03 11:23 torchrec/datasets/scripts/shuffle_preproc_criteo.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/datasets/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5308 b- defN 23-May-03 11:23 torchrec/datasets/test_utils/criteo_test_utils.py
+-rw-r--r--  2.0 unx     1912 b- defN 23-May-03 11:23 torchrec/distributed/__init__.py
+-rw-r--r--  2.0 unx    37254 b- defN 23-May-03 11:23 torchrec/distributed/batched_embedding_kernel.py
+-rw-r--r--  2.0 unx     2069 b- defN 23-May-03 11:23 torchrec/distributed/collective_utils.py
+-rw-r--r--  2.0 unx     4925 b- defN 23-May-03 11:23 torchrec/distributed/comm.py
+-rw-r--r--  2.0 unx    55820 b- defN 23-May-03 11:23 torchrec/distributed/comm_ops.py
+-rw-r--r--  2.0 unx    35443 b- defN 23-May-03 11:23 torchrec/distributed/dist_data.py
+-rw-r--r--  2.0 unx    29817 b- defN 23-May-03 11:23 torchrec/distributed/embedding.py
+-rw-r--r--  2.0 unx     4443 b- defN 23-May-03 11:23 torchrec/distributed/embedding_kernel.py
+-rw-r--r--  2.0 unx    27279 b- defN 23-May-03 11:23 torchrec/distributed/embedding_lookup.py
+-rw-r--r--  2.0 unx    14990 b- defN 23-May-03 11:23 torchrec/distributed/embedding_sharding.py
+-rw-r--r--  2.0 unx    37089 b- defN 23-May-03 11:23 torchrec/distributed/embedding_tower_sharding.py
+-rw-r--r--  2.0 unx    15021 b- defN 23-May-03 11:23 torchrec/distributed/embedding_types.py
+-rw-r--r--  2.0 unx    34625 b- defN 23-May-03 11:23 torchrec/distributed/embeddingbag.py
+-rw-r--r--  2.0 unx     7373 b- defN 23-May-03 11:23 torchrec/distributed/fbgemm_qcomm_codec.py
+-rw-r--r--  2.0 unx     5273 b- defN 23-May-03 11:23 torchrec/distributed/fused_embedding.py
+-rw-r--r--  2.0 unx     5110 b- defN 23-May-03 11:23 torchrec/distributed/fused_embeddingbag.py
+-rw-r--r--  2.0 unx     1699 b- defN 23-May-03 11:23 torchrec/distributed/fused_params.py
+-rw-r--r--  2.0 unx     3807 b- defN 23-May-03 11:23 torchrec/distributed/grouped_position_weighted.py
+-rw-r--r--  2.0 unx    19528 b- defN 23-May-03 11:23 torchrec/distributed/model_parallel.py
+-rw-r--r--  2.0 unx    13636 b- defN 23-May-03 11:23 torchrec/distributed/quant_embedding.py
+-rw-r--r--  2.0 unx    14006 b- defN 23-May-03 11:23 torchrec/distributed/quant_embedding_kernel.py
+-rw-r--r--  2.0 unx    10833 b- defN 23-May-03 11:23 torchrec/distributed/quant_embeddingbag.py
+-rw-r--r--  2.0 unx     9261 b- defN 23-May-03 11:23 torchrec/distributed/shard.py
+-rw-r--r--  2.0 unx    19218 b- defN 23-May-03 11:23 torchrec/distributed/sharding_plan.py
+-rw-r--r--  2.0 unx    22330 b- defN 23-May-03 11:23 torchrec/distributed/train_pipeline.py
+-rw-r--r--  2.0 unx    24927 b- defN 23-May-03 11:23 torchrec/distributed/types.py
+-rw-r--r--  2.0 unx    11373 b- defN 23-May-03 11:23 torchrec/distributed/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/distributed/composable/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 23-May-03 11:23 torchrec/distributed/composable/table_batched_embedding_slice.py
+-rw-r--r--  2.0 unx     1025 b- defN 23-May-03 11:23 torchrec/distributed/planner/__init__.py
+-rw-r--r--  2.0 unx     3135 b- defN 23-May-03 11:23 torchrec/distributed/planner/constants.py
+-rw-r--r--  2.0 unx    10318 b- defN 23-May-03 11:23 torchrec/distributed/planner/enumerators.py
+-rw-r--r--  2.0 unx    12485 b- defN 23-May-03 11:23 torchrec/distributed/planner/partitioners.py
+-rw-r--r--  2.0 unx      824 b- defN 23-May-03 11:23 torchrec/distributed/planner/perf_models.py
+-rw-r--r--  2.0 unx    12224 b- defN 23-May-03 11:23 torchrec/distributed/planner/planners.py
+-rw-r--r--  2.0 unx    11094 b- defN 23-May-03 11:23 torchrec/distributed/planner/proposers.py
+-rw-r--r--  2.0 unx    40173 b- defN 23-May-03 11:23 torchrec/distributed/planner/shard_estimators.py
+-rw-r--r--  2.0 unx    21410 b- defN 23-May-03 11:23 torchrec/distributed/planner/stats.py
+-rw-r--r--  2.0 unx     9125 b- defN 23-May-03 11:23 torchrec/distributed/planner/storage_reservations.py
+-rw-r--r--  2.0 unx    12879 b- defN 23-May-03 11:23 torchrec/distributed/planner/types.py
+-rw-r--r--  2.0 unx     1119 b- defN 23-May-03 11:23 torchrec/distributed/planner/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/distributed/sharding/__init__.py
+-rw-r--r--  2.0 unx     2539 b- defN 23-May-03 11:23 torchrec/distributed/sharding/cw_sequence_sharding.py
+-rw-r--r--  2.0 unx     9519 b- defN 23-May-03 11:23 torchrec/distributed/sharding/cw_sharding.py
+-rw-r--r--  2.0 unx     2802 b- defN 23-May-03 11:23 torchrec/distributed/sharding/dp_sequence_sharding.py
+-rw-r--r--  2.0 unx     7452 b- defN 23-May-03 11:23 torchrec/distributed/sharding/dp_sharding.py
+-rw-r--r--  2.0 unx     5041 b- defN 23-May-03 11:23 torchrec/distributed/sharding/rw_sequence_sharding.py
+-rw-r--r--  2.0 unx    12850 b- defN 23-May-03 11:23 torchrec/distributed/sharding/rw_sharding.py
+-rw-r--r--  2.0 unx     3114 b- defN 23-May-03 11:23 torchrec/distributed/sharding/sequence_sharding.py
+-rw-r--r--  2.0 unx     7609 b- defN 23-May-03 11:23 torchrec/distributed/sharding/tw_sequence_sharding.py
+-rw-r--r--  2.0 unx    16061 b- defN 23-May-03 11:23 torchrec/distributed/sharding/tw_sharding.py
+-rw-r--r--  2.0 unx     1284 b- defN 23-May-03 11:23 torchrec/distributed/sharding/twcw_sharding.py
+-rw-r--r--  2.0 unx    19840 b- defN 23-May-03 11:23 torchrec/distributed/sharding/twrw_sharding.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/__init__.py
+-rw-r--r--  2.0 unx    10125 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/infer_utils.py
+-rw-r--r--  2.0 unx     4868 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/multi_process.py
+-rw-r--r--  2.0 unx    34114 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/test_model.py
+-rw-r--r--  2.0 unx    11193 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/test_model_parallel.py
+-rw-r--r--  2.0 unx    25075 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/test_model_parallel_base.py
+-rw-r--r--  2.0 unx    15367 b- defN 23-May-03 11:23 torchrec/distributed/test_utils/test_sharding.py
+-rw-r--r--  2.0 unx      422 b- defN 23-May-03 11:23 torchrec/fx/__init__.py
+-rw-r--r--  2.0 unx     6477 b- defN 23-May-03 11:23 torchrec/fx/tracer.py
+-rw-r--r--  2.0 unx     4401 b- defN 23-May-03 11:23 torchrec/fx/utils.py
+-rw-r--r--  2.0 unx     1223 b- defN 23-May-03 11:23 torchrec/inference/__init__.py
+-rw-r--r--  2.0 unx     3614 b- defN 23-May-03 11:23 torchrec/inference/client.py
+-rw-r--r--  2.0 unx     3957 b- defN 23-May-03 11:23 torchrec/inference/model_packager.py
+-rw-r--r--  2.0 unx     7834 b- defN 23-May-03 11:23 torchrec/inference/modules.py
+-rw-r--r--  2.0 unx     3797 b- defN 23-May-03 11:23 torchrec/inference/state_dict_transform.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/metrics/__init__.py
+-rw-r--r--  2.0 unx    12728 b- defN 23-May-03 11:23 torchrec/metrics/auc.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-May-03 11:23 torchrec/metrics/calibration.py
+-rw-r--r--  2.0 unx     3465 b- defN 23-May-03 11:23 torchrec/metrics/ctr.py
+-rw-r--r--  2.0 unx     3836 b- defN 23-May-03 11:23 torchrec/metrics/mae.py
+-rw-r--r--  2.0 unx    17573 b- defN 23-May-03 11:23 torchrec/metrics/metric_module.py
+-rw-r--r--  2.0 unx     6615 b- defN 23-May-03 11:23 torchrec/metrics/metrics_config.py
+-rw-r--r--  2.0 unx     3643 b- defN 23-May-03 11:23 torchrec/metrics/metrics_namespace.py
+-rw-r--r--  2.0 unx     3904 b- defN 23-May-03 11:23 torchrec/metrics/model_utils.py
+-rw-r--r--  2.0 unx     4631 b- defN 23-May-03 11:23 torchrec/metrics/mse.py
+-rw-r--r--  2.0 unx     5605 b- defN 23-May-03 11:23 torchrec/metrics/multiclass_recall.py
+-rw-r--r--  2.0 unx     6811 b- defN 23-May-03 11:23 torchrec/metrics/ne.py
+-rw-r--r--  2.0 unx    30635 b- defN 23-May-03 11:23 torchrec/metrics/rec_metric.py
+-rw-r--r--  2.0 unx    10490 b- defN 23-May-03 11:23 torchrec/metrics/recall_session.py
+-rw-r--r--  2.0 unx     6057 b- defN 23-May-03 11:23 torchrec/metrics/throughput.py
+-rw-r--r--  2.0 unx    10622 b- defN 23-May-03 11:23 torchrec/metrics/tower_qps.py
+-rw-r--r--  2.0 unx     2867 b- defN 23-May-03 11:23 torchrec/metrics/weighted_avg.py
+-rw-r--r--  2.0 unx    16441 b- defN 23-May-03 11:23 torchrec/metrics/test_utils/__init__.py
+-rw-r--r--  2.0 unx      913 b- defN 23-May-03 11:23 torchrec/models/__init__.py
+-rw-r--r--  2.0 unx    11410 b- defN 23-May-03 11:23 torchrec/models/deepfm.py
+-rw-r--r--  2.0 unx    30000 b- defN 23-May-03 11:23 torchrec/models/dlrm.py
+-rw-r--r--  2.0 unx        0 b- defN 23-May-03 11:23 torchrec/models/experimental/__init__.py
+-rw-r--r--  2.0 unx     9823 b- defN 23-May-03 11:23 torchrec/models/experimental/test_transformerdlrm.py
+-rw-r--r--  2.0 unx     7434 b- defN 23-May-03 11:23 torchrec/models/experimental/transformerdlrm.py
+-rw-r--r--  2.0 unx     1179 b- defN 23-May-03 11:23 torchrec/modules/__init__.py
+-rw-r--r--  2.0 unx     1456 b- defN 23-May-03 11:23 torchrec/modules/activation.py
+-rw-r--r--  2.0 unx    15163 b- defN 23-May-03 11:23 torchrec/modules/crossnet.py
+-rw-r--r--  2.0 unx     8415 b- defN 23-May-03 11:23 torchrec/modules/deepfm.py
+-rw-r--r--  2.0 unx     5131 b- defN 23-May-03 11:23 torchrec/modules/embedding_configs.py
+-rw-r--r--  2.0 unx    12822 b- defN 23-May-03 11:23 torchrec/modules/embedding_modules.py
+-rw-r--r--  2.0 unx     4858 b- defN 23-May-03 11:23 torchrec/modules/embedding_tower.py
+-rw-r--r--  2.0 unx    12360 b- defN 23-May-03 11:23 torchrec/modules/feature_processor.py
+-rw-r--r--  2.0 unx    31184 b- defN 23-May-03 11:23 torchrec/modules/fused_embedding_modules.py
+-rw-r--r--  2.0 unx    10696 b- defN 23-May-03 11:23 torchrec/modules/lazy_extension.py
+-rw-r--r--  2.0 unx     6309 b- defN 23-May-03 11:23 torchrec/modules/mlp.py
+-rw-r--r--  2.0 unx     4022 b- defN 23-May-03 11:23 torchrec/modules/utils.py
+-rw-r--r--  2.0 unx     1639 b- defN 23-May-03 11:23 torchrec/optim/__init__.py
+-rw-r--r--  2.0 unx     2012 b- defN 23-May-03 11:23 torchrec/optim/apply_optimizer_in_backward.py
+-rw-r--r--  2.0 unx     1569 b- defN 23-May-03 11:23 torchrec/optim/clipping.py
+-rw-r--r--  2.0 unx     1353 b- defN 23-May-03 11:23 torchrec/optim/fused.py
+-rw-r--r--  2.0 unx    16069 b- defN 23-May-03 11:23 torchrec/optim/keyed.py
+-rw-r--r--  2.0 unx     4420 b- defN 23-May-03 11:23 torchrec/optim/optimizers.py
+-rw-r--r--  2.0 unx     7405 b- defN 23-May-03 11:23 torchrec/optim/rowwise_adagrad.py
+-rw-r--r--  2.0 unx     4865 b- defN 23-May-03 11:23 torchrec/optim/warmup.py
+-rw-r--r--  2.0 unx      560 b- defN 23-May-03 11:23 torchrec/optim/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1140 b- defN 23-May-03 11:23 torchrec/quant/__init__.py
+-rw-r--r--  2.0 unx    23109 b- defN 23-May-03 11:23 torchrec/quant/embedding_modules.py
+-rw-r--r--  2.0 unx     3691 b- defN 23-May-03 11:23 torchrec/quant/utils.py
+-rw-r--r--  2.0 unx     1163 b- defN 23-May-03 11:23 torchrec/sparse/__init__.py
+-rw-r--r--  2.0 unx    52763 b- defN 23-May-03 11:23 torchrec/sparse/jagged_tensor.py
+-rw-r--r--  2.0 unx     1430 b- defN 23-May-03 11:23 torchrec/sparse/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5661 b- defN 23-May-03 11:23 torchrec/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1530 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5011 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    12974 b- defN 23-May-03 11:28 torchrec_nightly-2023.5.3.dist-info/RECORD
+139 files, 1381525 bytes uncompressed, 318740 bytes compressed:  76.9%
```

## zipnote {}

```diff
@@ -396,23 +396,23 @@
 
 Filename: torchrec/sparse/test_utils/__init__.py
 Comment: 
 
 Filename: torchrec/test_utils/__init__.py
 Comment: 
 
-Filename: torchrec_nightly-2023.5.1.dist-info/LICENSE
+Filename: torchrec_nightly-2023.5.3.dist-info/LICENSE
 Comment: 
 
-Filename: torchrec_nightly-2023.5.1.dist-info/METADATA
+Filename: torchrec_nightly-2023.5.3.dist-info/METADATA
 Comment: 
 
-Filename: torchrec_nightly-2023.5.1.dist-info/WHEEL
+Filename: torchrec_nightly-2023.5.3.dist-info/WHEEL
 Comment: 
 
-Filename: torchrec_nightly-2023.5.1.dist-info/top_level.txt
+Filename: torchrec_nightly-2023.5.3.dist-info/top_level.txt
 Comment: 
 
-Filename: torchrec_nightly-2023.5.1.dist-info/RECORD
+Filename: torchrec_nightly-2023.5.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchrec/distributed/dist_data.py

```diff
@@ -17,15 +17,15 @@
     all_gather_base_pooled,
     alltoall_pooled,
     alltoall_sequence,
     reduce_scatter_base_pooled,
     reduce_scatter_v_pooled,
 )
 from torchrec.distributed.embedding_types import KJTList
-from torchrec.distributed.types import Awaitable, NoWait, QuantizedCommCodecs
+from torchrec.distributed.types import Awaitable, QuantizedCommCodecs
 from torchrec.fx.utils import fx_marker
 from torchrec.sparse.jagged_tensor import KeyedJaggedTensor
 
 try:
     torch.ops.load_library("//deeplearning/fbgemm/fbgemm_gpu:sparse_ops")
     torch.ops.load_library("//deeplearning/fbgemm/fbgemm_gpu:sparse_ops_cpu")
     torch.ops.load_library("//deeplearning/fbgemm/fbgemm_gpu:merge_pooled_embeddings")
@@ -494,15 +494,15 @@
         world_size: int,
     ) -> None:
         super().__init__()
         self._splits = splits
         self._world_size = world_size
         assert self._world_size == len(splits)
 
-    def forward(self, kjt: KeyedJaggedTensor) -> Awaitable[KJTList]:
+    def forward(self, kjt: KeyedJaggedTensor) -> KJTList:
         """
         Splits features first and then sends the slices to the corresponding devices.
 
         Args:
             kjt (KeyedJaggedTensor): the input features.
 
         Returns:
@@ -510,15 +510,15 @@
         """
         fx_marker("KJT_ONE_TO_ALL_FORWARD_BEGIN", kjt)
         kjts: List[KeyedJaggedTensor] = kjt.split(self._splits)
         dist_kjts = [
             kjts[rank].to(torch.device("cuda", rank), non_blocking=True)
             for rank in range(self._world_size)
         ]
-        ret = NoWait(KJTList(dist_kjts))
+        ret = KJTList(dist_kjts)
         fx_marker("KJT_ONE_TO_ALL_FORWARD_END", kjt)
         return ret
 
 
 class PooledEmbeddingsAwaitable(Awaitable[torch.Tensor]):
     """
     Awaitable for pooled embeddings after collective operation.
@@ -671,34 +671,32 @@
         cat_dim: int,
     ) -> None:
         super().__init__()
         self._device = device
         self._world_size = world_size
         self._cat_dim = cat_dim
 
-    def forward(self, tensors: List[torch.Tensor]) -> Awaitable[torch.Tensor]:
+    def forward(self, tensors: List[torch.Tensor]) -> torch.Tensor:
         """
         Performs AlltoOne operation on pooled/sequence embeddings tensors.
 
         Args:
             tensors (List[torch.Tensor]): list of embedding tensors.
 
         Returns:
             Awaitable[torch.Tensor]: awaitable of the merged embeddings.
         """
         assert len(tensors) == self._world_size
         non_cat_size = tensors[0].size(1 - self._cat_dim)
-        return NoWait(
-            torch.ops.fbgemm.merge_pooled_embeddings(
-                tensors,
-                non_cat_size,
-                # syntax for torchscript
-                str(self._device),
-                self._cat_dim,
-            )
+        return torch.ops.fbgemm.merge_pooled_embeddings(
+            tensors,
+            non_cat_size,
+            # syntax for torchscript
+            str(self._device),
+            self._cat_dim,
         )
 
 
 class SeqEmbeddingsAllToOne(nn.Module):
     """
     Merges the pooled/sequence embedding tensor on each device into single tensor.
 
@@ -714,31 +712,29 @@
         device: torch.device,
         world_size: int,
     ) -> None:
         super().__init__()
         self._device = device
         self._world_size = world_size
 
-    def forward(self, tensors: List[torch.Tensor]) -> Awaitable[List[torch.Tensor]]:
+    def forward(self, tensors: List[torch.Tensor]) -> List[torch.Tensor]:
         """
         Performs AlltoOne operation on pooled embeddings tensors.
 
         Args:
             tensors (List[torch.Tensor]): list of pooled embedding tensors.
 
         Returns:
             Awaitable[torch.Tensor]: awaitable of the merged pooled embeddings.
         """
 
         assert len(tensors) == self._world_size
-        return NoWait(
-            torch.ops.fbgemm.all_to_one_device(
-                tensors,
-                self._device,
-            )
+        return torch.ops.fbgemm.all_to_one_device(
+            tensors,
+            self._device,
         )
 
 
 class PooledEmbeddingsReduceScatter(nn.Module):
     """
     The module class that wraps reduce-scatter communication primitives for pooled
     embedding communication in row-wise and twrw sharding.
```

## torchrec/distributed/embedding_sharding.py

```diff
@@ -3,15 +3,15 @@
 # All rights reserved.
 #
 # This source code is licensed under the BSD-style license found in the
 # LICENSE file in the root directory of this source tree.
 
 import abc
 from dataclasses import dataclass, field
-from typing import Any, Dict, Generic, List, Optional, Tuple, TypeVar
+from typing import Any, Dict, Generic, List, Optional, Tuple, TypeVar, Union
 
 import torch
 from torch import nn
 from torchrec.distributed.dist_data import KJTAllToAllTensorsAwaitable
 from torchrec.distributed.embedding_types import (
     BaseEmbeddingLookup,
     BaseGroupedFeatureProcessor,
@@ -357,29 +357,29 @@
     Converts input from data-parallel to model-parallel.
     """
 
     @abc.abstractmethod
     def forward(
         self,
         sparse_features: KeyedJaggedTensor,
-    ) -> Awaitable[Awaitable[F]]:
+    ) -> Union[Awaitable[Awaitable[F]], F]:
         pass
 
 
 class BaseEmbeddingDist(abc.ABC, nn.Module, Generic[C, T, W]):
     """
     Converts output of EmbeddingLookup from model-parallel to data-parallel.
     """
 
     @abc.abstractmethod
     def forward(
         self,
         local_embs: T,
         sharding_ctx: Optional[C] = None,
-    ) -> Awaitable[W]:
+    ) -> Union[Awaitable[W], W]:
         pass
 
 
 class EmbeddingSharding(abc.ABC, Generic[C, F, T, W], FeatureShardingMixIn):
     """
     Used to implement different sharding types for `EmbeddingBagCollection`, e.g.
     table_wise.
```

## torchrec/distributed/embeddingbag.py

```diff
@@ -260,37 +260,49 @@
 def _check_need_pos(module: EmbeddingBagCollectionInterface) -> bool:
     for config in module.embedding_bag_configs():
         if config.need_pos:
             return True
     return False
 
 
+def construct_output_kt(
+    embeddings: List[torch.Tensor],
+    embedding_names: List[str],
+    embedding_dims: List[int],
+) -> KeyedTensor:
+    cat_embeddings: torch.Tensor
+    if len(embeddings) == 1:
+        cat_embeddings = embeddings[0]
+    else:
+        cat_embeddings = torch.cat(embeddings, dim=1)
+    return KeyedTensor(
+        keys=embedding_names,
+        length_per_key=embedding_dims,
+        values=cat_embeddings,
+        key_dim=1,
+    )
+
+
 class EmbeddingBagCollectionAwaitable(LazyAwaitable[KeyedTensor]):
     def __init__(
         self,
         awaitables: List[Awaitable[torch.Tensor]],
         embedding_dims: List[int],
         embedding_names: List[str],
     ) -> None:
         super().__init__()
         self._awaitables = awaitables
         self._embedding_dims = embedding_dims
         self._embedding_names = embedding_names
 
     def _wait_impl(self) -> KeyedTensor:
-        embeddings = [w.wait() for w in self._awaitables]
-        if len(embeddings) == 1:
-            embeddings = embeddings[0]
-        else:
-            embeddings = torch.cat(embeddings, dim=1)
-        return KeyedTensor(
-            keys=self._embedding_names,
-            length_per_key=self._embedding_dims,
-            values=embeddings,
-            key_dim=1,
+        return construct_output_kt(
+            embeddings=[w.wait() for w in self._awaitables],
+            embedding_names=self._embedding_names,
+            embedding_dims=self._embedding_dims,
         )
 
 
 @dataclass
 class EmbeddingBagCollectionContext(Multistreamable):
     sharding_contexts: List[Optional[EmbeddingShardingContext]] = field(
         default_factory=list
```

## torchrec/distributed/quant_embedding.py

```diff
@@ -290,15 +290,15 @@
                 )
             features_by_sharding = features.split(
                 self._feature_splits,
             )
 
             return ListOfKJTList(
                 [
-                    self._input_dists[i].forward(features_by_sharding[i]).wait().wait()
+                    self._input_dists[i].forward(features_by_sharding[i])
                     for i in range(len(self._input_dists))
                 ]
             )
 
     def compute(
         self, ctx: EmbeddingCollectionContext, dist_input: ListOfKJTList
     ) -> List[List[torch.Tensor]]:
@@ -320,15 +320,15 @@
         emb_per_sharding: List[List[torch.Tensor]] = []
         features_per_sharding: List[List[KeyedJaggedTensor]] = []
         for odist, embeddings, sharding_ctx in zip(
             self._output_dists,
             output,
             ctx.sharding_contexts,
         ):
-            emb_per_sharding.append(odist.forward(embeddings, sharding_ctx).wait())
+            emb_per_sharding.append(odist.forward(embeddings, sharding_ctx))
             features_per_sharding.append(sharding_ctx.features)
 
         return output_jt_dict(
             emb_per_sharding, features_per_sharding, self._need_indices
         )
 
     # pyre-ignore
```

## torchrec/distributed/quant_embeddingbag.py

```diff
@@ -21,16 +21,16 @@
     FeatureShardingMixIn,
     GroupedEmbeddingConfig,
     KJTList,
     ListOfKJTList,
     ShardedEmbeddingModule,
 )
 from torchrec.distributed.embeddingbag import (
+    construct_output_kt,
     create_sharding_infos_by_sharding,
-    EmbeddingBagCollectionAwaitable,
 )
 from torchrec.distributed.fused_params import (
     get_tbes_to_register_from_iterable,
     is_fused_param_register_tbe,
 )
 from torchrec.distributed.sharding.tw_sharding import InferTwEmbeddingSharding
 from torchrec.distributed.types import (
@@ -213,15 +213,15 @@
                     self._features_order,
                     # pyre-ignore [6]
                     self._features_order_tensor,
                 )
             features_by_shards = features.split(self._feature_splits)
             return ListOfKJTList(
                 [
-                    self._input_dists[i].forward(features_by_shards[i]).wait().wait()
+                    self._input_dists[i].forward(features_by_shards[i])
                     for i in range(len(self._input_dists))
                 ]
             )
 
     def compute(
         self,
         ctx: NullShardedModuleContext,
@@ -232,22 +232,21 @@
 
     # pyre-ignore
     def output_dist(
         self,
         ctx: NullShardedModuleContext,
         output: List[List[torch.Tensor]],
     ) -> KeyedTensor:
-        return EmbeddingBagCollectionAwaitable(
-            awaitables=[
+        return construct_output_kt(
+            embeddings=[
                 dist.forward(output[i]) for i, dist in enumerate(self._output_dists)
             ],
-            # syntax for torchscript
             embedding_dims=self._embedding_dims,
             embedding_names=self._embedding_names,
-        ).wait()
+        )
 
     # pyre-ignore
     def compute_and_output_dist(
         self, ctx: NullShardedModuleContext, input: ListOfKJTList
     ) -> KeyedTensor:
         return self.output_dist(ctx, self.compute(ctx, input))
```

## torchrec/distributed/planner/partitioners.py

```diff
@@ -138,14 +138,18 @@
             topology.devices[1].perf = (1,2) + (3,4)
 
             # The topology updates are done after the end of all the placements (the other
             # in the example is just for clarity).
         """
 
         _topology: Topology = copy.deepcopy(storage_constraint)
+        # shallow copy to keep an almost sorted list around
+        # we try to not modify the order of devices in the topology
+        # since _get_host_level_devices relies on the order
+        sorted_devices = _topology.devices.copy()
         _host_level_devices = GreedyPerfPartitioner._get_host_level_devices(_topology)
 
         # first partition the uniform sharding options (RW & DP)
         uniform_sharding_options = _get_uniform_sharding_options(proposal)
         GreedyPerfPartitioner._uniform_partition(
             uniform_sharding_options, _topology.devices
         )
@@ -167,15 +171,15 @@
                 == PartitionByType.DEVICE.value
             ):
                 assert (
                     len(sharding_option_group.sharding_options) == 1
                 ), f"Unexpected length for sharding options: {len(sharding_option_group.sharding_options)}"
                 GreedyPerfPartitioner._device_partition(
                     sharding_option_group.sharding_options[0],
-                    _topology.devices,
+                    sorted_devices,
                     _topology.local_world_size,
                 )
             else:
                 raise RuntimeError(
                     f"Unexpected sharding option group {sharding_option_group}"
                 )
         # pyre-ignore [16]: `GreedyPerfPartitioner` has no attribute `_topology`.
@@ -185,25 +189,27 @@
     @staticmethod
     def _device_partition(
         sharding_option: ShardingOption,
         devices: List[DeviceHardware],
         local_world_size: int = 1,
     ) -> None:
         for shard in sharding_option.shards:
-            sorted_devices = sorted(
-                devices,
+            devices.sort(
                 # We use the "local_rank" as the secondary key for sorting. This
                 # is to even out the pressure on different hosts. For example, in UVM
                 # case, we will allocate UVM table with the global rank order, and host0
                 # will use a lot more CPU memory than the others. With local rank as the
                 # secondary key, we could even out CPU memory pressure on different host
-                key=lambda device: (device.perf, device.rank % local_world_size),
+                key=lambda device: (
+                    device.perf,
+                    device.rank % local_world_size,
+                )
             )
             success = False
-            for device in sorted_devices:
+            for device in devices:
                 if cast(Storage, shard.storage).fits_in(device.storage):
                     shard.rank = device.rank
                     device.storage -= cast(Storage, shard.storage)
                     device.perf += cast(float, shard.perf)
                     success = True
                     break
             if not success:
@@ -249,14 +255,16 @@
                         )
                 except PlannerError:
                     success = False
                     break
             if success:
                 # successfully found a host and partitioned on that host
                 # need to update the devices
+                # resorting host_devices before copying data back
+                host_devices.sort(key=lambda device: device.rank)
                 for device, device_copy in zip(devices, host_devices):
                     device.storage = device_copy.storage
                     device.perf = device_copy.perf
                 return
         raise PlannerError(
             error_type=PlannerErrorType.PARTITION,
             message=f"can't find a host for sharding option group {sharding_option_group}",
```

## torchrec/distributed/planner/planners.py

```diff
@@ -52,14 +52,20 @@
     ShardingPlan,
     ShardingPlanner,
     ShardingType,
     ShardMetadata,
 )
 
 
+def _reset_shard_rank(proposal: List[ShardingOption]) -> None:
+    for sharding_option in proposal:
+        for shard in sharding_option.shards:
+            shard.rank = None
+
+
 def _to_sharding_plan(
     sharding_options: List[ShardingOption],
     topology: Topology,
 ) -> ShardingPlan:
 
     compute_device = topology.compute_device
     local_size = topology.local_world_size
@@ -233,23 +239,24 @@
                         perf_rating=perf_rating,
                     )
                     proposal = proposer.propose()
                     continue
 
                 self._num_proposals += 1
                 try:
+                    # plan is just proposal where shard.rank is populated
                     plan = self._partitioner.partition(
-                        proposal=copy.deepcopy(proposal),
+                        proposal=proposal,
                         storage_constraint=storage_constraint,
                     )
                     self._num_plans += 1
                     perf_rating = self._perf_model.rate(plan=plan)
                     if perf_rating < best_perf_rating:
                         best_perf_rating = perf_rating
-                        best_plan = plan
+                        best_plan = copy.deepcopy(plan)
                     proposal_cache[proposal_key] = (True, plan, perf_rating)
                     proposer.feedback(
                         partitionable=True, plan=plan, perf_rating=perf_rating
                     )
                 except PlannerError:
                     current_storage = cast(
                         Storage,
@@ -263,14 +270,16 @@
                         ),
                     )
                     if current_storage < lowest_storage:
                         lowest_storage = current_storage
                     proposal_cache[proposal_key] = (False, None, None)
                     proposer.feedback(partitionable=False)
 
+                # clear shard.rank for each sharding_option
+                _reset_shard_rank(proposal)
                 proposal = proposer.propose()
 
         if best_plan:
             self._best_plan = best_plan
             sharding_plan = _to_sharding_plan(best_plan, self._topology)
 
             end_time = perf_counter()
```

## torchrec/distributed/planner/types.py

```diff
@@ -215,14 +215,15 @@
         self.sharding_type = sharding_type
         self.partition_by = partition_by
         self.compute_kernel = compute_kernel
         # relevant to planner output, must be populated if sharding option
         # part of final solution
         self.shards = shards
         self.dependency = dependency
+        self._is_pooled: Optional[bool] = None
 
     @property
     def tensor(self) -> torch.Tensor:
         return self._tensor
 
     @property
     def module(self) -> Tuple[str, nn.Module]:
@@ -249,22 +250,28 @@
         storage: Storage = Storage(hbm=0, ddr=0)
         for shard in self.shards:
             storage += cast(Storage, shard.storage)
         return storage
 
     @property
     def is_pooled(self) -> bool:
+        if self._is_pooled is not None:
+            return self._is_pooled
+
         if isinstance(self.module[1], EmbeddingCollectionInterface):
-            return False
+            self._is_pooled = False
+            return self.is_pooled
         for module in self.module[1].modules():
             if isinstance(module, EmbeddingCollectionInterface):
                 for name, _ in module.named_parameters():
                     if self.name in name:
-                        return False
-        return True
+                        self._is_pooled = False
+                        return self._is_pooled
+        self._is_pooled = True
+        return self._is_pooled
 
     def __hash__(self) -> int:
         return hash(
             (
                 self.fqn,
                 self.sharding_type,
                 self.compute_kernel,
```

## torchrec/distributed/sharding/tw_sequence_sharding.py

```diff
@@ -167,15 +167,15 @@
         super().__init__()
         self._dist: SeqEmbeddingsAllToOne = SeqEmbeddingsAllToOne(device, world_size)
 
     def forward(
         self,
         local_embs: List[torch.Tensor],
         sharding_ctx: Optional[InferSequenceShardingContext] = None,
-    ) -> Awaitable[List[torch.Tensor]]:
+    ) -> List[torch.Tensor]:
         """
         Performs AlltoOne operation on sequence embeddings tensor.
 
         Args:
             local_embs (List[orch.Tensor]): tensor of values to distribute.
             sharding_ctx (InferSequenceShardingContext): shared context from KJTAllToOne
                 operation.
```

## torchrec/distributed/sharding/tw_sharding.py

```diff
@@ -368,25 +368,25 @@
             features_per_rank,
             world_size,
         )
 
     def forward(
         self,
         sparse_features: KeyedJaggedTensor,
-    ) -> Awaitable[Awaitable[KJTList]]:
+    ) -> KJTList:
         """
         Performs OnetoAll operation on sparse features.
 
         Args:
             sparse_features (KeyedJaggedTensor): sparse features to redistribute.
 
         Returns:
             Awaitable[Awaitable[KeyedJaggedTensor]]: awaitable of awaitable of KeyedJaggedTensor.
         """
-        return NoWait(self._dist.forward(sparse_features))
+        return self._dist.forward(sparse_features)
 
 
 class InferTwPooledEmbeddingDist(
     BaseEmbeddingDist[NullShardingContext, List[torch.Tensor], torch.Tensor]
 ):
     """
     Merges pooled embedding tensor from each device for inference.
@@ -404,15 +404,15 @@
         super().__init__()
         self._dist: EmbeddingsAllToOne = EmbeddingsAllToOne(device, world_size, 1)
 
     def forward(
         self,
         local_embs: List[torch.Tensor],
         sharding_ctx: Optional[NullShardingContext] = None,
-    ) -> Awaitable[torch.Tensor]:
+    ) -> torch.Tensor:
         """
         Performs AlltoOne operation on pooled embedding tensors.
 
         Args:
             local_embs (List[torch.Tensor]): pooled embedding tensors with
                 `len(local_embs) == world_size`.
```

## Comparing `torchrec_nightly-2023.5.1.dist-info/LICENSE` & `torchrec_nightly-2023.5.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchrec_nightly-2023.5.1.dist-info/METADATA` & `torchrec_nightly-2023.5.3.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: torchrec-nightly
-Version: 2023.5.1
+Version: 2023.5.3
 Summary: Pytorch domain library for recommendation systems
 Home-page: https://github.com/pytorch/torchrec
 Author: TorchRec Team
 Author-email: packages@pytorch.org
 License: BSD-3
 Keywords: pytorch,recommendation systems,sharding
 Classifier: Development Status :: 4 - Beta
```

## Comparing `torchrec_nightly-2023.5.1.dist-info/RECORD` & `torchrec_nightly-2023.5.3.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -13,60 +13,60 @@
 torchrec/datasets/test_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/datasets/test_utils/criteo_test_utils.py,sha256=Ob2fJniGOsfbWNF_Gy2RJhrGAVnLAFPSlUTJOp2kay4,5308
 torchrec/distributed/__init__.py,sha256=VCy8GKOM-1dejxUWNSA3gozG3HQ4x5-Y9c9-WFbAMGg,1912
 torchrec/distributed/batched_embedding_kernel.py,sha256=av-G7HZpWNhzaDaFSc1SzlU6n_-qC5EW41zqrBpkXfY,37254
 torchrec/distributed/collective_utils.py,sha256=r7Aawq-KSVC-HjjEd6U8k0vNnRMx_-8_sAhYdElGaJw,2069
 torchrec/distributed/comm.py,sha256=ag0NuBUMGpTY8DLKfd7IVs6yRgVdsRwQFKXyDiqjoVg,4925
 torchrec/distributed/comm_ops.py,sha256=vrTHX7h81UxYoS5X3ufY4lE6WC2Ufo4mMJpGQC4dyV0,55820
-torchrec/distributed/dist_data.py,sha256=-4k2EZGr8FhBLg_7xn14f9F3QiTetPgNbXOPTEjJnuw,35588
+torchrec/distributed/dist_data.py,sha256=TwNB0e24k_tmn0XARaXKjUCf-92bgFDzBncj8BcXobY,35443
 torchrec/distributed/embedding.py,sha256=Ew0hvvY4XxwxYmvtCXa4Y6D_yhNkYdcHx0BOA_2nKbw,29817
 torchrec/distributed/embedding_kernel.py,sha256=X24OliYNs5D2taRYncTQGN5MdUKWYZvv4FpXzgSho0c,4443
 torchrec/distributed/embedding_lookup.py,sha256=u_mAudcQEdgMdyahhwIB1hfa9CF5D7whaVFDQ-EBzpI,27279
-torchrec/distributed/embedding_sharding.py,sha256=oHJOZ_1MbazMRya_qqkHeiNBCcYywT0eHHzhwmjdC48,14963
+torchrec/distributed/embedding_sharding.py,sha256=_4xDvyTm9Ut5XwnQGiANmvko3AzkgaB50d9Ooxr55xI,14990
 torchrec/distributed/embedding_tower_sharding.py,sha256=ypr4JbTZUh_35dYUoKWoOSJNEVG3c6gV9g5gt-fs6lQ,37089
 torchrec/distributed/embedding_types.py,sha256=RAyug556PJTk5QlutuiXbo3ip4r9YXAmJupTNLhqKuk,15021
-torchrec/distributed/embeddingbag.py,sha256=_Rdr7IrssDEjAUDO9qipEbOZs94TR5mDAQtLUUFmTYk,34339
+torchrec/distributed/embeddingbag.py,sha256=6H9HFiHGgX9LXt67GKfFazrp5iWrbD30HleJZP8btas,34625
 torchrec/distributed/fbgemm_qcomm_codec.py,sha256=StYltKC6Eq6SE_YiX6GsVW3ZF0VyqTcGHXuCYmPAFlU,7373
 torchrec/distributed/fused_embedding.py,sha256=uIgeaoEPujTgkcq8S2OPRYBe03J4TUF04uOOy_iRsMk,5273
 torchrec/distributed/fused_embeddingbag.py,sha256=7DIMc5sHdsDAgqCnNomcPo6V4aIH1wlkzyshHeJB3pc,5110
 torchrec/distributed/fused_params.py,sha256=WnPW8Crs9SSDFF33f4RZx_ok51I8-ql0yeA8N93aMtw,1699
 torchrec/distributed/grouped_position_weighted.py,sha256=q-QE0U306BiPkXIAlJGIQ80EUDZj-FXTbWwjz3EyvLI,3807
 torchrec/distributed/model_parallel.py,sha256=yfk_pYRT7ZrBh6UrukOgYbzHpg0yd6pFSU5NfA4SXxo,19528
-torchrec/distributed/quant_embedding.py,sha256=quHodtz0aqeUy2b_Pv0OqoCZASgfcA5dDZC4GTXyWlU,13657
+torchrec/distributed/quant_embedding.py,sha256=tXcvkorboPGLH8-KhDgt1i0IRF1LAQYhCAa4FOJXHNQ,13636
 torchrec/distributed/quant_embedding_kernel.py,sha256=GoZ7VWhmgFqie9GB96DCOGMhuywvgyk-TtyktRZ2OtE,14006
-torchrec/distributed/quant_embeddingbag.py,sha256=qcZM2X_JOT6PHuo-zq1G8Kg0E7faYsq98G3Jhs0Wx0I,10915
+torchrec/distributed/quant_embeddingbag.py,sha256=IdPIlh1LjUEloqKU73pMsoyRy2lsi_6NhR6ayz7Gkhc,10833
 torchrec/distributed/shard.py,sha256=4Dr5ixWCoMEFEuL5WN4fL2gIdl9wmSUjZWsiF-kdCdQ,9261
 torchrec/distributed/sharding_plan.py,sha256=1CZyCmPp_lJPeitOgufRNfe77aXp9WtxsKDqVOm8XnU,19218
 torchrec/distributed/train_pipeline.py,sha256=RotmGdM_UduxSbrf3l8JKrowIeMhrB2DaNJHFwXPjIY,22330
 torchrec/distributed/types.py,sha256=tNd_B5PXjaOqPmUXxSZLXB3a71sjU8LBtSY9kuGxe7I,24927
 torchrec/distributed/utils.py,sha256=isPXbyPhHlbjiA0280ZY0V_wPUBuvDvvH4w6ziP5XA8,11373
 torchrec/distributed/composable/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/distributed/composable/table_batched_embedding_slice.py,sha256=x439M8TTXQtzoihan1OKKbmGYkqJlAxxTHCDz5295RY,3207
 torchrec/distributed/planner/__init__.py,sha256=UWnxb-SuE211uJGdwtSkKRVADT3plQozB2l6fvs6Ve0,1025
 torchrec/distributed/planner/constants.py,sha256=MkeVqYO2QGg57i6fs29lZb2dScaaR9mdQVsee4NxyFc,3135
 torchrec/distributed/planner/enumerators.py,sha256=hZzhnfMrOz65lBoddxTKBb01hm43R_5tdpysJFSzCLE,10318
-torchrec/distributed/planner/partitioners.py,sha256=I2fTqnKWcaL7G8Cz6aXC61S1sa3ZtHSz1Xp8xGbEAr8,12103
+torchrec/distributed/planner/partitioners.py,sha256=AwIuyCyyJGhy_WmzEipIdglx9PbOGAEmvp6qgLVCzpU,12485
 torchrec/distributed/planner/perf_models.py,sha256=srFJ0AkOhLVhumdcuyYzl5J1qzezt6knt-TxZvK6Zy4,824
-torchrec/distributed/planner/planners.py,sha256=ZccWhKg9QUA2f3oAtVnFWjPfKEiSjRzZVCMGcenyG7c,11869
+torchrec/distributed/planner/planners.py,sha256=dPfT-SjjKgv9jYZ-piwWPw9fhgzVWSqSb1CNCJt2Zcg,12224
 torchrec/distributed/planner/proposers.py,sha256=A-adzCCshGZUIyCP7DPOGDuh4hbojawWuObxsIf-rOg,11094
 torchrec/distributed/planner/shard_estimators.py,sha256=H9rZJV9_BwgRyImzXT0YXiymJ6FhRzudTg9NXrLoTOs,40173
 torchrec/distributed/planner/stats.py,sha256=my1OB-r1HO-ixc7joRz1CltZlQJDBrjJOtLIXDY7B5c,21410
 torchrec/distributed/planner/storage_reservations.py,sha256=rPqeD03f3mg8yoSqy9CZfOmxVMUGrCz98nPqwvxhApc,9125
-torchrec/distributed/planner/types.py,sha256=R-NCgPEq_QImroSWlreGPCtq_fNBKkdiIegzM8CJF4Y,12611
+torchrec/distributed/planner/types.py,sha256=16iitmowdWqzenpyeCvkWzQZW7OSOMjbpZzKDo3WDUo,12879
 torchrec/distributed/planner/utils.py,sha256=YwOUrqb-D6HaVtWCRFz9RWhBdMRBqjMaYYhoQgyOkQg,1119
 torchrec/distributed/sharding/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/distributed/sharding/cw_sequence_sharding.py,sha256=o8RZAs2zivNP8MZ5QUWF0alw7KwWZnJVG9iKHM4EJdE,2539
 torchrec/distributed/sharding/cw_sharding.py,sha256=PLJHgxkQvX_sNI8RJlz4xIemb4vO2KHOe_9PylZ2je8,9519
 torchrec/distributed/sharding/dp_sequence_sharding.py,sha256=zQmERguEGVd5IAbmVsIhqdKXb0f2g8tQamN838qVvfM,2802
 torchrec/distributed/sharding/dp_sharding.py,sha256=GwbU9BOOdqucVD6qqW3SwbBOcB8No5P5wvXqSeSaUAs,7452
 torchrec/distributed/sharding/rw_sequence_sharding.py,sha256=66tW1TyIp8kSdT3t8EsVsHcv7_kAzbCX3xT_pDmc_NE,5041
 torchrec/distributed/sharding/rw_sharding.py,sha256=k3ym1GGjdQeRnLeUheZm5o9D8co_U4dUQgHrF771XUQ,12850
 torchrec/distributed/sharding/sequence_sharding.py,sha256=LBUW1PvAV9WNwf-m-x9bYIcU96PL75mPE_uQK9dLAOU,3114
-torchrec/distributed/sharding/tw_sequence_sharding.py,sha256=Y4zgIRQbNVPhEOxQGa7ADe5RZbB6x2XicLu_qxrDy8k,7620
-torchrec/distributed/sharding/tw_sharding.py,sha256=cwyKT1wrBrqJUdnRSlB4_SpnsLvEmbmAK3Yc6c_shyo,16102
+torchrec/distributed/sharding/tw_sequence_sharding.py,sha256=MNlniKPVzshZWYwGoqA3bt5FsZrhBISk9b2TOrMa4eU,7609
+torchrec/distributed/sharding/tw_sharding.py,sha256=dbVwsNbUtLntRODZJjMWMFQq-IVsSn9iiTmnMG6644E,16061
 torchrec/distributed/sharding/twcw_sharding.py,sha256=LyOowcADWmRdkRn-eEW6QB0b0xYi0vM-Ubrr6N2zwwA,1284
 torchrec/distributed/sharding/twrw_sharding.py,sha256=4X-o1XLFBux3sY0dDXw1WQQye1DGLNt-ZMyoe4Dj6fY,19840
 torchrec/distributed/test_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/distributed/test_utils/infer_utils.py,sha256=OTsqNFGMgT5VWF-Ww94JbLsCDmv2KqB5R50O-ynK0Sg,10125
 torchrec/distributed/test_utils/multi_process.py,sha256=6AxYe2cO44EGdw-Lt5YNwqmWZW8Ldu7bVGTluIIOFbA,4868
 torchrec/distributed/test_utils/test_model.py,sha256=x4DxsGp6YfBBJMyrkBwOZTFpotm6VB8V6tLxuanHNFM,34114
 torchrec/distributed/test_utils/test_model_parallel.py,sha256=oN_fl-QOaAarrZwnr7q5Lz4f9xiqxxof9Bk_4PM13hM,11193
@@ -128,12 +128,12 @@
 torchrec/quant/__init__.py,sha256=A6NIA6ztq6iP1JTLRLNzlgnCcd-LaN8efnxGub3Ii4A,1140
 torchrec/quant/embedding_modules.py,sha256=DUTwXLCIofNUwdZW7gFp8s3YH4Yy9pd6BB73PdSrcGw,23109
 torchrec/quant/utils.py,sha256=2oUJIsrzE7ijvPs5DYUa06wOfmRvU1KdU1aQI7DUccs,3691
 torchrec/sparse/__init__.py,sha256=dLqSye4Jo6obnNNTUKdPDxPQb9sL2U4weemSn-DjpYk,1163
 torchrec/sparse/jagged_tensor.py,sha256=MoyqQJC8ezXOPLLEGK1o4zzI_SQ18tUUQ3zzOlW-_b4,52763
 torchrec/sparse/test_utils/__init__.py,sha256=BLxfGKJvwjjCiQM64O5wGAA_Cea0sG-buw9lTDWuqug,1430
 torchrec/test_utils/__init__.py,sha256=JncJcXS4N3gI7-fsizQ2-qiWM6MhIrpvskF_9gDf0Go,5661
-torchrec_nightly-2023.5.1.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
-torchrec_nightly-2023.5.1.dist-info/METADATA,sha256=1O-TqVxp7rNK0MV75SfaOmMZGHx0MfTLM6hP7KmjZ3c,5011
-torchrec_nightly-2023.5.1.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
-torchrec_nightly-2023.5.1.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
-torchrec_nightly-2023.5.1.dist-info/RECORD,,
+torchrec_nightly-2023.5.3.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
+torchrec_nightly-2023.5.3.dist-info/METADATA,sha256=URYGxobSLkQJffwDR9UT-lG-Rq9vOeJbsuNMOfM74Qs,5011
+torchrec_nightly-2023.5.3.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
+torchrec_nightly-2023.5.3.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
+torchrec_nightly-2023.5.3.dist-info/RECORD,,
```

