# Comparing `tmp/c65faucet-1.0.49.tar.gz` & `tmp/c65faucet-1.0.50.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "c65faucet-1.0.49.tar", last modified: Wed Apr 19 23:05:29 2023, max compression
+gzip compressed data, was "c65faucet-1.0.50.tar", last modified: Wed May  3 00:46:50 2023, max compression
```

## Comparing `c65faucet-1.0.49.tar` & `c65faucet-1.0.50.tar`

### file list

```diff
@@ -1,374 +1,374 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.920353 c65faucet-1.0.49/
--rw-r--r--   0 runner    (1001) docker     (123)      142 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.codecov.yml
--rw-r--r--   0 runner    (1001) docker     (123)       34 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.dockerignore
--rw-r--r--   0 runner    (1001) docker     (123)       31 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.flake8
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.868353 c65faucet-1.0.49/.github/
--rw-r--r--   0 runner    (1001) docker     (123)      346 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/dependabot.yml
--rw-r--r--   0 runner    (1001) docker     (123)      322 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/renovate.json
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.868353 c65faucet-1.0.49/.github/workflows/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.868353 c65faucet-1.0.49/.github/workflows/disabled/
--rw-r--r--   0 runner    (1001) docker     (123)     1230 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/workflows/disabled/periodic.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1631 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/workflows/disabled/release-debian.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4199 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/workflows/release-docker.yml
--rw-r--r--   0 runner    (1001) docker     (123)      741 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/workflows/release-python.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3502 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/workflows/tests-codecheck.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3256 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/workflows/tests-docs.yml
--rw-r--r--   0 runner    (1001) docker     (123)     7898 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/workflows/tests-integration.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4044 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/workflows/tests-unit.yml
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.github/workflows/tests-yaml-lint.yml
--rw-r--r--   0 runner    (1001) docker     (123)      641 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.pylintrc
--rw-r--r--   0 runner    (1001) docker     (123)      197 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.readthedocs.yml
--rw-r--r--   0 runner    (1001) docker     (123)       88 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.renovaterc.json
--rw-r--r--   0 runner    (1001) docker     (123)      125 2023-04-19 23:05:27.000000 c65faucet-1.0.49/.stickler.yml
--rw-r--r--   0 runner    (1001) docker     (123)       36 2023-04-19 23:05:29.000000 c65faucet-1.0.49/AUTHORS
--rw-r--r--   0 runner    (1001) docker     (123)      830 2023-04-19 23:05:27.000000 c65faucet-1.0.49/CONTRIBUTING.rst
--rw-r--r--   0 runner    (1001) docker     (123)       41 2023-04-19 23:05:29.000000 c65faucet-1.0.49/ChangeLog
--rw-r--r--   0 runner    (1001) docker     (123)      221 2023-04-19 23:05:27.000000 c65faucet-1.0.49/Dockerfile.faucet
--rw-r--r--   0 runner    (1001) docker     (123)      485 2023-04-19 23:05:27.000000 c65faucet-1.0.49/Dockerfile.fuzz-config
--rw-r--r--   0 runner    (1001) docker     (123)      464 2023-04-19 23:05:27.000000 c65faucet-1.0.49/Dockerfile.fuzz-packet
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-04-19 23:05:27.000000 c65faucet-1.0.49/Dockerfile.gauge
--rw-r--r--   0 runner    (1001) docker     (123)      129 2023-04-19 23:05:27.000000 c65faucet-1.0.49/Dockerfile.tests
--rw-r--r--   0 runner    (1001) docker     (123)    10912 2023-04-19 23:05:27.000000 c65faucet-1.0.49/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-04-19 23:05:27.000000 c65faucet-1.0.49/Makefile
--rw-r--r--   0 runner    (1001) docker     (123)      787 2023-04-19 23:05:29.920353 c65faucet-1.0.49/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     3610 2023-04-19 23:05:27.000000 c65faucet-1.0.49/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.868353 c65faucet-1.0.49/adapters/
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.856353 c65faucet-1.0.49/adapters/vendors/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.868353 c65faucet-1.0.49/adapters/vendors/faucetagent/
--rw-r--r--   0 runner    (1001) docker     (123)     1086 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/faucetagent/Dockerfile
--rw-r--r--   0 runner    (1001) docker     (123)      985 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/faucetagent/README.rst
--rw-r--r--   0 runner    (1001) docker     (123)      451 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/faucetagent/docker-compose.yaml
--rwxr-xr-x   0 runner    (1001) docker     (123)      694 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/faucetagent/example_client.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)     1142 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/faucetagent/gencerts.sh
--rw-r--r--   0 runner    (1001) docker     (123)       56 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/faucetagent/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.872353 c65faucet-1.0.49/adapters/vendors/rabbitmq/
--rw-r--r--   0 runner    (1001) docker     (123)      763 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/rabbitmq/Dockerfile
--rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/rabbitmq/README.rst
--rw-r--r--   0 runner    (1001) docker     (123)      152 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/rabbitmq/docker-compose-rabbitmq.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/rabbitmq/docker-compose.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/rabbitmq/example_consumer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.872353 c65faucet-1.0.49/adapters/vendors/rabbitmq/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)      131 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/rabbitmq/hooks/pre_build
--rw-r--r--   0 runner    (1001) docker     (123)     6487 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/rabbitmq/rabbit.py
--rw-r--r--   0 runner    (1001) docker     (123)       12 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/rabbitmq/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     3779 2023-04-19 23:05:27.000000 c65faucet-1.0.49/adapters/vendors/rabbitmq/test_rabbit.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.872353 c65faucet-1.0.49/c65faucet.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)      787 2023-04-19 23:05:29.000000 c65faucet-1.0.49/c65faucet.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     9704 2023-04-19 23:05:29.000000 c65faucet-1.0.49/c65faucet.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-04-19 23:05:29.000000 c65faucet-1.0.49/c65faucet.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      156 2023-04-19 23:05:29.000000 c65faucet-1.0.49/c65faucet.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-04-19 23:05:29.000000 c65faucet-1.0.49/c65faucet.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)       47 2023-04-19 23:05:29.000000 c65faucet-1.0.49/c65faucet.egg-info/pbr.json
--rw-r--r--   0 runner    (1001) docker     (123)      149 2023-04-19 23:05:29.000000 c65faucet-1.0.49/c65faucet.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        7 2023-04-19 23:05:29.000000 c65faucet-1.0.49/c65faucet.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.876353 c65faucet-1.0.49/clib/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      540 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/clib_mininet_test.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    32146 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/clib_mininet_test_main.py
--rw-r--r--   0 runner    (1001) docker     (123)     5995 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/clib_mininet_tests.py
--rw-r--r--   0 runner    (1001) docker     (123)    24759 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/config_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)    12531 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/docker_host.py
--rw-r--r--   0 runner    (1001) docker     (123)    49833 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/fakeoftable.py
--rw-r--r--   0 runner    (1001) docker     (123)   135065 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/mininet_test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    33547 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/mininet_test_base_topo.py
--rw-r--r--   0 runner    (1001) docker     (123)    27383 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/mininet_test_topo.py
--rw-r--r--   0 runner    (1001) docker     (123)     7087 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/mininet_test_util.py
--rw-r--r--   0 runner    (1001) docker     (123)     9471 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/mininet_test_watcher.py
--rw-r--r--   0 runner    (1001) docker     (123)     4992 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/tcpdump_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)   109310 2023-04-19 23:05:27.000000 c65faucet-1.0.49/clib/valve_test_lib.py
--rw-r--r--   0 runner    (1001) docker     (123)       48 2023-04-19 23:05:27.000000 c65faucet-1.0.49/codecheck-requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.876353 c65faucet-1.0.49/debian/
--rw-r--r--   0 runner    (1001) docker     (123)      134 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/changelog
--rw-r--r--   0 runner    (1001) docker     (123)        3 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/compat
--rw-r--r--   0 runner    (1001) docker     (123)     3364 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/control
--rw-r--r--   0 runner    (1001) docker     (123)      905 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/copyright
--rw-r--r--   0 runner    (1001) docker     (123)       54 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/faucet-all-in-one.install
--rw-r--r--   0 runner    (1001) docker     (123)       28 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/faucet-docs.docs
--rw-r--r--   0 runner    (1001) docker     (123)      224 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/faucet.default
--rw-r--r--   0 runner    (1001) docker     (123)      204 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/faucet.install
--rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/faucet.postinst
--rw-r--r--   0 runner    (1001) docker     (123)      421 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/faucet.service
--rw-r--r--   0 runner    (1001) docker     (123)      216 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/gauge.default
--rw-r--r--   0 runner    (1001) docker     (123)       35 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/gauge.install
--rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/gauge.postinst
--rw-r--r--   0 runner    (1001) docker     (123)      385 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/gauge.service
--rwxr-xr-x   0 runner    (1001) docker     (123)      555 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/rules
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.876353 c65faucet-1.0.49/debian/source/
--rw-r--r--   0 runner    (1001) docker     (123)       12 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/source/format
--rw-r--r--   0 runner    (1001) docker     (123)      344 2023-04-19 23:05:27.000000 c65faucet-1.0.49/debian/watch
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.880353 c65faucet-1.0.49/docker/
--rwxr-xr-x   0 runner    (1001) docker     (123)      989 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docker/fuzz_config.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      624 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docker/fuzz_packet.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)     1176 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docker/install-faucet.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)     1232 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docker/localtest.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      797 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docker/pip_deps.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      220 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docker/retrycmd.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)     7205 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docker/runtests.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      697 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docker/shard_tests.sh
--rw-r--r--   0 runner    (1001) docker     (123)     2208 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docker-compose.yaml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.880353 c65faucet-1.0.49/docs/
--rw-r--r--   0 runner    (1001) docker     (123)       22 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)      620 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/Makefile
--rw-r--r--   0 runner    (1001) docker     (123)      308 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/README.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.860353 c65faucet-1.0.49/docs/_static/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.880353 c65faucet-1.0.49/docs/_static/css/
--rw-r--r--   0 runner    (1001) docker     (123)      365 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/css/responsive-tables.css
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.884353 c65faucet-1.0.49/docs/_static/deployments/
--rw-r--r--   0 runner    (1001) docker     (123)    89120 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/deployments/ONF_Faucet_deploy1.png
--rw-r--r--   0 runner    (1001) docker     (123)   984943 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/deployments/nznog17-physical-network.jpg
--rw-r--r--   0 runner    (1001) docker     (123)  1106115 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/deployments/nznog17-virtual-network.jpg
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.884353 c65faucet-1.0.49/docs/_static/grafana-dashboards/
--rw-r--r--   0 runner    (1001) docker     (123)    23629 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/grafana-dashboards/faucet_instrumentation.json
--rw-r--r--   0 runner    (1001) docker     (123)    22310 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/grafana-dashboards/faucet_inventory.json
--rw-r--r--   0 runner    (1001) docker     (123)    18566 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/grafana-dashboards/faucet_port_statistics.json
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.888353 c65faucet-1.0.49/docs/_static/images/
--rwxr-xr-x   0 runner    (1001) docker     (123)     8466 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/8021X-conf-diagram.svg
--rw-r--r--   0 runner    (1001) docker     (123)    97737 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/faucet-architecture.svg
--rw-r--r--   0 runner    (1001) docker     (123)    23716 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/faucet-pipeline.png
--rw-r--r--   0 runner    (1001) docker     (123)    58351 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/faucet-pipeline.svg
--rw-r--r--   0 runner    (1001) docker     (123)     2421 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/faucet-pipeline.txt
--rw-r--r--   0 runner    (1001) docker     (123)   136689 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/gauge-nznog17.png
--rw-r--r--   0 runner    (1001) docker     (123)   285918 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/gauge-snapshot1.png
--rw-r--r--   0 runner    (1001) docker     (123)   452164 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/gauge-snapshot2.png
--rw-r--r--   0 runner    (1001) docker     (123)   467326 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/gauge-snapshot3.png
--rw-r--r--   0 runner    (1001) docker     (123)    51732 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-acls.svg
--rw-r--r--   0 runner    (1001) docker     (123)    77255 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-bgp-routing.svg
--rw-r--r--   0 runner    (1001) docker     (123)    47652 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-ivr.svg
--rw-r--r--   0 runner    (1001) docker     (123)   120062 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-multi-root-stack.svg
--rw-r--r--   0 runner    (1001) docker     (123)    87114 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-nfv-services.svg
--rw-r--r--   0 runner    (1001) docker     (123)   105019 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-stack-loop.svg
--rw-r--r--   0 runner    (1001) docker     (123)    78230 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-stack-tunnel.svg
--rw-r--r--   0 runner    (1001) docker     (123)    53594 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-stack.svg
--rw-r--r--   0 runner    (1001) docker     (123)   109550 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-stackwithivr.svg
--rw-r--r--   0 runner    (1001) docker     (123)    59007 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-static-routing.svg
--rw-r--r--   0 runner    (1001) docker     (123)   172474 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/images/tutorial-vlans.svg
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.892353 c65faucet-1.0.49/docs/_static/tutorial/
--rw-r--r--   0 runner    (1001) docker     (123)      390 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/tutorial/add_tagged_interface
--rw-r--r--   0 runner    (1001) docker     (123)      134 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/tutorial/as_ns
--rw-r--r--   0 runner    (1001) docker     (123)     1449 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/tutorial/cleanup
--rw-r--r--   0 runner    (1001) docker     (123)      389 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/tutorial/create_ns
--rw-r--r--   0 runner    (1001) docker     (123)      154 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/tutorial/destroy_ns
--rw-r--r--   0 runner    (1001) docker     (123)      793 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/_static/tutorial/inter_switch_link
--rw-r--r--   0 runner    (1001) docker     (123)     5063 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/architecture.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.892353 c65faucet-1.0.49/docs/autogen/
--rw-r--r--   0 runner    (1001) docker     (123)        6 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/autogen/.gitignore
--rw-r--r--   0 runner    (1001) docker     (123)     7237 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/conf.py
--rw-r--r--   0 runner    (1001) docker     (123)    40109 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/configuration.rst
--rw-r--r--   0 runner    (1001) docker     (123)    14496 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/developer_guide.rst
--rw-r--r--   0 runner    (1001) docker     (123)     1499 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/external_resources.rst
--rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/fuzzing.rst
--rw-r--r--   0 runner    (1001) docker     (123)      492 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)     9939 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/installation.rst
--rw-r--r--   0 runner    (1001) docker     (123)     5125 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/intro.rst
--rw-r--r--   0 runner    (1001) docker     (123)      930 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/monitoring.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.892353 c65faucet-1.0.49/docs/recipe_book/
--rw-r--r--   0 runner    (1001) docker     (123)       22 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/recipe_book/forwarding.rst
--rw-r--r--   0 runner    (1001) docker     (123)      261 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/recipe_book/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)       14 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/recipe_book/policy.rst
--rw-r--r--   0 runner    (1001) docker     (123)       16 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/recipe_book/routing.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.892353 c65faucet-1.0.49/docs/release_notes/
--rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/release_notes/1.7.0.rst
--rw-r--r--   0 runner    (1001) docker     (123)     3271 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/release_notes/1.9.0.rst
--rw-r--r--   0 runner    (1001) docker     (123)       99 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.892353 c65faucet-1.0.49/docs/source/
--rw-r--r--   0 runner    (1001) docker     (123)       73 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/source/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)    11599 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/testing.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.892353 c65faucet-1.0.49/docs/tutorials/
--rw-r--r--   0 runner    (1001) docker     (123)    13460 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/tutorials/acls.rst
--rw-r--r--   0 runner    (1001) docker     (123)    13064 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/tutorials/conntrack.rst
--rw-r--r--   0 runner    (1001) docker     (123)    18441 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/tutorials/first_time.rst
--rw-r--r--   0 runner    (1001) docker     (123)      142 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/tutorials/index.rst
--rw-r--r--   0 runner    (1001) docker     (123)    17927 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/tutorials/nfv_services.rst
--rw-r--r--   0 runner    (1001) docker     (123)    15268 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/tutorials/routing.rst
--rw-r--r--   0 runner    (1001) docker     (123)    30053 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/tutorials/stacking.rst
--rw-r--r--   0 runner    (1001) docker     (123)     8477 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/tutorials/vlans.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/docs/vendors/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/docs/vendors/allied-telesis/
--rw-r--r--   0 runner    (1001) docker     (123)    14233 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/allied-telesis/README_Allied_Telesis.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/docs/vendors/cisco/
--rwxr-xr-x   0 runner    (1001) docker     (123)     9404 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/cisco/README_Cisco.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/docs/vendors/hpe/
--rw-r--r--   0 runner    (1001) docker     (123)    15292 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/hpe/README_Aruba.rst
--rw-r--r--   0 runner    (1001) docker     (123)      405 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/index.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/docs/vendors/lagopus/
--rw-r--r--   0 runner    (1001) docker     (123)     3638 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/lagopus/README_Lagopus.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/docs/vendors/northboundnetworks/
--rw-r--r--   0 runner    (1001) docker     (123)     1661 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/northboundnetworks/README_ZodiacFX.rst
--rw-r--r--   0 runner    (1001) docker     (123)      605 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/northboundnetworks/README_ZodiacGX.rst
--rwxr-xr-x   0 runner    (1001) docker     (123)     1292 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/northboundnetworks/conf-zodiac.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/docs/vendors/noviflow/
--rw-r--r--   0 runner    (1001) docker     (123)     3286 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/noviflow/README_noviflow.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/docs/vendors/ovs/
--rw-r--r--   0 runner    (1001) docker     (123)     6413 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/ovs/README_OVS-DPDK.rst
--rw-r--r--   0 runner    (1001) docker     (123)    18223 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/ovs/faucet_ovs_test.png
--rw-r--r--   0 runner    (1001) docker     (123)     9332 2023-04-19 23:05:27.000000 c65faucet-1.0.49/docs/vendors/ovs/faucet_testing_with_OVS_on_hardware.rst
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.860353 c65faucet-1.0.49/etc/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/etc/default/
--rw-r--r--   0 runner    (1001) docker     (123)      223 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/default/faucet
--rw-r--r--   0 runner    (1001) docker     (123)      215 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/default/gauge
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/etc/faucet/
--rw-r--r--   0 runner    (1001) docker     (123)     2826 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/faucet/acls.yaml
--rw-r--r--   0 runner    (1001) docker     (123)     2423 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/faucet/faucet.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      965 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/faucet/gauge.yaml
--rw-r--r--   0 runner    (1001) docker     (123)      403 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/faucet/os_ken.conf
--rw-r--r--   0 runner    (1001) docker     (123)      403 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/faucet/ryu.conf
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/etc/logrotate.d/
--rw-r--r--   0 runner    (1001) docker     (123)      243 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/logrotate.d/faucet
--rw-r--r--   0 runner    (1001) docker     (123)      241 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/logrotate.d/gauge
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.896353 c65faucet-1.0.49/etc/prometheus/
--rw-r--r--   0 runner    (1001) docker     (123)      660 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/prometheus/faucet.rules.yml
--rw-r--r--   0 runner    (1001) docker     (123)      263 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/prometheus/prometheus-docker-compose.yml
--rw-r--r--   0 runner    (1001) docker     (123)      872 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/prometheus/prometheus.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.860353 c65faucet-1.0.49/etc/systemd/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.900353 c65faucet-1.0.49/etc/systemd/system/
--rw-r--r--   0 runner    (1001) docker     (123)      388 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/systemd/system/faucet.service
--rw-r--r--   0 runner    (1001) docker     (123)      352 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/systemd/system/gauge.service
--rw-r--r--   0 runner    (1001) docker     (123)      355 2023-04-19 23:05:27.000000 c65faucet-1.0.49/etc/systemd/system/prometheus.service
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.904353 c65faucet-1.0.49/faucet/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5723 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/__main__.py
--rw-r--r--   0 runner    (1001) docker     (123)    35363 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/acl.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2276 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/check_faucet_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     9540 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/conf.py
--rw-r--r--   0 runner    (1001) docker     (123)    14246 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/config_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     7721 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/config_parser_util.py
--rw-r--r--   0 runner    (1001) docker     (123)    71188 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/dp.py
--rw-r--r--   0 runner    (1001) docker     (123)    14561 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/faucet.py
--rw-r--r--   0 runner    (1001) docker     (123)     9912 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/faucet_bgp.py
--rw-r--r--   0 runner    (1001) docker     (123)    15483 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/faucet_dot1x.py
--rw-r--r--   0 runner    (1001) docker     (123)     4921 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/faucet_event.py
--rw-r--r--   0 runner    (1001) docker     (123)      440 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/faucet_metadata.py
--rw-r--r--   0 runner    (1001) docker     (123)    11238 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/faucet_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     7322 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/faucet_pipeline.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     7620 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/fctl.py
--rw-r--r--   0 runner    (1001) docker     (123)     8303 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/gauge.py
--rw-r--r--   0 runner    (1001) docker     (123)     9579 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/gauge_influx.py
--rw-r--r--   0 runner    (1001) docker     (123)     9861 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/gauge_pollers.py
--rw-r--r--   0 runner    (1001) docker     (123)     8661 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/gauge_prom.py
--rw-r--r--   0 runner    (1001) docker     (123)     1724 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/meter.py
--rw-r--r--   0 runner    (1001) docker     (123)    31122 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/port.py
--rw-r--r--   0 runner    (1001) docker     (123)     3490 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/prom_client.py
--rw-r--r--   0 runner    (1001) docker     (123)     6089 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/router.py
--rw-r--r--   0 runner    (1001) docker     (123)    19588 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/stack.py
--rw-r--r--   0 runner    (1001) docker     (123)     6703 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/tfm_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)    73345 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve.py
--rw-r--r--   0 runner    (1001) docker     (123)    30131 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_acl.py
--rw-r--r--   0 runner    (1001) docker     (123)     3036 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_coprocessor.py
--rw-r--r--   0 runner    (1001) docker     (123)     8518 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_lldp.py
--rw-r--r--   0 runner    (1001) docker     (123)     1106 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_manager_base.py
--rw-r--r--   0 runner    (1001) docker     (123)    42091 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_of.py
--rw-r--r--   0 runner    (1001) docker     (123)     1072 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_of_old.py
--rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_outonly.py
--rw-r--r--   0 runner    (1001) docker     (123)    28984 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_packet.py
--rw-r--r--   0 runner    (1001) docker     (123)     8441 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (123)    49835 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_route.py
--rw-r--r--   0 runner    (1001) docker     (123)     7609 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_ryuapp.py
--rw-r--r--   0 runner    (1001) docker     (123)    18431 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_stack.py
--rw-r--r--   0 runner    (1001) docker     (123)     3429 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_switch.py
--rw-r--r--   0 runner    (1001) docker     (123)    30701 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_switch_stack.py
--rw-r--r--   0 runner    (1001) docker     (123)    52744 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_switch_standalone.py
--rw-r--r--   0 runner    (1001) docker     (123)    12574 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_table.py
--rw-r--r--   0 runner    (1001) docker     (123)     6406 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valve_util.py
--rw-r--r--   0 runner    (1001) docker     (123)    18634 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/valves_manager.py
--rw-r--r--   0 runner    (1001) docker     (123)    26393 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/vlan.py
--rw-r--r--   0 runner    (1001) docker     (123)     6424 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/watcher.py
--rw-r--r--   0 runner    (1001) docker     (123)     6940 2023-04-19 23:05:27.000000 c65faucet-1.0.49/faucet/watcher_conf.py
--rw-r--r--   0 runner    (1001) docker     (123)       43 2023-04-19 23:05:27.000000 c65faucet-1.0.49/fuzz-requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.904353 c65faucet-1.0.49/git-hook/
--rwxr-xr-x   0 runner    (1001) docker     (123)      456 2023-04-19 23:05:27.000000 c65faucet-1.0.49/git-hook/pre-commit
--rw-r--r--   0 runner    (1001) docker     (123)      358 2023-04-19 23:05:27.000000 c65faucet-1.0.49/helper-funcs
--rw-r--r--   0 runner    (1001) docker     (123)     1061 2023-04-19 23:05:27.000000 c65faucet-1.0.49/hw_switch_config.yaml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.904353 c65faucet-1.0.49/ofctl_rest/
--rw-r--r--   0 runner    (1001) docker     (123)    26377 2023-04-19 23:05:27.000000 c65faucet-1.0.49/ofctl_rest/ofctl_rest.py
--rw-r--r--   0 runner    (1001) docker     (123)     9460 2023-04-19 23:05:27.000000 c65faucet-1.0.49/ofctl_rest/wsgi.py
--rw-r--r--   0 runner    (1001) docker     (123)      149 2023-04-19 23:05:27.000000 c65faucet-1.0.49/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (123)     1064 2023-04-19 23:05:29.924353 c65faucet-1.0.49/setup.cfg
--rwxr-xr-x   0 runner    (1001) docker     (123)     3674 2023-04-19 23:05:27.000000 c65faucet-1.0.49/setup.py
--rw-r--r--   0 runner    (1001) docker     (123)      196 2023-04-19 23:05:27.000000 c65faucet-1.0.49/test-requirements.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.908353 c65faucet-1.0.49/tests/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.908353 c65faucet-1.0.49/tests/codecheck/
--rwxr-xr-x   0 runner    (1001) docker     (123)      278 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/codecheck/flake8.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      500 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/codecheck/min_pylint.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      234 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/codecheck/pylint.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      342 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/codecheck/pytype.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      701 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/codecheck/src_files.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.860353 c65faucet-1.0.49/tests/generative/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.860353 c65faucet-1.0.49/tests/generative/fuzzer/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.908353 c65faucet-1.0.49/tests/generative/fuzzer/config/
--rw-r--r--   0 runner    (1001) docker     (123)      405 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/config/config.dict
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.908353 c65faucet-1.0.49/tests/generative/fuzzer/config/examples/
--rw-r--r--   0 runner    (1001) docker     (123)      432 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/config/examples/ex0
--rw-r--r--   0 runner    (1001) docker     (123)     1004 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/config/fuzz_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4755 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/config/generate_dict.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.908353 c65faucet-1.0.49/tests/generative/fuzzer/packet/
--rw-r--r--   0 runner    (1001) docker     (123)     1548 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/display_packet_crash.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.912353 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/
--rw-r--r--   0 runner    (1001) docker     (123)       65 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/aoe.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      121 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/arp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)       85 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/arp.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/asap.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      183 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/asap.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/diameter.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      169 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/dns.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      277 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/dns.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      465 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/http.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      925 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/http.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      933 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/icmp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)     2885 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/icmp.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/icmp.ex3
--rw-r--r--   0 runner    (1001) docker     (123)      141 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/icmp.ex4
--rw-r--r--   0 runner    (1001) docker     (123)      121 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/igmpv2.ex1
--rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/ipv4.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      381 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/ipv6.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      193 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/irc.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      265 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/irc.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      311 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/irc.ex3
--rw-r--r--   0 runner    (1001) docker     (123)      249 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/lacp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)     1001 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/msger.ex1
--rw-r--r--   0 runner    (1001) docker     (123)       63 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/packet.dict
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/tcp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/tcp.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      109 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/tcp.ex3
--rw-r--r--   0 runner    (1001) docker     (123)      125 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/tcp.ex4
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/tcp.ex5
--rw-r--r--   0 runner    (1001) docker     (123)      121 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/udp.ex1
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/udp.ex2
--rw-r--r--   0 runner    (1001) docker     (123)      157 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/udp.ex3
--rw-r--r--   0 runner    (1001) docker     (123)      147 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/udp.ex4
--rw-r--r--   0 runner    (1001) docker     (123)      715 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/fake_packet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1406 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/fuzzer/packet/fuzz_packet.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.912353 c65faucet-1.0.49/tests/generative/integration/
--rwxr-xr-x   0 runner    (1001) docker     (123)     2288 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/integration/fault_tolerance_main.py
--rw-r--r--   0 runner    (1001) docker     (123)    17828 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/integration/fault_tolerance_tests.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.912353 c65faucet-1.0.49/tests/generative/unit/
--rwxr-xr-x   0 runner    (1001) docker     (123)    12548 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/generative/unit/test_topology.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.916353 c65faucet-1.0.49/tests/integration/
--rwxr-xr-x   0 runner    (1001) docker     (123)      565 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/integration/mininet_main.py
--rw-r--r--   0 runner    (1001) docker     (123)   143643 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/integration/mininet_multidp_tests.py
--rw-r--r--   0 runner    (1001) docker     (123)   292341 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/integration/mininet_tests.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      574 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/run_unit_tests.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      244 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/sysctls_for_tests.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.864353 c65faucet-1.0.49/tests/unit/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.916353 c65faucet-1.0.49/tests/unit/clib/
--rwxr-xr-x   0 runner    (1001) docker     (123)     8000 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/clib/test_topo.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.920353 c65faucet-1.0.49/tests/unit/faucet/
--rwxr-xr-x   0 runner    (1001) docker     (123)    10401 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_check_config.py
--rwxr-xr-x   0 runner    (1001) docker     (123)   134114 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_config.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     6666 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_fctl.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1519 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_main.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5773 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_port.py
--rw-r--r--   0 runner    (1001) docker     (123)    31562 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_valve.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    39653 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_valve_config.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     4960 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_valve_dot1x.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5875 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_valve_egress.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2968 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_valve_of.py
--rw-r--r--   0 runner    (1001) docker     (123)   154810 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_valve_stack.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2770 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_valveapp_smoke.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     9217 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/faucet/test_vlan.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.920353 c65faucet-1.0.49/tests/unit/gauge/
--rwxr-xr-x   0 runner    (1001) docker     (123)     5559 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/gauge/test_config_gauge.py
--rwxr-xr-x   0 runner    (1001) docker     (123)    37880 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/gauge/test_gauge.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1494 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/gauge/test_main.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-04-19 23:05:29.920353 c65faucet-1.0.49/tests/unit/packaging/
--rwxr-xr-x   0 runner    (1001) docker     (123)     5354 2023-04-19 23:05:27.000000 c65faucet-1.0.49/tests/unit/packaging/test_packaging.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:50.001045 c65faucet-1.0.50/
+-rw-r--r--   0 runner    (1001) docker     (123)      142 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.codecov.yml
+-rw-r--r--   0 runner    (1001) docker     (123)       34 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.dockerignore
+-rw-r--r--   0 runner    (1001) docker     (123)       31 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.flake8
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.961045 c65faucet-1.0.50/.github/
+-rw-r--r--   0 runner    (1001) docker     (123)      346 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/dependabot.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      322 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/renovate.json
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.961045 c65faucet-1.0.50/.github/workflows/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.961045 c65faucet-1.0.50/.github/workflows/disabled/
+-rw-r--r--   0 runner    (1001) docker     (123)     1230 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/workflows/disabled/periodic.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1631 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/workflows/disabled/release-debian.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4199 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/workflows/release-docker.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      741 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/workflows/release-python.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3871 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/workflows/tests-codecheck.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3256 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/workflows/tests-docs.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     7898 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/workflows/tests-integration.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4044 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/workflows/tests-unit.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.github/workflows/tests-yaml-lint.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      641 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.pylintrc
+-rw-r--r--   0 runner    (1001) docker     (123)      197 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.readthedocs.yml
+-rw-r--r--   0 runner    (1001) docker     (123)       88 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.renovaterc.json
+-rw-r--r--   0 runner    (1001) docker     (123)      125 2023-05-03 00:46:43.000000 c65faucet-1.0.50/.stickler.yml
+-rw-r--r--   0 runner    (1001) docker     (123)       48 2023-05-03 00:46:49.000000 c65faucet-1.0.50/AUTHORS
+-rw-r--r--   0 runner    (1001) docker     (123)      830 2023-05-03 00:46:43.000000 c65faucet-1.0.50/CONTRIBUTING.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       32 2023-05-03 00:46:49.000000 c65faucet-1.0.50/ChangeLog
+-rw-r--r--   0 runner    (1001) docker     (123)      221 2023-05-03 00:46:43.000000 c65faucet-1.0.50/Dockerfile.faucet
+-rw-r--r--   0 runner    (1001) docker     (123)      485 2023-05-03 00:46:43.000000 c65faucet-1.0.50/Dockerfile.fuzz-config
+-rw-r--r--   0 runner    (1001) docker     (123)      464 2023-05-03 00:46:43.000000 c65faucet-1.0.50/Dockerfile.fuzz-packet
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-05-03 00:46:43.000000 c65faucet-1.0.50/Dockerfile.gauge
+-rw-r--r--   0 runner    (1001) docker     (123)      129 2023-05-03 00:46:43.000000 c65faucet-1.0.50/Dockerfile.tests
+-rw-r--r--   0 runner    (1001) docker     (123)    10912 2023-05-03 00:46:43.000000 c65faucet-1.0.50/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-05-03 00:46:43.000000 c65faucet-1.0.50/Makefile
+-rw-r--r--   0 runner    (1001) docker     (123)      787 2023-05-03 00:46:50.001045 c65faucet-1.0.50/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     3610 2023-05-03 00:46:43.000000 c65faucet-1.0.50/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.961045 c65faucet-1.0.50/adapters/
+-rw-r--r--   0 runner    (1001) docker     (123)      141 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.953045 c65faucet-1.0.50/adapters/vendors/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.961045 c65faucet-1.0.50/adapters/vendors/faucetagent/
+-rw-r--r--   0 runner    (1001) docker     (123)     1086 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/faucetagent/Dockerfile
+-rw-r--r--   0 runner    (1001) docker     (123)      985 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/faucetagent/README.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      451 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/faucetagent/docker-compose.yaml
+-rwxr-xr-x   0 runner    (1001) docker     (123)      694 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/faucetagent/example_client.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1142 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/faucetagent/gencerts.sh
+-rw-r--r--   0 runner    (1001) docker     (123)       56 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/faucetagent/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.961045 c65faucet-1.0.50/adapters/vendors/rabbitmq/
+-rw-r--r--   0 runner    (1001) docker     (123)      763 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/rabbitmq/Dockerfile
+-rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/rabbitmq/README.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      152 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/rabbitmq/docker-compose-rabbitmq.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      659 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/rabbitmq/docker-compose.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     1140 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/rabbitmq/example_consumer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.961045 c65faucet-1.0.50/adapters/vendors/rabbitmq/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)      131 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/rabbitmq/hooks/pre_build
+-rw-r--r--   0 runner    (1001) docker     (123)     6579 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/rabbitmq/rabbit.py
+-rw-r--r--   0 runner    (1001) docker     (123)       12 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/rabbitmq/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     3669 2023-05-03 00:46:43.000000 c65faucet-1.0.50/adapters/vendors/rabbitmq/test_rabbit.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.965045 c65faucet-1.0.50/c65faucet.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)      787 2023-05-03 00:46:49.000000 c65faucet-1.0.50/c65faucet.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     9704 2023-05-03 00:46:49.000000 c65faucet-1.0.50/c65faucet.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-03 00:46:49.000000 c65faucet-1.0.50/c65faucet.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      156 2023-05-03 00:46:49.000000 c65faucet-1.0.50/c65faucet.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-03 00:46:49.000000 c65faucet-1.0.50/c65faucet.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)       47 2023-05-03 00:46:49.000000 c65faucet-1.0.50/c65faucet.egg-info/pbr.json
+-rw-r--r--   0 runner    (1001) docker     (123)      149 2023-05-03 00:46:49.000000 c65faucet-1.0.50/c65faucet.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-05-03 00:46:49.000000 c65faucet-1.0.50/c65faucet.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.965045 c65faucet-1.0.50/clib/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      540 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/clib_mininet_test.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    34289 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/clib_mininet_test_main.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6261 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/clib_mininet_tests.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25032 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/config_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13136 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/docker_host.py
+-rw-r--r--   0 runner    (1001) docker     (123)    51086 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/fakeoftable.py
+-rw-r--r--   0 runner    (1001) docker     (123)   143491 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/mininet_test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35487 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/mininet_test_base_topo.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28930 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/mininet_test_topo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7292 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/mininet_test_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9612 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/mininet_test_watcher.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5131 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/tcpdump_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)   118212 2023-05-03 00:46:43.000000 c65faucet-1.0.50/clib/valve_test_lib.py
+-rw-r--r--   0 runner    (1001) docker     (123)       62 2023-05-03 00:46:43.000000 c65faucet-1.0.50/codecheck-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.965045 c65faucet-1.0.50/debian/
+-rw-r--r--   0 runner    (1001) docker     (123)      134 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/changelog
+-rw-r--r--   0 runner    (1001) docker     (123)        3 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/compat
+-rw-r--r--   0 runner    (1001) docker     (123)     3364 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/control
+-rw-r--r--   0 runner    (1001) docker     (123)      905 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/copyright
+-rw-r--r--   0 runner    (1001) docker     (123)       54 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/faucet-all-in-one.install
+-rw-r--r--   0 runner    (1001) docker     (123)       28 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/faucet-docs.docs
+-rw-r--r--   0 runner    (1001) docker     (123)      224 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/faucet.default
+-rw-r--r--   0 runner    (1001) docker     (123)      204 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/faucet.install
+-rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/faucet.postinst
+-rw-r--r--   0 runner    (1001) docker     (123)      421 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/faucet.service
+-rw-r--r--   0 runner    (1001) docker     (123)      216 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/gauge.default
+-rw-r--r--   0 runner    (1001) docker     (123)       35 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/gauge.install
+-rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/gauge.postinst
+-rw-r--r--   0 runner    (1001) docker     (123)      385 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/gauge.service
+-rwxr-xr-x   0 runner    (1001) docker     (123)      555 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/rules
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.965045 c65faucet-1.0.50/debian/source/
+-rw-r--r--   0 runner    (1001) docker     (123)       12 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/source/format
+-rw-r--r--   0 runner    (1001) docker     (123)      344 2023-05-03 00:46:43.000000 c65faucet-1.0.50/debian/watch
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.969045 c65faucet-1.0.50/docker/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      989 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docker/fuzz_config.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      624 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docker/fuzz_packet.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1176 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docker/install-faucet.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1232 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docker/localtest.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      797 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docker/pip_deps.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      220 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docker/retrycmd.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7205 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docker/runtests.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      697 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docker/shard_tests.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     2208 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docker-compose.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.969045 c65faucet-1.0.50/docs/
+-rw-r--r--   0 runner    (1001) docker     (123)       22 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)      620 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/Makefile
+-rw-r--r--   0 runner    (1001) docker     (123)      308 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/README.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.957045 c65faucet-1.0.50/docs/_static/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.969045 c65faucet-1.0.50/docs/_static/css/
+-rw-r--r--   0 runner    (1001) docker     (123)      365 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/css/responsive-tables.css
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.973045 c65faucet-1.0.50/docs/_static/deployments/
+-rw-r--r--   0 runner    (1001) docker     (123)    89120 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/deployments/ONF_Faucet_deploy1.png
+-rw-r--r--   0 runner    (1001) docker     (123)   984943 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/deployments/nznog17-physical-network.jpg
+-rw-r--r--   0 runner    (1001) docker     (123)  1106115 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/deployments/nznog17-virtual-network.jpg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.973045 c65faucet-1.0.50/docs/_static/grafana-dashboards/
+-rw-r--r--   0 runner    (1001) docker     (123)    23629 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/grafana-dashboards/faucet_instrumentation.json
+-rw-r--r--   0 runner    (1001) docker     (123)    22310 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/grafana-dashboards/faucet_inventory.json
+-rw-r--r--   0 runner    (1001) docker     (123)    18566 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/grafana-dashboards/faucet_port_statistics.json
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.977045 c65faucet-1.0.50/docs/_static/images/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8466 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/8021X-conf-diagram.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    97737 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/faucet-architecture.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    23716 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/faucet-pipeline.png
+-rw-r--r--   0 runner    (1001) docker     (123)    58351 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/faucet-pipeline.svg
+-rw-r--r--   0 runner    (1001) docker     (123)     2421 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/faucet-pipeline.txt
+-rw-r--r--   0 runner    (1001) docker     (123)   136689 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/gauge-nznog17.png
+-rw-r--r--   0 runner    (1001) docker     (123)   285918 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/gauge-snapshot1.png
+-rw-r--r--   0 runner    (1001) docker     (123)   452164 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/gauge-snapshot2.png
+-rw-r--r--   0 runner    (1001) docker     (123)   467326 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/gauge-snapshot3.png
+-rw-r--r--   0 runner    (1001) docker     (123)    51732 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-acls.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    77255 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-bgp-routing.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    47652 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-ivr.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   120062 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-multi-root-stack.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    87114 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-nfv-services.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   105019 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-stack-loop.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    78230 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-stack-tunnel.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    53594 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-stack.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   109550 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-stackwithivr.svg
+-rw-r--r--   0 runner    (1001) docker     (123)    59007 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-static-routing.svg
+-rw-r--r--   0 runner    (1001) docker     (123)   172474 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/images/tutorial-vlans.svg
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/_static/tutorial/
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/tutorial/add_tagged_interface
+-rw-r--r--   0 runner    (1001) docker     (123)      134 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/tutorial/as_ns
+-rw-r--r--   0 runner    (1001) docker     (123)     1449 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/tutorial/cleanup
+-rw-r--r--   0 runner    (1001) docker     (123)      389 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/tutorial/create_ns
+-rw-r--r--   0 runner    (1001) docker     (123)      154 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/tutorial/destroy_ns
+-rw-r--r--   0 runner    (1001) docker     (123)      793 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/_static/tutorial/inter_switch_link
+-rw-r--r--   0 runner    (1001) docker     (123)     5063 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/architecture.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/autogen/
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/autogen/.gitignore
+-rw-r--r--   0 runner    (1001) docker     (123)     7338 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)    40109 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/configuration.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    14496 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/developer_guide.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     1499 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/external_resources.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     1395 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/fuzzing.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      492 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     9939 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/installation.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     5125 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/intro.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      930 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/monitoring.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/recipe_book/
+-rw-r--r--   0 runner    (1001) docker     (123)       22 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/recipe_book/forwarding.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      261 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/recipe_book/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       14 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/recipe_book/policy.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       16 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/recipe_book/routing.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/release_notes/
+-rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/release_notes/1.7.0.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     3271 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/release_notes/1.9.0.rst
+-rw-r--r--   0 runner    (1001) docker     (123)       99 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/source/
+-rw-r--r--   0 runner    (1001) docker     (123)       73 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/source/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    11599 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/testing.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/tutorials/
+-rw-r--r--   0 runner    (1001) docker     (123)    13460 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/tutorials/acls.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    13064 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/tutorials/conntrack.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    18441 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/tutorials/first_time.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      142 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/tutorials/index.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    17927 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/tutorials/nfv_services.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    15268 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/tutorials/routing.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    30053 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/tutorials/stacking.rst
+-rw-r--r--   0 runner    (1001) docker     (123)     8477 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/tutorials/vlans.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/vendors/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/vendors/allied-telesis/
+-rw-r--r--   0 runner    (1001) docker     (123)    14233 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/allied-telesis/README_Allied_Telesis.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/vendors/cisco/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9404 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/cisco/README_Cisco.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/vendors/hpe/
+-rw-r--r--   0 runner    (1001) docker     (123)    15292 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/hpe/README_Aruba.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      405 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/index.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/vendors/lagopus/
+-rw-r--r--   0 runner    (1001) docker     (123)     3638 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/lagopus/README_Lagopus.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/vendors/northboundnetworks/
+-rw-r--r--   0 runner    (1001) docker     (123)     1661 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/northboundnetworks/README_ZodiacFX.rst
+-rw-r--r--   0 runner    (1001) docker     (123)      605 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/northboundnetworks/README_ZodiacGX.rst
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1292 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/northboundnetworks/conf-zodiac.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/vendors/noviflow/
+-rw-r--r--   0 runner    (1001) docker     (123)     3286 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/noviflow/README_noviflow.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/docs/vendors/ovs/
+-rw-r--r--   0 runner    (1001) docker     (123)     6413 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/ovs/README_OVS-DPDK.rst
+-rw-r--r--   0 runner    (1001) docker     (123)    18223 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/ovs/faucet_ovs_test.png
+-rw-r--r--   0 runner    (1001) docker     (123)     9332 2023-05-03 00:46:43.000000 c65faucet-1.0.50/docs/vendors/ovs/faucet_testing_with_OVS_on_hardware.rst
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.957045 c65faucet-1.0.50/etc/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.981045 c65faucet-1.0.50/etc/default/
+-rw-r--r--   0 runner    (1001) docker     (123)      223 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/default/faucet
+-rw-r--r--   0 runner    (1001) docker     (123)      215 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/default/gauge
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.985045 c65faucet-1.0.50/etc/faucet/
+-rw-r--r--   0 runner    (1001) docker     (123)     2826 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/faucet/acls.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)     2423 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/faucet/faucet.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      965 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/faucet/gauge.yaml
+-rw-r--r--   0 runner    (1001) docker     (123)      403 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/faucet/os_ken.conf
+-rw-r--r--   0 runner    (1001) docker     (123)      403 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/faucet/ryu.conf
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.985045 c65faucet-1.0.50/etc/logrotate.d/
+-rw-r--r--   0 runner    (1001) docker     (123)      243 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/logrotate.d/faucet
+-rw-r--r--   0 runner    (1001) docker     (123)      241 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/logrotate.d/gauge
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.985045 c65faucet-1.0.50/etc/prometheus/
+-rw-r--r--   0 runner    (1001) docker     (123)      660 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/prometheus/faucet.rules.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      263 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/prometheus/prometheus-docker-compose.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      872 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/prometheus/prometheus.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.957045 c65faucet-1.0.50/etc/systemd/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.985045 c65faucet-1.0.50/etc/systemd/system/
+-rw-r--r--   0 runner    (1001) docker     (123)      388 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/systemd/system/faucet.service
+-rw-r--r--   0 runner    (1001) docker     (123)      352 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/systemd/system/gauge.service
+-rw-r--r--   0 runner    (1001) docker     (123)      355 2023-05-03 00:46:43.000000 c65faucet-1.0.50/etc/systemd/system/prometheus.service
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.993045 c65faucet-1.0.50/faucet/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5773 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/__main__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    35363 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/acl.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2276 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/check_faucet_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10214 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14246 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/config_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7721 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/config_parser_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    71188 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/dp.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14920 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/faucet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10311 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/faucet_bgp.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16098 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/faucet_dot1x.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4998 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/faucet_event.py
+-rw-r--r--   0 runner    (1001) docker     (123)      440 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/faucet_metadata.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11424 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/faucet_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7478 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/faucet_pipeline.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     7755 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/fctl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8487 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/gauge.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9975 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/gauge_influx.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9969 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/gauge_pollers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8703 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/gauge_prom.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1721 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/meter.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31120 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/port.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3572 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/prom_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6167 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/router.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19588 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/stack.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6703 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/tfm_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)    73345 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30131 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_acl.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3036 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_coprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8518 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_lldp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1106 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_manager_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    42091 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1072 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_of_old.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1783 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_outonly.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28984 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_packet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8841 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)    49835 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_route.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7696 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_ryuapp.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18431 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_stack.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3447 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_switch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30701 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_switch_stack.py
+-rw-r--r--   0 runner    (1001) docker     (123)    52744 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_switch_standalone.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12574 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_table.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6406 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valve_util.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18634 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/valves_manager.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26043 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/vlan.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6538 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/watcher.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7013 2023-05-03 00:46:43.000000 c65faucet-1.0.50/faucet/watcher_conf.py
+-rw-r--r--   0 runner    (1001) docker     (123)       43 2023-05-03 00:46:43.000000 c65faucet-1.0.50/fuzz-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.993045 c65faucet-1.0.50/git-hook/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      456 2023-05-03 00:46:43.000000 c65faucet-1.0.50/git-hook/pre-commit
+-rw-r--r--   0 runner    (1001) docker     (123)      358 2023-05-03 00:46:43.000000 c65faucet-1.0.50/helper-funcs
+-rw-r--r--   0 runner    (1001) docker     (123)     1061 2023-05-03 00:46:43.000000 c65faucet-1.0.50/hw_switch_config.yaml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.993045 c65faucet-1.0.50/ofctl_rest/
+-rw-r--r--   0 runner    (1001) docker     (123)    27150 2023-05-03 00:46:43.000000 c65faucet-1.0.50/ofctl_rest/ofctl_rest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9752 2023-05-03 00:46:43.000000 c65faucet-1.0.50/ofctl_rest/wsgi.py
+-rw-r--r--   0 runner    (1001) docker     (123)      149 2023-05-03 00:46:43.000000 c65faucet-1.0.50/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     1064 2023-05-03 00:46:50.001045 c65faucet-1.0.50/setup.cfg
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3802 2023-05-03 00:46:43.000000 c65faucet-1.0.50/setup.py
+-rw-r--r--   0 runner    (1001) docker     (123)      196 2023-05-03 00:46:43.000000 c65faucet-1.0.50/test-requirements.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.993045 c65faucet-1.0.50/tests/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.993045 c65faucet-1.0.50/tests/codecheck/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      278 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/codecheck/flake8.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      500 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/codecheck/min_pylint.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      234 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/codecheck/pylint.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      342 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/codecheck/pytype.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      701 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/codecheck/src_files.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.957045 c65faucet-1.0.50/tests/generative/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.957045 c65faucet-1.0.50/tests/generative/fuzzer/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.993045 c65faucet-1.0.50/tests/generative/fuzzer/config/
+-rw-r--r--   0 runner    (1001) docker     (123)      405 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/config/config.dict
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.993045 c65faucet-1.0.50/tests/generative/fuzzer/config/examples/
+-rw-r--r--   0 runner    (1001) docker     (123)      432 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/config/examples/ex0
+-rw-r--r--   0 runner    (1001) docker     (123)     1004 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/config/fuzz_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4996 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/config/generate_dict.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.993045 c65faucet-1.0.50/tests/generative/fuzzer/packet/
+-rw-r--r--   0 runner    (1001) docker     (123)     1592 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/display_packet_crash.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.997045 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/
+-rw-r--r--   0 runner    (1001) docker     (123)       65 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/aoe.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/arp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)       85 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/arp.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/asap.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      183 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/asap.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/diameter.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      169 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/dns.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      277 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/dns.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      465 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/http.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      925 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/http.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      933 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/icmp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)     2885 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/icmp.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      141 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/icmp.ex3
+-rw-r--r--   0 runner    (1001) docker     (123)      141 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/icmp.ex4
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/igmpv2.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/ipv4.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      381 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/ipv6.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      193 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/irc.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      265 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/irc.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      311 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/irc.ex3
+-rw-r--r--   0 runner    (1001) docker     (123)      249 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/lacp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)     1001 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/msger.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)       63 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/packet.dict
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/tcp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/tcp.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      109 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/tcp.ex3
+-rw-r--r--   0 runner    (1001) docker     (123)      125 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/tcp.ex4
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/tcp.ex5
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/udp.ex1
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/udp.ex2
+-rw-r--r--   0 runner    (1001) docker     (123)      157 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/udp.ex3
+-rw-r--r--   0 runner    (1001) docker     (123)      147 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/udp.ex4
+-rw-r--r--   0 runner    (1001) docker     (123)      715 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/fake_packet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1458 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/fuzzer/packet/fuzz_packet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.997045 c65faucet-1.0.50/tests/generative/integration/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2304 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/integration/fault_tolerance_main.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18304 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/integration/fault_tolerance_tests.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.997045 c65faucet-1.0.50/tests/generative/unit/
+-rwxr-xr-x   0 runner    (1001) docker     (123)    12779 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/generative/unit/test_topology.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.997045 c65faucet-1.0.50/tests/integration/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      565 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/integration/mininet_main.py
+-rw-r--r--   0 runner    (1001) docker     (123)   157767 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/integration/mininet_multidp_tests.py
+-rw-r--r--   0 runner    (1001) docker     (123)   308688 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/integration/mininet_tests.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      574 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/run_unit_tests.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      244 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/sysctls_for_tests.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.957045 c65faucet-1.0.50/tests/unit/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:49.997045 c65faucet-1.0.50/tests/unit/clib/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     8815 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/clib/test_topo.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:50.001045 c65faucet-1.0.50/tests/unit/faucet/
+-rwxr-xr-x   0 runner    (1001) docker     (123)    10401 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_check_config.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)   134114 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_config.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     6666 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_fctl.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1519 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_main.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5773 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_port.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31562 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_valve.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    39651 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_valve_config.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4960 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_valve_dot1x.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5875 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_valve_egress.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2968 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_valve_of.py
+-rw-r--r--   0 runner    (1001) docker     (123)   154810 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_valve_stack.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2770 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_valveapp_smoke.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9217 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/faucet/test_vlan.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:50.001045 c65faucet-1.0.50/tests/unit/gauge/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5590 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/gauge/test_config_gauge.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)    36917 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/gauge/test_gauge.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1516 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/gauge/test_main.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-03 00:46:50.001045 c65faucet-1.0.50/tests/unit/packaging/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5501 2023-05-03 00:46:43.000000 c65faucet-1.0.50/tests/unit/packaging/test_packaging.py
```

### Comparing `c65faucet-1.0.49/.github/workflows/disabled/periodic.yml` & `c65faucet-1.0.50/.github/workflows/disabled/periodic.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/.github/workflows/disabled/release-debian.yml` & `c65faucet-1.0.50/.github/workflows/disabled/release-debian.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/.github/workflows/release-docker.yml` & `c65faucet-1.0.50/.github/workflows/release-docker.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/.github/workflows/release-python.yml` & `c65faucet-1.0.50/.github/workflows/release-python.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/.github/workflows/tests-codecheck.yml` & `c65faucet-1.0.50/.github/workflows/tests-codecheck.yml`

 * *Files 8% similar despite different names*

```diff
@@ -53,14 +53,23 @@
       - name: Set up python-${{ env.CODECHECK_PY_VER }}
         uses: actions/setup-python@v4
         with:
           python-version: ${{ env.CODECHECK_PY_VER }}
       - name: Install dependencies
         run: |
           ./docker/pip_deps.sh --extra-requirements="codecheck-requirements.txt"
+      - name: Run black formatter
+        run: |
+          if [[ "${{ env.FILES_CHANGED }}" == "all" || ! -z "${{ env.RQ_FILES_CHANGED }}" ]]; then
+            echo "Running black on everything"
+            black . --check
+          else
+            echo "Running black on ${{ env.PY_FILES_CHANGED }}"
+            black ${{ env.PY_FILES_CHANGED }} --check
+          fi
       - if: ${{ env.FILES_CHANGED == 'all' || env.PY_FILES_CHANGED }}
         name: Run pylint
         run: |
           cd ./tests/codecheck
           if [[ "${{ env.FILES_CHANGED }}" == "all" || ! -z "${{ env.RQ_FILES_CHANGED }}" ]]; then
             echo "Running pylint on everything"
             ./pylint.sh
```

### Comparing `c65faucet-1.0.49/.github/workflows/tests-docs.yml` & `c65faucet-1.0.50/.github/workflows/tests-docs.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/.github/workflows/tests-integration.yml` & `c65faucet-1.0.50/.github/workflows/tests-integration.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/.github/workflows/tests-unit.yml` & `c65faucet-1.0.50/.github/workflows/tests-unit.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/.pylintrc` & `c65faucet-1.0.50/.pylintrc`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/CONTRIBUTING.rst` & `c65faucet-1.0.50/CONTRIBUTING.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/LICENSE` & `c65faucet-1.0.50/LICENSE`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/Makefile` & `c65faucet-1.0.50/Makefile`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/PKG-INFO` & `c65faucet-1.0.50/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 1.2
 Name: c65faucet
-Version: 1.0.49
+Version: 1.0.50
 Summary: Faucet is an OpenFlow controller that implements a layer 2 and layer 3 switch.
 Home-page: https://faucet.nz
 Author: Faucet development team
 Author-email: faucetsdn@googlegroups.com
 License: Apache-2
 Description: UNKNOWN
 Keywords: openflow,openvswitch,ryu
```

### Comparing `c65faucet-1.0.49/README.rst` & `c65faucet-1.0.50/README.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/adapters/vendors/faucetagent/Dockerfile` & `c65faucet-1.0.50/adapters/vendors/faucetagent/Dockerfile`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/adapters/vendors/faucetagent/README.rst` & `c65faucet-1.0.50/adapters/vendors/faucetagent/README.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/adapters/vendors/faucetagent/example_client.sh` & `c65faucet-1.0.50/adapters/vendors/faucetagent/example_client.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/adapters/vendors/faucetagent/gencerts.sh` & `c65faucet-1.0.50/adapters/vendors/faucetagent/gencerts.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/adapters/vendors/rabbitmq/Dockerfile` & `c65faucet-1.0.50/adapters/vendors/rabbitmq/Dockerfile`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/adapters/vendors/rabbitmq/README.rst` & `c65faucet-1.0.50/adapters/vendors/rabbitmq/README.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/adapters/vendors/rabbitmq/docker-compose.yaml` & `c65faucet-1.0.50/adapters/vendors/rabbitmq/docker-compose.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/adapters/vendors/rabbitmq/example_consumer.py` & `c65faucet-1.0.50/adapters/vendors/rabbitmq/example_consumer.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,38 +3,38 @@
 import logging
 
 import pika
 
 
 def callback(chan, method, properties, body):
     """Callback that has the message that was received"""
-    logging.info(" [X] %s UTC %r:%r:%r:%r",
-                 str(datetime.datetime.utcnow()),
-                 chan,
-                 method.routing_key,
-                 properties,
-                 body)
+    logging.info(
+        " [X] %s UTC %r:%r:%r:%r",
+        str(datetime.datetime.utcnow()),
+        chan,
+        method.routing_key,
+        properties,
+        body,
+    )
 
 
 def main():
     """Creates the connection to RabbitMQ as a consumer and binds to the queue
     waiting for messages
     """
     params = pika.ConnectionParameters(host="0.0.0.0", port=5672)
     connection = pika.BlockingConnection(params)
     channel = connection.channel()
 
-    channel.exchange_declare(exchange='topic_recs', exchange_type='topic')
-    result = channel.queue_declare('faucet')
+    channel.exchange_declare(exchange="topic_recs", exchange_type="topic")
+    result = channel.queue_declare("faucet")
     queue_name = result.method.queue
 
     binding_key = "FAUCET.Event"
-    channel.queue_bind(exchange='topic_recs',
-                       queue=queue_name,
-                       routing_key=binding_key)
+    channel.queue_bind(exchange="topic_recs", queue=queue_name, routing_key=binding_key)
 
     return channel, queue_name
 
 
 if __name__ == "__main__":
     CHANNEL, QUEUE = main()
     CHANNEL.basic_consume(queue=QUEUE, on_message_callback=callback)
```

### Comparing `c65faucet-1.0.49/adapters/vendors/rabbitmq/rabbit.py` & `c65faucet-1.0.50/adapters/vendors/rabbitmq/rabbit.py`

 * *Files 8% similar despite different names*

```diff
@@ -34,17 +34,19 @@
     a virtual environment"""
 
     # Find the appropriate prefix for config and log file default locations
     # in case Faucet is run in a virtual environment. virtualenv marks the
     # original path in sys.real_prefix. If this value exists, and is
     # different from sys.prefix, then we are most likely running in a
     # virtualenv. Also check for Py3.3+ pyvenv.
-    sysprefix = ''
-    if (getattr(sys, 'real_prefix', sys.prefix) != sys.prefix
-            or getattr(sys, 'base_prefix', sys.prefix) != sys.prefix):
+    sysprefix = ""
+    if (
+        getattr(sys, "real_prefix", sys.prefix) != sys.prefix
+        or getattr(sys, "base_prefix", sys.prefix) != sys.prefix
+    ):
         sysprefix = sys.prefix
 
     return sysprefix
 
 
 # pylint: disable=too-many-instance-attributes
 class RabbitAdapter:
@@ -56,64 +58,69 @@
         super().__init__()
 
         # setup socket
         self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
 
         # get environment variables and set defaults
         self.channel = None
-        self.event_sock = os.getenv('FAUCET_EVENT_SOCK', '0')
-        self.host = os.getenv('FA_RABBIT_HOST', '')
-        self.port = os.getenv('FA_RABBIT_PORT')
+        self.event_sock = os.getenv("FAUCET_EVENT_SOCK", "0")
+        self.host = os.getenv("FA_RABBIT_HOST", "")
+        self.port = os.getenv("FA_RABBIT_PORT")
         if not self.port:
             self.port = 5672
         else:
             try:
                 self.port = int(self.port)
             except ValueError:
                 self.port = 5672
-        self.exchange = os.getenv('FA_RABBIT_EXCHANGE')
+        self.exchange = os.getenv("FA_RABBIT_EXCHANGE")
         if not self.exchange:
-            self.exchange = 'topic_recs'
-        self.exchange_type = os.getenv('FA_RABBIT_EXCHANGE_TYPE')
+            self.exchange = "topic_recs"
+        self.exchange_type = os.getenv("FA_RABBIT_EXCHANGE_TYPE")
         if not self.exchange_type:
-            self.exchange_type = 'topic'
-        self.routing_key = os.getenv('FA_RABBIT_ROUTING_KEY', 'FAUCET.Event')
+            self.exchange_type = "topic"
+        self.routing_key = os.getenv("FA_RABBIT_ROUTING_KEY", "FAUCET.Event")
         if not self.routing_key:
-            self.routing_key = 'FAUCET.Event'
+            self.routing_key = "FAUCET.Event"
 
     def rabbit_conn(self):
         """Make connection to rabbit to send events"""
         # check if a rabbit host was specified
         if not self.host:
-            print('Not connecting to any RabbitMQ, host is None.')
+            print("Not connecting to any RabbitMQ, host is None.")
             return False
 
         # create connection to rabbit
-        params = pika.ConnectionParameters(host=self.host,
-                                           port=self.port,
-                                           heartbeat=600,
-                                           blocked_connection_timeout=300)
+        params = pika.ConnectionParameters(
+            host=self.host,
+            port=self.port,
+            heartbeat=600,
+            blocked_connection_timeout=300,
+        )
         try:
             self.channel = pika.BlockingConnection(params).channel()
-            self.channel.exchange_declare(exchange=self.exchange,
-                                          exchange_type=self.exchange_type)
+            self.channel.exchange_declare(
+                exchange=self.exchange, exchange_type=self.exchange_type
+            )
         except (pika.exceptions.AMQPError, socket.gaierror, OSError) as err:
-            print(f"Unable to connect to RabbitMQ at {self.host}:{self.port} because: {err}")
+            print(
+                f"Unable to connect to RabbitMQ at {self.host}:{self.port} because: {err}"
+            )
             return False
         print(f"Connected to RabbitMQ at {self.host}:{self.port}")
         return True
 
     def socket_conn(self):
         """Make connection to sock to receive events"""
         # check if socket events are enabled
-        if self.event_sock == '0':
-            print('Not connecting to any socket, FAUCET_EVENT_SOCK is none.')
+        if self.event_sock == "0":
+            print("Not connecting to any socket, FAUCET_EVENT_SOCK is none.")
             return False
-        if self.event_sock == '1':
-            self.event_sock = get_sys_prefix() + '/var/run/faucet/faucet.sock'
+        if self.event_sock == "1":
+            self.event_sock = get_sys_prefix() + "/var/run/faucet/faucet.sock"
         # otherwise it's a path
 
         # create connection to unix socket
         try:
             self.sock.connect(self.event_sock)
             self.sock.setblocking(False)
         except socket.error as err:
@@ -121,26 +128,26 @@
             return False
         print(f"Connected to the socket at {self.event_sock}")
         return True
 
     def poll_events(self):
         """Poll FAUCET socket for events."""
         socket_ok = False
-        event_buffer = b''
+        event_buffer = b""
         read_ready, _, _ = select.select([self.sock], [], [])
         if self.sock in read_ready:
             socket_ok = True
             while True:
                 try:
                     event_buffer += self.sock.recv(1024)
                 except socket.error as err:
                     socket_ok = err.errno == errno.EWOULDBLOCK
                     break
 
-        events = event_buffer.strip().split(b'\n')
+        events = event_buffer.strip().split(b"\n")
         return (socket_ok, events)
 
     def main(self):
         """Make connections to sock and rabbit and receive messages from sock
         to sent to rabbit
         """
         # ensure connections to the socket and rabbit before getting messages
@@ -152,18 +159,23 @@
                     socket_ok, events = self.poll_events()
                 try:
                     for event in events:
                         self.channel.basic_publish(
                             exchange=self.exchange,
                             routing_key=self.routing_key,
                             body=event,
-                            properties=pika.BasicProperties(delivery_mode=2,))
+                            properties=pika.BasicProperties(
+                                delivery_mode=2,
+                            ),
+                        )
                     events = []
                 except pika.exceptions.AMQPError as err:
-                    print(f"Unable to send events {events} to RabbitMQ: {err}, retrying")
+                    print(
+                        f"Unable to send events {events} to RabbitMQ: {err}, retrying"
+                    )
                     time.sleep(1)
                     self.rabbit_conn()
                     sys.stdout.flush()
             self.sock.close()
 
 
 if __name__ == "__main__":  # pragma: no cover
```

### Comparing `c65faucet-1.0.49/adapters/vendors/rabbitmq/test_rabbit.py` & `c65faucet-1.0.50/adapters/vendors/rabbitmq/test_rabbit.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,37 +10,41 @@
 class MockPikaChannel(pika.channel.Channel):
     """Mock class for testing pika calls"""
 
     def __init__(self):
         # pylint: disable=super-init-not-called
         pass
 
-    def basic_publish(self,
-                      exchange,  # pylint: disable=unused-argument
-                      routing_key,  # pylint: disable=unused-argument
-                      body,  # pylint: disable=unused-argument
-                      properties=None,  # pylint: disable=unused-argument
-                      mandatory=False):  # pylint: disable=unused-argument
+    def basic_publish(
+        self,
+        exchange,  # pylint: disable=unused-argument
+        routing_key,  # pylint: disable=unused-argument
+        body,  # pylint: disable=unused-argument
+        properties=None,  # pylint: disable=unused-argument
+        mandatory=False,
+    ):  # pylint: disable=unused-argument
         return True
 
 
 class MockPikaBadAMQP(pika.channel.Channel):
     """Mock class for testing pika AMQP failures"""
 
     def __init__(self):
         # pylint: disable=super-init-not-called
         pass
 
-    def basic_publish(self,
-                      exchange,  # pylint: disable=unused-argument
-                      routing_key,  # pylint: disable=unused-argument
-                      body,  # pylint: disable=unused-argument
-                      properties=None,  # pylint: disable=unused-argument
-                      mandatory=False):  # pylint: disable=unused-argument
-        raise pika.exceptions.AMQPError('failure')
+    def basic_publish(
+        self,
+        exchange,  # pylint: disable=unused-argument
+        routing_key,  # pylint: disable=unused-argument
+        body,  # pylint: disable=unused-argument
+        properties=None,  # pylint: disable=unused-argument
+        mandatory=False,
+    ):  # pylint: disable=unused-argument
+        raise pika.exceptions.AMQPError("failure")
 
 
 class MockRabbitAdapter(rabbit.RabbitAdapter):
     """Mock class for testing RabbitAdapter"""
 
     def rabbit_conn(self):
         return True
@@ -53,68 +57,68 @@
     """Test no rabbit host set"""
     rabbit_adapter = rabbit.RabbitAdapter()
     rabbit_adapter.main()
 
 
 def test_no_rabbit_connection():
     """Test no connection available to rabbit"""
-    os.environ['FA_RABBIT_HOST'] = 'localhost'
+    os.environ["FA_RABBIT_HOST"] = "localhost"
     rabbit_adapter = rabbit.RabbitAdapter()
     rabbit_adapter.main()
-    assert rabbit_adapter.host == 'localhost'
+    assert rabbit_adapter.host == "localhost"
 
 
 def test_no_socket_path():
     """Test no socket path set"""
     rabbit_adapter = rabbit.RabbitAdapter()
     rabbit_adapter.socket_conn()
 
 
 def test_no_socket_connection():
     """Test no connection available to socket"""
-    os.environ['FAUCET_EVENT_SOCK'] = '1'
+    os.environ["FAUCET_EVENT_SOCK"] = "1"
     rabbit_adapter = rabbit.RabbitAdapter()
     rabbit_adapter.socket_conn()
-    assert rabbit_adapter.event_sock == '/var/run/faucet/faucet.sock'
+    assert rabbit_adapter.event_sock == "/var/run/faucet/faucet.sock"
 
 
 def test_socket_connection():
     """Test connection available to socket"""
-    os.environ['FAUCET_EVENT_SOCK'] = '/var/run/faucet/faucet-event.sock'
+    os.environ["FAUCET_EVENT_SOCK"] = "/var/run/faucet/faucet-event.sock"
     rabbit_adapter = rabbit.RabbitAdapter()
     rabbit_adapter.socket_conn()
-    assert rabbit_adapter.event_sock == '/var/run/faucet/faucet-event.sock'
+    assert rabbit_adapter.event_sock == "/var/run/faucet/faucet-event.sock"
 
 
 def test_port_set_int():
     """Test port was set and it was an int"""
-    os.environ['FA_RABBIT_PORT'] = '9999'
+    os.environ["FA_RABBIT_PORT"] = "9999"
     rabbit_adapter = rabbit.RabbitAdapter()
     assert rabbit_adapter.port == 9999
 
 
 def test_port_set_not_int():
     """Test port was set and it was not an int"""
-    os.environ['FA_RABBIT_PORT'] = 'bad'
+    os.environ["FA_RABBIT_PORT"] = "bad"
     rabbit_adapter = rabbit.RabbitAdapter()
     assert rabbit_adapter.port == 5672
 
 
 def test_routing_key_not_set():
     """Test routing_key was not set"""
-    os.environ['FA_RABBIT_ROUTING_KEY'] = ''
+    os.environ["FA_RABBIT_ROUTING_KEY"] = ""
     rabbit_adapter = rabbit.RabbitAdapter()
-    assert rabbit_adapter.routing_key == 'FAUCET.Event'
+    assert rabbit_adapter.routing_key == "FAUCET.Event"
 
 
 def test_routing_key_set():
     """Test routing_key was set"""
-    os.environ['FA_RABBIT_ROUTING_KEY'] = 'foo'
+    os.environ["FA_RABBIT_ROUTING_KEY"] = "foo"
     rabbit_adapter = rabbit.RabbitAdapter()
-    assert rabbit_adapter.routing_key == 'foo'
+    assert rabbit_adapter.routing_key == "foo"
 
 
 def test_rabbit_socket_true():
     """Test if rabbit_conn and socket_conn are True"""
     rabbit_adapter = MockRabbitAdapter()
     rabbit_adapter.channel = MockPikaChannel()
     rabbit_adapter.main()
```

### Comparing `c65faucet-1.0.49/c65faucet.egg-info/PKG-INFO` & `c65faucet-1.0.50/c65faucet.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 1.2
 Name: c65faucet
-Version: 1.0.49
+Version: 1.0.50
 Summary: Faucet is an OpenFlow controller that implements a layer 2 and layer 3 switch.
 Home-page: https://faucet.nz
 Author: Faucet development team
 Author-email: faucetsdn@googlegroups.com
 License: Apache-2
 Description: UNKNOWN
 Keywords: openflow,openvswitch,ryu
```

### Comparing `c65faucet-1.0.49/c65faucet.egg-info/SOURCES.txt` & `c65faucet-1.0.50/c65faucet.egg-info/SOURCES.txt`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/clib/clib_mininet_test.py` & `c65faucet-1.0.50/clib/clib_mininet_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,9 +9,9 @@
 It is strongly recommended to run these tests via Docker, to ensure you have
 all dependencies correctly installed. See ../docs/.
 """
 
 from clib_mininet_test_main import test_main
 import clib_mininet_tests
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     test_main([clib_mininet_tests.__name__])
```

### Comparing `c65faucet-1.0.49/clib/clib_mininet_test_main.py` & `c65faucet-1.0.50/clib/clib_mininet_test_main.py`

 * *Files 8% similar despite different names*

```diff
@@ -39,270 +39,314 @@
 from mininet.log import setLogLevel
 from mininet.clean import Cleanup
 import psutil
 
 from clib import mininet_test_util
 from clib.valve_test_lib import yaml_load, yaml_dump
 
-DEFAULT_HARDWARE = 'Open vSwitch'
+DEFAULT_HARDWARE = "Open vSwitch"
 
 # Only these hardware types will be tested with meters.
 SUPPORTS_METERS = (
     DEFAULT_HARDWARE,  # TODO: tested with OVS userspace only.
-    'Aruba',
-    'NoviFlow',
-    'ZodiacGX',
+    "Aruba",
+    "NoviFlow",
+    "ZodiacGX",
 )
 
-SUPPORTS_METADATA = (
-    DEFAULT_HARDWARE,
-)
+SUPPORTS_METADATA = (DEFAULT_HARDWARE,)
 
 
 EXTERNAL_DEPENDENCIES = (
-    ('osken-manager', ['--version'],
-     'osken-manager', r'osken-manager\s+(\d+\.\d+\.\d+)\n', "2.1"),
-    ('ovs-vsctl', ['--version'], 'Open vSwitch',
-     r'ovs-vsctl\s+\(Open vSwitch\)\s+(\d+\.\d+)\.\d+\n', "2.3"),
-    ('tcpdump', ['-h'], 'tcpdump',
-     r'tcpdump\s+version\s+(\d+\.\d+)\.\d+\n', "4.5"),
-    ('nc', ['-h'], 'OpenBSD netcat', '', 0),
-    ('vconfig', [], 'the VLAN you are talking about', '', 0),
-    ('fuser', ['-V'], r'fuser \(PSmisc\)',
-     r'fuser \(PSmisc\) (\d+\.\d+|UNKNOWN)\n', "22.0"),
-    ('lsof', ['-v'], r'lsof version',
-     r'revision: (\d+\.\d+(\.\d+)?)\n', "4.86"),
-    ('mn', ['--version'], r'\d+\.\d+.\d+',
-     r'(\d+\.\d+).\d+', "2.2"),
-    ('exabgp', ['--version'], 'ExaBGP',
-     r'ExaBGP : (\d+\.\d+).\d+', "4.0"),
-    ('pip3', ['show', 'influxdb'], 'influxdb',
-     r'Version:\s+(\d+\.\d+)\.\d+', "3.0"),
-    ('curl', ['--version'], 'libcurl',
-     r'curl (\d+\.\d+).\d+', "7.3"),
-    ('ladvd', ['-h'], 'ladvd',
-     r'ladvd version (\d+\.\d+)\.\d+', "0.9"),
-    ('iperf', ['--version'], 'iperf',
-     r'iperf version (\d+\.\d+)\.\d+', "2.0"),
-    ('fping', ['-v'], 'fping',
-     r'fping: Version (\d+\.\d+)', "3.10"),
-    ('rdisc6', ['-V'], 'ndisc6',
-     r'ndisc6.+tool (\d+\.\d+)', "1.0"),
-    ('tshark', ['-v'], r'(?i)tshark',
-     r'TShark\s*[a-zA-Z\(\)]*\s*([\d\.]+)', "2.1"),
-    ('scapy', ['-h'], 'Usage: scapy', '', 0),
+    (
+        "osken-manager",
+        ["--version"],
+        "osken-manager",
+        r"osken-manager\s+(\d+\.\d+\.\d+)\n",
+        "2.1",
+    ),
+    (
+        "ovs-vsctl",
+        ["--version"],
+        "Open vSwitch",
+        r"ovs-vsctl\s+\(Open vSwitch\)\s+(\d+\.\d+)\.\d+\n",
+        "2.3",
+    ),
+    ("tcpdump", ["-h"], "tcpdump", r"tcpdump\s+version\s+(\d+\.\d+)\.\d+\n", "4.5"),
+    ("nc", ["-h"], "OpenBSD netcat", "", 0),
+    ("vconfig", [], "the VLAN you are talking about", "", 0),
+    (
+        "fuser",
+        ["-V"],
+        r"fuser \(PSmisc\)",
+        r"fuser \(PSmisc\) (\d+\.\d+|UNKNOWN)\n",
+        "22.0",
+    ),
+    ("lsof", ["-v"], r"lsof version", r"revision: (\d+\.\d+(\.\d+)?)\n", "4.86"),
+    ("mn", ["--version"], r"\d+\.\d+.\d+", r"(\d+\.\d+).\d+", "2.2"),
+    ("exabgp", ["--version"], "ExaBGP", r"ExaBGP : (\d+\.\d+).\d+", "4.0"),
+    ("pip3", ["show", "influxdb"], "influxdb", r"Version:\s+(\d+\.\d+)\.\d+", "3.0"),
+    ("curl", ["--version"], "libcurl", r"curl (\d+\.\d+).\d+", "7.3"),
+    ("ladvd", ["-h"], "ladvd", r"ladvd version (\d+\.\d+)\.\d+", "0.9"),
+    ("iperf", ["--version"], "iperf", r"iperf version (\d+\.\d+)\.\d+", "2.0"),
+    ("fping", ["-v"], "fping", r"fping: Version (\d+\.\d+)", "3.10"),
+    ("rdisc6", ["-V"], "ndisc6", r"ndisc6.+tool (\d+\.\d+)", "1.0"),
+    ("tshark", ["-v"], r"(?i)tshark", r"TShark\s*[a-zA-Z\(\)]*\s*([\d\.]+)", "2.1"),
+    ("scapy", ["-h"], "Usage: scapy", "", 0),
 )
 
 # see hw_switch_config.yaml for how to bridge in an external hardware switch.
-HW_SWITCH_CONFIG_FILE = 'hw_switch_config.yaml'
-CONFIG_FILE_DIRS = ['/etc/faucet', './', '/faucet-src']
+HW_SWITCH_CONFIG_FILE = "hw_switch_config.yaml"
+CONFIG_FILE_DIRS = ["/etc/faucet", "./", "/faucet-src"]
 REQUIRED_TEST_PORTS = 4
 
 REQUIRED_HW_CONFIG = {
-    'dp_ports': (dict,),
-    'cpn_intf': (str,),
-    'dpid': (int,),
-    'hardware': (str,),
-    'hw_switch': (bool,),
-    'of_port': (int,),
-    'gauge_of_port': (int,),
+    "dp_ports": (dict,),
+    "cpn_intf": (str,),
+    "dpid": (int,),
+    "hardware": (str,),
+    "hw_switch": (bool,),
+    "of_port": (int,),
+    "gauge_of_port": (int,),
 }
 
 OPTIONAL_HW_CONFIG = {
-    'cpn_ipv6': (bool,),
-    'ctl_privkey': (str,),
-    'ca_certs': (str,),
-    'ctl_cert': (str,),
+    "cpn_ipv6": (bool,),
+    "ctl_privkey": (str,),
+    "ca_certs": (str,),
+    "ctl_cert": (str,),
 }
 
 ALL_HW_CONFIG = {**REQUIRED_HW_CONFIG, **OPTIONAL_HW_CONFIG}
 
 
 def import_hw_config():
     """Import configuration for physical switch testing."""
     for config_file_dir in CONFIG_FILE_DIRS:
         config_file_name = os.path.join(config_file_dir, HW_SWITCH_CONFIG_FILE)
         if os.path.isfile(config_file_name):
             break
     if os.path.isfile(config_file_name):
-        print(f'Using config from {config_file_name}')
+        print("Using config from %s" % config_file_name)
     else:
-        print(f'Cannot find {HW_SWITCH_CONFIG_FILE} in {CONFIG_FILE_DIRS}')
+        print("Cannot find %s in %s" % (HW_SWITCH_CONFIG_FILE, CONFIG_FILE_DIRS))
         sys.exit(-1)
     try:
-        with open(config_file_name, 'r', encoding='utf-8') as config_file:
+        with open(config_file_name, "r", encoding="utf-8") as config_file:
             config = yaml_load(config_file)
     except IOError:
-        print(f'Could not load YAML config data from {config_file_name}')
+        print("Could not load YAML config data from %s" % config_file_name)
         sys.exit(-1)
-    if config.get('hw_switch', False):
+    if config.get("hw_switch", False):
         unknown_keys = set(config.keys()) - set(ALL_HW_CONFIG.keys())
         if unknown_keys:
-            print(f'unknown config {unknown_keys} in {config_file_name}')
+            print("unknown config %s in %s" % (unknown_keys, config_file_name))
             sys.exit(-1)
         missing_required_keys = set(REQUIRED_HW_CONFIG.keys()) - set(config.keys())
         if missing_required_keys:
-            print(f'missing required config: {missing_required_keys}')
+            print("missing required config: %s" % missing_required_keys)
             sys.exit(-1)
         for config_key, config_value in config.items():
             valid_types = ALL_HW_CONFIG[config_key]
             valid_values = [
-                config_value for valid_type in valid_types
-                if isinstance(config_value, valid_type)]
+                config_value
+                for valid_type in valid_types
+                if isinstance(config_value, valid_type)
+            ]
             if not valid_values:
-                print(f'{config_key} ({config_value}) must be of type {valid_types} in {config_file_name}')
+                print(
+                    "%s (%s) must be of type %s in %s"
+                    % (config_key, config_value, valid_types, config_file_name)
+                )
                 sys.exit(-1)
-        dp_ports = config['dp_ports']
+        dp_ports = config["dp_ports"]
         if len(dp_ports) < REQUIRED_TEST_PORTS:
-            print(f'At least {REQUIRED_TEST_PORTS} dataplane ports are required, '
-                  f'{len(dp_ports)} are provided in {config_file_name}.')
+            print(
+                "At least %u dataplane ports are required, "
+                "%d are provided in %s."
+                % (REQUIRED_TEST_PORTS, len(dp_ports), config_file_name)
+            )
             sys.exit(-1)
         return config
     return None
 
 
 def check_dependencies():
     """Verify dependant libraries/binaries are present with correct versions."""
-    print('Checking library/binary dependencies')
-    for (binary, binary_get_version, binary_present_re,
-         binary_version_re, binary_minversion) in EXTERNAL_DEPENDENCIES:
+    print("Checking library/binary dependencies")
+    for (
+        binary,
+        binary_get_version,
+        binary_present_re,
+        binary_version_re,
+        binary_minversion,
+    ) in EXTERNAL_DEPENDENCIES:
         binary_args = [binary] + binary_get_version
-        required_binary = f'required binary/library {" ".join(binary_args)}'
+        required_binary = "required binary/library %s" % (" ".join(binary_args))
         try:
             with subprocess.Popen(
-                    binary_args,
-                    stdin=mininet_test_util.DEVNULL,
-                    stdout=subprocess.PIPE,
-                    stderr=subprocess.STDOUT,
-                    close_fds=True) as proc:
+                binary_args,
+                stdin=mininet_test_util.DEVNULL,
+                stdout=subprocess.PIPE,
+                stderr=subprocess.STDOUT,
+                close_fds=True,
+            ) as proc:
                 proc_out, proc_err = proc.communicate()
                 binary_output = proc_out.decode()
                 if proc_err is not None:
                     binary_output += proc_err.decode()
         except subprocess.CalledProcessError:
             # Might have run successfully, need to parse output
             pass
         except OSError:
-            print(f'could not run {required_binary}')
+            print("could not run %s" % required_binary)
             return False
         present_match = re.search(binary_present_re, binary_output)
         if not present_match:
-            print(f'{required_binary} not present or did not return expected string '
-                  f'{binary_present_re} ({binary_output})')
+            print(
+                "%s not present or did not return expected string %s (%s)"
+                % (required_binary, binary_present_re, binary_output)
+            )
             return False
         if binary_version_re:
             version_match = re.search(binary_version_re, binary_output)
             if version_match is None:
-                print(f'could not get version from {required_binary} ({binary_output})')
+                print(
+                    "could not get version from %s (%s)"
+                    % (required_binary, binary_output)
+                )
                 return False
             try:
                 binary_version = version_match.group(1)
             except ValueError:
-                print(f'cannot parse version {version_match} for {required_binary}')
+                print(
+                    "cannot parse version %s for %s" % (version_match, required_binary)
+                )
                 return False
-            if binary == 'fuser' and binary_version == 'UNKNOWN':
+            if binary == "fuser" and binary_version == "UNKNOWN":
                 # Workaround for psmisc 23.3
                 return True
             if version.parse(binary_version) < version.parse(binary_minversion):
-                print(f'{required_binary} version {binary_version} is less than required version {binary_minversion}')
+                print(
+                    "%s version %s is less than required version %s"
+                    % (required_binary, binary_version, binary_minversion)
+                )
                 return False
     return True
 
 
-def make_suite(tc_class, hw_config, root_tmpdir, ports_sock, max_test_load,
-               port_order, start_port):
+def make_suite(
+    tc_class, hw_config, root_tmpdir, ports_sock, max_test_load, port_order, start_port
+):
     """Compose test suite based on test class names."""
     testloader = unittest.TestLoader()
     testnames = testloader.getTestCaseNames(tc_class)
     suite = unittest.TestSuite()
     for name in testnames:
-        suite.addTest(tc_class(
-            name, hw_config, root_tmpdir, ports_sock, max_test_load,
-            port_order, start_port))
+        suite.addTest(
+            tc_class(
+                name,
+                hw_config,
+                root_tmpdir,
+                ports_sock,
+                max_test_load,
+                port_order,
+                start_port,
+            )
+        )
     return suite
 
 
 def pipeline_superset_report(decoded_pcap_logs):
     """Report on matches, instructions, and actions by table from tshark logs."""
 
     def parse_flow(flow_lines):
         table_id = None
         group_id = None
-        last_oxm_match = ''
+        last_oxm_match = ""
         matches_count = 0
         actions_count = 0
         instructions_count = 0
-        oxm_match_re = re.compile(r'.*Field: (\S+).*')
-        oxm_mask_match_re = re.compile(r'.*Has mask: True.*')
-        type_match_re = re.compile(r'Type: (\S+).+')
+        oxm_match_re = re.compile(r".*Field: (\S+).*")
+        oxm_mask_match_re = re.compile(r".*Has mask: True.*")
+        type_match_re = re.compile(r"Type: (\S+).+")
 
         for flow_line, depth, section_stack in flow_lines:
             if depth == 1:
-                if flow_line.startswith('Type: OFPT_'):
-                    if not (flow_line.startswith('Type: OFPT_FLOW_MOD')
-                            or flow_line.startswith('Type: OFPT_GROUP_MOD')):
+                if flow_line.startswith("Type: OFPT_"):
+                    if not (
+                        flow_line.startswith("Type: OFPT_FLOW_MOD")
+                        or flow_line.startswith("Type: OFPT_GROUP_MOD")
+                    ):
                         return
-                if flow_line.startswith('Table ID'):
-                    if not flow_line.startswith('Table ID: OFPTT_ALL'):
+                if flow_line.startswith("Table ID"):
+                    if not flow_line.startswith("Table ID: OFPTT_ALL"):
                         table_id = int(flow_line.split()[-1])
                     else:
                         return
-                if flow_line.startswith('Group ID'):
-                    if not flow_line.startswith('Group ID: OFPG_ALL'):
+                if flow_line.startswith("Group ID"):
+                    if not flow_line.startswith("Group ID: OFPG_ALL"):
                         group_id = int(flow_line.split()[-1])
                     else:
                         return
                 continue
             if depth > 1:
                 section_name = section_stack[-1]
                 if table_id is not None:
-                    if 'Match' in section_stack:
-                        if section_name == 'OXM field':
+                    if "Match" in section_stack:
+                        if section_name == "OXM field":
                             oxm_match = oxm_match_re.match(flow_line)
                             if oxm_match:
                                 table_matches[table_id].add(oxm_match.group(1))
                                 last_oxm_match = oxm_match.group(1)
                                 matches_count += 1
                                 if matches_count > table_matches_max[table_id]:
                                     table_matches_max[table_id] = matches_count
                             else:
                                 oxm_mask_match = oxm_mask_match_re.match(flow_line)
                                 if oxm_mask_match:
-                                    table_matches[table_id].add(last_oxm_match + '/Mask')
-                    elif 'Instruction' in section_stack:
+                                    table_matches[table_id].add(
+                                        last_oxm_match + "/Mask"
+                                    )
+                    elif "Instruction" in section_stack:
                         type_match = type_match_re.match(flow_line)
                         if type_match:
-                            if section_name == 'Instruction':
+                            if section_name == "Instruction":
                                 table_instructions[table_id].add(type_match.group(1))
                                 instructions_count += 1
-                                if instructions_count > table_instructions_max[table_id]:
-                                    table_instructions_max[table_id] = instructions_count
-                            elif section_name == 'Action':
+                                if (
+                                    instructions_count
+                                    > table_instructions_max[table_id]
+                                ):
+                                    table_instructions_max[
+                                        table_id
+                                    ] = instructions_count
+                            elif section_name == "Action":
                                 table_actions[table_id].add(type_match.group(1))
                                 actions_count += 1
                                 if actions_count > table_actions_max[table_id]:
                                     table_actions_max[table_id] = actions_count
                 elif group_id is not None:
-                    if 'Bucket' in section_stack:
+                    if "Bucket" in section_stack:
                         type_match = type_match_re.match(flow_line)
                         if type_match:
-                            if section_name == 'Action':
+                            if section_name == "Action":
                                 group_actions.add(type_match.group(1))
 
     group_actions = set()
     table_matches = collections.defaultdict(set)
     table_matches_max = collections.defaultdict(lambda: 0)
     table_instructions = collections.defaultdict(set)
     table_instructions_max = collections.defaultdict(lambda: 0)
     table_actions = collections.defaultdict(set)
     table_actions_max = collections.defaultdict(lambda: 0)
 
     for log in decoded_pcap_logs:
-        with open(log, encoding='utf-8') as log_file:
-            packets = re.compile(r'\n{2,}').split(log_file.read())
+        with open(log, encoding="utf-8") as log_file:
+            packets = re.compile(r"\n{2,}").split(log_file.read())
         for packet in packets:
             last_packet_line = None
             indent_count = 0
             last_indent_count = 0
             section_stack = []
             flow_lines = []
 
@@ -322,30 +366,39 @@
                 depth = len(section_stack)
                 last_indent_count = indent_count
                 last_packet_line = packet_line
                 flow_lines.append((packet_line, depth, copy.copy(section_stack)))
             parse_flow(flow_lines)
 
     for table in sorted(table_matches):
-        print(f'table: {table}')
-        print(f'  matches: {sorted(table_matches[table])} (max {table_matches_max[table]})')
-        print(f'  table_instructions: {sorted(table_instructions[table])} (max {table_instructions_max[table]})')
-        print(f'  table_actions: {sorted(table_actions[table])} (max {table_actions_max[table]})')
+        print("table: %u" % table)
+        print(
+            "  matches: %s (max %u)"
+            % (sorted(table_matches[table]), table_matches_max[table])
+        )
+        print(
+            "  table_instructions: %s (max %u)"
+            % (sorted(table_instructions[table]), table_instructions_max[table])
+        )
+        print(
+            "  table_actions: %s (max %u)"
+            % (sorted(table_actions[table]), table_actions_max[table])
+        )
     if group_actions:
-        print('group bucket actions:')
-        print(f'  {sorted(group_actions)}')
+        print("group bucket actions:")
+        print("  %s" % sorted(group_actions))
 
 
 def filter_test_hardware(test_obj, hw_config):
     test_hosts = test_obj.N_TAGGED + test_obj.N_UNTAGGED + test_obj.N_EXTENDED
     test_links = test_hosts * test_obj.LINKS_PER_HOST
     testing_hardware = hw_config is not None
     test_hardware = DEFAULT_HARDWARE
     if testing_hardware:
-        test_hardware = hw_config['hardware']
+        test_hardware = hw_config["hardware"]
 
     if test_obj.REQUIRES_METERS and test_hardware not in SUPPORTS_METERS:
         return False
 
     if test_obj.REQUIRES_METADATA and test_hardware not in SUPPORTS_METADATA:
         return False
 
@@ -363,128 +416,147 @@
     return True
 
 
 def max_loadavg():
     return int(psutil.cpu_count(logical=False) * 1.5)
 
 
-def expand_tests(modules, requested_test_classes, regex_test_classes, excluded_test_classes,
-                 hw_config, root_tmpdir, ports_sock, serial,
-                 port_order, start_port):
+def expand_tests(
+    modules,
+    requested_test_classes,
+    regex_test_classes,
+    excluded_test_classes,
+    hw_config,
+    root_tmpdir,
+    ports_sock,
+    serial,
+    port_order,
+    start_port,
+):
     sanity_test_suites = []
     single_test_suites = []
     parallel_test_suites = []
 
     for module in modules:
         for full_name, test_obj in inspect.getmembers(sys.modules[module]):
-            test_name = full_name.split('.')[-1]
+            test_name = full_name.split(".")[-1]
             if not inspect.isclass(test_obj):
                 continue
             if regex_test_classes or requested_test_classes:
-                if not ((regex_test_classes and regex_test_classes.match(test_name)) or (
-                        requested_test_classes and test_name in requested_test_classes)):
+                if not (
+                    (regex_test_classes and regex_test_classes.match(test_name))
+                    or (requested_test_classes and test_name in requested_test_classes)
+                ):
                     continue
             if excluded_test_classes and test_name in excluded_test_classes:
                 continue
-            if test_name.endswith('Test') and test_name.startswith('Faucet'):
+            if test_name.endswith("Test") and test_name.startswith("Faucet"):
                 if not filter_test_hardware(test_obj, hw_config):
                     continue
-                print(f'adding test {test_name}')
+                print("adding test %s" % test_name)
                 test_suite = make_suite(
-                    test_obj, hw_config, root_tmpdir, ports_sock, max_loadavg(),
-                    port_order, start_port)
-                if test_name.startswith('FaucetSanity'):
+                    test_obj,
+                    hw_config,
+                    root_tmpdir,
+                    ports_sock,
+                    max_loadavg(),
+                    port_order,
+                    start_port,
+                )
+                if test_name.startswith("FaucetSanity"):
                     sanity_test_suites.append(test_suite)
                 else:
-                    if serial or test_name.startswith('FaucetSingle') or test_obj.NETNS:
+                    if serial or test_name.startswith("FaucetSingle") or test_obj.NETNS:
                         single_test_suites.append(test_suite)
                     else:
                         parallel_test_suites.append(test_suite)
 
         sanity_tests = unittest.TestSuite()
         single_tests = unittest.TestSuite()
         parallel_tests = unittest.TestSuite()
 
         if len(parallel_test_suites) == 1:
             single_test_suites.extend(parallel_test_suites)
             parallel_test_suites = []
         if parallel_test_suites:
             seed = time.time()
-            print(f'seeding parallel test shuffle with {seed}')
+            print("seeding parallel test shuffle with %f" % seed)
             random.seed(seed)
             random.shuffle(parallel_test_suites)
             for test_suite in parallel_test_suites:
                 parallel_tests.addTest(test_suite)
 
         for test_suite in sanity_test_suites:
             sanity_tests.addTest(test_suite)
         for test_suite in single_test_suites:
             single_tests.addTest(test_suite)
     return (sanity_tests, single_tests, parallel_tests)
 
 
 class FaucetResult(unittest.runner.TextTestResult):  # pytype: disable=module-attr
-
     root_tmpdir = None
     test_duration_secs = {}
     unexpected_success = False
 
     def _test_tmpdir(self, test):
         return os.path.join(
-            self.root_tmpdir, mininet_test_util.flat_test_name(test.id()))
+            self.root_tmpdir, mininet_test_util.flat_test_name(test.id())
+        )
 
     def _set_test_duration_secs(self, test):
-        duration_file_name = os.path.join(self._test_tmpdir(test), 'test_duration_secs')
+        duration_file_name = os.path.join(self._test_tmpdir(test), "test_duration_secs")
         if test.id() not in self.test_duration_secs:
             self.test_duration_secs[test.id()] = 0
         try:
-            with open(duration_file_name, encoding='utf-8') as duration_file:
+            with open(duration_file_name, encoding="utf-8") as duration_file:
                 self.test_duration_secs[test.id()] = int(duration_file.read())
         except FileNotFoundError:
             pass
 
     def stopTest(self, test):
         self._set_test_duration_secs(test)
         super().stopTest(test)
 
     def addUnexpectedSuccess(self, test):
         self.unexpected_success = True
         super().addUnexpectedSuccess(test)
 
 
 class FaucetCleanupResult(FaucetResult):
-
     successes = []
 
     def addSuccess(self, test):
         self._set_test_duration_secs(test)
         shutil.rmtree(self._test_tmpdir(test))
-        self.successes.append((test, ''))
+        self.successes.append((test, ""))
         super().addSuccess(test)
 
 
 def debug_exception_handler(etype, value, trace):
     traceback.print_exception(etype, value, trace)
     print()
     pdb.pm()
 
 
 def test_runner(root_tmpdir, resultclass, failfast=False):
     resultclass.root_tmpdir = root_tmpdir
-    return unittest.TextTestRunner(verbosity=255, resultclass=resultclass, failfast=failfast)
+    return unittest.TextTestRunner(
+        verbosity=255, resultclass=resultclass, failfast=failfast
+    )
 
 
 def run_parallel_test_suites(root_tmpdir, resultclass, parallel_tests):
     results = []
     if parallel_tests.countTestCases():
         max_parallel_tests = min(parallel_tests.countTestCases(), max_loadavg())
-        print(f'running maximum of {max_parallel_tests} parallel tests')
+        print("running maximum of %u parallel tests" % max_parallel_tests)
         parallel_runner = test_runner(root_tmpdir, resultclass)
         parallel_suite = ConcurrentTestSuite(
-            parallel_tests, fork_for_tests(max_parallel_tests))
+            parallel_tests, fork_for_tests(max_parallel_tests)
+        )
         results.append(parallel_runner.run(parallel_suite))
     return results
 
 
 def run_single_test_suites(debug, root_tmpdir, resultclass, single_tests):
     results = []
     # TODO: Tests that are serialized generally depend on hardcoded ports.
@@ -506,81 +578,99 @@
     sanity_result = sanity_runner.run(sanity_tests)
     return sanity_result
 
 
 def report_tests(test_status, test_list, result):
     tests_json = {}
     for test_class, test_text in test_list:
-        test_text = test_text.replace('\n', '\t')
-        test_text = test_text.replace('"', '\'')
+        test_text = test_text.replace("\n", "\t")
+        test_text = test_text.replace('"', "'")
         test_duration_secs = result.test_duration_secs[test_class.id()]
-        tests_json.update({
-            test_class.id(): {
-                'status': test_status,
-                'output': test_text,
-                'test_duration_secs': test_duration_secs
-            }})
+        tests_json.update(
+            {
+                test_class.id(): {
+                    "status": test_status,
+                    "output": test_text,
+                    "test_duration_secs": test_duration_secs,
+                }
+            }
+        )
     return tests_json
 
 
 def report_results(results, hw_config, report_json_filename):
     if results:
         tests_json = {}
-        report_title = 'test results'
-        print('\n')
+        report_title = "test results"
+        print("\n")
         print(report_title)
-        print('=' * len(report_title))
+        print("=" * len(report_title))
         for result in results:
             test_lists = [
-                ('ERROR', result.errors),
-                ('FAIL', result.failures),
-                ('SKIPPED', result.skipped),
+                ("ERROR", result.errors),
+                ("FAIL", result.failures),
+                ("SKIPPED", result.skipped),
             ]
-            if hasattr(result, 'successes'):
-                test_lists.append(
-                    ('OK', result.successes))
+            if hasattr(result, "successes"):
+                test_lists.append(("OK", result.successes))
             for test_status, test_list in test_lists:
                 tests_json.update(report_tests(test_status, test_list, result))
         print(yaml_dump(tests_json))
         if report_json_filename:
             report_json = {
-                'hw_config': hw_config,
-                'tests': tests_json,
+                "hw_config": hw_config,
+                "tests": tests_json,
             }
-            with open(report_json_filename, 'w', encoding='utf-8') as report_json_file:
+            with open(report_json_filename, "w", encoding="utf-8") as report_json_file:
                 report_json_file.write(json.dumps(report_json))
 
 
-def run_test_suites(debug, report_json_filename, hw_config, root_tmpdir,
-                    resultclass, single_tests, parallel_tests, sanity_result):
-    print(f'running {parallel_tests.countTestCases()} tests in parallel and '
-          f'{single_tests.countTestCases()} tests serial')
+def run_test_suites(
+    debug,
+    report_json_filename,
+    hw_config,
+    root_tmpdir,
+    resultclass,
+    single_tests,
+    parallel_tests,
+    sanity_result,
+):
+    print(
+        "running %u tests in parallel and %u tests serial"
+        % (parallel_tests.countTestCases(), single_tests.countTestCases())
+    )
     results = []
     results.append(sanity_result)
     results.extend(run_parallel_test_suites(root_tmpdir, resultclass, parallel_tests))
-    results.extend(run_single_test_suites(debug, root_tmpdir, resultclass, single_tests))
+    results.extend(
+        run_single_test_suites(debug, root_tmpdir, resultclass, single_tests)
+    )
     report_results(results, hw_config, report_json_filename)
-    successful_results = [result for result in results
-                          if result.wasSuccessful() or result.unexpected_success]
+    successful_results = [
+        result
+        for result in results
+        if result.wasSuccessful() or result.unexpected_success
+    ]
     return len(results) == len(successful_results)
 
 
 def start_port_server(root_tmpdir, start_free_ports, min_free_ports):
-    ports_sock = os.path.join(root_tmpdir, '.ports-server')
+    ports_sock = os.path.join(root_tmpdir, ".ports-server")
     ports_server = threading.Thread(
         target=mininet_test_util.serve_ports,
-        args=(ports_sock, start_free_ports, min_free_ports))
+        args=(ports_sock, start_free_ports, min_free_ports),
+    )
     ports_server.setDaemon(True)
     ports_server.start()
     for _ in range(min_free_ports // 2):
         if os.path.exists(ports_sock):
             break
         time.sleep(1)
     if not os.path.exists(ports_sock):
-        print(f'ports server did not start ({ports_sock} not created)')
+        print("ports server did not start (%s not created)" % ports_sock)
         sys.exit(-1)
     return ports_sock
 
 
 def dump_failed_test_file(test_file, only_exts):
     dump_file = False
     if only_exts:
@@ -589,246 +679,354 @@
                 dump_file = True
                 break
     else:
         dump_file = True
 
     if dump_file:
         try:
-            with open(test_file, encoding='utf-8') as test_file_h:
+            with open(test_file, encoding="utf-8") as test_file_h:
                 test_file_content = test_file_h.read()
             if test_file_content:
                 print(test_file)
-                print('=' * len(test_file))
-                print('\n')
+                print("=" * len(test_file))
+                print("\n")
                 print(test_file_content)
         except UnicodeDecodeError:
             pass
     return dump_file
 
 
 def dump_failed_test(test_name, test_dir):
     print(test_name)
-    print('=' * len(test_name))
-    print('\n')
-    test_files = set(glob.glob(os.path.join(test_dir, '*')))
+    print("=" * len(test_name))
+    print("\n")
+    test_files = set(glob.glob(os.path.join(test_dir, "*")))
     dumped_test_files = set()
 
-    for only_exts in (['.yaml'], ['.log'], ['.cap.txt'], ['.txt']):
+    for only_exts in ([".yaml"], [".log"], [".cap.txt"], [".txt"]):
         for test_file in sorted(test_files):
             if test_file in dumped_test_files:
                 continue
             if dump_failed_test_file(test_file, only_exts):
                 dumped_test_files.add(test_file)
 
 
 def clean_test_dirs(root_tmpdir, all_successful, sanity, keep_logs, dumpfail):
     if all_successful:
         if not keep_logs or not os.listdir(root_tmpdir):
             shutil.rmtree(root_tmpdir)
     else:
-        print(f'\nlog/debug files for failed tests are in {root_tmpdir}\n')
+        print("\nlog/debug files for failed tests are in %s\n" % root_tmpdir)
         if not keep_logs:
             if sanity:
-                test_dirs = glob.glob(os.path.join(root_tmpdir, '*'))
+                test_dirs = glob.glob(os.path.join(root_tmpdir, "*"))
                 for test_dir in test_dirs:
                     test_name = os.path.basename(test_dir)
                     if dumpfail:
                         dump_failed_test(test_name, test_dir)
 
 
-def run_tests(modules, hw_config, requested_test_classes, regex_test_classes, dumpfail, debug,
-              keep_logs, serial, repeat, excluded_test_classes, report_json_filename,
-              port_order, start_port):
+def run_tests(
+    modules,
+    hw_config,
+    requested_test_classes,
+    regex_test_classes,
+    dumpfail,
+    debug,
+    keep_logs,
+    serial,
+    repeat,
+    excluded_test_classes,
+    report_json_filename,
+    port_order,
+    start_port,
+):
     """Actually run the test suites, potentially in parallel."""
     if repeat:
-        print('Will repeat tests until failure')
+        print("Will repeat tests until failure")
     if hw_config is not None:
-        print('Testing hardware, forcing test serialization')
+        print("Testing hardware, forcing test serialization")
         serial = True
-    root_tmpdir = tempfile.mkdtemp(prefix='faucet-tests-', dir='/var/tmp')
+    root_tmpdir = tempfile.mkdtemp(prefix="faucet-tests-", dir="/var/tmp")
     os.chmod(root_tmpdir, 0o755)
-    print(f'Logging test results in {root_tmpdir}')
+    print("Logging test results in %s" % root_tmpdir)
     start_free_ports = 10
     min_free_ports = 200
     if serial:
         start_free_ports = 5
         min_free_ports = 5
     ports_sock = start_port_server(root_tmpdir, start_free_ports, min_free_ports)
-    print('test ports server started')
+    print("test ports server started")
     resultclass = FaucetCleanupResult
     if keep_logs:
         resultclass = FaucetResult
     all_successful = False
     no_tests = True
 
     sanity_tests, single_tests, parallel_tests = expand_tests(
-        modules, requested_test_classes, regex_test_classes, excluded_test_classes,
-        hw_config, root_tmpdir, ports_sock, serial, port_order, start_port)
-
-    test_count = (sanity_tests.countTestCases() + single_tests.countTestCases() + parallel_tests.countTestCases())
+        modules,
+        requested_test_classes,
+        regex_test_classes,
+        excluded_test_classes,
+        hw_config,
+        root_tmpdir,
+        ports_sock,
+        serial,
+        port_order,
+        start_port,
+    )
+
+    test_count = (
+        sanity_tests.countTestCases()
+        + single_tests.countTestCases()
+        + parallel_tests.countTestCases()
+    )
 
     if test_count:
         no_tests = False
         sanity_result = run_sanity_test_suite(root_tmpdir, resultclass, sanity_tests)
         if sanity_result.wasSuccessful():
             while True:
                 all_successful = run_test_suites(
-                    debug, report_json_filename,
-                    hw_config, root_tmpdir, resultclass,
+                    debug,
+                    report_json_filename,
+                    hw_config,
+                    root_tmpdir,
+                    resultclass,
                     copy.deepcopy(single_tests),
                     copy.deepcopy(parallel_tests),
-                    sanity_result)
+                    sanity_result,
+                )
                 if not repeat:
                     break
                 if not all_successful:
                     break
-                print('repeating run')
+                print("repeating run")
         else:
             report_results([sanity_result], hw_config, report_json_filename)
 
     if no_tests:
-        print('no tests selected')
+        print("no tests selected")
         shutil.rmtree(root_tmpdir)
         sys.exit(0)
     else:
-        decoded_pcap_logs = glob.glob(os.path.join(
-            os.path.join(root_tmpdir, '*'), '*of.cap.txt'))
+        decoded_pcap_logs = glob.glob(
+            os.path.join(os.path.join(root_tmpdir, "*"), "*of.cap.txt")
+        )
         pipeline_superset_report(decoded_pcap_logs)
         clean_test_dirs(
-            root_tmpdir, all_successful,
-            sanity_result.wasSuccessful(), keep_logs, dumpfail)
+            root_tmpdir,
+            all_successful,
+            sanity_result.wasSuccessful(),
+            keep_logs,
+            dumpfail,
+        )
 
     if not all_successful:
         sys.exit(-1)
 
 
 def parse_args():
     """Parse command line arguments."""
 
-    parser = argparse.ArgumentParser(
-        prog='mininet_tests')
-    parser.add_argument(
-        '--regex', help='run tests that match regular expression pattern')
-    parser.add_argument(
-        '-c', '--clean', action='store_true', help='run mininet cleanup')
-    parser.add_argument(
-        '-d', '--dumpfail', action='store_true', help='dump logs for failed tests')
-    parser.add_argument(
-        '--debug', action='store_true', help='enter debug breakpoint on assertion failure')
-    parser.add_argument(
-        '-i', '--integration', default=True, action='store_true', help='run integration tests')
-    parser.add_argument(
-        '-j', '--jsonreport', help='write a json file with test results')
-    parser.add_argument(
-        '-k', '--keep_logs', action='store_true', help='keep logs even for OK tests')
-    loglevels = ('debug', 'error', 'warning', 'info', 'output')
-    parser.add_argument(
-        '-l', '--loglevel', choices=loglevels, default='warning',
-        help='set mininet logging level')
-    parser.add_argument(
-        '-n', '--nocheck', action='store_true', help='skip dependency check')
-    parser.add_argument(
-        '-o', '--order', default='random',
-        help='port order for tests: 0,1,2,3 | random (default: random)')
-    parser.add_argument(
-        '-p', '--profile', action='store_true',
-        help='use Cprofile to report elapsed wall clock time per function')
-    parser.add_argument(
-        '--port', default='random',
-        help='starting port number (integer) | random (default: random)')
-    parser.add_argument(
-        '-r', '--repeat', action='store_true', help='repeat tests until failure')
-    parser.add_argument(
-        '-s', '--serial', action='store_true', help='run tests serially')
+    parser = argparse.ArgumentParser(prog="mininet_tests")
     parser.add_argument(
-        '--generative_unit', default=False, action='store_true', help='run generative unit tests')
+        "--regex", help="run tests that match regular expression pattern"
+    )
     parser.add_argument(
-        '--generative_tolerance', default=False, action='store_true',
-        help='run generative integration fault-tolerance tests')
+        "-c", "--clean", action="store_true", help="run mininet cleanup"
+    )
     parser.add_argument(
-        '-x', help='list of test classes to exclude')
+        "-d", "--dumpfail", action="store_true", help="dump logs for failed tests"
+    )
+    parser.add_argument(
+        "--debug",
+        action="store_true",
+        help="enter debug breakpoint on assertion failure",
+    )
+    parser.add_argument(
+        "-i",
+        "--integration",
+        default=True,
+        action="store_true",
+        help="run integration tests",
+    )
+    parser.add_argument(
+        "-j", "--jsonreport", help="write a json file with test results"
+    )
+    parser.add_argument(
+        "-k", "--keep_logs", action="store_true", help="keep logs even for OK tests"
+    )
+    loglevels = ("debug", "error", "warning", "info", "output")
+    parser.add_argument(
+        "-l",
+        "--loglevel",
+        choices=loglevels,
+        default="warning",
+        help="set mininet logging level",
+    )
+    parser.add_argument(
+        "-n", "--nocheck", action="store_true", help="skip dependency check"
+    )
+    parser.add_argument(
+        "-o",
+        "--order",
+        default="random",
+        help="port order for tests: 0,1,2,3 | random (default: random)",
+    )
+    parser.add_argument(
+        "-p",
+        "--profile",
+        action="store_true",
+        help="use Cprofile to report elapsed wall clock time per function",
+    )
+    parser.add_argument(
+        "--port",
+        default="random",
+        help="starting port number (integer) | random (default: random)",
+    )
+    parser.add_argument(
+        "-r", "--repeat", action="store_true", help="repeat tests until failure"
+    )
+    parser.add_argument(
+        "-s", "--serial", action="store_true", help="run tests serially"
+    )
+    parser.add_argument(
+        "--generative_unit",
+        default=False,
+        action="store_true",
+        help="run generative unit tests",
+    )
+    parser.add_argument(
+        "--generative_tolerance",
+        default=False,
+        action="store_true",
+        help="run generative integration fault-tolerance tests",
+    )
+    parser.add_argument("-x", help="list of test classes to exclude")
 
     excluded_test_classes = []
     report_json_filename = None
 
     try:
         args, requested_test_classes = parser.parse_known_args(sys.argv[1:])
-        if args.order == 'random':
+        if args.order == "random":
             port_order = list(range(4))
             random.shuffle(port_order)
         else:
-            port_order = [int(s) for s in args.order.split(',')]
+            port_order = [int(s) for s in args.order.split(",")]
         if sorted(port_order) != sorted(range(len(port_order))):
-            print('Port order should be a permutation of 0,1,2,3')
+            print("Port order should be a permutation of 0,1,2,3")
             raise ValueError
-        if args.port == 'random':
+        if args.port == "random":
             start_port = random.randint(1, 10)
         else:
             start_port = int(args.port)
         regex_test_classes = None
         if args.regex:
             regex_test_classes = re.compile(args.regex)
-            print(f'Running tests on classes matching {args.regex}')
-    except(KeyError, IndexError, ValueError):
+            print("Running tests on classes matching %s" % args.regex)
+    except (KeyError, IndexError, ValueError):
         parser.print_usage()
         sys.exit(-1)
 
     if args.jsonreport:
         report_json_filename = args.jsonreport
     if args.x:
-        excluded_test_classes = args.x.split(',')
+        excluded_test_classes = args.x.split(",")
 
     return (
-        requested_test_classes, regex_test_classes, args.clean, args.dumpfail, args.debug,
-        args.keep_logs, args.nocheck, args.serial, args.repeat,
-        excluded_test_classes, report_json_filename, port_order, start_port,
-        args.loglevel, args.profile)
+        requested_test_classes,
+        regex_test_classes,
+        args.clean,
+        args.dumpfail,
+        args.debug,
+        args.keep_logs,
+        args.nocheck,
+        args.serial,
+        args.repeat,
+        excluded_test_classes,
+        report_json_filename,
+        port_order,
+        start_port,
+        args.loglevel,
+        args.profile,
+    )
 
 
 def test_main(modules, serial_override=None):
     """Test main."""
 
-    print(f'testing module {modules}')
+    print("testing module %s" % modules)
 
-    (requested_test_classes, regex_test_classes, clean, dumpfail, debug, keep_logs, nocheck,
-     serial, repeat, excluded_test_classes, report_json_filename, port_order,
-     start_port, loglevel, profile) = parse_args()
+    (
+        requested_test_classes,
+        regex_test_classes,
+        clean,
+        dumpfail,
+        debug,
+        keep_logs,
+        nocheck,
+        serial,
+        repeat,
+        excluded_test_classes,
+        report_json_filename,
+        port_order,
+        start_port,
+        loglevel,
+        profile,
+    ) = parse_args()
 
     if serial_override is not None:
-        print('overriding serial to ', serial_override)
+        print("overriding serial to ", serial_override)
         serial = serial_override
 
     setLogLevel(loglevel)
 
     if clean:
-        print('Cleaning up test interfaces, processes and openvswitch '
-              'configuration from previous test runs')
+        print(
+            "Cleaning up test interfaces, processes and openvswitch "
+            "configuration from previous test runs"
+        )
         Cleanup.cleanup()
         sys.exit(0)
 
     if nocheck:
-        print('Skipping dependency checks')
+        print("Skipping dependency checks")
     else:
         if not check_dependencies():
-            print('dependency check failed. check required library/binary '
-                  'list in header of this script')
+            print(
+                "dependency check failed. check required library/binary "
+                "list in header of this script"
+            )
             sys.exit(-1)
 
-    print('port order: -o', ','.join(str(i) for i in port_order))
-    print(f'start port: --port {start_port}')
+    print("port order: -o", ",".join(str(i) for i in port_order))
+    print("start port: --port %s" % start_port)
 
     hw_config = import_hw_config()
 
     if profile:
         # use wall clock time
         pr = cProfile.Profile(time.time)  # pylint: disable=invalid-name
         pr.enable()
 
     run_tests(
-        modules, hw_config, requested_test_classes, regex_test_classes, dumpfail, debug,
-        keep_logs, serial, repeat, excluded_test_classes, report_json_filename,
-        port_order, start_port)
+        modules,
+        hw_config,
+        requested_test_classes,
+        regex_test_classes,
+        dumpfail,
+        debug,
+        keep_logs,
+        serial,
+        repeat,
+        excluded_test_classes,
+        report_json_filename,
+        port_order,
+        start_port,
+    )
 
     if profile:
         pr.disable()
-        ps = pstats.Stats(pr).sort_stats('cumulative')  # pylint: disable=invalid-name
+        ps = pstats.Stats(pr).sort_stats("cumulative")  # pylint: disable=invalid-name
         ps.print_stats()
```

### Comparing `c65faucet-1.0.49/clib/clib_mininet_tests.py` & `c65faucet-1.0.50/clib/clib_mininet_tests.py`

 * *Files 9% similar despite different names*

```diff
@@ -37,132 +37,155 @@
                 native_vlan: 100
                 description: "b4"
 """
 
     def setUp(self):
         super().setUp()
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
-            n_extended=self.N_EXTENDED, e_cls=self.EXTENDED_CLS,
-            tmpdir=self.tmpdir, links_per_host=self.LINKS_PER_HOST,
-            hw_dpid=self.hw_dpid)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=self.N_TAGGED,
+            n_untagged=self.N_UNTAGGED,
+            n_extended=self.N_EXTENDED,
+            e_cls=self.EXTENDED_CLS,
+            tmpdir=self.tmpdir,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+        )
         self.start_net()
 
     def test_ping_all(self):
         """All hosts should have connectivity."""
         self.ping_all_when_learned()
 
 
 class FaucetTcpdumpHelperTest(FaucetSimpleTest):
     """Test for TcpdumpHelper class"""
 
     def _terminate_with_zero(self, tcpdump_helper):
         term_returns = tcpdump_helper.terminate()
-        self.assertEqual(
-            0, term_returns, msg=f'terminate code not 0: {term_returns}')
+        self.assertEqual(0, term_returns, msg="terminate code not 0: %d" % term_returns)
 
     def _terminate_with_nonzero(self, tcpdump_helper):
         term_returns = tcpdump_helper.terminate()
         self.assertNotEqual(
-            0, term_returns, msg=f'terminate code is 0: {term_returns}')
+            0, term_returns, msg="terminate code is 0: %d" % term_returns
+        )
 
     def test_tcpdump_execute(self):
         """Check tcpdump filter monitors ping using execute"""
         self.ping_all_when_learned()
         from_host = self.net.hosts[0]
         to_host = self.net.hosts[1]
-        tcpdump_filter = ('icmp')
-        tcpdump_helper = TcpdumpHelper(to_host, tcpdump_filter, [
-            lambda: from_host.cmd(f'ping -c1 {to_host.IP()}')])
+        tcpdump_filter = "icmp"
+        tcpdump_helper = TcpdumpHelper(
+            to_host,
+            tcpdump_filter,
+            [lambda: from_host.cmd("ping -c1 %s" % to_host.IP())],
+        )
         tcpdump_txt = tcpdump_helper.execute()
-        self.assertTrue(re.search(
-            f'{to_host.IP()}: ICMP echo request', tcpdump_txt))
+        self.assertTrue(re.search("%s: ICMP echo request" % to_host.IP(), tcpdump_txt))
         self._terminate_with_zero(tcpdump_helper)
 
     def test_tcpdump_pcap(self):
         """Check tcpdump creates pcap output"""
         self.ping_all_when_learned()
         from_host = self.net.hosts[0]
         to_host = self.net.hosts[1]
-        tcpdump_filter = ('icmp')
-        pcap_file = os.path.join(self.tmpdir, 'out.pcap')
+        tcpdump_filter = "icmp"
+        pcap_file = os.path.join(self.tmpdir, "out.pcap")
         tcpdump_helper = TcpdumpHelper(
-            to_host, tcpdump_filter,
-            [lambda: from_host.cmd(f'ping -c3 {to_host.IP()}')],
-            pcap_out=pcap_file, packets=None)
+            to_host,
+            tcpdump_filter,
+            [lambda: from_host.cmd("ping -c3 %s" % to_host.IP())],
+            pcap_out=pcap_file,
+            packets=None,
+        )
         tcpdump_helper.execute()
         self._terminate_with_zero(tcpdump_helper)
-        result = from_host.cmd(f'tcpdump -en -r {pcap_file}')
-        self.assertEqual(result.count('ICMP echo reply'), 3, 'three icmp echo replies')
+        result = from_host.cmd("tcpdump -en -r %s" % pcap_file)
+        self.assertEqual(result.count("ICMP echo reply"), 3, "three icmp echo replies")
 
     def test_tcpdump_noblock(self):
         """Check tcpdump uses nonblocking next_line"""
         self.ping_all_when_learned()
         from_host = self.net.hosts[0]
         to_host = self.net.hosts[1]
-        tcpdump_filter = ('icmp')
+        tcpdump_filter = "icmp"
         tcpdump_helper = TcpdumpHelper(
-            to_host, tcpdump_filter,
-            [lambda: from_host.cmd(f'ping -c10 {to_host.IP()}')],
-            blocking=False, packets=None)
+            to_host,
+            tcpdump_filter,
+            [lambda: from_host.cmd("ping -c10 %s" % to_host.IP())],
+            blocking=False,
+            packets=None,
+        )
         count = 0
         while tcpdump_helper.next_line():
             count = count + 1
-            self.assertTrue(count < 10, 'Too many ping results before noblock')
+            self.assertTrue(count < 10, "Too many ping results before noblock")
         self._terminate_with_nonzero(tcpdump_helper)
 
     def test_tcpdump_nextline(self):
         """Check tcpdump filter monitors ping using next_line"""
         self.ping_all_when_learned()
         from_host = self.net.hosts[0]
         to_host = self.net.hosts[1]
-        tcpdump_filter = ('icmp')
-        tcpdump_helper = TcpdumpHelper(to_host, tcpdump_filter, [
-            lambda: from_host.cmd(f'ping -c5 -i2 {to_host.IP()}')])
+        tcpdump_filter = "icmp"
+        tcpdump_helper = TcpdumpHelper(
+            to_host,
+            tcpdump_filter,
+            [lambda: from_host.cmd("ping -c5 -i2 %s" % to_host.IP())],
+        )
 
-        self.assertTrue(re.search('proto ICMP', tcpdump_helper.next_line()))
+        self.assertTrue(re.search("proto ICMP", tcpdump_helper.next_line()))
         next_line = tcpdump_helper.next_line()
-        self.assertTrue(re.search(f'{to_host.IP()}: ICMP echo request', next_line), next_line)
-        self.assertTrue(re.search('proto ICMP', tcpdump_helper.next_line()))
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % to_host.IP(), next_line), next_line
+        )
+        self.assertTrue(re.search("proto ICMP", tcpdump_helper.next_line()))
         next_line = tcpdump_helper.next_line()
-        self.assertTrue(re.search(f'{from_host.IP()}: ICMP echo reply', next_line), next_line)
-        self.assertFalse(re.search('ICMP', tcpdump_helper.next_line()))
+        self.assertTrue(
+            re.search("%s: ICMP echo reply" % from_host.IP(), next_line), next_line
+        )
+        self.assertFalse(re.search("ICMP", tcpdump_helper.next_line()))
         while tcpdump_helper.next_line():
             pass
         self._terminate_with_zero(tcpdump_helper)
 
 
 class FaucetDockerHostTest(FaucetSimpleTest):
     """Test basic docker host functionality"""
 
     N_UNTAGGED = 2
     N_EXTENDED = 2
-    EXTENDED_CLS = make_docker_host('faucet/test-host', startup_timeout_ms=90 * 1e3)
+    EXTENDED_CLS = make_docker_host("faucet/test-host", startup_timeout_ms=90 * 1e3)
 
     def test_containers(self):
         """Test containers to make sure they're actually docker."""
         count = 0
         host_name = None
 
         for host in self.net.hosts:
-            marker = host.cmd('cat /root/test_marker.txt').strip()
-            if marker == 'faucet-test-host':
+            marker = host.cmd("cat /root/test_marker.txt").strip()
+            if marker == "faucet-test-host":
                 host_name = host.name
                 count = count + 1
                 host.activate()
                 host.wait()
 
         self.assertTrue(
             count == self.N_EXTENDED,
-            f'Found {count} containers, expected {self.N_EXTENDED}')
+            "Found %d containers, expected %d" % (count, self.N_EXTENDED),
+        )
 
         self.assertTrue(
-            os.path.exists(
-                os.path.join(self.tmpdir, host_name, 'tmp')),
-            'container tmp dir missing')
+            os.path.exists(os.path.join(self.tmpdir, host_name, "tmp")),
+            "container tmp dir missing",
+        )
 
-        host_log = os.path.join(self.tmpdir, host_name, 'activate.log')
-        with open(host_log, 'r', encoding='utf-8') as host_log_file:
+        host_log = os.path.join(self.tmpdir, host_name, "activate.log")
+        with open(host_log, "r", encoding="utf-8") as host_log_file:
             lines = host_log_file.readlines()
-            output = ' '.join(lines).strip()
-            self.assertEqual(output, 'hello faucet')
+            output = " ".join(lines).strip()
+            self.assertEqual(output, "hello faucet")
```

### Comparing `c65faucet-1.0.49/clib/config_generator.py` & `c65faucet-1.0.50/clib/config_generator.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-
 """Mininet Topo class with YAML config generator"""
 
 # Copyright (C) 2015 Research and Innovation Advanced Network New Zealand Ltd.
 # Copyright (C) 2015--2019 The Contributors
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -19,29 +18,34 @@
 import random
 import string
 
 from mininet.log import output
 from mininet.topo import Topo
 
 from clib import mininet_test_util
-from clib.mininet_test_topo import FaucetHost, VLANHost, FaucetSwitch, NoControllerFaucetSwitch
+from clib.mininet_test_topo import (
+    FaucetHost,
+    VLANHost,
+    FaucetSwitch,
+    NoControllerFaucetSwitch,
+)
 from clib.valve_test_lib import yaml_dump
 
 
 class GenerationError(Exception):
     """Indicates a problem with generating the configuration file"""
 
 
 class FaucetTopoGenerator(Topo):
     """Creates a mininet topology and then provides a method to generate a YAML config file"""
 
     # Host CPU option
     CPUF = 0.5
     # Link delay option
-    DELAY = '1ms'
+    DELAY = "1ms"
 
     # Switch index map to switch name
     switches_by_id = {}
     # Switch index map to switch dpid
     dpids_by_id = {}
     # Host index map to host name
     hosts_by_id = {}
@@ -70,42 +74,46 @@
 
     def _create_link_port_map(self):
         """Switch pair link map to list of ports for that pair"""
         port_maps = {}
         for i, name in self.switches_by_id.items():
             for port, link in self.ports[name].items():
                 if self.isSwitch(link[0]):
-                    peer_id = self.nodeInfo(link[0])['switch_n']
+                    peer_id = self.nodeInfo(link[0])["switch_n"]
                     port_maps.setdefault((i, peer_id), [])
                     port_maps[(i, peer_id)].append(port)
         return port_maps
 
     def _create_host_port_map(self):
         """Host map to linked switches to list of ports from switch to host"""
         host_port_map = {}
         for host, name in self.hosts_by_id.items():
             host_port_map.setdefault(host, {})
             for link in self.ports[name].values():
-                switch_id = self.nodeInfo(link[0])['switch_n']
+                switch_id = self.nodeInfo(link[0])["switch_n"]
                 host_port_map[host].setdefault(switch_id, [])
                 host_port_map[host][switch_id].append(link[1])
         return host_port_map
 
     def _create_port_map(self):
         """Create a map from port to the true port"""
         port_maps = {}
         for i, dpid in self.dpids_by_id.items():
             switch_name = self.switches_by_id[i]
             ports = self.ports[switch_name].keys()
-            port_maps[dpid] = {f'port_{i}': port for i, port in enumerate(ports)}
+            port_maps[dpid] = {"port_%d" % i: port for i, port in enumerate(ports)}
         return port_maps
 
     def create_port_maps(self):
         """Return host port maps and link port maps"""
-        return self._create_port_map(), self._create_host_port_map(), self._create_link_port_map()
+        return (
+            self._create_port_map(),
+            self._create_host_port_map(),
+            self._create_link_port_map(),
+        )
 
     def rand_sequential_dpid(self, i):
         """Returns a DPID that is sequential to the other DPIDs in the topology"""
         # Some functions in the Faucet code relies on the DPID for determining the order
         #   of an operation; E.g. stack root nominations. This could mean that by using randomized
         #   DPIDs, the test could be flaky due to the random ordering of the DPIDs
         if i == 0 and self.hw_dpid:
@@ -132,43 +140,44 @@
                 dpid = int(prev_dpid) + 1
             if dpid not in self.dpids_by_id.values():
                 return str(dpid)
 
     @staticmethod
     def vlan_name(i):
         """VLAN name"""
-        return f'vlan-{i+1}'
+        return "vlan-%i" % (i + 1)
 
     @staticmethod
     def vlan_vid(i):
         """VLAN VID value"""
         return (i + 1) * 100
 
     @staticmethod
     def router_name(i):
         """Router name"""
-        return f'router-{i + 1}'
+        return "router-%s" % (i + 1)
 
     def __init__(self, *args, **kwargs):
         self.switches_by_id = {}
         self.dpids_by_id = {}
         self.hosts_by_id = {}
         self.num_dps = None
         self.descending_dpids = None
         super().__init__(*args, **kwargs)
 
     @staticmethod
     def _get_sid_prefix(ports_served):
         """Return a unique switch/host prefix for a test."""
         # Linux tools require short interface names.
-        id_chars = ''.join(sorted(  # pytype: disable=module-attr
-            string.ascii_letters + string.digits))
+        id_chars = "".join(
+            sorted(string.ascii_letters + string.digits)  # pytype: disable=module-attr
+        )
         id_a = int(ports_served / len(id_chars))
         id_b = ports_served - (id_a * len(id_chars))
-        return f'{id_chars[id_a]}{id_chars[id_b]}'
+        return "%s%s" % (id_chars[id_a], id_chars[id_b])
 
     @staticmethod
     def extend_port_order(port_order=None, max_length=16):
         """
         Extends the pattern of port_port order up to max_length
 
         Args:
@@ -216,63 +225,69 @@
             sid_prefix (str): SID prefix to generate the host name
             host_index (int): Host index to generate the host name
             vlans (list/None/int): Type of host/vlans the host belongs to
         """
         sid_prefix = self._generate_sid_prefix()
         host_opts = self.host_options.get(host_index, {})
         host_name = None
-        if 'cls' in host_opts:
-            host_name = 'e%s%1.1u' % (sid_prefix, host_index + 1)
+        if "cls" in host_opts:
+            host_name = "e%s%1.1u" % (sid_prefix, host_index + 1)
         else:
             if isinstance(vlans, int) or vlans is None:
-                host_name = 'u%s%1.1u' % (sid_prefix, host_index + 1)
-                host_opts['cls'] = FaucetHost
+                host_name = "u%s%1.1u" % (sid_prefix, host_index + 1)
+                host_opts["cls"] = FaucetHost
             elif isinstance(vlans, list):
-                host_name = 't%s%1.1u' % (sid_prefix, host_index + 1)
-                host_opts['vlans'] = [FaucetTopoGenerator.vlan_vid(vlan) for vlan in vlans]
-                host_opts['cls'] = VLANHost
+                host_name = "t%s%1.1u" % (sid_prefix, host_index + 1)
+                host_opts["vlans"] = [
+                    FaucetTopoGenerator.vlan_vid(vlan) for vlan in vlans
+                ]
+                host_opts["cls"] = VLANHost
             else:
-                raise GenerationError('Unknown host type')
+                raise GenerationError("Unknown host type")
         self.hosts_by_id[host_index] = host_name
         return self.addHost(
             cpu=self.CPUF,
             host_n=host_index,
             name=host_name,
             config_vlans=vlans,
-            **host_opts)
+            **host_opts
+        )
 
     def _add_faucet_switch(self, switch_index):
         """
         Adds a Faucet switch to the topology
 
         Args:
             sid_prefix (str): SID prefix to generate the switch name
             switch_index (int): Switch index to generate the host name
             dpid (int): Switch DP ID
         """
         sid_prefix = self._generate_sid_prefix()
         switch_cls = FaucetSwitch
-        switch_name = f's{sid_prefix}'
+        switch_name = "s%s" % sid_prefix
         if switch_index == 0 and self.hw_dpid:
             self.hw_name = switch_name
             self.dpids_by_id[switch_index] = self.hw_dpid
             dpid = str(int(self.hw_dpid) + 1)
-            output('bridging hardware switch DPID %s (%x) dataplane via OVS DPID %s (%x)\n' % (
-                self.hw_dpid, int(self.hw_dpid), dpid, int(dpid)))
+            output(
+                "bridging hardware switch DPID %s (%x) dataplane via OVS DPID %s (%x)\n"
+                % (self.hw_dpid, int(self.hw_dpid), dpid, int(dpid))
+            )
             switch_cls = NoControllerFaucetSwitch
         else:
             dpid = self.rand_sequential_dpid(switch_index)
             self.dpids_by_id[switch_index] = dpid
         self.switches_by_id[switch_index] = switch_name
         return self.addSwitch(
             name=switch_name,
             cls=switch_cls,
             datapath=self.ovs_type,
             dpid=mininet_test_util.mininet_dpid(dpid),
-            switch_n=switch_index)
+            switch_n=switch_index,
+        )
 
     def _add_link(self, node, peer_node, vlans):
         """
         Creates and adds a link between two nodes to the topology
 
         Args:
             node (str): Name of the node for the link, NOTE: should ALWAYS be a switch
@@ -285,23 +300,19 @@
             # Node is a switch, create port
             port1 = self._create_next_port(node)
         if self.isSwitch(peer_node):
             # Peer node is a switch, create port
             port2 = self._create_next_port(peer_node)
         else:
             # Peer node is a host, use delay & htb options
-            opts['delay'] = self.DELAY
-            opts['use_htb'] = True
+            opts["delay"] = self.DELAY
+            opts["use_htb"] = True
         return self.addLink(
-            node,
-            peer_node,
-            port1=port1,
-            port2=port2,
-            **opts,
-            config_vlans=vlans)
+            node, peer_node, port1=port1, port2=port2, **opts, config_vlans=vlans
+        )
 
     def add_switch_topology(self, switch_links, link_vlans):
         """
         Adds the switches and switch-switch links to the network topology
         Tagged links are mapped to a list of vlan indices whereas untagged links
             are mapped to a single vlan index, stack links are mapped to None
 
@@ -336,20 +347,32 @@
             for dp_i in links:
                 if dp_i not in self.switches_by_id:
                     self._add_faucet_switch(dp_i)
                 switch_name = self.switches_by_id[dp_i]
                 self._add_link(switch_name, host_name, vlans)
 
     # pylint: disable=arguments-differ
-    def build(self, ovs_type, ports_sock, test_name, num_dps, descending_dpids,
-              host_links, host_vlans, switch_links, link_vlans,
-              hw_dpid=None, hw_ports=None,
-              port_order=None, start_port=5,
-              get_serialno=mininet_test_util.get_serialno,
-              host_options=None):
+    def build(
+        self,
+        ovs_type,
+        ports_sock,
+        test_name,
+        num_dps,
+        descending_dpids,
+        host_links,
+        host_vlans,
+        switch_links,
+        link_vlans,
+        hw_dpid=None,
+        hw_ports=None,
+        port_order=None,
+        start_port=5,
+        get_serialno=mininet_test_util.get_serialno,
+        host_options=None,
+    ):
         """
         Creates a Faucet mininet topology
 
         Args:
             ovs_type (str): The OVS switch type
             ports_sock (str): Port socket
             test_name (str): Name of the test creating the mininet topology
@@ -393,107 +416,108 @@
         """Return the ACLs in dictionary format for the configuration file"""
         return acl_options.copy()
 
     def get_dps_config(self, dp_options, host_options, link_options, ignored_switches):
         """Return the DPs in dictionary format for the configuration file"""
         dps_config = {}
 
-        def get_interface_config(link_name, src_port, dst_node, dst_port, vlans, options, ignored):
+        def get_interface_config(
+            link_name, src_port, dst_node, dst_port, vlans, options, ignored
+        ):
             interface_config = {}
-            _type = 'switch-switch' if dst_port else 'switch-host'
+            _type = "switch-switch" if dst_port else "switch-host"
             if ignored:
                 # Link is to an outside network, so treat it as a output only link with more
                 #   specific options defined in the options dictionary
                 interface_config = {
-                    'name': f'{src_port}',
-                    'description': f'output only {link_name}',
+                    "name": "b%u" % src_port,
+                    "description": "output only %s" % link_name,
                 }
             elif isinstance(vlans, int):
                 # Untagged link
                 interface_config = {
-                    'name': f'b{src_port}',
-                    'description': f'untagged {link_name}',
-                    'native_vlan': self.vlan_name(vlans)
+                    "name": "b%u" % src_port,
+                    "description": "untagged %s" % link_name,
+                    "native_vlan": self.vlan_name(vlans),
                 }
             elif isinstance(vlans, list):
                 # Tagged link
                 interface_config = {
-                    'name': f'b{src_port}',
-                    'description': f'tagged {link_name}',
-                    'tagged_vlans': [self.vlan_name(vlan) for vlan in vlans]
+                    "name": "b%u" % src_port,
+                    "description": "tagged %s" % link_name,
+                    "tagged_vlans": [self.vlan_name(vlan) for vlan in vlans],
                 }
             elif dst_node and dst_port:
                 # Stack link
                 interface_config = {
-                    'name': f'b{src_port}',
-                    'description': f'stack {link_name}',
-                    'stack': {
-                        'dp': dst_node,
-                        'port': dst_port
-                    }
+                    "name": "b%u" % src_port,
+                    "description": "stack %s" % link_name,
+                    "stack": {"dp": dst_node, "port": dst_port},
                 }
             elif vlans is None:
                 # output only link or coprocessor, leave to more specific options to handle
                 interface_config = {
-                    'name': f'b{src_port}',
-                    'description': f'output only {link_name}',
+                    "name": "b%u" % src_port,
+                    "description": "output only %s" % link_name,
                 }
             else:
-                raise GenerationError(f'Unknown {_type} link type {vlans}')
+                raise GenerationError("Unknown %s link type %s" % (_type, vlans))
             if options:
                 for option_key, option_value in options.items():
                     interface_config[option_key] = option_value
             return interface_config
 
         def add_dp_config(src_node, dst_node, link_key, link_info, reverse=False):
             dp_config = dps_config[src_node]
             src_info, dst_info = self.nodeInfo(src_node), self.nodeInfo(dst_node)
-            vlans = link_info['config_vlans']
-            src_id = src_info['switch_n']
-            dp_config.setdefault('interfaces', {})
+            vlans = link_info["config_vlans"]
+            src_id = src_info["switch_n"]
+            dp_config.setdefault("interfaces", {})
             options = {}
             ignored = False
             if self.isSwitch(dst_node):
                 # Generate switch-switch config link
                 if reverse:
-                    src_port, dst_port = link_info['port2'], link_info['port1']
+                    src_port, dst_port = link_info["port2"], link_info["port1"]
                 else:
-                    src_port, dst_port = link_info['port1'], link_info['port2']
-                link_name = f'link #{link_key} to {dst_node}:{dst_port}'
+                    src_port, dst_port = link_info["port1"], link_info["port2"]
+                link_name = "link #%s to %s:%s" % (link_key, dst_node, dst_port)
                 options = {}
-                dst_id = dst_info['switch_n']
+                dst_id = dst_info["switch_n"]
                 if link_options and (src_id, dst_id) in link_options:
                     options.update(link_options[(src_id, dst_id)])
                 if dst_id in ignored_switches:
                     ignored = True
             else:
                 # Generate host-switch config link
-                src_port, dst_port = link_info['port1'], None
-                link_name = f'link #{link_key} to {dst_node}'
-                host_n = dst_info['host_n']
+                src_port, dst_port = link_info["port1"], None
+                link_name = "link #%s to %s" % (link_key, dst_node)
+                host_n = dst_info["host_n"]
                 if host_options and host_n in host_options:
                     options = host_options[host_n]
-            dp_config['interfaces'].setdefault(  # pytype: disable=attribute-error
+            dp_config["interfaces"].setdefault(  # pytype: disable=attribute-error
                 src_port,
                 get_interface_config(
-                    link_name, src_port, dst_node, dst_port, vlans, options, ignored))
+                    link_name, src_port, dst_node, dst_port, vlans, options, ignored
+                ),
+            )
 
         for links in self.links(withKeys=True, withInfo=True):
             src_node, dst_node, link_key, link_info = links
             src_info = self.nodeInfo(src_node)
             dst_info = self.nodeInfo(dst_node)
-            if self.isSwitch(src_node) and src_info['switch_n'] not in ignored_switches:
+            if self.isSwitch(src_node) and src_info["switch_n"] not in ignored_switches:
                 dps_config.setdefault(src_node, {})
-                src_dpid = self.dpids_by_id[src_info['switch_n']]
-                dps_config[src_node].setdefault('dp_id', int(src_dpid))
+                src_dpid = self.dpids_by_id[src_info["switch_n"]]
+                dps_config[src_node].setdefault("dp_id", int(src_dpid))
                 add_dp_config(src_node, dst_node, link_key, link_info)
-            if self.isSwitch(dst_node) and dst_info['switch_n'] not in ignored_switches:
+            if self.isSwitch(dst_node) and dst_info["switch_n"] not in ignored_switches:
                 dps_config.setdefault(dst_node, {})
-                dst_dpid = self.dpids_by_id[dst_info['switch_n']]
-                dps_config[dst_node].setdefault('dp_id', int(dst_dpid))
+                dst_dpid = self.dpids_by_id[dst_info["switch_n"]]
+                dps_config[dst_node].setdefault("dp_id", int(dst_dpid))
                 add_dp_config(dst_node, src_node, link_key, link_info, True)
         if dp_options:
             for dp_i, options in dp_options.items():
                 switch_name = self.switches_by_id[dp_i]
                 dps_config.setdefault(switch_name, {})
                 for option_key, option_value in options.items():
                     dps_config[switch_name][option_key] = option_value
@@ -506,17 +530,15 @@
         Args:
             n_vlans (int): Number of VLANs to generate
             vlan_options (dict): Additional options for each VLAN, keyed by vlan index
         """
         vlans_config = {}
         for vlan in range(n_vlans):
             vlan_name = self.vlan_name(vlan)
-            vlans_config[vlan_name] = {
-                'vid': FaucetTopoGenerator.vlan_vid(vlan)
-            }
+            vlans_config[vlan_name] = {"vid": FaucetTopoGenerator.vlan_vid(vlan)}
         if vlan_options:
             for vlan, options in vlan_options.items():
                 vlan_name = self.vlan_name(vlan)
                 for option_key, option_value in options.items():
                     vlans_config[vlan_name][option_key] = option_value
         return vlans_config
 
@@ -527,26 +549,37 @@
         Args:
             routers (dict): Router index to list of VLANs in the router
             router_options (dict): Additional options for each router, keyed by router index
         """
         routers_config = {}
         for router, vlans in routers.items():
             routers_config[self.router_name(router)] = {
-                'vlans': [self.vlan_name(vlan) for vlan in vlans]
+                "vlans": [self.vlan_name(vlan) for vlan in vlans]
             }
         if router_options:
             for router, options in router_options.items():
                 router_name = self.router_name(router)
                 for option_key, option_value in options.items():
                     routers_config[router_name][option_key] = option_value
         return routers_config
 
-    def get_config(self, n_vlans, acl_options=None, dp_options=None, host_options=None,
-                   link_options=None, vlan_options=None, routers=None, router_options=None,
-                   include=None, include_optional=None, ignored_switches=None):
+    def get_config(
+        self,
+        n_vlans,
+        acl_options=None,
+        dp_options=None,
+        host_options=None,
+        link_options=None,
+        vlan_options=None,
+        routers=None,
+        router_options=None,
+        include=None,
+        include_optional=None,
+        ignored_switches=None,
+    ):
         """
         Creates a Faucet YAML configuration file using the current topology
 
         Args:
             n_vlans (int): Number of VLANs to generate
             acl_options (dict): Acls in use in the Faucet configuration file
             dp_options (dict): Additional options for each DP, keyed by DP index
@@ -555,33 +588,34 @@
             vlan_options (dict): Additional options for each VLAN, keyed by vlan index
             routers (dict): Router index to list of VLANs in the router
             router_options (dict): Additional options for each router, keyed by router index
             include (list): Files to include using the the Faucet config 'include' key
             include_optional (list): File to include using the Faucet config 'include_optional' key
             ignored_switches (list): List of switches to not include for the configuration file
         """
-        config = {'version': 2}
+        config = {"version": 2}
         if include:
-            config['include'] = list(include)
+            config["include"] = list(include)
         if include_optional:
-            config['include-optional'] = list(include_optional)
+            config["include-optional"] = list(include_optional)
         if acl_options:
-            config['acls'] = self.get_acls_config(acl_options)
-        config['vlans'] = self.get_vlans_config(n_vlans, vlan_options)
+            config["acls"] = self.get_acls_config(acl_options)
+        config["vlans"] = self.get_vlans_config(n_vlans, vlan_options)
         if routers:
-            config['routers'] = self.get_routers_config(routers, router_options)
+            config["routers"] = self.get_routers_config(routers, router_options)
         if ignored_switches is None:
             ignored_switches = []
-        config['dps'] = self.get_dps_config(
-            dp_options, host_options, link_options, ignored_switches)
+        config["dps"] = self.get_dps_config(
+            dp_options, host_options, link_options, ignored_switches
+        )
         return yaml_dump(config)
 
 
 class FaucetFakeOFTopoGenerator(FaucetTopoGenerator):
     """Generates Faucet topologies for Unittests"""
 
     # NOTE: For now, we dont actually create the objects for the unittests
     #   so we can leave them as they are in the FaucetTopoGenerator function
     @staticmethod
     def dp_dpid(i):
         """DP DPID"""
-        return f'{i + 1}'
+        return "%u" % (i + 1)
```

### Comparing `c65faucet-1.0.49/clib/docker_host.py` & `c65faucet-1.0.50/clib/docker_host.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,16 +11,16 @@
 # pylint: disable=import-error
 from mininet.log import error, debug
 from mininet.node import Host
 from mininet.util import quietRun, errRun
 
 from clib.mininet_test_util import DEVNULL
 
-DEFAULT_NETWORK = 'none'
-DEFAULT_PREFIX = 'mininet'
+DEFAULT_NETWORK = "none"
+DEFAULT_PREFIX = "mininet"
 STARTUP_TIMEOUT_MS = 20000
 
 # pylint: disable=too-many-instance-attributes
 
 
 class DockerHost(Host):
     """Mininet host that encapsulates execution in a docker container"""
@@ -49,16 +49,26 @@
     startup_timeout_ms = None
     container = None
     pollIn = None
     active_log = None
     ps1 = chr(127)
 
     # pylint: disable=too-many-arguments
-    def __init__(self, name, image=None, tmpdir=None, prefix=None, env_vars=None, vol_maps=None,
-                 startup_timeout_ms=STARTUP_TIMEOUT_MS, network=None, **kwargs):
+    def __init__(
+        self,
+        name,
+        image=None,
+        tmpdir=None,
+        prefix=None,
+        env_vars=None,
+        vol_maps=None,
+        startup_timeout_ms=STARTUP_TIMEOUT_MS,
+        network=None,
+        **kwargs
+    ):
         self.image = image
         self.tmpdir = tmpdir
         self.prefix = prefix
         if env_vars is None:
             env_vars = []
         self.env_vars = env_vars
         if vol_maps is None:
@@ -68,87 +78,112 @@
         self.startup_timeout_ms = startup_timeout_ms
         self.name = name
         self.pullImage()
         Host.__init__(self, name, **kwargs)
 
     def pullImage(self):  # pylint: disable=invalid-name
         "Pull docker image if necessary"
-        if self.image not in quietRun('docker images'):
-            error(f'{self.name}: docker image', self.image,
-                  'not available locally - pulling\n')
-            _out, err, code = errRun('docker', 'pull', self.image)
+        if self.image not in quietRun("docker images"):
+            error(
+                "%s: docker image" % self.name,
+                self.image,
+                "not available locally - pulling\n",
+            )
+            _out, err, code = errRun("docker", "pull", self.image)
             if err or code:
-                error('docker pull failed with error', code, err, '\n')
+                error("docker pull failed with error", code, err, "\n")
 
     # pylint: disable=invalid-name
     def startShell(self, mnopts=None):
         """Start a shell process for running commands."""
         if self.shell:
-            error('shell is already running')
+            error("shell is already running")
             return
 
-        assert mnopts is None, 'mnopts not supported for DockerHost'
+        assert mnopts is None, "mnopts not supported for DockerHost"
 
-        self.container = f'{self.prefix}-{self.name}'
+        self.container = "%s-%s" % (self.prefix, self.name)
 
-        debug(f'Starting container {self.container} with image "{self.image}".')
+        debug('Starting container %s with image "%s".' % (self.container, self.image))
 
         self.kill(purge=True)
 
-        container_tmp_dir = os.path.join(os.path.abspath(self.tmpdir), 'tmp')
-        tmp_volume = container_tmp_dir + ':/tmp'
+        container_tmp_dir = os.path.join(os.path.abspath(self.tmpdir), "tmp")
+        tmp_volume = container_tmp_dir + ":/tmp"
 
-        base_cmd = ["docker", "run", "-ti", "--privileged", "--entrypoint", "env",
-                    "-h", self.name, "--name", self.container]
-        opt_args = [f'--net={self.network}']
-        env_vars = self.env_vars + ["TERM=dumb", f"PS1={self.ps1}"]
-        env_args = reduce(operator.add, (['--env', var] for var in env_vars), [])
-        vol_args = reduce(operator.add, (['-v', var] for var in self.vol_maps), ['-v', tmp_volume])
+        base_cmd = [
+            "docker",
+            "run",
+            "-ti",
+            "--privileged",
+            "--entrypoint",
+            "env",
+            "-h",
+            self.name,
+            "--name",
+            self.container,
+        ]
+        opt_args = ["--net=%s" % self.network]
+        env_vars = self.env_vars + ["TERM=dumb", "PS1=%s" % self.ps1]
+        env_args = reduce(operator.add, (["--env", var] for var in env_vars), [])
+        vol_args = reduce(
+            operator.add, (["-v", var] for var in self.vol_maps), ["-v", tmp_volume]
+        )
         image_args = [self.image, "bash", "--norc", "-is", "mininet:" + self.name]
         cmd = base_cmd + opt_args + env_args + vol_args + image_args
         self.master, self.slave = pty.openpty()
-        debug(f'docker command {" ".join(cmd)}, fd {self.master}, fd {self.slave}')
+        debug(
+            'docker command "%s", fd %d, fd %d'
+            % (" ".join(cmd), self.master, self.slave)
+        )
         try:
-            self.shell = self._popen(cmd, stdin=self.slave, stdout=self.slave, stderr=self.slave)
-            self.stdin = os.fdopen(self.master, 'r')
+            self.shell = self._popen(
+                cmd, stdin=self.slave, stdout=self.slave, stderr=self.slave
+            )
+            self.stdin = os.fdopen(self.master, "r")
             self.stdout = self.stdin
             self.pollOut = select.poll()  # pylint: disable=invalid-name
             self.pollOut.register(self.stdout)  # pylint: disable=no-member
             self.outToNode[self.stdout.fileno()] = self  # pylint: disable=no-member
             self.pollIn = select.poll()  # pylint: disable=invalid-name
-            self.pollIn.register(self.stdout, select.POLLIN)  # pylint: disable=no-member
+            self.pollIn.register(
+                self.stdout, select.POLLIN
+            )  # pylint: disable=no-member
             self.inToNode[self.stdin.fileno()] = self  # pylint: disable=no-member
             self.execed = False
             self.lastCmd = None  # pylint: disable=invalid-name
             self.lastPid = None  # pylint: disable=invalid-name
-            self.readbuf = ''
+            self.readbuf = ""
             self.waiting = True
-            data = ''
+            data = ""
             while True:
                 data = self.read(1)
                 if data[-1] == self.ps1:
                     break
-            self.readbuf = ''
+            self.readbuf = ""
             self.waiting = False
         except Exception:
-            error(f'docker cmd: {" ".join(cmd)}')
+            error("docker cmd: %s" % " ".join(cmd))
             if self.shell.returncode:
-                error(f'returncode: {self.shell.returncode}')
+                error("returncode: %d" % self.shell.returncode)
             if self.shell:
                 self.shell.poll()
             raise
 
         self.pid = self.inspect_pid()
-        debug(f"Container {self.container} created pid {self.pid}/{self.shell.pid}.")
+        debug(
+            "Container %s created pid %s/%s."
+            % (self.container, self.pid, self.shell.pid)
+        )
 
-        self.cmd('unset HISTFILE; stty -echo; set +m')  # pylint: disable=no-member
+        self.cmd("unset HISTFILE; stty -echo; set +m")  # pylint: disable=no-member
 
     def kill(self, purge=False):
         """Kill a container."""
-        debug(f'killing container {self.container}.')
+        debug("killing container %s." % self.container)
         if purge:
             kill_cmd = ["docker", "rm", "-f", self.container]
         else:
             kill_cmd = ["docker", "kill", self.container]
         kill_pipe = None
         try:
             kill_pipe = self._popen(kill_cmd, stdin=DEVNULL, stdout=PIPE, stderr=STDOUT)
@@ -172,74 +207,87 @@
             if pid_pipe is not None:
                 pid_pipe.poll()
             raise
 
     def open_log(self):
         """Open a log file for writing and return it."""
         # pylint: disable=consider-using-with
-        return open(os.path.join(self.tmpdir, 'activate.log'), 'w')
+        return open(os.path.join(self.tmpdir, "activate.log"), "w")
 
     def activate(self):
         """Activate a container and return STDOUT to it."""
-        assert not self.active_pipe, f'container {self.container} already activated'
-        debug(f'activating container {self.container}.')
+        assert not self.active_pipe, "container %s already activated" % self.container
+        debug("activating container %s." % self.container)
         inspect_cmd = ["docker", "inspect", "--format={{json .Config}}", self.image]
         inspect_pipe = None
         try:
-            inspect_pipe = self._popen(inspect_cmd, stdin=DEVNULL, stdout=PIPE, stderr=STDOUT)
+            inspect_pipe = self._popen(
+                inspect_cmd, stdin=DEVNULL, stdout=PIPE, stderr=STDOUT
+            )
             config_json = inspect_pipe.stdout.readlines()
             inspect_pipe.stdout.close()
-            assert len(config_json) == 1, f"Expected 1 config line, found {len(config_json)}"
+            assert len(config_json) == 1, "Expected 1 config line, found %s" % len(
+                config_json
+            )
             config = json.loads(config_json[0].decode())
-            entryconfig = config['Entrypoint']
-            entrypoint = entryconfig if entryconfig else ['/usr/bin/env']
-            cmd = config['Cmd'] if 'Cmd' in config else []
+            entryconfig = config["Entrypoint"]
+            entrypoint = entryconfig if entryconfig else ["/usr/bin/env"]
+            cmd = config["Cmd"] if "Cmd" in config else []
             docker_cmd = entrypoint + (cmd if cmd else [])
-            debug(f'logging to activate.log for {docker_cmd}')
+            debug("logging to activate.log for %s" % docker_cmd)
             stdout = self.open_log()
             self.active_log = stdout
         except Exception:
             if inspect_pipe:
                 inspect_pipe.poll()
             raise
         self.active_pipe_returncode = None
-        self.active_pipe = self.popen(docker_cmd, stdin=DEVNULL, stdout=stdout, stderr=STDOUT)
+        self.active_pipe = self.popen(
+            docker_cmd, stdin=DEVNULL, stdout=stdout, stderr=STDOUT
+        )
         pipe_out = self.active_pipe.stdout
         out_fd = pipe_out.fileno() if pipe_out else None
-        debug(f'Active_pipe container {self.container} pid {self.active_pipe.pid} fd {out_fd}')
+        debug(
+            "Active_pipe container %s pid %s fd %s"
+            % (self.container, self.active_pipe.pid, out_fd)
+        )
         return self.active_pipe
 
     def wait(self):
         """Wait for an activated container to terminate."""
         try:
             if self.active_pipe_returncode is not None:
                 return self.active_pipe_returncode
-            debug(f'Waiting for container {self.container}.')
+            debug("Waiting for container %s." % self.container)
             assert self.active_pipe, "container not activated"
             self.active_pipe.communicate()
             self.active_pipe.returncode = self.active_pipe.wait()
             self.terminate()
             return self.active_pipe_returncode
         except Exception as err:
-            error(f'Exception waiting for {self.container}: {err}')
+            error("Exception waiting for %s: %s" % (self.container, err))
             self.terminate()
             raise
 
     def read(self, size=1024):
         """Read from an activated container."""
         poll_results = self.pollIn.poll(self.startup_timeout_ms)
         data_ready = poll_results and (poll_results[0][1] & select.POLLIN)
-        assert data_ready, (
-            'Timeout waiting for read data on %d after %ds' %
-            (self.stdout.fileno(), self.startup_timeout_ms / 1e3))
+        assert data_ready, "Timeout waiting for read data on %d after %ds" % (
+            self.stdout.fileno(),
+            self.startup_timeout_ms / 1e3,
+        )
         return Host.read(self, size)
 
     def terminate(self):
         """Override Mininet terminate() to partially avoid pty leak."""
-        debug(f'Terminating container {self.container}, shell {self.shell}, pipe {self.active_pipe}')
+        debug(
+            "Terminating container %s, shell %s, pipe %s"
+            % (self.container, self.shell, self.active_pipe)
+        )
         if self.slave:
             os.close(self.slave)
             self.slave = None
         if self.shell is not None:
             self.stdin.close()
             self.stdin = None
             self.master = None
@@ -260,55 +308,57 @@
                 self.active_log.close()
                 self.active_log = None
         self.cleanup()  # pylint: disable=no-member
         return self.active_pipe_returncode
 
     def popen(self, *args, **kwargs):
         """Return a Popen() object in node's namespace
-           args: Popen() args, single list, or string
-           kwargs: Popen() keyword args"""
+        args: Popen() args, single list, or string
+        kwargs: Popen() keyword args"""
         # -t is necessary to prevent docker from buffering output. It might cause
         # problems with some commands like shells that then assume they can output
         # all sorts of crazy control characters b/c it's a terminal.
-        mncmd = ['docker', 'exec', '--env', 'TERM=dumb', '-t', self.container]
+        mncmd = ["docker", "exec", "--env", "TERM=dumb", "-t", self.container]
         pipe = Host.popen(self, mncmd=mncmd, *args, **kwargs)
         if pipe:
-            debug(f'docker pid {pipe.pid}: {mncmd} {args} {kwargs}')
+            debug("docker pid %d: %s %s %s" % (pipe.pid, mncmd, args, kwargs))
         return pipe
 
     def _popen(self, cmd, **params):
         # Docker is different than mininet in that it doesn't handle signals like
         # a normal interactive terminal would. So, put it in a separate process group
         # so it doesn't receive stray SIGINTs, rather relying on the message sent
         # from the owning process through the pty.
-        if 'preexec_fn' not in params:
-            params['preexec_fn'] = os.setpgrp
+        if "preexec_fn" not in params:
+            params["preexec_fn"] = os.setpgrp
         pipe = super()._popen(cmd, **params)
         if pipe:
             stdout = pipe.stdout
             out_fd = pipe.stdout.fileno() if stdout else None
-            debug(f'docker pid {pipe.pid}: {cmd}, fd {out_fd}')
+            debug("docker pid %d: %s, fd %s" % (pipe.pid, cmd, out_fd))
         return pipe
 
 
-def make_docker_host(image, prefix=DEFAULT_PREFIX, network=DEFAULT_NETWORK,
-                     startup_timeout_ms=None):
+def make_docker_host(
+    image, prefix=DEFAULT_PREFIX, network=DEFAULT_NETWORK, startup_timeout_ms=None
+):
     """Utility function to create a docker-host class that can be passed to mininet"""
 
     class _ImageHost(DockerHost):
         """Internal class that represents a docker image host"""
+
         def __init__(self, *args, **kwargs):
             host_name = args[0]
-            kwargs['image'] = image
-            assert kwargs['tmpdir'], 'tmpdir required for docker host'
-            kwargs['tmpdir'] = os.path.join(kwargs['tmpdir'], host_name)
-            kwargs['prefix'] = prefix
-            kwargs['network'] = network
+            kwargs["image"] = image
+            assert kwargs["tmpdir"], "tmpdir required for docker host"
+            kwargs["tmpdir"] = os.path.join(kwargs["tmpdir"], host_name)
+            kwargs["prefix"] = prefix
+            kwargs["network"] = network
             if startup_timeout_ms:
-                kwargs['startup_timeout_ms'] = startup_timeout_ms
-            elif 'DOCKER_STARTUP_TIMEOUT_MS' in os.environ:
-                env_val = os.environ['DOCKER_STARTUP_TIMEOUT_MS']
+                kwargs["startup_timeout_ms"] = startup_timeout_ms
+            elif "DOCKER_STARTUP_TIMEOUT_MS" in os.environ:
+                env_val = os.environ["DOCKER_STARTUP_TIMEOUT_MS"]
                 if env_val:
-                    kwargs['startup_timeout_ms'] = int(env_val)
+                    kwargs["startup_timeout_ms"] = int(env_val)
             super().__init__(*args, **kwargs)
 
     return _ImageHost
```

### Comparing `c65faucet-1.0.49/clib/fakeoftable.py` & `c65faucet-1.0.50/clib/fakeoftable.py`

 * *Files 4% similar despite different names*

```diff
@@ -129,17 +129,17 @@
 
     def apply_ofmsgs(self, dp_id, ofmsgs, ignore_errors=False):
         """Applies ofmsgs to a FakeOFTable for DP ID"""
         self.tables[dp_id].apply_ofmsgs(ofmsgs, ignore_errors=ignore_errors)
 
     def print_table(self, dp_id):
         """Prints the table in string format to STDERR"""
-        sys.stderr.write('TABLE %x' % dp_id)
-        sys.stderr.write(str(self.tables[dp_id]) + '\n')
-        sys.stderr.write('======================\n\n')
+        sys.stderr.write("TABLE %x" % dp_id)
+        sys.stderr.write(str(self.tables[dp_id]) + "\n")
+        sys.stderr.write("======================\n\n")
 
     def shortest_path_len(self, src_dpid, dst_dpid):
         """Returns the length of the shortest path from the source to the destination"""
         if src_dpid == dst_dpid:
             return 1
         src_valve = self.valves_manager.valves[src_dpid]
         dst_valve = self.valves_manager.valves[dst_dpid]
@@ -181,51 +181,53 @@
             pkt = dict(pkt)
             if dp_id == dst_dpid:
                 # A packet has reached the destination, so test for the output
                 found = self.tables[dp_id].is_full_output(pkt, port, vid, trace=trace)
                 if not found and trace:
                     # A packet on the destination DP is not output in the expected state so
                     #   continue searching (flood reflection)
-                    sys.stderr.write('Output is away from destination\n')
+                    sys.stderr.write("Output is away from destination\n")
             if not found:
                 # Packet not reached destination, so continue traversing
                 if trace:
-                    sys.stderr.write(f'FakeOFTable {dp_id}: {pkt}\n')
+                    sys.stderr.write("FakeOFTable %s: %s\n" % (dp_id, pkt))
                 port_outputs = self.tables[dp_id].get_port_outputs(pkt, trace=trace)
                 valve = self.valves_manager.valves[dp_id]
                 for out_port, out_pkts in port_outputs.items():
                     if out_port == IN_PORT:
                         # Rebind output to the packet in_port value
-                        out_port = pkt['in_port']
+                        out_port = pkt["in_port"]
                     if out_port not in valve.dp.ports:
                         # Ignore output to improper ports & controller
                         # TODO: Here we should actually send the packet to the
                         #   controller, and maybe install necessary rules to
                         #   help testing routing implementations
                         continue
                     for out_pkt in out_pkts:
                         port_obj = valve.dp.ports[out_port]
                         if port_obj.stack:
                             # Need to continue traversing through the FakeOFNetwork
-                            adj_port = port_obj.stack['port']
-                            adj_dpid = port_obj.stack['dp'].dp_id
+                            adj_port = port_obj.stack["port"]
+                            adj_dpid = port_obj.stack["dp"].dp_id
                             new_pkt = out_pkt.copy()
-                            new_pkt['in_port'] = adj_port.number
+                            new_pkt["in_port"] = adj_port.number
                             if not dfs.has_visited(adj_dpid, new_pkt):
                                 # Add packet to the heap if we have not visited the node with
                                 #   this packet before
                                 priority = self.shortest_path_len(adj_dpid, dst_dpid)
                                 dfs.push(adj_dpid, new_pkt, priority)
                                 dfs.visit(adj_dpid, new_pkt)
                         elif trace:
                             # Output to non-stack port, can ignore this output
                             sys.stderr.write(
-                                f'Ignoring non-stack output {valve.dp.name}:{out_port}\n')
+                                "Ignoring non-stack output %s:%s\n"
+                                % (valve.dp.name, out_port)
+                            )
             if trace:
-                sys.stderr.write('\n')
+                sys.stderr.write("\n")
         return found
 
     def table_state(self, dp_id):
         """Return tuple of table hash & table str"""
         return self.tables[dp_id].table_state()
 
     def hash_table(self, dp_id):
@@ -265,49 +267,48 @@
                 self.groups = {}
                 return
             if group_id in self.groups:
                 del self.groups[group_id]
 
         def _add(ofmsg, group_id):
             if group_id in self.groups:
-                raise FakeOFTableException(
-                    f'group already in group table: {ofmsg}')
+                raise FakeOFTableException("group already in group table: %s" % ofmsg)
             self.groups[group_id] = ofmsg
 
         def _modify(ofmsg, group_id):
             if group_id not in self.groups:
-                raise FakeOFTableException(
-                    f'group not in group table: {ofmsg}')
+                raise FakeOFTableException("group not in group table: %s" % ofmsg)
             self.groups[group_id] = ofmsg
 
         _groupmod_handlers = {
             ofp.OFPGC_DELETE: _del,
             ofp.OFPGC_ADD: _add,
             ofp.OFPGC_MODIFY: _modify,
         }
 
         _groupmod_handlers[ofmsg.command](ofmsg, ofmsg.group_id)
 
     def _apply_flowmod(self, ofmsg):
         """Adds, Deletes and modify flow modification messages are applied
-           according to section 6.4 of the OpenFlow 1.3 specification."""
+        according to section 6.4 of the OpenFlow 1.3 specification."""
 
         def _validate_flowmod_tfm(table_id, tfm_body, ofmsg):
             if not self.requires_tfm:
                 return
 
             if table_id == ofp.OFPTT_ALL:
                 if ofmsg.match.items() and not self.tfm:
                     raise FakeOFTableException(
-                        f'got {ofmsg} with matches before TFM that defines tables')
+                        "got %s with matches before TFM that defines tables" % ofmsg
+                    )
                 return
 
             if tfm_body is None:
                 raise FakeOFTableException(
-                    f'got {ofmsg} before TFM that defines table {table_id}'
+                    "got %s before TFM that defines table %u" % (ofmsg, table_id)
                 )
 
         def _add(table, flowmod):
             # From the 1.3 spec, section 6.4:
             # For add requests (OFPFC_ADD) with the
             # OFPFF_CHECK_OVERLAP flag set, the switch must first
             # check for any overlapping flow entries in the
@@ -326,15 +327,17 @@
             # obviously unnacceptable so we will assume this is
             # always set
             for fte in table:
                 if flowmod.fte_matches(fte, strict=True):
                     table.remove(fte)
                     break
                 if flowmod.overlaps(fte):
-                    raise FakeOFTableException(f'Overlapping flowmods {flowmod} and {fte}')
+                    raise FakeOFTableException(
+                        "Overlapping flowmods {} and {}".format(flowmod, fte)
+                    )
             table.append(flowmod)
 
         def _del(table, flowmod):
             removals = [fte for fte in table if flowmod.fte_matches(fte)]
             for fte in removals:
                 table.remove(fte)
 
@@ -377,18 +380,24 @@
         for table in tables:
             _flowmod_handlers[ofmsg.command](table, flowmod)
 
         if tfm_body:
             for table in tables:
                 entries = len(table)
                 if entries > tfm_body.max_entries:
-                    tfm_table_details = f'self.dp_id: table {table_id} {tfm_body.name} ' \
-                                        f'full ({entries}/{tfm_body.max_entries})'
-                    flow_dump = '\n\n'.join(
-                        (tfm_table_details, str(ofmsg), str(tfm_body)))
+                    tfm_table_details = "%s : table %u %s full (%u/%u)" % (
+                        self.dp_id,
+                        table_id,
+                        tfm_body.name,
+                        entries,
+                        tfm_body.max_entries,
+                    )
+                    flow_dump = "\n\n".join(
+                        (tfm_table_details, str(ofmsg), str(tfm_body))
+                    )
                     raise FakeOFTableException(flow_dump)
 
     def _apply_tfm(self, ofmsg):
         self.tfm = {body.table_id: body for body in ofmsg.body}
 
     def _apply_flowstats(self, ofmsg):
         """Update state of flow tables to match an OFPFlowStatsReply message.
@@ -433,15 +442,15 @@
                     self._apply_flowstats(ofmsg)
                     self.sort_tables()
                     continue
             except FakeOFTableException:
                 if not ignore_errors:
                     raise
             if not ignore_errors:
-                raise FakeOFTableException('Unsupported flow %s' % str(ofmsg))
+                raise FakeOFTableException("Unsupported flow %s" % str(ofmsg))
 
     def single_table_lookup(self, match, table_id, trace=False):
         """
         Searches through a single table with `table_id` for entries
         that will be applied to the packet with fields represented by match
         Args:
             match (dict): A dictionary keyed by header field names with values
@@ -455,15 +464,15 @@
         matching_fte = None
         # Find matching flowmods
         for fte in table:
             if fte.pkt_matches(packet_dict):
                 matching_fte = fte
                 break
         if trace:
-            sys.stderr.write(f'{table_id}: {matching_fte}\n')
+            sys.stderr.write("%s: %s\n" % (table_id, matching_fte))
         return matching_fte
 
     def _process_instruction(self, match, instruction):
         """
         Process an instructions actions into an output dictionary
         Args:
             match (dict): A dictionary keyed by header field names with values
@@ -485,39 +494,46 @@
 
             pending_actions.append(action)
 
             if action.type == ofp.OFPAT_SET_FIELD:
                 # Set field, modify a packet header
                 packet_dict[action.key] = action.value
             elif action.type == ofp.OFPAT_PUSH_VLAN:
-                if 'vlan_vid' in packet_dict and packet_dict['vlan_vid'] & ofp.OFPVID_PRESENT:
+                if (
+                    "vlan_vid" in packet_dict
+                    and packet_dict["vlan_vid"] & ofp.OFPVID_PRESENT
+                ):
                     # Pushing on another tag, so create another
                     #   field for the encapsulated VID
-                    packet_dict['encap_vid'] = packet_dict['vlan_vid']
+                    packet_dict["encap_vid"] = packet_dict["vlan_vid"]
                 # Push the VLAN header to the packet
-                packet_dict['vlan_vid'] = ofp.OFPVID_PRESENT
+                packet_dict["vlan_vid"] = ofp.OFPVID_PRESENT
             elif action.type == ofp.OFPAT_POP_VLAN:
                 # Remove VLAN header from the packet
-                packet_dict.pop('vlan_vid')
-                if 'vlan_pcp' in packet_dict:
+                packet_dict.pop("vlan_vid")
+                if "vlan_pcp" in packet_dict:
                     # Also make sure to pop off any VLAN header information too
-                    packet_dict.pop('vlan_pcp')
-                if 'encap_vid' in packet_dict:
+                    packet_dict.pop("vlan_pcp")
+                if "encap_vid" in packet_dict:
                     # Move the encapsulated VID to the front
-                    packet_dict['vlan_vid'] = packet_dict['encap_vid']
-                    packet_dict.pop('encap_vid')
+                    packet_dict["vlan_vid"] = packet_dict["encap_vid"]
+                    packet_dict.pop("encap_vid")
                 else:
-                    packet_dict['vlan_vid'] = 0
+                    packet_dict["vlan_vid"] = 0
             elif action.type == ofp.OFPAT_GROUP:
                 # Group mod so make sure that we process the group buckets
                 if action.group_id not in self.groups:
-                    raise FakeOFTableException(f'output group not in group table: {action}')
+                    raise FakeOFTableException(
+                        "output group not in group table: %s" % action
+                    )
                 buckets = self.groups[action.group_id].buckets
                 for bucket in buckets:
-                    bucket_outputs, _, _ = self._process_instruction(packet_dict, bucket)
+                    bucket_outputs, _, _ = self._process_instruction(
+                        packet_dict, bucket
+                    )
                     for out_port, out_pkts in bucket_outputs.items():
                         outputs.setdefault(out_port, [])
                         outputs[out_port].extend(out_pkts)
                         pending_actions = []
 
         return outputs, packet_dict, pending_actions
 
@@ -541,34 +557,41 @@
         pending_actions = []
         if matching_fte:
             for instruction in matching_fte.instructions:
                 if instruction.type == ofp.OFPIT_GOTO_TABLE:
                     if table_id < instruction.table_id:
                         next_table = instruction.table_id
                     else:
-                        raise FakeOFTableException('goto to lower table ID')
+                        raise FakeOFTableException("goto to lower table ID")
                 elif instruction.type == ofp.OFPIT_APPLY_ACTIONS:
                     if not instruction.actions:
-                        raise FakeOFTableException('no-op instruction actions')
-                    instruction_outputs, packet_dict, pending_actions = self._process_instruction(
-                        packet_dict, instruction)
+                        raise FakeOFTableException("no-op instruction actions")
+                    (
+                        instruction_outputs,
+                        packet_dict,
+                        pending_actions,
+                    ) = self._process_instruction(packet_dict, instruction)
                     for out_port, out_pkts in instruction_outputs.items():
                         outputs.setdefault(out_port, [])
                         outputs[out_port].extend(out_pkts)
                 elif instruction.type == ofp.OFPIT_WRITE_METADATA:
-                    metadata = packet_dict.get('metadata', 0)
+                    metadata = packet_dict.get("metadata", 0)
                     mask = instruction.metadata_mask
                     mask_compl = mask ^ 0xFFFFFFFFFFFFFFFF
-                    packet_dict['metadata'] = (metadata & mask_compl)\
-                        | (instruction.metadata & mask)
+                    packet_dict["metadata"] = (metadata & mask_compl) | (
+                        instruction.metadata & mask
+                    )
         if next_table:
             pending_actions = []
         if pending_actions:
-            raise FakeOFTableException(f'flow performs actions on packet after \
-                                       output with no goto: {matching_fte}')
+            raise FakeOFTableException(
+                "flow performs actions on packet after \
+                                       output with no goto: %s"
+                % matching_fte
+            )
         return outputs, packet_dict, next_table
 
     def get_output(self, match, trace=False):
         """
         Get all of the outputs of the tables with the output packets
         for each table in the FakeOFTable that match progresses through
         Args:
@@ -581,15 +604,16 @@
         table_outputs = {}
         table_id = 0
         next_table = True
         packet_dict = match.copy()
         while next_table:
             next_table = False
             outputs, packet_dict, next_table_id = self.get_table_output(
-                packet_dict, table_id, trace)
+                packet_dict, table_id, trace
+            )
             table_outputs[table_id] = outputs
             next_table = next_table_id is not None
             table_id = next_table_id
         return table_outputs
 
     def get_port_outputs(self, match, trace=False):
         """
@@ -604,15 +628,16 @@
         port_outputs = {}
         table_id = 0
         next_table = True
         packet_dict = match.copy()
         while next_table:
             next_table = False
             outputs, packet_dict, next_table_id = self.get_table_output(
-                packet_dict, table_id, trace)
+                packet_dict, table_id, trace
+            )
             for out_port, out_pkts in outputs.items():
                 port_outputs.setdefault(out_port, [])
                 # Remove duplicate entries from the list
                 for out_pkt in out_pkts:
                     if out_pkt not in port_outputs[out_port]:
                         port_outputs[out_port].append(out_pkt)
             next_table = next_table_id is not None
@@ -632,40 +657,44 @@
             vid: The expected output vid
             trace (bool): Print the trace of traversing the tables
         Returns:
             true if packets with match fields is output to port with correct VLAN
         """
         table_outputs = self.get_output(match, trace)
         if trace:
-            sys.stderr.write(pprint.pformat(table_outputs) + '\n')
-        in_port = match.get('in_port')
+            sys.stderr.write(pprint.pformat(table_outputs) + "\n")
+        in_port = match.get("in_port")
         for table_outputs in table_outputs.values():
             for out_port, out_pkts in table_outputs.items():
                 for out_pkt in out_pkts:
-                    if port == out_port and port == out_pkt['in_port']:
+                    if port == out_port and port == out_pkt["in_port"]:
                         continue
                     if port is None:
                         # Port is None & outputting so return true
                         return True
                     if vid is None:
                         # Vid is None, return true if output to specified port
                         if port == out_port:
                             return True
                         if out_port == ofp.OFPP_IN_PORT and port == in_port:
                             # In some cases we want to match to specifically ofp.OFPP_IN_PORT
                             #   otherwise we treat ofp.OFPP_IN_PORT as the match in_port
                             return True
-                    if port == out_port or (out_port == ofp.OFPP_IN_PORT and port == in_port):
+                    if port == out_port or (
+                        out_port == ofp.OFPP_IN_PORT and port == in_port
+                    ):
                         # Matching port, so check matching VID
                         if vid & ofp.OFPVID_PRESENT == 0:
                             # If OFPVID_PRESENT bit is 0 then packet should not have a VLAN tag
-                            return ('vlan_vid' not in out_pkt
-                                    or out_pkt['vlan_vid'] & ofp.OFPVID_PRESENT == 0)
+                            return (
+                                "vlan_vid" not in out_pkt
+                                or out_pkt["vlan_vid"] & ofp.OFPVID_PRESENT == 0
+                            )
                         # VID specified, check if matching expected
-                        return 'vlan_vid' in out_pkt and vid == out_pkt['vlan_vid']
+                        return "vlan_vid" in out_pkt and vid == out_pkt["vlan_vid"]
         return False
 
     def lookup(self, match, trace=False):
         """Return the entries from flowmods that matches match.
 
         Searches each table in the pipeline for the entries that will be
         applied to the packet with fields represented by match.
@@ -690,32 +719,33 @@
             for fte in table:
                 if fte.pkt_matches(packet_dict):
                     matching_fte = fte
                     break
             # if a flowmod is found, make modifications to the match values and
             # determine if another lookup is necessary
             if trace:
-                sys.stderr.write(f'{table_id}: {matching_fte}\n')
+                sys.stderr.write("%d: %s\n" % (table_id, matching_fte))
             if matching_fte:
                 for instruction in matching_fte.instructions:
                     instructions.append(instruction)
                     if instruction.type == ofp.OFPIT_GOTO_TABLE:
                         if table_id < instruction.table_id:
                             table_id = instruction.table_id
                             goto_table = True
                     elif instruction.type == ofp.OFPIT_APPLY_ACTIONS:
                         for action in instruction.actions:
                             if action.type == ofp.OFPAT_SET_FIELD:
                                 packet_dict[action.key] = action.value
                     elif instruction.type == ofp.OFPIT_WRITE_METADATA:
-                        metadata = packet_dict.get('metadata', 0)
+                        metadata = packet_dict.get("metadata", 0)
                         mask = instruction.metadata_mask
                         mask_compl = mask ^ 0xFFFFFFFFFFFFFFFF
-                        packet_dict['metadata'] = (metadata & mask_compl)\
-                            | (instruction.metadata & mask)
+                        packet_dict["metadata"] = (metadata & mask_compl) | (
+                            instruction.metadata & mask
+                        )
         return (instructions, packet_dict)
 
     def flow_count(self):
         """Return number of flow tables rules"""
         return sum(map(len, self.tables))
 
     def is_output(self, match, port=None, vid=None, trace=False):
@@ -736,15 +766,15 @@
         """
 
         full_output = self.is_full_output(match.copy(), port, vid, trace)
 
         def _output_result(action, vid_stack, port, vid):
             if port is None:
                 return True
-            in_port = match.get('in_port')
+            in_port = match.get("in_port")
             result = None
             if action.port == port:
                 if port == in_port:
                     result = None
                 elif vid is None:
                     result = True
                 elif vid & ofp.OFPVID_PRESENT == 0:
@@ -757,61 +787,67 @@
 
         def _process_vid_stack(action, vid_stack):
             if action.type == ofp.OFPAT_PUSH_VLAN:
                 vid_stack.append(ofp.OFPVID_PRESENT)
             elif action.type == ofp.OFPAT_POP_VLAN:
                 vid_stack.pop()
             elif action.type == ofp.OFPAT_SET_FIELD:
-                if action.key == 'vlan_vid':
+                if action.key == "vlan_vid":
                     vid_stack[-1] = action.value
             return vid_stack
 
         if trace:
             sys.stderr.write(
-                f'tracing packet flow {match} matching to port {port}, vid {vid}\n')
+                "tracing packet flow %s matching to port %s, vid %s\n"
+                % (match, port, vid)
+            )
 
         # vid_stack represents the packet's vlan stack, innermost label listed
         # first
-        match_vid = match.get('vlan_vid', 0)
+        match_vid = match.get("vlan_vid", 0)
         vid_stack = []
         if match_vid & ofp.OFPVID_PRESENT != 0:
             vid_stack.append(match_vid)
         instructions, _ = self.lookup(match, trace=trace)
 
         for instruction in instructions:
             if instruction.type != ofp.OFPIT_APPLY_ACTIONS:
                 continue
             for action in instruction.actions:
                 vid_stack = _process_vid_stack(action, vid_stack)
                 if action.type == ofp.OFPAT_OUTPUT:
-                    output_result = _output_result(
-                        action, vid_stack, port, vid)
+                    output_result = _output_result(action, vid_stack, port, vid)
                     if output_result is not None:
                         if output_result != full_output:
-                            raise FakeOFTableException('Output functions do not match')
+                            raise FakeOFTableException("Output functions do not match")
                         return output_result
                 elif action.type == ofp.OFPAT_GROUP:
                     if action.group_id not in self.groups:
                         raise FakeOFTableException(
-                            f'output group not in group table: {action}')
+                            "output group not in group table: %s" % action
+                        )
                     buckets = self.groups[action.group_id].buckets
                     for bucket in buckets:
                         bucket_vid_stack = vid_stack
                         for bucket_action in bucket.actions:
                             bucket_vid_stack = _process_vid_stack(
-                                bucket_action, bucket_vid_stack)
+                                bucket_action, bucket_vid_stack
+                            )
                             if bucket_action.type == ofp.OFPAT_OUTPUT:
                                 output_result = _output_result(
-                                    bucket_action, vid_stack, port, vid)
+                                    bucket_action, vid_stack, port, vid
+                                )
                                 if output_result is not None:
                                     if output_result != full_output:
-                                        raise FakeOFTableException('Output functions do not match')
+                                        raise FakeOFTableException(
+                                            "Output functions do not match"
+                                        )
                                     return output_result
         if full_output is not False:
-            raise FakeOFTableException('Output functions do not match')
+            raise FakeOFTableException("Output functions do not match")
         return False
 
     def apply_instructions_to_packet(self, match):
         """
         Send packet through the fake OF table pipeline
         Args:
             match (dict): A dict keyed by header fields with values, represents
@@ -820,50 +856,57 @@
             dict: Modified match dict, represents packet that has been through
                 the pipeline with values possibly altered
         """
         _, packet_dict = self.lookup(match)
         return packet_dict
 
     def __str__(self):
-        string = ''
+        string = ""
         for table_id, table in enumerate(self.tables):
-            string += f'\n----- Table {table_id} -----\n'
-            string += '\n'.join(sorted([str(flowmod) for flowmod in table]))
+            string += "\n----- Table %u -----\n" % (table_id)
+            string += "\n".join(sorted([str(flowmod) for flowmod in table]))
         return string
 
     def sort_tables(self):
         """Sort flows in tables by priority order."""
         self.tables = [sorted(table, reverse=True) for table in self.tables]
 
 
 class FlowMod:
     """Represents a flow modification message and its corresponding entry in
     the flow table.
     """
+
     MAC_MATCH_FIELDS = (
-        'eth_src', 'eth_dst', 'arp_sha', 'arp_tha', 'ipv6_nd_sll',
-        'ipv6_nd_tll'
+        "eth_src",
+        "eth_dst",
+        "arp_sha",
+        "arp_tha",
+        "ipv6_nd_sll",
+        "ipv6_nd_tll",
     )
-    IPV4_MATCH_FIELDS = ('ipv4_src', 'ipv4_dst', 'arp_spa', 'arp_tpa')
-    IPV6_MATCH_FIELDS = ('ipv6_src', 'ipv6_dst', 'ipv6_nd_target')
-    HEX_FIELDS = ('eth_type')
+    IPV4_MATCH_FIELDS = ("ipv4_src", "ipv4_dst", "arp_spa", "arp_tpa")
+    IPV6_MATCH_FIELDS = ("ipv6_src", "ipv6_dst", "ipv6_nd_target")
+    HEX_FIELDS = "eth_type"
 
     def __init__(self, flowmod):
         """flowmod is a ryu flow modification message object"""
         self.priority = flowmod.priority
         self.cookie = flowmod.cookie
         self.instructions = flowmod.instructions
         self.validate_instructions()
         self.match_values = {}
         self.match_masks = {}
         self.out_port = None
         # flowmod can be an OFPFlowMod or an OFPStats
         if isinstance(flowmod, parser.OFPFlowMod):
-            if flowmod.command in (ofp.OFPFC_DELETE, ofp.OFPFC_DELETE_STRICT)\
-                    and flowmod.out_port != ofp.OFPP_ANY:
+            if (
+                flowmod.command in (ofp.OFPFC_DELETE, ofp.OFPFC_DELETE_STRICT)
+                and flowmod.out_port != ofp.OFPP_ANY
+            ):
                 self.out_port = flowmod.out_port
 
         for key, val in flowmod.match.items():
             if isinstance(val, tuple):
                 val, mask = val
             else:
                 mask = -1
@@ -874,15 +917,17 @@
             self.match_masks[key] = mask
 
     def validate_instructions(self):
         instruction_types = set()
         for instruction in self.instructions:
             if instruction.type in instruction_types:
                 raise FakeOFTableException(
-                    f'FlowMod with Multiple instructions of the same type: {self.instructions}')
+                    "FlowMod with Multiple instructions of the "
+                    "same type: {}".format(self.instructions)
+                )
             instruction_types.add(instruction.type)
 
     def out_port_matches(self, other):
         """returns True if other has an output action to this flowmods
         output_port"""
         if self.out_port is None or self.out_port == ofp.OFPP_ANY:
             return True
@@ -910,17 +955,19 @@
                 return False
             val_bits = self.match_to_bits(key, pkt_dict[key])
             if val_bits != (val & self.match_masks[key]):
                 return False
         return True
 
     def _matches_match(self, other):
-        return (self.priority == other.priority
-                and self.match_values == other.match_values
-                and self.match_masks == other.match_masks)
+        return (
+            self.priority == other.priority
+            and self.match_values == other.match_values
+            and self.match_masks == other.match_masks
+        )
 
     def fte_matches(self, other, strict=False):
         """returns True if the flow table entry other matches this flowmod.
 
         used for finding existing flow table entries that match with this
         flowmod.
 
@@ -937,15 +984,15 @@
             if key not in other.match_values:
                 return False
             if other.match_values[key] & self.match_masks[key] != val:
                 return False
         return True
 
     def overlaps(self, other):
-        """ returns True if any packet can match both self and other."""
+        """returns True if any packet can match both self and other."""
         # This is different from the matches method as matches assumes an
         # undefined field is a failed match. In this case an undefined field is
         # potentially an overlap and therefore is considered success
         if other.priority != self.priority:
             return False
         for key, val in self.match_values.items():
             if key in other.match_values:
@@ -981,51 +1028,55 @@
         if key in self.MAC_MATCH_FIELDS:
             result = addrconv.mac.bin_to_text(val.tobytes())
         elif key in self.IPV4_MATCH_FIELDS:
             result = addrconv.ipv4.bin_to_text(val.tobytes())
         elif key in self.IPV6_MATCH_FIELDS:
             result = addrconv.ipv6.bin_to_text(val.tobytes())
         elif key in self.HEX_FIELDS:
-            result = str(val.hex.lstrip('0'))
+            result = str(val.hex.lstrip("0"))
         else:
             result = str(val.int)
         return result
 
     def __lt__(self, other):
         return self.priority < other.priority
 
     def __eq__(self, other):
-        return (self._matches_match(other)
-                and self.out_port == other.out_port
-                and self.instructions == other.instructions)
+        return (
+            self._matches_match(other)
+            and self.out_port == other.out_port
+            and self.instructions == other.instructions
+        )
 
     def __hash__(self):
-        return hash((
-            self.priority,
-            self.match_values,
-            self.match_masks,
-            self.out_port,
-            self.instructions,
-        ))
+        return hash(
+            (
+                self.priority,
+                self.match_values,
+                self.match_masks,
+                self.out_port,
+                self.instructions,
+            )
+        )
 
     def _pretty_field_str(self, key, value, mask=None):
         mask_str = ""
         value_int = value
         mask_int = mask
         if isinstance(value, Bits):
             value_int = value.int
         if isinstance(mask, Bits):
             mask_int = mask.int  # pytype: disable=attribute-error
         elif mask is None:
             mask_int = -1
-        if key == 'vlan_vid':
+        if key == "vlan_vid":
             if value_int & ofp.OFPVID_PRESENT == 0:
-                result = 'vlan untagged'
-            elif key == 'vlan_vid' and mask_int == ofp.OFPVID_PRESENT:
-                result = 'vlan tagged'
+                result = "vlan untagged"
+            elif key == "vlan_vid" and mask_int == ofp.OFPVID_PRESENT:
+                result = "vlan tagged"
             else:
                 result = str(value_int ^ ofp.OFPVID_PRESENT)
                 if mask_int != -1:
                     mask_str = str(mask_int ^ ofp.OFPVID_PRESENT)
         elif isinstance(value, Bits):
             result = self.bits_to_str(key, value)
             if mask is not None and mask_int != -1:
@@ -1040,74 +1091,75 @@
                 if mask is not None and mask != -1:
                     mask_str = hex(mask)
             else:
                 result = str(value)
                 if mask is not None and mask != -1:
                     mask_str = str(mask)
         if mask_str:
-            result += f"/{mask_str}"
+            result += "/{}".format(mask_str)
         return result
 
     def _pretty_action_str(self, action):
         actions_names_attrs = {
-            parser.OFPActionPushVlan.__name__: ('push_vlan', 'ethertype'),
-            parser.OFPActionPopVlan.__name__: ('pop_vlan', None),
-            parser.OFPActionGroup.__name__: ('group', 'group_id'),
-            parser.OFPActionDecNwTtl.__name__: ('dec_nw_ttl', None)}
+            parser.OFPActionPushVlan.__name__: ("push_vlan", "ethertype"),
+            parser.OFPActionPopVlan.__name__: ("pop_vlan", None),
+            parser.OFPActionGroup.__name__: ("group", "group_id"),
+            parser.OFPActionDecNwTtl.__name__: ("dec_nw_ttl", None),
+        }
         value = None
         if isinstance(action, parser.OFPActionOutput):
-            name = 'output'
+            name = "output"
             if action.port == CONTROLLER_PORT:
-                value = 'CONTROLLER'
+                value = "CONTROLLER"
             elif action.port == IN_PORT:
-                value = 'IN_PORT'
+                value = "IN_PORT"
             else:
                 value = str(action.port)
         elif isinstance(action, parser.OFPActionSetField):
-            name = f'set_{action.key}'
+            name = "set_{}".format(action.key)
             value = self._pretty_field_str(action.key, action.value)
         else:
             name, attr = actions_names_attrs[type(action).__name__]
             if attr:
                 value = getattr(action, attr)
         result = name
         if value:
-            result += f" {value}"
+            result += " {}".format(value)
         return result
 
     def __str__(self):
-        result = f'Priority: {self.priority} | Match: '
+        result = "Priority: {0} | Match: ".format(self.priority)
 
         for key in sorted(self.match_values.keys()):
             val = self.match_values[key]
             mask = self.match_masks[key]
-            result += f" {key} {self._pretty_field_str(key, val, mask)},"
-        result = result.rstrip(',')
+            result += " {} {},".format(key, self._pretty_field_str(key, val, mask))
+        result = result.rstrip(",")
         result += " | Instructions :"
         if not self.instructions:
-            result += ' drop'
+            result += " drop"
         for instruction in self.instructions:
             if isinstance(instruction, parser.OFPInstructionGotoTable):
-                result += f' goto {instruction.table_id}'
+                result += " goto {}".format(instruction.table_id)
             elif isinstance(instruction, parser.OFPInstructionActions):
                 for action in instruction.actions:
-                    result += f" {self._pretty_action_str(action)},"
+                    result += " {},".format(self._pretty_action_str(action))
             else:
                 result += str(instruction)
-        result = result.rstrip(',')
+        result = result.rstrip(",")
         return result
 
     def __repr__(self):
-        string = f'priority: {self.priority} cookie: {self.cookie}'
+        string = "priority: {0} cookie: {1}".format(self.priority, self.cookie)
         for key in sorted(self.match_values.keys()):
             mask = self.match_masks[key]
-            string += f' {key}: {self.match_values[key]}'
+            string += " {0}: {1}".format(key, self.match_values[key])
             if mask.int != -1:  # pytype: disable=attribute-error
-                string += f'/{mask}'
-        string += f' Instructions: {str(self.instructions)}'
+                string += "/{0}".format(mask)
+        string += " Instructions: {0}".format(str(self.instructions))
         return string
 
 
 class FakeRyuDp:  # pylint: disable=too-few-public-methods
     """Fake ryu Datapath object.
 
     Just needed to provide a parser to allow us to extract ryu objects from
@@ -1118,120 +1170,124 @@
         """Create fake ryu DP"""
         self.ofproto_parser = parser
 
 
 def parse_print_args():
     """Parse arguments for the print command"""
     arg_parser = argparse.ArgumentParser(
-        prog='fakeoftable',
-        description='Prints a JSON flow table in a human readable format',
+        prog="fakeoftable",
+        description="Prints a JSON flow table in a human readable format",
         usage="""
     Print a flow table in a human readable format
     {argv0} print -f FILE
-""".format(argv0=sys.argv[0])
+""".format(
+            argv0=sys.argv[0]
+        ),
     )
     arg_parser.add_argument(
-        '-f',
-        '--file',
-        help='file containing an OFPFlowStatsReply message in JSON format'
+        "-f",
+        "--file",
+        help="file containing an OFPFlowStatsReply message in JSON format",
     )
     args = arg_parser.parse_args(sys.argv[2:])
-    return {'filename': args.file}
+    return {"filename": args.file}
 
 
 def parse_probe_args():
     """Parse arguments for the probe command"""
     arg_parser = argparse.ArgumentParser(
-        prog='fakeoftable',
-        description='Performs a packet lookup on a JSON openflow table',
+        prog="fakeoftable",
+        description="Performs a packet lookup on a JSON openflow table",
         usage="""
     Find the flow table entries in a given flow table that match a given packet
     {argv0} probe -f FILE -p PACKET_STRING
-""".format(argv0=sys.argv[0])
+""".format(
+            argv0=sys.argv[0]
+        ),
     )
     arg_parser.add_argument(
-        '-p',
-        '--packet',
-        metavar='PACKET_STRING',
+        "-p",
+        "--packet",
+        metavar="PACKET_STRING",
         help=(
-            '''string representation of a packet dictionary eg. '''
-            '''"{'in_port': 1, 'eth_dst': '01:80:c2:00:00:02', 'eth_type': '''
-            '''34825}"''')
+            """string representation of a packet dictionary eg. """
+            """"{'in_port': 1, 'eth_dst': '01:80:c2:00:00:02', 'eth_type': """
+            '''34825}"'''
+        ),
     )
     arg_parser.add_argument(
-        '-f',
-        '--file',
-        metavar='FILE',
-        help='file containing an OFPFlowStatsReply message in JSON format'
+        "-f",
+        "--file",
+        metavar="FILE",
+        help="file containing an OFPFlowStatsReply message in JSON format",
     )
     args = arg_parser.parse_args(sys.argv[2:])
     packet = args.packet
     packet = ast.literal_eval(args.packet)
     # fix vlan vid
-    if 'vlan_vid' in packet:
-        packet['vlan_vid'] |= ofp.OFPVID_PRESENT
-    return {'packet': packet, 'filename': args.file}
+    if "vlan_vid" in packet:
+        packet["vlan_vid"] |= ofp.OFPVID_PRESENT
+    return {"packet": packet, "filename": args.file}
 
 
 def parse_args():
     """parse arguments"""
     arg_parser = argparse.ArgumentParser(
-        prog='fakeoftable',
-        description='Performs operations on JSON openflow tables',
+        prog="fakeoftable",
+        description="Performs operations on JSON openflow tables",
         usage="""
     {argv0} <command> <args>
 
-""".format(argv0=sys.argv[0])
-    )
-    arg_parser.add_argument(
-        'command',
-        help='Subcommand, either "print" or "probe"'
+""".format(
+            argv0=sys.argv[0]
+        ),
     )
+    arg_parser.add_argument("command", help='Subcommand, either "print" or "probe"')
     args = arg_parser.parse_args(sys.argv[1:2])
     try:
-        if args.command == 'probe':
+        if args.command == "probe":
             command_args = parse_probe_args()
-        elif args.command == 'print':
+        elif args.command == "print":
             command_args = parse_print_args()
     except (KeyError, IndexError, ValueError, AttributeError) as err:
         print(err)
         arg_parser.print_help()
         sys.exit(-1)
     return (args.command, command_args)
 
 
 def _print(filename, **_kwargs):
     """Prints the JSON flow table from a file in a human readable format"""
-    with open(filename, 'r', encoding='utf-8') as file_handle:
+    with open(filename, "r", encoding="utf-8") as file_handle:
         msg = json.load(file_handle)
     datapath = FakeRyuDp()
     ofmsg = ofp_parser.ofp_msg_from_jsondict(datapath, msg)
     table = FakeOFTable(1)
     table.apply_ofmsgs([ofmsg])
     print(table)
 
 
 def probe(filename, packet):
     """Prints the actions applied to packet by the table from the file"""
-    with open(filename, 'r', encoding='utf-8') as file_handle:
+    with open(filename, "r", encoding="utf-8") as file_handle:
         msg = json.load(file_handle)
     datapath = FakeRyuDp()
     ofmsg = ofp_parser.ofp_msg_from_jsondict(datapath, msg)
     table = FakeOFTable(1)
     table.apply_ofmsgs([ofmsg])
     instructions, out_packet = table.lookup(packet)
     print(packet)
     for instruction in instructions:
         print(instruction)
     print(out_packet)
 
 
 def main():
     command, kwargs = parse_args()
-    if command == 'probe':
+    if command == "probe":
         probe(**kwargs)
-    elif command == 'print':
+    elif command == "print":
         _print(**kwargs)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `c65faucet-1.0.49/clib/mininet_test_base.py` & `c65faucet-1.0.50/clib/mininet_test_base.py`

 * *Files 15% similar despite different names*

```diff
@@ -40,16 +40,16 @@
 
 
 MAX_TEST_VID = 512
 OFPVID_PRESENT = 0x1000
 MIN_FLAP_TIME = 1
 PEER_BGP_AS = 2**16 + 1
 IPV4_ETH = 0x0800
-IPV6_ETH = 0x86dd
-FPING_ARGS = '-s -T 1 -A'
+IPV6_ETH = 0x86DD
+FPING_ARGS = "-s -T 1 -A"
 
 
 class FaucetTestBase(unittest.TestCase):
     """Base class for all FAUCET unit tests."""
 
     # Number of Faucet controllers to create
     NUM_FAUCET_CONTROLLERS = 2
@@ -57,53 +57,53 @@
     NUM_GAUGE_CONTROLLERS = 1
 
     # List of switches (by switch index) to ignore (treating them as outside the Faucet network)
     IGNORED_SWITCHES = []
 
     CONTROLLER_CLASS = mininet_test_topo.FAUCET
 
-    DP_NAME = 'faucet-1'
+    DP_NAME = "faucet-1"
 
-    _PROM_LINE_RE = re.compile(r'^(.+)\s+([0-9\.\-\+e]+)$')
+    _PROM_LINE_RE = re.compile(r"^(.+)\s+([0-9\.\-\+e]+)$")
 
-    ONE_GOOD_PING = '1 packets transmitted, 1 received, 0% packet loss'
-    FAUCET_VIPV4 = ipaddress.ip_interface('10.0.0.254/24')
-    FAUCET_VIPV4_2 = ipaddress.ip_interface('172.16.0.254/24')
-    FAUCET_VIPV6 = ipaddress.ip_interface('fc00::1:254/112')
-    FAUCET_VIPV6_2 = ipaddress.ip_interface('fc01::1:254/112')
-    OFCTL = 'ovs-ofctl -OOpenFlow13'
-    VSCTL = 'ovs-vsctl'
-    OVS_TYPE = 'kernel'
-    BOGUS_MAC = '01:02:03:04:05:06'
-    FAUCET_MAC = '0e:00:00:00:00:01'
-    LADVD = 'ladvd -e lo -t -f'
-    ONEMBPS = (1024 * 1024)
+    ONE_GOOD_PING = "1 packets transmitted, 1 received, 0% packet loss"
+    FAUCET_VIPV4 = ipaddress.ip_interface("10.0.0.254/24")
+    FAUCET_VIPV4_2 = ipaddress.ip_interface("172.16.0.254/24")
+    FAUCET_VIPV6 = ipaddress.ip_interface("fc00::1:254/112")
+    FAUCET_VIPV6_2 = ipaddress.ip_interface("fc01::1:254/112")
+    OFCTL = "ovs-ofctl -OOpenFlow13"
+    VSCTL = "ovs-vsctl"
+    OVS_TYPE = "kernel"
+    BOGUS_MAC = "01:02:03:04:05:06"
+    FAUCET_MAC = "0e:00:00:00:00:01"
+    LADVD = "ladvd -e lo -t -f"
+    ONEMBPS = 1024 * 1024
     DB_TIMEOUT = 5
-    STAT_RELOAD = ''
-    EVENT_SOCK_HEARTBEAT = ''
+    STAT_RELOAD = ""
+    EVENT_SOCK_HEARTBEAT = ""
 
-    CONFIG = ''
-    CONFIG_GLOBAL = ''
-    GAUGE_CONFIG_DBS = ''
+    CONFIG = ""
+    CONFIG_GLOBAL = ""
+    GAUGE_CONFIG_DBS = ""
 
-    LOG_LEVEL = 'INFO'
+    LOG_LEVEL = "INFO"
 
     N_UNTAGGED = 0
     N_TAGGED = 0
     N_EXTENDED = 0
     EXTENDED_CLS = None
     NUM_DPS = 1
     LINKS_PER_HOST = 1
     SOFTWARE_ONLY = False
     NETNS = False
     EVENT_LOGGER_TIMEOUT = 120
 
     FPING_ARGS = FPING_ARGS
-    FPING_ARGS_SHORT = ' '.join((FPING_ARGS, '-i10 -p100 -t100'))
-    FPINGS_ARGS_ONE = ' '.join(('fping', FPING_ARGS, '-t100 -c 1'))
+    FPING_ARGS_SHORT = " ".join((FPING_ARGS, "-i10 -p100 -t100"))
+    FPINGS_ARGS_ONE = " ".join(("fping", FPING_ARGS, "-t100 -c 1"))
 
     REQUIRES_METERS = False
     REQUIRES_METADATA = False
 
     _PORT_ACL_TABLE = 0
     _VLAN_TABLE = 1
     _COPRO_TABLE = 2
@@ -114,18 +114,18 @@
     _VIP_TABLE = 7
     _ETH_DST_HAIRPIN_TABLE = 8
     _ETH_DST_TABLE = 9
     _FLOOD_TABLE = 10
 
     # Standard Gauge port counters.
     PORT_VARS = {
-        'of_port_rx_bytes',
-        'of_port_tx_bytes',
-        'of_port_rx_packets',
-        'of_port_tx_packets',
+        "of_port_rx_bytes",
+        "of_port_tx_bytes",
+        "of_port_rx_packets",
+        "of_port_tx_packets",
     }
 
     faucet_controllers = None
     faucet_of_ports = None
     faucet_prom_ports = None
 
     faucet_config_path = None
@@ -134,15 +134,15 @@
     gauge_of_ports = None
     gauge_controller = None
     gauge_of_port = None
 
     config = None
     dpid = None
     hw_dpid = None
-    hardware = 'Open vSwitch'
+    hardware = "Open vSwitch"
     hw_switch = False
     prom_port = None
     net = None
     of_port = None
     ctl_privkey = None
     ctl_cert = None
     ca_certs = None
@@ -156,16 +156,24 @@
     config_ports = {}
     event_sock_dir = None
     event_socks = []
     event_log = None
 
     rand_dpids = set()
 
-    def __init__(self, name, config, root_tmpdir, ports_sock, max_test_load,
-                 port_order=None, start_port=None):
+    def __init__(
+        self,
+        name,
+        config,
+        root_tmpdir,
+        ports_sock,
+        max_test_load,
+        port_order=None,
+        start_port=None,
+    ):
         super().__init__(name)
         self.env = collections.defaultdict(dict)
         self.faucet_controllers = []
         self.faucet_of_ports = []
         self.faucet_prom_ports = []
         self.gauge_controllers = []
         self.gauge_of_ports = []
@@ -206,186 +214,212 @@
     def _set_var(self, controller, var, value):
         """Set controller environment variable to value"""
         self.env[controller][var] = value
 
     def _set_vars(self):
         """Set controller additional variables"""
         for c_index in range(self.NUM_FAUCET_CONTROLLERS):
-            self._set_var(f'faucet-{c_index}', 'FAUCET_PROMETHEUS_PORT',
-                          str(self.faucet_prom_ports[c_index]))
+            self._set_var(
+                "faucet-%s" % c_index,
+                "FAUCET_PROMETHEUS_PORT",
+                str(self.faucet_prom_ports[c_index]),
+            )
 
     def _set_var_path(self, controller, var, path):
         """Update environment variable that is a file path to the correct tmpdir"""
         self._set_var(controller, var, os.path.join(self.tmpdir, path))
 
     def _set_static_vars(self):
         """Set static environment variables"""
         if self.event_sock_dir and os.path.exists(self.event_sock_dir):
             shutil.rmtree(self.event_sock_dir)
         self.event_sock_dir = tempfile.mkdtemp()
         self.event_socks = []
         for c_index in range(self.NUM_FAUCET_CONTROLLERS):
-            event_sock = os.path.join(self.event_sock_dir, f'event-{c_index}.sock')
+            event_sock = os.path.join(self.event_sock_dir, "event-%s.sock" % c_index)
             self.event_socks.append(event_sock)
 
-            self._set_var(f'faucet-{c_index}', 'FAUCET_LOG_LEVEL', str(self.LOG_LEVEL))
-            self._set_var(f'faucet-{c_index}', 'FAUCET_CONFIG_STAT_RELOAD', self.STAT_RELOAD)
-            self._set_var(f'faucet-{c_index}', 'FAUCET_EVENT_SOCK', event_sock)
-            self._set_var(f'faucet-{c_index}', 'FAUCET_EVENT_SOCK_HEARTBEAT',
-                          self.EVENT_SOCK_HEARTBEAT)
-            self._set_var(f'faucet-{c_index}', 'FAUCET_PROMETHEUS_ADDR',
-                          mininet_test_util.LOCALHOSTV6)
-            self._set_var_path(f'faucet-{c_index}', 'FAUCET_CONFIG', 'faucet.yaml')
-            self._set_var_path(f'faucet-{c_index}', 'FAUCET_LOG', f'faucet-{c_index}.log')
-            self._set_var_path(f'faucet-{c_index}', 'FAUCET_EXCEPTION_LOG',
-                               f'faucet-{c_index}-exception.log')
+            self._set_var(
+                "faucet-%s" % c_index, "FAUCET_LOG_LEVEL", str(self.LOG_LEVEL)
+            )
+            self._set_var(
+                "faucet-%s" % c_index, "FAUCET_CONFIG_STAT_RELOAD", self.STAT_RELOAD
+            )
+            self._set_var("faucet-%s" % c_index, "FAUCET_EVENT_SOCK", event_sock)
+            self._set_var(
+                "faucet-%s" % c_index,
+                "FAUCET_EVENT_SOCK_HEARTBEAT",
+                self.EVENT_SOCK_HEARTBEAT,
+            )
+            self._set_var(
+                "faucet-%s" % c_index,
+                "FAUCET_PROMETHEUS_ADDR",
+                mininet_test_util.LOCALHOSTV6,
+            )
+            self._set_var_path("faucet-%s" % c_index, "FAUCET_CONFIG", "faucet.yaml")
+            self._set_var_path(
+                "faucet-%s" % c_index, "FAUCET_LOG", "faucet-%s.log" % c_index
+            )
+            self._set_var_path(
+                "faucet-%s" % c_index,
+                "FAUCET_EXCEPTION_LOG",
+                "faucet-%s-exception.log" % c_index,
+            )
         for c_index in range(self.NUM_GAUGE_CONTROLLERS):
-            self._set_var_path(f'gauge-{c_index}', 'GAUGE_CONFIG', 'gauge.yaml')
-            self._set_var_path(f'gauge-{c_index}', 'GAUGE_LOG', f'gauge-{c_index}.log')
-            self._set_var_path(f'gauge-{c_index}', 'GAUGE_EXCEPTION_LOG',
-                               f'gauge-{c_index}-exception.log')
-        self.faucet_config_path = self.env['faucet-0']['FAUCET_CONFIG']
-        self.gauge_config_path = self.env['gauge-0']['GAUGE_CONFIG']
-        self.debug_log_path = os.path.join(
-            self.tmpdir, 'ofchannel.txt')
-        self.monitor_stats_file = os.path.join(
-            self.tmpdir, 'gauge-ports.txt')
-        self.monitor_state_file = os.path.join(
-            self.tmpdir, 'gauge-state.txt')
-        self.monitor_flow_table_dir = os.path.join(
-            self.tmpdir, 'gauge-flow')
-        self.monitor_meter_stats_file = os.path.join(
-            self.tmpdir, 'gauge-meter.txt')
+            self._set_var_path("gauge-%s" % c_index, "GAUGE_CONFIG", "gauge.yaml")
+            self._set_var_path(
+                "gauge-%s" % c_index, "GAUGE_LOG", "gauge-%s.log" % c_index
+            )
+            self._set_var_path(
+                "gauge-%s" % c_index,
+                "GAUGE_EXCEPTION_LOG",
+                "gauge-%s-exception.log" % c_index,
+            )
+        self.faucet_config_path = self.env["faucet-0"]["FAUCET_CONFIG"]
+        self.gauge_config_path = self.env["gauge-0"]["GAUGE_CONFIG"]
+        self.debug_log_path = os.path.join(self.tmpdir, "ofchannel.txt")
+        self.monitor_stats_file = os.path.join(self.tmpdir, "gauge-ports.txt")
+        self.monitor_state_file = os.path.join(self.tmpdir, "gauge-state.txt")
+        self.monitor_flow_table_dir = os.path.join(self.tmpdir, "gauge-flow")
+        self.monitor_meter_stats_file = os.path.join(self.tmpdir, "gauge-meter.txt")
         os.mkdir(self.monitor_flow_table_dir)
         if self.config is not None:
-            if 'hw_switch' in self.config:
-                self.hw_switch = self.config['hw_switch']
+            if "hw_switch" in self.config:
+                self.hw_switch = self.config["hw_switch"]
             if self.hw_switch:
-                self.dpid = self.config['dpid']
-                self.cpn_intf = self.config['cpn_intf']
-                if 'cpn_ipv6' in self.config:
-                    self.cpn_ipv6 = self.config['cpn_ipv6']
-                self.hardware = self.config['hardware']
-                if 'ctl_privkey' in self.config:
-                    self.ctl_privkey = self.config['ctl_privkey']
-                if 'ctl_cert' in self.config:
-                    self.ctl_cert = self.config['ctl_cert']
-                if 'ca_certs' in self.config:
-                    self.ca_certs = self.config['ca_certs']
-                dp_ports = self.config['dp_ports']
+                self.dpid = self.config["dpid"]
+                self.cpn_intf = self.config["cpn_intf"]
+                if "cpn_ipv6" in self.config:
+                    self.cpn_ipv6 = self.config["cpn_ipv6"]
+                self.hardware = self.config["hardware"]
+                if "ctl_privkey" in self.config:
+                    self.ctl_privkey = self.config["ctl_privkey"]
+                if "ctl_cert" in self.config:
+                    self.ctl_cert = self.config["ctl_cert"]
+                if "ca_certs" in self.config:
+                    self.ca_certs = self.config["ca_certs"]
+                dp_ports = self.config["dp_ports"]
                 self.switch_map = dp_ports.copy()
 
     def _enable_event_log(self, timeout=None):
         """Enable analsis of event log contents by copying events to a local log file"""
-        assert not self.event_log, 'event_log already enabled'
+        assert not self.event_log, "event_log already enabled"
         if not timeout:
             timeout = self.EVENT_LOGGER_TIMEOUT
-        self.event_log = os.path.join(self.tmpdir, 'event.log')
+        self.event_log = os.path.join(self.tmpdir, "event.log")
         self.prev_event_id = 0
         controller = self._get_controller()
-        sock = self.env[self.faucet_controllers[0].name]['FAUCET_EVENT_SOCK']
+        sock = self.env[self.faucet_controllers[0].name]["FAUCET_EVENT_SOCK"]
         # Relying on a timeout seems a bit brittle;
         # as an alternative we might possibly use something like
         # `with popen(cmd...) as proc`to clean up on exceptions
-        controller.cmd(mininet_test_util.timeout_cmd(
-            f'nc -U {sock} > {self.event_log} &', timeout))
+        controller.cmd(
+            mininet_test_util.timeout_cmd(
+                "nc -U %s > %s &" % (sock, self.event_log), timeout
+            )
+        )
 
     # pylint: disable=inconsistent-return-statements
     def _wait_until_matching_event(self, match_func, timeout=30):
         """Return the next matching event from the event sock, else fail"""
         assert timeout >= 1
         assert self.event_log and os.path.exists(self.event_log)
         for _ in range(timeout):
-            with open(self.event_log, encoding='utf-8') as events:
+            with open(self.event_log, encoding="utf-8") as events:
                 for event_str in events:
                     event = json.loads(event_str)
-                    event_id = event['event_id']
+                    event_id = event["event_id"]
                     if event_id <= self.prev_event_id:
                         continue
                     self.prev_event_id = event_id
                     try:
                         if match_func(event):
                             return event
                     except KeyError:
                         pass  # Allow for easy dict traversal.
                 time.sleep(1)
-        self.fail('matching event not found in event stream')
+        self.fail("matching event not found in event stream")
 
     @staticmethod
     def _read_yaml(yaml_path):
-        with open(yaml_path, encoding='utf-8') as yaml_file:
+        with open(yaml_path, encoding="utf-8") as yaml_file:
             content = yaml_load(yaml_file.read())
         return content
 
     def _get_faucet_conf(self):
         """Return the yaml content from the config file"""
         return self._read_yaml(self.faucet_config_path)
 
     @staticmethod
     def _annotate_interfaces_conf(yaml_conf):
         """Consistently name interface names/descriptions."""
-        if 'dps' not in yaml_conf:
+        if "dps" not in yaml_conf:
             return yaml_conf
         yaml_conf_remap = copy.deepcopy(yaml_conf)
-        for dp_key, dp_yaml in yaml_conf['dps'].items():
-            interfaces_yaml = dp_yaml.get('interfaces', None)
+        for dp_key, dp_yaml in yaml_conf["dps"].items():
+            interfaces_yaml = dp_yaml.get("interfaces", None)
             if interfaces_yaml is not None:
                 remap_interfaces_yaml = {}
                 for intf_key, orig_intf_conf in interfaces_yaml.items():
                     intf_conf = copy.deepcopy(orig_intf_conf)
                     port_no = None
                     if isinstance(intf_key, int):
                         port_no = intf_key
-                    number = intf_conf.get('number', port_no)
+                    number = intf_conf.get("number", port_no)
                     if isinstance(number, int):
                         port_no = number
-                    assert isinstance(number, int), f'{intf_key} {orig_intf_conf}'
-                    intf_name = f'b{port_no}'
-                    intf_conf.update({'name': intf_name, 'description': intf_name})
+                    assert isinstance(number, int), "%u %s" % (intf_key, orig_intf_conf)
+                    intf_name = "b%u" % port_no
+                    intf_conf.update({"name": intf_name, "description": intf_name})
                     remap_interfaces_yaml[intf_key] = intf_conf
-                yaml_conf_remap['dps'][dp_key]['interfaces'] = remap_interfaces_yaml
+                yaml_conf_remap["dps"][dp_key]["interfaces"] = remap_interfaces_yaml
         return yaml_conf_remap
 
     @staticmethod
     def _write_yaml_conf(yaml_path, yaml_conf):
         assert isinstance(yaml_conf, dict)
         new_conf_str = yaml_dump(yaml_conf).encode()
         with tempfile.NamedTemporaryFile(
-                prefix=os.path.basename(yaml_path),
-                dir=os.path.dirname(yaml_path),
-                delete=False) as conf_file_tmp:
+            prefix=os.path.basename(yaml_path),
+            dir=os.path.dirname(yaml_path),
+            delete=False,
+        ) as conf_file_tmp:
             conf_file_tmp_name = conf_file_tmp.name
             os.chmod(conf_file_tmp_name, 0o644)
             conf_file_tmp.write(new_conf_str)
-        with open(conf_file_tmp_name, 'rb', encoding=None) as conf_file_tmp:
+        with open(conf_file_tmp_name, "rb", encoding=None) as conf_file_tmp:
             conf_file_tmp_str = conf_file_tmp.read()
             assert new_conf_str == conf_file_tmp_str
         if os.path.exists(yaml_path):
-            shutil.copyfile(yaml_path, f'{yaml_path}.{time.time()}')
+            shutil.copyfile(yaml_path, "%s.%f" % (yaml_path, time.time()))
         os.rename(conf_file_tmp_name, yaml_path)
 
     def _init_faucet_config(self):
-        faucet_config = '\n'.join((
-            self.get_config_header(
-                self.CONFIG_GLOBAL,
-                self.debug_log_path, self.dpid, self.hardware),
-            self.CONFIG))
+        faucet_config = "\n".join(
+            (
+                self.get_config_header(
+                    self.CONFIG_GLOBAL, self.debug_log_path, self.dpid, self.hardware
+                ),
+                self.CONFIG,
+            )
+        )
         config_vars = {}
         for config_var in (self.config_ports, self.port_map):
             config_vars.update(config_var)
         faucet_config = faucet_config % config_vars
         yaml_conf = yaml_dump(self._annotate_interfaces_conf(yaml_load(faucet_config)))
         self._write_yaml_conf(self.faucet_config_path, yaml_load(yaml_conf))
 
     def _init_gauge_config(self):
         gauge_config = self.get_gauge_config(
             self.faucet_config_path,
             self.monitor_stats_file,
             self.monitor_state_file,
-            self.monitor_flow_table_dir)
+            self.monitor_flow_table_dir,
+        )
         if self.config_ports:
             gauge_config = gauge_config % self.config_ports
         self._write_yaml_conf(self.gauge_config_path, yaml_load(gauge_config))
 
     def _test_name(self):
         return mininet_test_util.flat_test_name(self.id())
 
@@ -396,183 +430,213 @@
 
     def _wait_load(self, load_retries=10):
         for _ in range(load_retries):
             time.sleep(random.randint(1, 5))
             load = os.getloadavg()[0]
             if load < self.max_test_load:
                 return
-            output(f'load average too high {load}, waiting')
-        self.fail(f'load average {load} consistently too high')
+            output("load average too high %f, waiting" % load)
+        self.fail("load average %f consistently too high" % load)
 
     def _allocate_config_ports(self):
         for port_name in self.config_ports:
             self.config_ports[port_name] = None
             for config in (self.CONFIG, self.CONFIG_GLOBAL, self.GAUGE_CONFIG_DBS):
                 if re.search(port_name, config):
                     port = mininet_test_util.find_free_port(
-                        self.ports_sock, self._test_name())
+                        self.ports_sock, self._test_name()
+                    )
                     self.config_ports[port_name] = port
-                    output(f'allocating port {port} for {port_name}')
+                    output("allocating port %u for %s" % (port, port_name))
 
     def _allocate_faucet_ports(self):
         for c_index in range(self.NUM_FAUCET_CONTROLLERS):
             if self.hw_switch and c_index == 0:
-                of_port = self.config['of_port']
+                of_port = self.config["of_port"]
             else:
                 of_port = mininet_test_util.find_free_port(
-                    self.ports_sock, self._test_name())
+                    self.ports_sock, self._test_name()
+                )
             prom_port = mininet_test_util.find_free_port(
-                self.ports_sock, self._test_name())
+                self.ports_sock, self._test_name()
+            )
             self.faucet_of_ports.append(of_port)
             self.faucet_prom_ports.append(prom_port)
         self.of_port = self.faucet_of_ports[0]
         self.prom_port = self.faucet_prom_ports[0]
 
     def _allocate_gauge_ports(self):
         for c_index in range(self.NUM_GAUGE_CONTROLLERS):
             if self.hw_switch and c_index == 0:
-                of_port = self.config['gauge_of_port']
+                of_port = self.config["gauge_of_port"]
             else:
                 of_port = mininet_test_util.find_free_port(
-                    self.ports_sock, self._test_name())
+                    self.ports_sock, self._test_name()
+                )
             self.gauge_of_ports.append(of_port)
         self.gauge_of_port = self.gauge_of_ports[0]
 
     def _stop_net(self):
         if self.net is not None:
             for switch in self.net.switches:
-                switch.cmd(
-                    self.VSCTL, 'del-controller', switch.name, '|| true')
+                switch.cmd(self.VSCTL, "del-controller", switch.name, "|| true")
             self.net.stop()
 
     def setUp(self):
-        if self.config and 'hw_switch' in self.config:
+        if self.config and "hw_switch" in self.config:
             # Simulating/running hardware switches so only 1 controller configured
             # TODO: Handle multiple controllers with hardware tests
             self.NUM_FAUCET_CONTROLLERS = 1
         self.start_time = time.time()
         self.tmpdir = self._tmpdir_name()
         self._set_static_vars()
         self.topo_class = partial(
-            mininet_test_topo.FaucetSwitchTopo, port_order=self.port_order,
-            switch_map=self.switch_map, start_port=self.start_port)
+            mininet_test_topo.FaucetSwitchTopo,
+            port_order=self.port_order,
+            switch_map=self.switch_map,
+            start_port=self.start_port,
+        )
         if self.hw_switch:
             self.hw_dpid = mininet_test_util.str_int_dpid(self.dpid)
             self.dpid = self.hw_dpid
         else:
             self.dpid = self.rand_dpid()
 
     @staticmethod
     def hostns(host):
-        return f'{host.name}'
+        return "%s" % host.name
 
     def dump_switch_flows(self, switch):
         """Dump switch information to tmpdir"""
         for dump_cmd in (
-                'dump-flows', 'dump-groups', 'dump-meters',
-                'dump-group-stats', 'dump-ports', 'dump-ports-desc',
-                'meter-stats'):
-            switch_dump_name = os.path.join(self.tmpdir, f'{switch.name}-{dump_cmd}.log')
+            "dump-flows",
+            "dump-groups",
+            "dump-meters",
+            "dump-group-stats",
+            "dump-ports",
+            "dump-ports-desc",
+            "meter-stats",
+        ):
+            switch_dump_name = os.path.join(
+                self.tmpdir, "%s-%s.log" % (switch.name, dump_cmd)
+            )
             # TODO: occasionally fails with socket error.
-            switch.cmd(f'{self.OFCTL} {dump_cmd} {switch.name} > {switch_dump_name}', success=None)
-        for other_cmd in ('show', 'list controller', 'list manager'):
-            other_dump_name = os.path.join(self.tmpdir, f'{other_cmd.replace(" ", "")}.log')
-            switch.cmd(f'{self.VSCTL} {other_cmd} > {other_dump_name}')
+            switch.cmd(
+                "%s %s %s > %s" % (self.OFCTL, dump_cmd, switch.name, switch_dump_name),
+                success=None,
+            )
+        for other_cmd in ("show", "list controller", "list manager"):
+            other_dump_name = os.path.join(
+                self.tmpdir, "%s.log" % other_cmd.replace(" ", "")
+            )
+            switch.cmd("%s %s > %s" % (self.VSCTL, other_cmd, other_dump_name))
 
     # pylint: disable=arguments-differ
     def tearDown(self, ignore_oferrors=False):
         """Clean up after a test.
-           ignore_oferrors: return OF errors rather than failing"""
+        ignore_oferrors: return OF errors rather than failing"""
         if self.NETNS:
             for host in self.hosts_name_ordered()[:1]:
                 if self.get_host_netns(host):
-                    self.quiet_commands(host, [f'ip netns del {self.hostns(host)}'])
+                    self.quiet_commands(host, ["ip netns del %s" % self.hostns(host)])
         first_switch = self.first_switch()
         if first_switch:
-            self.first_switch().cmd(f'ip link > {os.path.join(self.tmpdir, "ip-links.log")}')
+            self.first_switch().cmd(
+                "ip link > %s" % os.path.join(self.tmpdir, "ip-links.log")
+            )
         switch_names = []
         for switch in self.net.switches:
             switch_names.append(switch.name)
             self.dump_switch_flows(switch)
-            switch.cmd(f'{self.VSCTL} --if-exists del-br {switch.name}')
+            switch.cmd("%s --if-exists del-br %s" % (self.VSCTL, switch.name))
         self._stop_net()
         self.net = None
         if self.event_sock_dir and os.path.exists(self.event_sock_dir):
             shutil.rmtree(self.event_sock_dir)
-        mininet_test_util.return_free_ports(
-            self.ports_sock, self._test_name())
-        if 'OVS_LOGDIR' in os.environ:
-            ovs_log_dir = os.environ['OVS_LOGDIR']
+        mininet_test_util.return_free_ports(self.ports_sock, self._test_name())
+        if "OVS_LOGDIR" in os.environ:
+            ovs_log_dir = os.environ["OVS_LOGDIR"]
             if ovs_log_dir and os.path.exists(ovs_log_dir):
-                for ovs_log in glob.glob(os.path.join(ovs_log_dir, '*.log')):
+                for ovs_log in glob.glob(os.path.join(ovs_log_dir, "*.log")):
                     lines = []
                     for name in switch_names:
                         lines.extend(self.matching_lines_from_file(name, ovs_log))
                     if lines:
-                        switch_ovs_log_name = os.path.join(self.tmpdir, os.path.basename(ovs_log))
-                        with open(switch_ovs_log_name, 'w', encoding='utf-8') as switch_ovs_log:
-                            switch_ovs_log.write('\n'.join(lines))
-        with open(os.path.join(self.tmpdir, 'test_duration_secs'), 'w', encoding='utf-8') as duration_file:
+                        switch_ovs_log_name = os.path.join(
+                            self.tmpdir, os.path.basename(ovs_log)
+                        )
+                        with open(
+                            switch_ovs_log_name, "w", encoding="utf-8"
+                        ) as switch_ovs_log:
+                            switch_ovs_log.write("\n".join(lines))
+        with open(
+            os.path.join(self.tmpdir, "test_duration_secs"), "w", encoding="utf-8"
+        ) as duration_file:
             duration_file.write(str(int(time.time() - self.start_time)))
         # Must not be any controller exception.
         for controller_env in self.env.values():
-            if 'FAUCET_EXCEPTION_LOG' in controller_env:
-                self.verify_no_exception(controller_env['FAUCET_EXCEPTION_LOG'])
-            if 'GAUGE_EXCEPTION_LOG' in controller_env:
-                self.verify_no_exception(controller_env['GAUGE_EXCEPTION_LOG'])
-        oferrors = ''
+            if "FAUCET_EXCEPTION_LOG" in controller_env:
+                self.verify_no_exception(controller_env["FAUCET_EXCEPTION_LOG"])
+            if "GAUGE_EXCEPTION_LOG" in controller_env:
+                self.verify_no_exception(controller_env["GAUGE_EXCEPTION_LOG"])
+        oferrors = ""
         for controller_env in self.env.values():
-            if 'FAUCET_LOG' in controller_env:
-                logfile = controller_env['FAUCET_LOG']
-            elif 'GAUGE_LOG' in controller_env:
-                logfile = controller_env['GAUGE_LOG']
-            oldlogfile = '.'.join((logfile, 'old'))
+            if "FAUCET_LOG" in controller_env:
+                logfile = controller_env["FAUCET_LOG"]
+            elif "GAUGE_LOG" in controller_env:
+                logfile = controller_env["GAUGE_LOG"]
+            oldlogfile = ".".join((logfile, "old"))
             if os.path.exists(oldlogfile):
                 logfile = oldlogfile
             # Verify version is logged.
             self.assertTrue(
-                self.matching_lines_from_file(r'^.+version\s+(\S+)$', logfile),
-                msg=f'no version logged in {logfile}')
+                self.matching_lines_from_file(r"^.+version\s+(\S+)$", logfile),
+                msg="no version logged in %s" % logfile,
+            )
             # Verify no OFErrors.
-            oferrors += '\n\n'.join(self.matching_lines_from_file(r'^.+(OFError.+)$', logfile))
+            oferrors += "\n\n".join(
+                self.matching_lines_from_file(r"^.+(OFError.+)$", logfile)
+            )
             if not ignore_oferrors:
                 self.assertFalse(oferrors, msg=oferrors)
         return oferrors
 
     def _block_non_faucet_packets(self):
-
         def _cmd(cmd):
-            with subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as proc:
+            with subprocess.Popen(
+                cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
+            ) as proc:
                 stdout, stderr = proc.communicate()
-                self.assertFalse(stdout, msg=f'{stdout}: {cmd}')
-                self.assertFalse(stderr, msg=f'{stdout}: {cmd}')
+                self.assertFalse(stdout, msg="%s: %s" % (stdout, cmd))
+                self.assertFalse(stderr, msg="%s: %s" % (stderr, cmd))
 
-        _cmd('ebtables --f OUTPUT')
+        _cmd("ebtables --f OUTPUT")
         for phys_port in self.switch_map.values():
             phys_mac = self.get_mac_of_intf(phys_port)
             for cmd in (
-                    f'ip link set dev {phys_port} up',
-                    f'ip -4 addr flush dev {phys_port}',
-                    f'ip -6 addr flush dev {phys_port}',
-                    f'ebtables -A OUTPUT -s {phys_mac} -o {phys_port} -j DROP'):
+                "ip link set dev %s up" % phys_port,
+                "ip -4 addr flush dev %s" % phys_port,
+                "ip -6 addr flush dev %s" % phys_port,
+                "ebtables -A OUTPUT -s %s -o %s -j DROP" % (phys_mac, phys_port),
+            ):
                 _cmd(cmd)
 
     def _attach_physical_switch(self):
         """Bridge a physical switch into test topology.
 
-           We do this for now to enable us to reconnect
-           virtual ethernet interfaces which may already
-           exist on emulated hosts and other OVS instances.
-
-           (One alternative would be to create a Link() class
-           that uses the hardware interfaces directly.)
-
-           We repurpose the first OvS switch in the topology
-           as a patch panel that transparently connects the
-           hardware interfaces to the host/switch veth links."""
+        We do this for now to enable us to reconnect
+        virtual ethernet interfaces which may already
+        exist on emulated hosts and other OVS instances.
+
+        (One alternative would be to create a Link() class
+        that uses the hardware interfaces directly.)
+
+        We repurpose the first OvS switch in the topology
+        as a patch panel that transparently connects the
+        hardware interfaces to the host/switch veth links."""
         switch = self.first_switch()
         if not switch:
             return
         # hw_names are the names of the server hardware interfaces
         # that are cabled to the device under test, sorted by OF port number
         hw_names = [self.switch_map[port] for port in sorted(self.switch_map)]
         hw_macs = set()
@@ -581,41 +645,41 @@
         # The actual tests reorder them according to port_map
         ovs_ports = sorted(self.topo.switch_ports[switch.name])
         # Patch hardware interfaces through to to OvS interfaces
         for hw_name, ovs_port in zip(hw_names, ovs_ports):
             # Note we've already removed any Linux IP addresses from hw_name
             # and blocked traffic to/from its meaningless MAC
             hw_mac = self.get_mac_of_intf(hw_name)
-            self.assertFalse(hw_mac in hw_macs,
-                             f'duplicate hardware MAC {hw_mac}')
+            self.assertFalse(hw_mac in hw_macs, "duplicate hardware MAC %s" % hw_mac)
             hw_macs.add(hw_mac)
             # Create mininet Intf and attach it to the switch
             hw_intf = HWIntf(hw_name, node=switch)
             switch.attach(hw_intf)
             hw_port = switch.ports[hw_intf]
             # Connect hw_port <-> ovs_port
             src, dst = hw_port, ovs_port
             for flow in (
-                    # Drop anything to or from the meaningless hw_mac
-                    f'eth_src={hw_mac},priority=2,actions=drop',
-                    f'eth_dst={hw_mac},priority=2,actions=drop',
-                    # Forward traffic bidirectionally src <-> dst
-                    f'in_port={src},priority=1,actions=output:{dst}',
-                    f'in_port={dst},priority=1,actions=output:{src}'):
-                switch.cmd(self.OFCTL, 'add-flow', switch, flow)
+                # Drop anything to or from the meaningless hw_mac
+                "eth_src=%s,priority=2,actions=drop" % hw_mac,
+                "eth_dst=%s,priority=2,actions=drop" % hw_mac,
+                # Forward traffic bidirectionally src <-> dst
+                "in_port=%u,priority=1,actions=output:%u" % (src, dst),
+                "in_port=%u,priority=1,actions=output:%u" % (dst, src),
+            ):
+                switch.cmd(self.OFCTL, "add-flow", switch, flow)
 
     def create_port_map(self, dpid):
         """Return a port map {'port_1': port...} for a dpid in self.topo"""
         ports = self.topo.dpid_ports(dpid)
-        port_map = {'port_%d' % i: port for i, port in enumerate(ports, start=1)}
+        port_map = {"port_%d" % i: port for i, port in enumerate(ports, start=1)}
         return port_map
 
     def start_net(self):
         """Start Mininet network."""
-        controller_intf = 'lo'
+        controller_intf = "lo"
         controller_ipv6 = False
         if self.hw_switch:
             controller_intf = self.cpn_intf
             controller_ipv6 = self.cpn_ipv6
         if not self.port_map:
             # Sometimes created in build_net for config purposes, sometimes not
             self.port_map = self.create_port_map(self.dpid)
@@ -639,67 +703,75 @@
     @staticmethod
     def _start_gauge_check():
         return None
 
     def _start_check(self):
         # '_wait_controllers_connected' also checks the 'healthy' state
         if not self._wait_controllers_connected():
-            return 'not all controllers connected to switch'
+            return "not all controllers connected to switch"
         if not self._wait_ofctl_up():
-            return 'ofctl not up'
+            return "ofctl not up"
         if not self.wait_dp_status(1):
-            return 'prometheus port not up'
+            return "prometheus port not up"
         if not self._wait_controllers_healthy():
-            return 'not all controllers healthy after initial switch connection'
+            return "not all controllers healthy after initial switch connection"
         if self.config_ports:
             for port_name, port in self.config_ports.items():
-                if port is not None and not port_name.startswith('gauge'):
+                if port is not None and not port_name.startswith("gauge"):
                     if not self._get_controller().listen_port(port):
-                        return f'faucet not listening on {port} ({port_name})'
+                        return "faucet not listening on %u (%s)" % (port, port_name)
         return self._start_gauge_check()
 
     def _create_faucet_controller(self, index, intf, ipv6):
         port = self.faucet_of_ports[index]
-        name = f'faucet-{index}'
+        name = "faucet-%s" % index
         faucet_controller = self.CONTROLLER_CLASS(
-            name=name, tmpdir=self.tmpdir,
+            name=name,
+            tmpdir=self.tmpdir,
             controller_intf=intf,
             controller_ipv6=ipv6,
             env=self.env[name],
             ctl_privkey=self.ctl_privkey,
             ctl_cert=self.ctl_cert,
             ca_certs=self.ca_certs,
             ports_sock=self.ports_sock,
             prom_port=self.get_prom_port(name),
             port=port,
-            test_name=self._test_name())
+            test_name=self._test_name(),
+        )
         self.env[faucet_controller.name] = self.env.pop(name)
         return faucet_controller
 
     def _create_gauge_controller(self, index, intf, ipv6):
         port = self.gauge_of_ports[index]
-        name = f'gauge-{index}'
+        name = "gauge-%s" % index
         gauge_controller = mininet_test_topo.Gauge(
-            name=name, tmpdir=self.tmpdir,
+            name=name,
+            tmpdir=self.tmpdir,
             env=self.env[name],
             controller_intf=intf,
             controller_ipv6=ipv6,
             ctl_privkey=self.ctl_privkey,
             ctl_cert=self.ctl_cert,
             ca_certs=self.ca_certs,
-            port=port)
+            port=port,
+        )
         self.env[gauge_controller.name] = self.env.pop(name)
         return gauge_controller
 
     def _start_faucet(self, controller_intf, controller_ipv6):
-        self.assertIsNone(self.net, 'Cannot invoke _start_faucet() multiple times')
-        self.assertTrue(self.NUM_FAUCET_CONTROLLERS > 0, 'Define at least 1 Faucet controller')
-        self.assertTrue(self.NUM_GAUGE_CONTROLLERS > 0, 'Define at least 1 Gauge controller')
+        self.assertIsNone(self.net, "Cannot invoke _start_faucet() multiple times")
+        self.assertTrue(
+            self.NUM_FAUCET_CONTROLLERS > 0, "Define at least 1 Faucet controller"
+        )
+        self.assertTrue(
+            self.NUM_GAUGE_CONTROLLERS > 0, "Define at least 1 Gauge controller"
+        )
 
-        for log in glob.glob(os.path.join(self.tmpdir, '*.log')):
+        for log in glob.glob(os.path.join(self.tmpdir, "*.log")):
             os.remove(log)
 
         # Setup all static configuration
         self._allocate_config_ports()
         self._allocate_faucet_ports()
         self._allocate_gauge_ports()
 
@@ -707,38 +779,37 @@
 
         self._init_faucet_config()
         self._init_gauge_config()
 
         # Create all the controller instances
         self.faucet_controllers = []
         for c_index in range(self.NUM_FAUCET_CONTROLLERS):
-            controller = self._create_faucet_controller(c_index,
-                                                        controller_intf,
-                                                        controller_ipv6)
+            controller = self._create_faucet_controller(
+                c_index, controller_intf, controller_ipv6
+            )
             self.faucet_controllers.append(controller)
 
         self.gauge_controllers = []
         for c_index in range(self.NUM_GAUGE_CONTROLLERS):
-            controller = self._create_gauge_controller(c_index,
-                                                       controller_intf,
-                                                       controller_ipv6)
+            controller = self._create_gauge_controller(
+                c_index, controller_intf, controller_ipv6
+            )
             self.gauge_controllers.append(controller)
 
         # Use the first Gauge instance for Prometheus scraping
         self.gauge_controller = self.gauge_controllers[0]
 
         self._wait_load()
 
         last_error_txt = None
         for _ in range(3):
             # Start Mininet, connected to the first controller
             self.net = Mininet(
-                self.topo,
-                link=FaucetLink,
-                controller=self.faucet_controllers[0])
+                self.topo, link=FaucetLink, controller=self.faucet_controllers[0]
+            )
 
             # Add all the remaining Faucet controllers
             #  and all the Gauge controllers to the network
             for controller in self.faucet_controllers[1:]:
                 self.net.addController(controller)
             for controller in self.gauge_controllers:
                 self.net.addController(controller)
@@ -751,66 +822,70 @@
 
             last_error_txt = self._start_check()
             if last_error_txt is None:
                 break
 
             # Existing controllers will be reused on the next cycle
             self._stop_net()
-            last_error_txt += '\n\n' + self._dump_controller_logs()
-            error(f'{self._test_name()}: {last_error_txt}')
+            last_error_txt += "\n\n" + self._dump_controller_logs()
+            error("%s: %s" % (self._test_name(), last_error_txt))
             time.sleep(mininet_test_util.MIN_PORT_AGE)
 
         if last_error_txt is not None:
             self.fail(last_error_txt)
 
         # All controllers are OK, so prepare to keep running the test
         self._config_tableids()
         self._wait_load()
 
         if self.NETNS:
             # TODO: seemingly can't have more than one namespace.
             for host in self.hosts_name_ordered()[:1]:
                 hostns = self.hostns(host)
                 if self.get_host_netns(host):
-                    self.quiet_commands(host, [f'ip netns del {hostns}'])
-                self.quiet_commands(host, [f'ip netns add {hostns}'])
+                    self.quiet_commands(host, ["ip netns del %s" % hostns])
+                self.quiet_commands(host, ["ip netns add %s" % hostns])
 
         self.post_start_net()
 
     def _ofctl_rest_url(self, req):
         """Return control URL for Ryu ofctl module."""
-        return f'http://[{mininet_test_util.LOCALHOSTV6}]:{self._get_controller().ofctl_port}/{req}'
+        return "http://[%s]:%u/%s" % (
+            mininet_test_util.LOCALHOSTV6,
+            self._get_controller().ofctl_port,
+            req,
+        )
 
     @staticmethod
     def _ofctl(req, params=None):
         if params is None:
             params = {}
         try:
             ofctl_result = requests.get(req, params=params).json()
         except requests.exceptions.ConnectionError:
             return None
         return ofctl_result
 
     def _ofctl_up(self):
-        switches = self._ofctl(self._ofctl_rest_url('stats/switches'))
+        switches = self._ofctl(self._ofctl_rest_url("stats/switches"))
         return isinstance(switches, list) and switches
 
     def _wait_ofctl_up(self, timeout=10):
         for _ in range(timeout):
             if self._ofctl_up():
                 return True
             time.sleep(1)
         return False
 
     def _ofctl_post(self, int_dpid, req, timeout, params=None):
         for _ in range(timeout):
             try:
                 ofctl_result = requests.post(
-                    self._ofctl_rest_url(req),
-                    json=params).json()
+                    self._ofctl_rest_url(req), json=params
+                ).json()
                 return ofctl_result[int_dpid]
             except (ValueError, TypeError, requests.exceptions.ConnectionError):
                 # Didn't get valid JSON, try again
                 time.sleep(1)
                 continue
         return []
 
@@ -823,67 +898,66 @@
                 # Didn't get valid JSON, try again
                 time.sleep(1)
                 continue
         return []
 
     def _portmod(self, int_dpid, port_no, config, mask):
         result = requests.post(
-            self._ofctl_rest_url('stats/portdesc/modify'),
-            json={'dpid': str(int_dpid), 'port_no': str(port_no),
-                  'config': str(config), 'mask': str(mask)})
+            self._ofctl_rest_url("stats/portdesc/modify"),
+            json={
+                "dpid": str(int_dpid),
+                "port_no": str(port_no),
+                "config": str(config),
+                "mask": str(mask),
+            },
+        )
         # ofctl doesn't use barriers, so cause port_mod to be sent.
         self.get_port_stats_from_dpid(int_dpid, port_no)
         return result
 
     @staticmethod
     def _signal_proc_on_port(host, port, signal):
-        tcp_pattern = f'{port}/tcp'
-        fuser_out = host.cmd(f'fuser {tcp_pattern} -k -{signal}')
-        return re.search(r'%s:\s+\d+' % tcp_pattern, fuser_out)
+        tcp_pattern = "%s/tcp" % port
+        fuser_out = host.cmd("fuser %s -k -%u" % (tcp_pattern, signal))
+        return re.search(r"%s:\s+\d+" % tcp_pattern, fuser_out)
 
     def _get_ofchannel_logs(self):
         ofchannel_logs = []
         config = self._get_faucet_conf()
-        for dp_name, dp_config in config['dps'].items():
-            if 'ofchannel_log' in dp_config:
-                debug_log = dp_config['ofchannel_log']
+        for dp_name, dp_config in config["dps"].items():
+            if "ofchannel_log" in dp_config:
+                debug_log = dp_config["ofchannel_log"]
                 ofchannel_logs.append((dp_name, debug_log))
         return ofchannel_logs
 
     def _dump_controller_logs(self):
-        dump_txt = ''
-        test_logs = sorted(glob.glob(os.path.join(self.tmpdir, '*.log')))
+        dump_txt = ""
+        test_logs = sorted(glob.glob(os.path.join(self.tmpdir, "*.log")))
         for controller in self.net.controllers:
-            header_txt = controller.name + ' log files'
-            dump_txt += '\n'.join((
-                '',
-                '#' * len(header_txt),
-                header_txt,
-                '#' * len(header_txt),
-                ''))
+            header_txt = controller.name + " log files"
+            dump_txt += "\n".join(
+                ("", "#" * len(header_txt), header_txt, "#" * len(header_txt), "")
+            )
             for test_log_name in test_logs:
                 basename = os.path.basename(test_log_name)
                 if basename.startswith(controller.name_no_pid):
-                    with open(test_log_name, encoding='utf-8') as test_log:
-                        dump_txt += '\n'.join((
-                            '',
-                            basename,
-                            '=' * len(basename),
-                            '',
-                            test_log.read()))
+                    with open(test_log_name, encoding="utf-8") as test_log:
+                        dump_txt += "\n".join(
+                            ("", basename, "=" * len(basename), "", test_log.read())
+                        )
         return dump_txt
 
     def _controllers_healthy(self):
         for controller in self.net.controllers:
             if not controller.healthy():
                 return False
         for c_index in range(self.NUM_FAUCET_CONTROLLERS):
             event_sock = self.event_socks[c_index]
             if event_sock and not os.path.exists(event_sock):
-                error(f'event socket {event_sock} not created\n')
+                error("event socket %s not created\n" % event_sock)
                 return False
         return True
 
     def _controllers_connected(self):
         for controller in self.net.controllers:
             if not controller.connected():
                 return False
@@ -904,70 +978,92 @@
         return False
 
     def _wait_debug_log(self):
         """Require all switches to have exchanged flows with controller."""
         ofchannel_logs = self._get_ofchannel_logs()
         for _, debug_log in ofchannel_logs:
             for _ in range(60):
-                if (os.path.exists(debug_log)
-                        and os.path.getsize(debug_log) > 0):
+                if os.path.exists(debug_log) and os.path.getsize(debug_log) > 0:
                     return True
                 time.sleep(1)
         return False
 
     def verify_no_exception(self, exception_log_name):
         if not os.path.exists(exception_log_name):
             return
-        with open(exception_log_name, encoding='utf-8') as exception_log:
+        with open(exception_log_name, encoding="utf-8") as exception_log:
             exception_contents = exception_log.read()
             self.assertEqual(
-                '',
+                "",
                 exception_contents,
-                msg=f'{exception_log_name} log contains {exception_contents}')
+                msg="%s log contains %s" % (exception_log_name, exception_contents),
+            )
 
     @staticmethod
     def tcpdump_helper(*args, **kwargs):
         return TcpdumpHelper(*args, **kwargs).execute()
 
     @staticmethod
     def scapy_template(packet, iface, count=1):
-        return (f'python3 -c \"from scapy.all import * ; sendp({packet}, iface=\'{iface}\', count={count})"')
-
-    def scapy_base_udp(self, mac, iface, src_ip, dst_ip, dport, sport, count=1, dst=None):
+        return (
+            "python3 -c \"from scapy.all import * ; sendp(%s, iface='%s', count=%u)\""
+            % (packet, iface, count)
+        )
+
+    def scapy_base_udp(
+        self, mac, iface, src_ip, dst_ip, dport, sport, count=1, dst=None
+    ):
         if dst is None:
-            dst = 'ff:ff:ff:ff:ff:ff'
+            dst = "ff:ff:ff:ff:ff:ff"
         return self.scapy_template(
-            (f'Ether(dst=\'{dst}\', src=\'{mac}\', type={IPV4_ETH}) / '
-             f'IP(src=\'{src_ip}\', dst=\'{dst_ip}\') / UDP(dport={dport},sport={sport}) '),
-            iface, count)
+            (
+                "Ether(dst='%s', src='%s', type=%u) / "
+                "IP(src='%s', dst='%s') / UDP(dport=%s,sport=%s) "
+                % (dst, mac, IPV4_ETH, src_ip, dst_ip, dport, sport)
+            ),
+            iface,
+            count,
+        )
 
     def scapy_dhcp(self, mac, iface, count=1, dst=None):
         if dst is None:
-            dst = 'ff:ff:ff:ff:ff:ff'
+            dst = "ff:ff:ff:ff:ff:ff"
         return self.scapy_template(
-            (f'Ether(dst=\'{dst}\', src=\'{mac}\', type={IPV4_ETH}) / '
-             'IP(src=\'0.0.0.0\', dst=\'255.255.255.255\') / UDP(dport=67,sport=68) / '
-             'BOOTP(op=1) / DHCP(options=[(\'message-type\', \'discover\'), (\'end\')])'),
-            iface, count)
+            (
+                "Ether(dst='%s', src='%s', type=%u) / "
+                "IP(src='0.0.0.0', dst='255.255.255.255') / UDP(dport=67,sport=68) / "
+                "BOOTP(op=1) / DHCP(options=[('message-type', 'discover'), ('end')])"
+            )
+            % (dst, mac, IPV4_ETH),
+            iface,
+            count,
+        )
 
     def scapy_icmp(self, mac, iface, src_ip, dst_ip, count=1, dst=None):
         if dst is None:
-            dst = 'ff:ff:ff:ff:ff:ff'
+            dst = "ff:ff:ff:ff:ff:ff"
         return self.scapy_template(
-            (f'Ether(dst=\'{dst}\', src=\'{mac}\', type={IPV4_ETH}) / '
-             f'IP(src=\'{src_ip}\', dst=\'{dst_ip}\') / ICMP()'),
-            iface, count)
+            ("Ether(dst='%s', src='%s', type=%u) / " "IP(src='%s', dst='%s') / ICMP()")
+            % (dst, mac, IPV4_ETH, src_ip, dst_ip),
+            iface,
+            count,
+        )
 
     def scapy_dscp(self, src_mac, dst_mac, dscp_value, iface, count=1):
         # creates a packet with L2-L4 headers using scapy
         return self.scapy_template(
-            (f'Ether(dst=\'{dst_mac}\', src=\'{src_mac}\', type={IPV4_ETH}) / '
-             f'IP(src=\'0.0.0.0\', dst=\'255.255.255.255\', tos={dscp_value}) / UDP(dport=67,sport=68) / '
-             'BOOTP(op=1)'),
-            iface, count)
+            (
+                "Ether(dst='%s', src='%s', type=%u) / "
+                "IP(src='0.0.0.0', dst='255.255.255.255', tos=%s) / UDP(dport=67,sport=68) / "
+                "BOOTP(op=1)"
+            )
+            % (dst_mac, src_mac, IPV4_ETH, dscp_value),
+            iface,
+            count,
+        )
 
     def scapy_bcast(self, host, count=1):
         return self.scapy_dhcp(host.MAC(), host.defaultIntf(), count)
 
     @staticmethod
     def pre_start_net():
         """Hook called after Mininet initialization, before Mininet started."""
@@ -984,342 +1080,482 @@
 %s
 dps:
     %s:
         ofchannel_log: %s
         dp_id: 0x%x
         hardware: "%s"
         cookie: %u
-""" % (config_global, self.DP_NAME, debug_log,
-            int(dpid), hardware, random.randint(1, 2**64 - 1))
+""" % (
+            config_global,
+            self.DP_NAME,
+            debug_log,
+            int(dpid),
+            hardware,
+            random.randint(1, 2**64 - 1),
+        )
 
     def get_gauge_watcher_config(self):
-        return f"""
+        return """
     port_stats:
-        dps: ['{self.DP_NAME}']
+        dps: ['%s']
         type: 'port_stats'
         interval: 5
         db: 'stats_file'
     port_state:
-        dps: ['{self.DP_NAME}']
+        dps: ['%s']
         type: 'port_state'
         interval: 5
         db: 'state_file'
     flow_table:
-        dps: ['{self.DP_NAME}']
+        dps: ['%s']
         type: 'flow_table'
         interval: 5
         db: 'flow_dir'
-"""
-
-    def get_gauge_config(self, faucet_config_file,
-                         monitor_stats_file,
-                         monitor_state_file,
-                         monitor_flow_table_dir):
+""" % (
+            self.DP_NAME,
+            self.DP_NAME,
+            self.DP_NAME,
+        )
+
+    def get_gauge_config(
+        self,
+        faucet_config_file,
+        monitor_stats_file,
+        monitor_state_file,
+        monitor_flow_table_dir,
+    ):
         """Build Gauge config."""
-        return f"""
+        return """
 faucet_configs:
-    - {faucet_config_file}
+    - %s
 watchers:
-    {self.get_gauge_watcher_config()}
+    %s
 dbs:
     stats_file:
         type: 'text'
-        file: {monitor_stats_file}
+        file: %s
     state_file:
         type: 'text'
-        file: {monitor_state_file}
+        file: %s
     flow_dir:
         type: 'text'
-        path: {monitor_flow_table_dir}
-{self.GAUGE_CONFIG_DBS}
-"""
+        path: %s
+%s
+""" % (
+            faucet_config_file,
+            self.get_gauge_watcher_config(),
+            monitor_stats_file,
+            monitor_state_file,
+            monitor_flow_table_dir,
+            self.GAUGE_CONFIG_DBS,
+        )
 
     @staticmethod
-    def get_exabgp_conf(peer, peer_config=''):
+    def get_exabgp_conf(peer, peer_config=""):
         return """
   neighbor %s {
     router-id 2.2.2.2;
     local-address %s;
     connect %s;
     peer-as 1;
     local-as %s;
     %s
   }
-""" % (peer, peer, '%(bgp_port)d', PEER_BGP_AS, peer_config)
+""" % (
+            peer,
+            peer,
+            "%(bgp_port)d",
+            PEER_BGP_AS,
+            peer_config,
+        )
 
     def get_all_groups_desc_from_dpid(self, dpid, timeout=2):
         int_dpid = mininet_test_util.str_int_dpid(dpid)
-        return self._ofctl_get(
-            int_dpid, f'stats/groupdesc/{int_dpid}', timeout)
+        return self._ofctl_get(int_dpid, "stats/groupdesc/%s" % int_dpid, timeout)
 
     def get_all_flows_from_dpid(self, dpid, table_id, timeout=10, match=None):
         """Return all flows from DPID."""
         int_dpid = mininet_test_util.str_int_dpid(dpid)
         params = {}
-        params['table_id'] = table_id
+        params["table_id"] = table_id
         if match is not None:
-            params['match'] = match
+            params["match"] = match
         return self._ofctl_post(
-            int_dpid, f'stats/flow/{int_dpid}', timeout, params=params)
+            int_dpid, "stats/flow/%s" % int_dpid, timeout, params=params
+        )
 
     @staticmethod
     def _port_stat(port_stats, port):
         if port_stats:
             for port_stat in port_stats:
-                if port_stat['port_no'] == port:
+                if port_stat["port_no"] == port:
                     return port_stat
         return None
 
     def get_port_stats_from_dpid(self, dpid, port, timeout=2):
         """Return port stats for a port."""
         int_dpid = mininet_test_util.str_int_dpid(dpid)
         port_stats = self._ofctl_get(
-            int_dpid, f'stats/port/{int_dpid}/{port}', timeout)
+            int_dpid, "stats/port/%s/%s" % (int_dpid, port), timeout
+        )
         return self._port_stat(port_stats, port)
 
     def get_port_desc_from_dpid(self, dpid, port, timeout=2):
         """Return port desc for a port."""
         int_dpid = mininet_test_util.str_int_dpid(dpid)
         port_stats = self._ofctl_get(
-            int_dpid, f'stats/portdesc/{int_dpid}/{port}', timeout)
+            int_dpid, "stats/portdesc/%s/%s" % (int_dpid, port), timeout
+        )
         return self._port_stat(port_stats, port)
 
     def get_all_meters_from_dpid(self, dpid):
         """Return all meters from DPID"""
         int_dpid = mininet_test_util.str_int_dpid(dpid)
-        return self._ofctl_get(
-            int_dpid, f'stats/meterconfig/{int_dpid}', timeout=10)
+        return self._ofctl_get(int_dpid, "stats/meterconfig/%s" % int_dpid, timeout=10)
 
     def wait_matching_in_group_table(self, action, group_id, timeout=10):
-        groupdump = os.path.join(self.tmpdir, f'groupdump-{self.dpid}.txt')
+        groupdump = os.path.join(self.tmpdir, "groupdump-%s.txt" % self.dpid)
         for _ in range(timeout):
             group_dump = self.get_all_groups_desc_from_dpid(self.dpid, 1)
-            with open(groupdump, 'w', encoding='utf-8') as groupdump_file:
+            with open(groupdump, "w", encoding="utf-8") as groupdump_file:
                 for group_dict in group_dump:
-                    groupdump_file.write(str(group_dict) + '\n')
-                    if group_dict['group_id'] == group_id:
-                        actions = set(group_dict['buckets'][0]['actions'])
+                    groupdump_file.write(str(group_dict) + "\n")
+                    if group_dict["group_id"] == group_id:
+                        actions = set(group_dict["buckets"][0]["actions"])
                         if set([action]).issubset(actions):
                             return True
             time.sleep(1)
         return False
 
     # TODO: Should this have meter_confs as well or can we just match meter_ids
     def get_matching_meters_on_dpid(self, dpid):
-        meterdump = os.path.join(self.tmpdir, f'meterdump-{dpid}.log')
+        meterdump = os.path.join(self.tmpdir, "meterdump-%s.log" % dpid)
         meter_dump = self.get_all_meters_from_dpid(dpid)
-        with open(meterdump, 'w', encoding='utf-8') as meterdump_file:
+        with open(meterdump, "w", encoding="utf-8") as meterdump_file:
             meterdump_file.write(str(meter_dump))
         return meterdump
 
-    def get_matching_flows_on_dpid(self, dpid, match, table_id, timeout=10,
-                                   actions=None, hard_timeout=0, cookie=None,
-                                   ofa_match=True):
-
+    def get_matching_flows_on_dpid(
+        self,
+        dpid,
+        match,
+        table_id,
+        timeout=10,
+        actions=None,
+        hard_timeout=0,
+        cookie=None,
+        ofa_match=True,
+    ):
         # TODO: Ryu ofctl serializes to old matches.
         def to_old_match(match):
             old_matches = {
-                'tcp_dst': 'tp_dst',
-                'ip_proto': 'nw_proto',
-                'eth_dst': 'dl_dst',
-                'eth_type': 'dl_type',
+                "tcp_dst": "tp_dst",
+                "ip_proto": "nw_proto",
+                "eth_dst": "dl_dst",
+                "eth_type": "dl_type",
             }
             if match is not None:
                 for new_match, old_match in old_matches.items():
                     if new_match in match:
                         match[old_match] = match[new_match]
                         del match[new_match]
             return match
 
-        flowdump = os.path.join(self.tmpdir, f'flowdump-{dpid}.log')
+        flowdump = os.path.join(self.tmpdir, "flowdump-%s.log" % dpid)
         match = to_old_match(match)
         match_set = None
         exact_mask_match_set = None
         if match:
             # Different OFAs handle matches with an exact mask, different.
             # Most (including OVS) drop the redundant exact mask. But others
             # include an exact mask. So we must handle both.
-            mac_exact = str(netaddr.EUI(2**48 - 1)).replace('-', ':').lower()
+            mac_exact = str(netaddr.EUI(2**48 - 1)).replace("-", ":").lower()
             match_set = frozenset(match.items())
             exact_mask_match = {}
             for field, value in match.items():
-                if isinstance(value, str) and '/' not in value:
+                if isinstance(value, str) and "/" not in value:
                     value_mac = None
                     value_ip = None
                     try:
                         value_mac = netaddr.EUI(value)
                         value_ip = ipaddress.ip_address(value)
                     except (ValueError, netaddr.core.AddrFormatError):
                         pass
                     if value_mac:
-                        value = '/'.join((value, mac_exact))
+                        value = "/".join((value, mac_exact))
                     elif value_ip:
-                        ip_exact = str(ipaddress.ip_address(2**value_ip.max_prefixlen - 1))
-                        value = '/'.join((value, ip_exact))
+                        ip_exact = str(
+                            ipaddress.ip_address(2**value_ip.max_prefixlen - 1)
+                        )
+                        value = "/".join((value, ip_exact))
                 exact_mask_match[field] = value
             exact_mask_match_set = frozenset(exact_mask_match.items())
         actions_set = None
         if actions:
             actions_set = frozenset(actions)
 
         for _ in range(timeout):
             flow_dicts = []
             if ofa_match:
                 flow_dump = self.get_all_flows_from_dpid(dpid, table_id, match=match)
             else:
                 flow_dump = self.get_all_flows_from_dpid(dpid, table_id)
-            with open(flowdump, 'w', encoding='utf-8') as flowdump_file:
+            with open(flowdump, "w", encoding="utf-8") as flowdump_file:
                 flowdump_file.write(str(flow_dump))
             for flow_dict in flow_dump:
-                if (cookie is not None
-                        and cookie != flow_dict['cookie']):
+                if cookie is not None and cookie != flow_dict["cookie"]:
                     continue
                 if hard_timeout:
-                    if 'hard_timeout' not in flow_dict:
+                    if "hard_timeout" not in flow_dict:
                         continue
-                    if flow_dict['hard_timeout'] < hard_timeout:
+                    if flow_dict["hard_timeout"] < hard_timeout:
                         continue
                 if actions is not None:
-                    flow_actions_set = frozenset(flow_dict['actions'])
+                    flow_actions_set = frozenset(flow_dict["actions"])
                     if actions:
                         if not actions_set.issubset(  # pytype: disable=attribute-error
-                                flow_actions_set):
+                            flow_actions_set
+                        ):
                             continue
                     else:
-                        if flow_dict['actions']:
+                        if flow_dict["actions"]:
                             continue
                 if not ofa_match and match is not None:
-                    flow_match_set = frozenset(flow_dict['match'].items())
+                    flow_match_set = frozenset(flow_dict["match"].items())
                     # pytype: disable=attribute-error
-                    if not (match_set.issubset(flow_match_set)
-                            or exact_mask_match_set.issubset(flow_match_set)):
+                    if not (
+                        match_set.issubset(flow_match_set)
+                        or exact_mask_match_set.issubset(flow_match_set)
+                    ):
                         continue
                     # pytype: enable=attribute-error
                 flow_dicts.append(flow_dict)
             if flow_dicts:
                 return flow_dicts
             time.sleep(1)
         return flow_dicts
 
-    def get_matching_flow_on_dpid(self, dpid, match, table_id, timeout=10,
-                                  actions=None, hard_timeout=0, cookie=None,
-                                  ofa_match=True):
+    def get_matching_flow_on_dpid(
+        self,
+        dpid,
+        match,
+        table_id,
+        timeout=10,
+        actions=None,
+        hard_timeout=0,
+        cookie=None,
+        ofa_match=True,
+    ):
         flow_dicts = self.get_matching_flows_on_dpid(
-            dpid, match, table_id, timeout=timeout,
-            actions=actions, hard_timeout=hard_timeout, cookie=cookie,
-            ofa_match=ofa_match)
+            dpid,
+            match,
+            table_id,
+            timeout=timeout,
+            actions=actions,
+            hard_timeout=hard_timeout,
+            cookie=cookie,
+            ofa_match=ofa_match,
+        )
         if flow_dicts:
             return flow_dicts[0]
         return []
 
-    def get_matching_flow(self, match, table_id, timeout=10,
-                          actions=None, hard_timeout=0,
-                          cookie=None, ofa_match=True):
+    def get_matching_flow(
+        self,
+        match,
+        table_id,
+        timeout=10,
+        actions=None,
+        hard_timeout=0,
+        cookie=None,
+        ofa_match=True,
+    ):
         return self.get_matching_flow_on_dpid(
-            self.dpid, match, table_id, timeout=timeout,
-            actions=actions, hard_timeout=hard_timeout,
-            cookie=cookie, ofa_match=ofa_match)
+            self.dpid,
+            match,
+            table_id,
+            timeout=timeout,
+            actions=actions,
+            hard_timeout=hard_timeout,
+            cookie=cookie,
+            ofa_match=ofa_match,
+        )
 
     def get_group_id_for_matching_flow(self, match, table_id, timeout=10):
         for _ in range(timeout):
             flow_dict = self.get_matching_flow(match, table_id, timeout=timeout)
             if flow_dict:
-                for action in flow_dict['actions']:
-                    if action.startswith('GROUP'):
-                        _, group_id = action.split(':')
+                for action in flow_dict["actions"]:
+                    if action.startswith("GROUP"):
+                        _, group_id = action.split(":")
                         return int(group_id)
             time.sleep(1)
         return None
 
-    def matching_flow_present_on_dpid(self, dpid, match, table_id, timeout=10,
-                                      actions=None, hard_timeout=0, cookie=None,
-                                      ofa_match=True):
+    def matching_flow_present_on_dpid(
+        self,
+        dpid,
+        match,
+        table_id,
+        timeout=10,
+        actions=None,
+        hard_timeout=0,
+        cookie=None,
+        ofa_match=True,
+    ):
         """Return True if matching flow is present on a DPID."""
         return self.get_matching_flow_on_dpid(
-            dpid, match, table_id, timeout=timeout,
-            actions=actions, hard_timeout=hard_timeout, cookie=cookie,
-            ofa_match=ofa_match)
-
-    def matching_flow_present(self, match, table_id, timeout=10,
-                              actions=None, hard_timeout=0, cookie=None,
-                              ofa_match=True):
+            dpid,
+            match,
+            table_id,
+            timeout=timeout,
+            actions=actions,
+            hard_timeout=hard_timeout,
+            cookie=cookie,
+            ofa_match=ofa_match,
+        )
+
+    def matching_flow_present(
+        self,
+        match,
+        table_id,
+        timeout=10,
+        actions=None,
+        hard_timeout=0,
+        cookie=None,
+        ofa_match=True,
+    ):
         """Return True if matching flow is present on default DPID."""
         return self.matching_flow_present_on_dpid(
-            self.dpid, match, table_id, timeout=timeout,
-            actions=actions, hard_timeout=hard_timeout, cookie=cookie,
-            ofa_match=ofa_match)
-
-    def wait_until_matching_flow(self, match, table_id, timeout=10,
-                                 actions=None, hard_timeout=0, cookie=None,
-                                 ofa_match=True, dpid=None):
+            self.dpid,
+            match,
+            table_id,
+            timeout=timeout,
+            actions=actions,
+            hard_timeout=hard_timeout,
+            cookie=cookie,
+            ofa_match=ofa_match,
+        )
+
+    def wait_until_matching_flow(
+        self,
+        match,
+        table_id,
+        timeout=10,
+        actions=None,
+        hard_timeout=0,
+        cookie=None,
+        ofa_match=True,
+        dpid=None,
+    ):
         """Wait (require) for flow to be present on default DPID."""
         if dpid is None:
             dpid = self.dpid
         self.assertTrue(
             self.matching_flow_present_on_dpid(
-                dpid, match, table_id, timeout=timeout,
-                actions=actions, hard_timeout=hard_timeout, cookie=cookie,
-                ofa_match=ofa_match),
-            msg=(f'match: {match} table_id: {table_id} actions: {actions}'))
-
-    def wait_until_no_matching_flow(self, match, table_id, timeout=10,
-                                    actions=None, hard_timeout=0, cookie=None,
-                                    ofa_match=True, dpid=None):
+                dpid,
+                match,
+                table_id,
+                timeout=timeout,
+                actions=actions,
+                hard_timeout=hard_timeout,
+                cookie=cookie,
+                ofa_match=ofa_match,
+            ),
+            msg=("match: %s table_id: %u actions: %s" % (match, table_id, actions)),
+        )
+
+    def wait_until_no_matching_flow(
+        self,
+        match,
+        table_id,
+        timeout=10,
+        actions=None,
+        hard_timeout=0,
+        cookie=None,
+        ofa_match=True,
+        dpid=None,
+    ):
         """Wait for a flow not to be present."""
         if dpid is None:
             dpid = self.dpid
         for _ in range(timeout):
             matching_flow = self.matching_flow_present_on_dpid(
-                dpid, match, table_id, timeout=1,
-                actions=actions, hard_timeout=hard_timeout, cookie=cookie,
-                ofa_match=ofa_match)
+                dpid,
+                match,
+                table_id,
+                timeout=1,
+                actions=actions,
+                hard_timeout=hard_timeout,
+                cookie=cookie,
+                ofa_match=ofa_match,
+            )
             if not matching_flow:
                 return
-        self.fail('%s present' % matching_flow)
+        self.fail("%s present" % matching_flow)
 
     def wait_until_controller_flow(self):
         self.wait_until_matching_flow(
-            None, table_id=self._ETH_SRC_TABLE, actions=['OUTPUT:CONTROLLER'])
+            None, table_id=self._ETH_SRC_TABLE, actions=["OUTPUT:CONTROLLER"]
+        )
 
     def mac_learned(self, mac, timeout=10, in_port=None, hard_timeout=1):
         """Return True if a MAC has been learned on default DPID."""
         for eth_field, table_id in (
-                ('dl_src', self._ETH_SRC_TABLE),
-                ('dl_dst', self._ETH_DST_TABLE)):
-            match = {eth_field: f'{mac}'}
+            ("dl_src", self._ETH_SRC_TABLE),
+            ("dl_dst", self._ETH_DST_TABLE),
+        ):
+            match = {eth_field: "%s" % mac}
             match_hard_timeout = 0
             if table_id == self._ETH_SRC_TABLE:
                 if in_port is not None:
-                    match['in_port'] = in_port
+                    match["in_port"] = in_port
                 match_hard_timeout = hard_timeout
             if not self.matching_flow_present(
-                    match, table_id, timeout=timeout, hard_timeout=match_hard_timeout):
+                match, table_id, timeout=timeout, hard_timeout=match_hard_timeout
+            ):
                 return False
         return True
 
     def scrape_port_counters(self, ports, port_vars):
         """Scrape Gauge for list of ports and list of variables."""
         port_counters = {port: {} for port in ports}
         for port in ports:
             port_labels = self.port_labels(self.port_map[port])
             for port_var in port_vars:
                 val = self.scrape_prometheus_var(
-                    port_var, labels=port_labels,
-                    controller=self.gauge_controller.name, dpid=True, retries=3)
-                self.assertIsNotNone(val, f'{port_var} missing for port {port}')
+                    port_var,
+                    labels=port_labels,
+                    controller=self.gauge_controller.name,
+                    dpid=True,
+                    retries=3,
+                )
+                self.assertIsNotNone(val, "%s missing for port %s" % (port_var, port))
                 port_counters[port][port_var] = val
             # Require port to be up and reporting non-zero speed.
             speed = self.scrape_prometheus_var(
-                'of_port_curr_speed', labels=port_labels,
-                controller=self.gauge_controller.name, retries=3)
-            self.assertTrue(speed and speed > 0, msg=f'of_port_curr_speed {port_labels}: {speed}')
+                "of_port_curr_speed",
+                labels=port_labels,
+                controller=self.gauge_controller.name,
+                retries=3,
+            )
+            self.assertTrue(
+                speed and speed > 0,
+                msg="%s %s: %s" % ("of_port_curr_speed", port_labels, speed),
+            )
             state = self.scrape_prometheus_var(
-                'of_port_state', labels=port_labels,
-                controller=self.gauge_controller.name, retries=3)
-            self.assertFalse(state & ofp.OFPPS_LINK_DOWN, msg=f'of_port_state {port_labels}: {state}')
+                "of_port_state",
+                labels=port_labels,
+                controller=self.gauge_controller.name,
+                retries=3,
+            )
+            self.assertFalse(
+                state & ofp.OFPPS_LINK_DOWN,
+                msg="%s %s: %s" % ("of_port_state", port_labels, state),
+            )
         return port_counters
 
     def wait_ports_updating(self, ports, port_vars, stimulate_counters_func=None):
         """Return True if list of ports have list of variables all updated."""
         if stimulate_counters_func is None:
             stimulate_counters_func = self.ping_all_when_learned
         ports_not_updated = set(ports)
@@ -1341,118 +1577,130 @@
             if ports_not_updated:
                 time.sleep(1)
             else:
                 break
 
         end_time = time.time()
 
-        error(f'counter latency up to {end_time - start_time} sec\n')
+        error("counter latency up to %u sec\n" % (end_time - start_time))
         return not ports_not_updated
 
     @staticmethod
     def mac_as_int(mac):
-        return int(mac.replace(':', ''), 16)
+        return int(mac.replace(":", ""), 16)
 
     @staticmethod
     def mac_from_int(mac_int):
-        mac_int_str = '%012x' % int(mac_int)
-        return ':'.join(mac_int_str[i:i + 2] for i in range(0, len(mac_int_str), 2))
+        mac_int_str = "%012x" % int(mac_int)
+        return ":".join(mac_int_str[i : i + 2] for i in range(0, len(mac_int_str), 2))
 
     def prom_macs_learned(self, port=None, vlan=None):
         labels = {
-            'n': r'\d+',
-            'port': r'b\d+',
-            'vlan': r'\d+',
+            "n": r"\d+",
+            "port": r"b\d+",
+            "vlan": r"\d+",
         }
         if port:
             labels.update(self.port_labels(port))
         if vlan:
-            labels['vlan'] = str(vlan)
+            labels["vlan"] = str(vlan)
         port_learned_macs_prom = self.scrape_prometheus_var(
-            'learned_macs', labels=labels, default=[], multiple=True, dpid=True)
-        macs = [self.mac_from_int(mac_int) for _, mac_int in port_learned_macs_prom if mac_int]
+            "learned_macs", labels=labels, default=[], multiple=True, dpid=True
+        )
+        macs = [
+            self.mac_from_int(mac_int)
+            for _, mac_int in port_learned_macs_prom
+            if mac_int
+        ]
         return macs
 
     def prom_mac_learned(self, mac, port=None, vlan=None):
         return mac in self.prom_macs_learned(port=port, vlan=vlan)
 
     def host_learned(self, host, timeout=10, in_port=None, hard_timeout=1):
         """Return True if a host has been learned on default DPID."""
         return self.mac_learned(host.MAC(), timeout, in_port, hard_timeout=hard_timeout)
 
     @staticmethod
     def get_host_intf_mac(host, intf):
-        return host.cmd(f'cat /sys/class/net/{intf}/address').strip()
+        return host.cmd("cat /sys/class/net/%s/address" % intf).strip()
 
     def get_host_netns(self, host):
         hostns = self.hostns(host)
-        nses = [netns.split()[0] for netns in host.cmd('ip netns list').splitlines()]
+        nses = [netns.split()[0] for netns in host.cmd("ip netns list").splitlines()]
         return hostns in nses
 
     @staticmethod
     def host_ip(host, family, family_re):
         host_ip_cmd = (
-            r'ip -o -f %s addr show %s|'
-            'grep -m 1 -Eo "%s %s"|cut -f2 -d " "' % (
-                family,
-                host.defaultIntf(),
-                family,
-                family_re))
+            r"ip -o -f %s addr show %s|"
+            'grep -m 1 -Eo "%s %s"|cut -f2 -d " "'
+            % (family, host.defaultIntf(), family, family_re)
+        )
         return host.cmd(host_ip_cmd).strip()
 
     def host_ipv4(self, host):
         """Return first IPv4/netmask for host's default interface."""
-        return self.host_ip(host, 'inet', r'[0-9\\.]+\/[0-9]+')
+        return self.host_ip(host, "inet", r"[0-9\\.]+\/[0-9]+")
 
     def host_ipv6(self, host):
         """Return first IPv6/netmask for host's default interface."""
-        return self.host_ip(host, 'inet6', r'[0-9a-f\:]+\/[0-9]+')
+        return self.host_ip(host, "inet6", r"[0-9a-f\:]+\/[0-9]+")
 
     @staticmethod
     def reset_ipv4_prefix(host, prefix=24):
         host.setIP(host.IP(), prefixLen=prefix)
 
     def reset_all_ipv4_prefix(self, prefix=24):
         for host in self.hosts_name_ordered():
             self.reset_ipv4_prefix(host, prefix)
 
     def stimulate_host_learn(self, host):
-        unicast_learn_cli = self.scapy_dhcp(host.MAC(), host.defaultIntf(), dst=self.FAUCET_MAC)
+        unicast_learn_cli = self.scapy_dhcp(
+            host.MAC(), host.defaultIntf(), dst=self.FAUCET_MAC
+        )
         bcast_learn_cli = self.scapy_dhcp(host.MAC(), host.defaultIntf())
         results = []
         for learn_cli in (unicast_learn_cli, bcast_learn_cli):
             results.append(host.cmd(learn_cli))
-        return ' '.join(results)
+        return " ".join(results)
 
     def require_host_learned(self, host, retries=8, in_port=None, hard_timeout=1):
         """Require a host be learned on default DPID."""
         for _ in range(retries):
-            if self.host_learned(host, timeout=1, in_port=in_port, hard_timeout=hard_timeout):
+            if self.host_learned(
+                host, timeout=1, in_port=in_port, hard_timeout=hard_timeout
+            ):
                 return
             learn_result = self.stimulate_host_learn(host)
-        self.fail(f'Could not learn host {host} ({host.MAC()}): {learn_result}')
+        self.fail("Could not learn host %s (%s): %s" % (host, host.MAC(), learn_result))
 
     def get_prom_port(self, controller=None):
         if controller is None:
             controller = self.faucet_controllers[0].name
-        return int(self.env[controller]['FAUCET_PROMETHEUS_PORT'])
+        return int(self.env[controller]["FAUCET_PROMETHEUS_PORT"])
 
     def get_prom_addr(self, controller=None):
         if controller is None:
             controller = self.faucet_controllers[0].name
-        return self.env[controller]['FAUCET_PROMETHEUS_ADDR']
+        return self.env[controller]["FAUCET_PROMETHEUS_ADDR"]
 
     def _prometheus_url(self, controller):
-        if 'faucet' in controller:
-            return f'http://[{self.get_prom_addr()}]:{self.get_prom_port()}'
-        if 'gauge' in controller:
-            return f'http://[{self.get_prom_addr()}]:{self.config_ports["gauge_prom_port"]}'
+        if "faucet" in controller:
+            return "http://[%s]:%u" % (self.get_prom_addr(), self.get_prom_port())
+        if "gauge" in controller:
+            return "http://[%s]:%u" % (
+                self.get_prom_addr(),
+                self.config_ports["gauge_prom_port"],
+            )
         raise NotImplementedError
 
-    def scrape_prometheus(self, controller=None, timeout=15, var=None, verify_consistent=False):
+    def scrape_prometheus(
+        self, controller=None, timeout=15, var=None, verify_consistent=False
+    ):
         """
         Obtain prometheus statistics
 
         Args:
             controller (str): name of the controller for the prometheus variable to scrape for
             timeout (int): Timeout for scrape request
             var (str): Variable to match on & return
@@ -1467,24 +1715,33 @@
         else:
             controller_iter = self.gauge_controllers
         for cont in controller_iter:
             controller_name = cont.name
             url = self._prometheus_url(controller_name)
             try:
                 prom_raw = requests.get(url, {}, timeout=timeout).text
-            except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
+            except (
+                requests.exceptions.ConnectionError,
+                requests.exceptions.ReadTimeout,
+            ):
                 return []
-            log_filename = os.path.join(self.tmpdir, f'{controller_name}-prometheus.log')
-            with open(log_filename, 'w', encoding='utf-8') as prom_log:
+            log_filename = os.path.join(
+                self.tmpdir, "%s-prometheus.log" % controller_name
+            )
+            with open(log_filename, "w", encoding="utf-8") as prom_log:
                 prom_log.write(prom_raw)
             prom_lines = [
-                prom_line for prom_line in prom_raw.splitlines() if not prom_line.startswith('#')]
+                prom_line
+                for prom_line in prom_raw.splitlines()
+                if not prom_line.startswith("#")
+            ]
             if var:
                 prom_lines = [
-                    prom_line for prom_line in prom_lines if prom_line.startswith(var)]
+                    prom_line for prom_line in prom_lines if prom_line.startswith(var)
+                ]
             all_prom_lines.append(prom_lines)
         if verify_consistent:
             self.verify_prom_var(all_prom_lines)
         cont = self.net.get(controller)
         index = controller_iter.index(cont)
         return all_prom_lines[index]
 
@@ -1508,45 +1765,75 @@
                     self.assertIsNotNone(match_a)
                     self.assertIsNotNone(match_b)
                     var_a = match_a.group(1)
                     var_b = match_b.group(1)
                     self.assertEqual(var_a, var_b)
                     val_a = int(float(match_a.group(2)))
                     val_b = int(float(match_b.group(2)))
-                    self.assertEqual(val_a, val_b, msg=f'{prom_line_a} {prom_line_b} inconsistent')
+                    self.assertEqual(
+                        val_a,
+                        val_b,
+                        msg="%s %s inconsistent" % (prom_line_a, prom_line_b),
+                    )
 
     def parse_prom_var(self, prom_line):
         """Parse prometheus variable, return tuple of variable name, variable value"""
         prom_line_match = self._PROM_LINE_RE.match(prom_line)
         self.assertIsNotNone(
-            prom_line_match,
-            msg=f'Invalid prometheus line {prom_line}')
+            prom_line_match, msg="Invalid prometheus line %s" % prom_line
+        )
         prom_var = prom_line_match.group(1)
         prom_val = int(float(prom_line_match.group(2)))
         return (prom_var, prom_val)
 
-    def wait_for_prometheus_var(self, var, result_wanted, labels=None,
-                                any_labels=False, default=None, dpid=True,
-                                multiple=False, controller=None, retries=3,
-                                timeout=5, orgreater=False):
+    def wait_for_prometheus_var(
+        self,
+        var,
+        result_wanted,
+        labels=None,
+        any_labels=False,
+        default=None,
+        dpid=True,
+        multiple=False,
+        controller=None,
+        retries=3,
+        timeout=5,
+        orgreater=False,
+    ):
         if controller is None:
             controller = self.faucet_controllers[0].name
         for _ in range(timeout):
             result = self.scrape_prometheus_var(
-                var, labels=labels, any_labels=any_labels, default=default,
-                dpid=dpid, multiple=multiple, controller=controller, retries=retries)
+                var,
+                labels=labels,
+                any_labels=any_labels,
+                default=default,
+                dpid=dpid,
+                multiple=multiple,
+                controller=controller,
+                retries=retries,
+            )
             if result == result_wanted:
                 return True
             if orgreater and result is not None and result > result_wanted:
                 return True
             time.sleep(1)
         return False
 
-    def scrape_prometheus_var(self, var, labels=None, any_labels=False, default=None,
-                              dpid=True, multiple=False, controller=None, retries=3):
+    def scrape_prometheus_var(
+        self,
+        var,
+        labels=None,
+        any_labels=False,
+        default=None,
+        dpid=True,
+        multiple=False,
+        controller=None,
+        retries=3,
+    ):
         """
         Return parsed, prometheus variable
 
         Args:
             var (str): Prometheus variable to scrape for
             labels (dict): Labels to apply for the variable search
             any_labels (bool): Wildcard label match
@@ -1563,28 +1850,28 @@
                 dpid = int(self.dpid)
             else:
                 dpid = int(dpid)
         if dpid and self.dpid_names:
             dp_name = self.dpid_names[str(dpid)]
         else:
             dp_name = self.DP_NAME
-        label_values_re = r''
+        label_values_re = r""
         if any_labels:
-            label_values_re = r'\{[^\}]+\}'
+            label_values_re = r"\{[^\}]+\}"
         else:
             if labels is None:
                 labels = {}
             if dpid:
-                labels.update({'dp_id': '0x%x' % dpid, 'dp_name': dp_name})
+                labels.update({"dp_id": "0x%x" % dpid, "dp_name": dp_name})
             if labels:
                 label_values = []
                 for label, value in sorted(labels.items()):
-                    label_values.append(f'{label}="{value}"')
-                label_values_re = r'\{%s\}' % r'\S+'.join(label_values)
-        var_re = re.compile(r'^%s%s$' % (var, label_values_re))
+                    label_values.append('%s="%s"' % (label, value))
+                label_values_re = r"\{%s\}" % r"\S+".join(label_values)
+        var_re = re.compile(r"^%s%s$" % (var, label_values_re))
         for i in range(retries):
             results = []
             prom_lines = self.scrape_prometheus(controller=controller, var=var)
             for prom_line in prom_lines:
                 prom_var, prom_val = self.parse_prom_var(prom_line)
                 if var_re.match(prom_var):
                     results.append((var, prom_val))
@@ -1595,195 +1882,267 @@
                     return results
                 return results[0][1]
             if i < (retries - 1):
                 time.sleep(1)
         return default
 
     def gauge_smoke_test(self):
-        watcher_files = set([
-            self.monitor_stats_file,
-            self.monitor_state_file,
-        ])
+        watcher_files = set(
+            [
+                self.monitor_stats_file,
+                self.monitor_state_file,
+            ]
+        )
         found_watcher_files = set()
         for _ in range(60):
             for watcher_file in watcher_files:
-                if (os.path.exists(watcher_file)
-                        and os.path.getsize(watcher_file)):
+                if os.path.exists(watcher_file) and os.path.getsize(watcher_file):
                     found_watcher_files.add(watcher_file)
-            if watcher_files == found_watcher_files \
-                    and bool(os.listdir(self.monitor_flow_table_dir)):
+            if watcher_files == found_watcher_files and bool(
+                os.listdir(self.monitor_flow_table_dir)
+            ):
                 break
-            self.verify_no_exception(self.env[self.gauge_controller.name]['GAUGE_EXCEPTION_LOG'])
+            self.verify_no_exception(
+                self.env[self.gauge_controller.name]["GAUGE_EXCEPTION_LOG"]
+            )
             time.sleep(1)
             found_watcher_files = set()
         missing_watcher_files = watcher_files - found_watcher_files
         self.assertEqual(
-            missing_watcher_files, set(), msg=f'Gauge missing logs: {missing_watcher_files}')
+            missing_watcher_files,
+            set(),
+            msg="Gauge missing logs: %s" % missing_watcher_files,
+        )
         self.hup_controller(self.gauge_controller.name)
-        self.verify_no_exception(self.env[self.faucet_controllers[0].name]['FAUCET_EXCEPTION_LOG'])
+        self.verify_no_exception(
+            self.env[self.faucet_controllers[0].name]["FAUCET_EXCEPTION_LOG"]
+        )
 
     def prometheus_smoke_test(self):
-        prom_out = '\n'.join(self.scrape_prometheus())
+        prom_out = "\n".join(self.scrape_prometheus())
         for nonzero_var in (
-                r'of_packet_ins', r'of_flowmsgs_sent', r'of_dp_connections',
-                r'faucet_config\S+name=\"flood\"', r'faucet_pbr_version\S+version='):
+            r"of_packet_ins",
+            r"of_flowmsgs_sent",
+            r"of_dp_connections",
+            r"faucet_config\S+name=\"flood\"",
+            r"faucet_pbr_version\S+version=",
+        ):
             self.assertTrue(
-                re.search(r'%s\S+\s+[1-9]+' % nonzero_var, prom_out),
-                msg=f'expected {nonzero_var} to be nonzero ({prom_out})')
-        for zero_var in (
-                'of_errors', 'of_dp_disconnections'):
+                re.search(r"%s\S+\s+[1-9]+" % nonzero_var, prom_out),
+                msg="expected %s to be nonzero (%s)" % (nonzero_var, prom_out),
+            )
+        for zero_var in ("of_errors", "of_dp_disconnections"):
             self.assertTrue(
-                re.search(r'%s\S+\s+0' % zero_var, prom_out),
-                msg=f'expected {zero_var} to be present and zero ({prom_out})')
+                re.search(r"%s\S+\s+0" % zero_var, prom_out),
+                msg="expected %s to be present and zero (%s)" % (zero_var, prom_out),
+            )
 
     def get_configure_count(self, retries=5, controller=None):
         """Return the number of times FAUCET has processed a reload request."""
         if controller is None:
             controller = self.faucet_controllers[0].name
         for _ in range(retries):
             count = self.scrape_prometheus_var(
-                'faucet_config_reload_requests_total',
-                dpid=False, controller=controller)
+                "faucet_config_reload_requests_total", dpid=False, controller=controller
+            )
             if count:
                 break
             time.sleep(1)
-        self.assertTrue(count, msg='configure count stayed zero')
+        self.assertTrue(count, msg="configure count stayed zero")
         return count
 
     def hup_controller(self, controller=None):
         """Send a HUP signal to the controller."""
         if controller is None:
             controller = self.faucet_controllers[0].name
         cont_obj = self.net.get(controller)
-        self.assertTrue(
-            self._signal_proc_on_port(cont_obj, int(cont_obj.port), 1))
-
-    def reload_conf(self, yaml_conf, conf_path, restart, cold_start,
-                    change_expected=True, host_cache=None, hup=True, dpid=True):
+        self.assertTrue(self._signal_proc_on_port(cont_obj, int(cont_obj.port), 1))
 
+    def reload_conf(
+        self,
+        yaml_conf,
+        conf_path,
+        restart,
+        cold_start,
+        change_expected=True,
+        host_cache=None,
+        hup=True,
+        dpid=True,
+    ):
         def _update_conf(conf_path, yaml_conf):
             if yaml_conf:
                 yaml_conf = self._annotate_interfaces_conf(yaml_conf)
                 self._write_yaml_conf(conf_path, yaml_conf)
 
         update_conf_func = partial(_update_conf, conf_path, yaml_conf)
         verify_faucet_reconf_func = partial(
             self.verify_faucet_reconf,
             cold_start=cold_start,
             change_expected=change_expected,
-            reconf_funcs=[update_conf_func], hup=hup, dpid=dpid)
+            reconf_funcs=[update_conf_func],
+            hup=hup,
+            dpid=dpid,
+        )
 
         if restart:
             if host_cache:
                 vlan_labels = dict(vlan=host_cache)
-                old_mac_table = sorted(self.scrape_prometheus_var(
-                    'learned_macs', labels=vlan_labels, multiple=True, default=[], dpid=dpid))
+                old_mac_table = sorted(
+                    self.scrape_prometheus_var(
+                        "learned_macs",
+                        labels=vlan_labels,
+                        multiple=True,
+                        default=[],
+                        dpid=dpid,
+                    )
+                )
                 verify_faucet_reconf_func()
-                new_mac_table = sorted(self.scrape_prometheus_var(
-                    'learned_macs', labels=vlan_labels, multiple=True, default=[], dpid=dpid))
+                new_mac_table = sorted(
+                    self.scrape_prometheus_var(
+                        "learned_macs",
+                        labels=vlan_labels,
+                        multiple=True,
+                        default=[],
+                        dpid=dpid,
+                    )
+                )
                 self.assertFalse(
-                    cold_start, msg='host cache is not maintained with cold start')
+                    cold_start, msg="host cache is not maintained with cold start"
+                )
                 self.assertTrue(
-                    new_mac_table, msg=f'no host cache for VLAN {host_cache}')
+                    new_mac_table, msg="no host cache for VLAN %u" % host_cache
+                )
                 self.assertEqual(
-                    old_mac_table, new_mac_table,
-                    msg=f'host cache for VLAN {host_cache} not same over reload '
-                        f'(old {old_mac_table}, new {new_mac_table})')
+                    old_mac_table,
+                    new_mac_table,
+                    msg="host cache for VLAN %u not same over reload (old %s, new %s)"
+                    % (host_cache, old_mac_table, new_mac_table),
+                )
             else:
                 verify_faucet_reconf_func()
             return
 
         update_conf_func()
 
     def coldstart_conf(self, hup=True):
         orig_conf = self._get_faucet_conf()
         cold_start_conf = copy.deepcopy(orig_conf)
-        if 'routers' in cold_start_conf:
-            del cold_start_conf['routers']
+        if "routers" in cold_start_conf:
+            del cold_start_conf["routers"]
         used_vids = set()
-        for vlan_name, vlan_conf in cold_start_conf['vlans'].items():
-            used_vids.add(vlan_conf.get('vid', vlan_name))
+        for vlan_name, vlan_conf in cold_start_conf["vlans"].items():
+            used_vids.add(vlan_conf.get("vid", vlan_name))
         unused_vids = list(set(range(2, max(used_vids))) - used_vids)
         assert len(unused_vids) >= len(self.port_map)
         # Ensure cold start by moving all ports to new, unused VLANs,
         # then back again.
-        for dp_conf in cold_start_conf['dps'].values():
-            dp_conf['interfaces'] = {
-                self.port_map[port]: {'native_vlan': unused_vids[i]}
-                for i, port in enumerate(self.port_map.keys(), start=0)}
+        for dp_conf in cold_start_conf["dps"].values():
+            dp_conf["interfaces"] = {
+                self.port_map[port]: {"native_vlan": unused_vids[i]}
+                for i, port in enumerate(self.port_map.keys(), start=0)
+            }
         for conf in (cold_start_conf, orig_conf):
             self.reload_conf(
-                conf, self.faucet_config_path,
-                restart=True, cold_start=True, hup=hup)
+                conf, self.faucet_config_path, restart=True, cold_start=True, hup=hup
+            )
 
-    def add_port_config(self, port, port_config, conf=None,
-                        restart=True, cold_start=False,
-                        hup=True):
+    def add_port_config(
+        self, port, port_config, conf=None, restart=True, cold_start=False, hup=True
+    ):
         if conf is None:
             conf = self._get_faucet_conf()
-        conf['dps'][self.DP_NAME]['interfaces'][port] = port_config
-        self.reload_conf(
-            conf, self.faucet_config_path,
-            restart, cold_start, hup=hup)
+        conf["dps"][self.DP_NAME]["interfaces"][port] = port_config
+        self.reload_conf(conf, self.faucet_config_path, restart, cold_start, hup=hup)
 
-    def change_port_config(self, port, config_name, config_value,
-                           conf=None, restart=True, cold_start=False,
-                           hup=True, change_expected=True):
+    def change_port_config(
+        self,
+        port,
+        config_name,
+        config_value,
+        conf=None,
+        restart=True,
+        cold_start=False,
+        hup=True,
+        change_expected=True,
+    ):
         if conf is None:
             conf = self._get_faucet_conf()
         if config_name is None:
-            del conf['dps'][self.DP_NAME]['interfaces'][port]
+            del conf["dps"][self.DP_NAME]["interfaces"][port]
         else:
             if config_value is None:
-                del conf['dps'][self.DP_NAME]['interfaces'][port][config_name]
+                del conf["dps"][self.DP_NAME]["interfaces"][port][config_name]
             else:
-                conf['dps'][self.DP_NAME]['interfaces'][port][config_name] = config_value
+                conf["dps"][self.DP_NAME]["interfaces"][port][
+                    config_name
+                ] = config_value
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart, cold_start, hup=hup, change_expected=change_expected)
+            conf,
+            self.faucet_config_path,
+            restart,
+            cold_start,
+            hup=hup,
+            change_expected=change_expected,
+        )
 
-    def change_vlan_config(self, vlan, config_name, config_value,
-                           conf=None, restart=True, cold_start=False,
-                           hup=True):
+    def change_vlan_config(
+        self,
+        vlan,
+        config_name,
+        config_value,
+        conf=None,
+        restart=True,
+        cold_start=False,
+        hup=True,
+    ):
         if conf is None:
             conf = self._get_faucet_conf()
-        conf['vlans'][vlan][config_name] = config_value
-        self.reload_conf(
-            conf, self.faucet_config_path,
-            restart, cold_start, hup=hup)
+        conf["vlans"][vlan][config_name] = config_value
+        self.reload_conf(conf, self.faucet_config_path, restart, cold_start, hup=hup)
 
     def ipv4_vip_bcast(self):
         return self.FAUCET_VIPV4.network.broadcast_address
 
     def verify_traveling_dhcp_mac(self, retries=10):
-        mac = '0e:00:00:00:00:ff'
+        mac = "0e:00:00:00:00:ff"
         locations = set()
         for host in self.hosts_name_ordered():
             for _ in range(retries):
                 host.cmd(self.scapy_dhcp(mac, host.defaultIntf()))
                 new_locations = set()
-                for line in self.scrape_prometheus(var='learned_macs'):
+                for line in self.scrape_prometheus(var="learned_macs"):
                     location, mac_float = self.parse_prom_var(line)
                     if self.mac_from_int(int(float(mac_float))) == mac:
                         new_locations.add(location)
                 if locations != new_locations:
                     break
                 time.sleep(1)
             # TODO: verify port/host association, not just that host moved.
             self.assertNotEqual(locations, new_locations)
             locations = new_locations
 
-    def _verify_xcast(self, received_expected, packets, tcpdump_filter, scapy_cmd, host_a, host_b):
+    def _verify_xcast(
+        self, received_expected, packets, tcpdump_filter, scapy_cmd, host_a, host_b
+    ):
         received_packets = False
         for _ in range(packets):
             tcpdump_txt = self.tcpdump_helper(
-                host_b, tcpdump_filter,
+                host_b,
+                tcpdump_filter,
                 [partial(host_a.cmd, scapy_cmd)],
-                packets=1, timeout=2)
-            msg = f'{host_a} ({host_a.MAC()}) -> {host_b} ({host_b.MAC()}): {tcpdump_txt}'
+                packets=1,
+                timeout=2,
+            )
+            msg = "%s (%s) -> %s (%s): %s" % (
+                host_a,
+                host_a.MAC(),
+                host_b,
+                host_b.MAC(),
+                tcpdump_txt,
+            )
             received_no_packets = self.tcpdump_rx_packets(tcpdump_txt, packets=0)
             received_packets = received_packets or not received_no_packets
             if received_packets:
                 if received_expected is not False:
                     return True
                 self.assertTrue(received_expected, msg=msg)
             time.sleep(1)
@@ -1794,160 +2153,202 @@
         return None
 
     def verify_broadcast(self, hosts=None, broadcast_expected=True, packets=3):
         host_a = self.hosts_name_ordered()[0]
         host_b = self.hosts_name_ordered()[-1]
         if hosts is not None:
             host_a, host_b = hosts
-        tcpdump_filter = ' and '.join((
-            'ether dst host ff:ff:ff:ff:ff:ff',
-            f'ether src host {host_a.MAC()}',
-            'udp'))
+        tcpdump_filter = " and ".join(
+            (
+                "ether dst host ff:ff:ff:ff:ff:ff",
+                "ether src host %s" % host_a.MAC(),
+                "udp",
+            )
+        )
         scapy_cmd = self.scapy_bcast(host_a, count=packets)
-        return self._verify_xcast(broadcast_expected, packets, tcpdump_filter,
-                                  scapy_cmd, host_a, host_b)
+        return self._verify_xcast(
+            broadcast_expected, packets, tcpdump_filter, scapy_cmd, host_a, host_b
+        )
 
     def verify_unicast(self, hosts, unicast_expected=True, packets=3):
         host_a = self.hosts_name_ordered()[0]
         host_b = self.hosts_name_ordered()[-1]
         if hosts is not None:
             host_a, host_b = hosts
-        tcpdump_filter = ' and '.join((
-            f'ether dst {host_b.MAC()}',
-            f'ether src {host_a.MAC()}',
-            'udp'))
+        tcpdump_filter = " and ".join(
+            ("ether dst %s" % host_b.MAC(), "ether src %s" % host_a.MAC(), "udp")
+        )
         scapy_cmd = self.scapy_template(
-            (f'Ether(src=\'{host_a.MAC()}\', dst=\'{host_b.MAC()}\', type={IPV4_ETH}) / '
-             f'IP(src=\'{host_a.IP()}\', dst=\'{host_b.IP()}\') / UDP(dport=67,sport=68)'),
-            host_a.defaultIntf(), count=packets)
-        return self._verify_xcast(unicast_expected, packets, tcpdump_filter,
-                                  scapy_cmd, host_a, host_b)
+            (
+                "Ether(src='%s', dst='%s', type=%u) / "
+                "IP(src='%s', dst='%s') / UDP(dport=67,sport=68)"
+            )
+            % (host_a.MAC(), host_b.MAC(), IPV4_ETH, host_a.IP(), host_b.IP()),
+            host_a.defaultIntf(),
+            count=packets,
+        )
+        return self._verify_xcast(
+            unicast_expected, packets, tcpdump_filter, scapy_cmd, host_a, host_b
+        )
 
     def verify_empty_caps(self, cap_files):
         cap_file_cmds = [
-            'tcpdump -n -v -A -r %s 2> /dev/null' % cap_file for cap_file in cap_files]
+            "tcpdump -n -v -A -r %s 2> /dev/null" % cap_file for cap_file in cap_files
+        ]
         self.quiet_commands(self.net.controllers[0], cap_file_cmds)
 
     def verify_no_bcast_to_self(self, timeout=3):
         bcast_cap_files = []
         tcpdump_timeout = timeout * len(self.hosts_name_ordered()) * 2
         for host in self.hosts_name_ordered():
-            tcpdump_filter = f'-Q in ether src {host.MAC()}'
-            bcast_cap_file = os.path.join(self.tmpdir, f'{host}-bcast.cap')
+            tcpdump_filter = "-Q in ether src %s" % host.MAC()
+            bcast_cap_file = os.path.join(self.tmpdir, "%s-bcast.cap" % host)
             bcast_cap_files.append(bcast_cap_file)
-            host.cmd(mininet_test_util.timeout_cmd(
-                f'tcpdump -U -n -c 1 -i {host.defaultIntf()} -w {bcast_cap_file} {tcpdump_filter} &',
-                tcpdump_timeout))
+            host.cmd(
+                mininet_test_util.timeout_cmd(
+                    "tcpdump -U -n -c 1 -i %s -w %s %s &"
+                    % (host.defaultIntf(), bcast_cap_file, tcpdump_filter),
+                    tcpdump_timeout,
+                )
+            )
         for host in self.hosts_name_ordered():
             for bcast_cmd in (
-                    (f'ndisc6 -w1 fe80::1 {host.defaultIntf()}'),
-                    (f'ping -b -i0.1 -c3 {self.ipv4_vip_bcast()}')):
+                ("ndisc6 -w1 fe80::1 %s" % host.defaultIntf()),
+                ("ping -b -i0.1 -c3 %s" % self.ipv4_vip_bcast()),
+            ):
                 host.cmd(mininet_test_util.timeout_cmd(bcast_cmd, timeout))
         self.verify_empty_caps(bcast_cap_files)
 
     def verify_unicast_not_looped(self, packets=3):
-        unicast_mac1 = '0e:00:00:00:00:02'
-        unicast_mac2 = '0e:00:00:00:00:03'
+        unicast_mac1 = "0e:00:00:00:00:02"
+        unicast_mac2 = "0e:00:00:00:00:03"
         hello_template = (
-            'Ether(src=\'%s\', dst=\'%s\')/'
-            'IP(src=\'10.0.0.100\', dst=\'10.0.0.255\')/'
-            'UDP(dport=9)/'
-            'b\'hello\'')
-        tcpdump_filter = f'-Q in ether src {unicast_mac1}'
+            "Ether(src='%s', dst='%s')/"
+            "IP(src='10.0.0.100', dst='10.0.0.255')/"
+            "UDP(dport=9)/"
+            "b'hello'"
+        )
+        tcpdump_filter = "-Q in ether src %s" % unicast_mac1
         for host in self.hosts_name_ordered():
             host.cmd(
                 self.scapy_template(
-                    hello_template % (unicast_mac1, 'ff:ff:ff:ff:ff:ff'),
-                    host.defaultIntf()))
+                    hello_template % (unicast_mac1, "ff:ff:ff:ff:ff:ff"),
+                    host.defaultIntf(),
+                )
+            )
             host.cmd(
                 self.scapy_template(
-                    hello_template % (unicast_mac2, 'ff:ff:ff:ff:ff:ff'),
-                    host.defaultIntf()))
+                    hello_template % (unicast_mac2, "ff:ff:ff:ff:ff:ff"),
+                    host.defaultIntf(),
+                )
+            )
             tcpdump_txt = self.tcpdump_helper(
-                host, tcpdump_filter, [
-                    partial(host.cmd, (
-                        self.scapy_template(
-                            hello_template % (unicast_mac1, unicast_mac2),
-                            host.defaultIntf(),
-                            count=packets)))],
-                timeout=(packets - 1), vflags='-vv', packets=1)
+                host,
+                tcpdump_filter,
+                [
+                    partial(
+                        host.cmd,
+                        (
+                            self.scapy_template(
+                                hello_template % (unicast_mac1, unicast_mac2),
+                                host.defaultIntf(),
+                                count=packets,
+                            )
+                        ),
+                    )
+                ],
+                timeout=(packets - 1),
+                vflags="-vv",
+                packets=1,
+            )
             self.verify_no_packets(tcpdump_txt)
 
-    def verify_controller_fping(self, host, faucet_vip,
-                                total_packets=100, packet_interval_ms=100, size=64):
-        fping_bin = 'fping'
+    def verify_controller_fping(
+        self, host, faucet_vip, total_packets=100, packet_interval_ms=100, size=64
+    ):
+        fping_bin = "fping"
         if faucet_vip.version == 6:
-            fping_bin = 'fping6'
-        fping_cli = f'{fping_bin} {self.FPING_ARGS_SHORT} -b {size}' \
-                    f' -c {total_packets} -i {packet_interval_ms} {faucet_vip.ip}'
+            fping_bin = "fping6"
+        fping_cli = "%s %s -b %u -c %u -i %u %s" % (
+            fping_bin,
+            self.FPING_ARGS_SHORT,
+            size,
+            total_packets,
+            packet_interval_ms,
+            faucet_vip.ip,
+        )
         timeout = int(((1000.0 / packet_interval_ms) * total_packets) * 1.5)
-        fping_out = host.cmd(mininet_test_util.timeout_cmd(
-            fping_cli, timeout))
-        error(f'{self._test_name()}: {fping_out}')
+        fping_out = host.cmd(mininet_test_util.timeout_cmd(fping_cli, timeout))
+        error("%s: %s" % (self._test_name(), fping_out))
         self.assertTrue(
-            re.search(r'\s+[1-9][0-9]* ICMP Echo Replies received', fping_out),
-            msg=fping_out)
+            re.search(r"\s+[1-9][0-9]* ICMP Echo Replies received", fping_out),
+            msg=fping_out,
+        )
 
     def verify_learn_counters(self, vlan, ports, verify_neighbors=False):
         # Need to synchronize with stats update thread.
         for _ in range(7):
             vlan_hosts_learned = self.scrape_prometheus_var(
-                'vlan_hosts_learned',
-                {'vlan': str(vlan)})
+                "vlan_hosts_learned", {"vlan": str(vlan)}
+            )
             port_vlan_hosts_learned = 0
             prom_macs_learned = 0
             for port in ports:
-                port_no = self.port_map[f'port_{port}']
-                labels = {'vlan': str(vlan)}
+                port_no = self.port_map["port_%u" % port]
+                labels = {"vlan": str(vlan)}
                 labels.update(self.port_labels(port_no))
                 port_vlan_hosts_learned += self.scrape_prometheus_var(
-                    'port_vlan_hosts_learned', labels, default=0)
-                prom_macs_learned += len(self.prom_macs_learned(
-                    vlan=vlan, port=port_no))
-            if (vlan_hosts_learned == port_vlan_hosts_learned
-                    and vlan_hosts_learned == prom_macs_learned):
+                    "port_vlan_hosts_learned", labels, default=0
+                )
+                prom_macs_learned += len(
+                    self.prom_macs_learned(vlan=vlan, port=port_no)
+                )
+            if (
+                vlan_hosts_learned == port_vlan_hosts_learned
+                and vlan_hosts_learned == prom_macs_learned
+            ):
                 break
             time.sleep(1)
         self.assertEqual(vlan_hosts_learned, port_vlan_hosts_learned)
         self.assertEqual(vlan_hosts_learned, prom_macs_learned)
         if verify_neighbors:
             vlan_neighbors = self.scrape_prometheus_var(
-                'vlan_neighbors',
-                {'vlan': str(vlan)})
+                "vlan_neighbors", {"vlan": str(vlan)}
+            )
             self.assertEqual(vlan_hosts_learned, vlan_neighbors)
         return vlan_hosts_learned
 
     def verify_learning(self, test_net, learn_ip, min_hosts, max_hosts, learn_pps=20):
-
         # TODO: test environment is pretty hard on test host, with this many macvlans
         def simplify_intf_conf(host, intf):
             for conf_cmd in (
-                    'echo 1 > /proc/sys/net/ipv6/conf/%s/disable_ipv6',
-                    'echo 300 > /proc/sys/net/ipv4/neigh/%s/gc_stale_time',
-                    'ip link set dev %s arp off',):
-                self.assertEqual('', host.cmd(conf_cmd % intf))
+                "echo 1 > /proc/sys/net/ipv6/conf/%s/disable_ipv6",
+                "echo 300 > /proc/sys/net/ipv4/neigh/%s/gc_stale_time",
+                "ip link set dev %s arp off",
+            ):
+                self.assertEqual("", host.cmd(conf_cmd % intf))
 
         def generate_test_ipas():
             test_ipas = []
             for ipa in sorted(test_net.hosts()):
-                if str(ipa).endswith('.0'):
+                if str(ipa).endswith(".0"):
                     continue
-                if str(ipa).endswith('.255'):
+                if str(ipa).endswith(".255"):
                     continue
                 test_ipas.append(ipa)
                 if len(test_ipas) == max_hosts + len(self.hosts_name_ordered()):
                     break
-            base_ipas = test_ipas[-len(self.hosts_name_ordered()):]
+            base_ipas = test_ipas[-len(self.hosts_name_ordered()) :]
             return (base_ipas, test_ipas)
 
         def generate_mac_intfs(test_ipas, other_hosts):
             mac_intf_ipv4s = []
             for i in range(0, max_hosts):
                 host = other_hosts[i % len(other_hosts)]
-                mac_intf = f'mac{i}'
+                mac_intf = "mac%u" % i
                 mac_ipv4 = str(test_ipas[i])
                 mac_intf_ipv4s.append((host, mac_intf, mac_ipv4))
             return mac_intf_ipv4s
 
         first_host = self.hosts_name_ordered()[0]
         other_hosts = self.hosts_name_ordered()[1:]
 
@@ -1957,420 +2358,550 @@
         for i, host in enumerate(self.hosts_name_ordered()):
             host.setIP(str(base_ipas[i]), prefixLen=test_net.prefixlen)
         self.ping_all_when_learned()
 
         learn_hosts = min_hosts
         successful_learn_hosts = 0
 
-        fping_prefix = f'fping {self.FPING_ARGS_SHORT} -q -c 1'
+        fping_prefix = "fping %s -q -c 1" % self.FPING_ARGS_SHORT
         pps_ms = 1e3 / learn_pps
         while learn_hosts <= max_hosts and successful_learn_hosts < max_hosts:
-            error(f'will learn {learn_hosts} hosts\n')
+            error("will learn %u hosts\n" % learn_hosts)
             start_time = time.time()
             learn_host_list = mac_intf_ipv4s[successful_learn_hosts:learn_hosts]
             random.shuffle(learn_host_list)
             # configure macvlan interfaces and stimulate learning
             for host, mac_intf, mac_ipv4 in learn_host_list:
                 fping_conf_start = time.time()
                 self.add_macvlan(host, mac_intf, mac_ipv4, ipm=test_net.prefixlen)
                 simplify_intf_conf(host, mac_intf)
-                host.cmd(f'{fping_prefix} -I{mac_intf} {str(learn_ip)}')
+                host.cmd("%s -I%s %s" % (fping_prefix, mac_intf, str(learn_ip)))
                 fping_ms = (time.time() - fping_conf_start) * 1e3
                 if fping_ms < pps_ms:
                     time.sleep((pps_ms - fping_ms) / 1e3)
 
             def verify_connectivity(learn_hosts):
-                error('verifying connectivity')
+                error("verifying connectivity")
                 all_unverified_ips = [str(ipa) for ipa in test_ipas[:learn_hosts]]
                 random.shuffle(all_unverified_ips)
-                loss_re = re.compile(
-                    r'^(\S+) : xmt\/rcv\/\%loss = \d+\/\d+\/(\d+)\%.+')
+                loss_re = re.compile(r"^(\S+) : xmt\/rcv\/\%loss = \d+\/\d+\/(\d+)\%.+")
                 while all_unverified_ips:
                     unverified_ips = set()
                     for _ in range(min(learn_pps, len(all_unverified_ips))):
                         unverified_ips.add(all_unverified_ips.pop())
                     for _ in range(10):
-                        error('.')
+                        error(".")
                         random_unverified_ips = list(unverified_ips)
                         random.shuffle(random_unverified_ips)
-                        fping_cmd = f'{fping_prefix} {" ".join(random_unverified_ips)}'
+                        fping_cmd = "%s %s" % (
+                            fping_prefix,
+                            " ".join(random_unverified_ips),
+                        )
                         fping_lines = first_host.cmd(fping_cmd).splitlines()
                         for fping_line in fping_lines:
                             loss_match = loss_re.match(fping_line)
                             if loss_match:
                                 ipa = loss_match.group(1)
                                 loss = int(loss_match.group(2))
                                 if loss == 0:
                                     unverified_ips.remove(ipa)
                         if unverified_ips:
                             time.sleep(0.1 * len(unverified_ips))
                         else:
                             break
                     if unverified_ips:
-                        error(f'could not verify connectivity for all hosts: {unverified_ips}\n')
+                        error(
+                            "could not verify connectivity for all hosts: %s\n"
+                            % unverified_ips
+                        )
                         return False
 
                 return self.wait_for_prometheus_var(
-                    'vlan_hosts_learned', learn_hosts, labels={'vlan': '100'},
-                    timeout=15, orgreater=True)
+                    "vlan_hosts_learned",
+                    learn_hosts,
+                    labels={"vlan": "100"},
+                    timeout=15,
+                    orgreater=True,
+                )
 
             if verify_connectivity(learn_hosts):
                 learn_time = time.time() - start_time
                 # dump_packet_counters()
-                error(f'verified {learn_hosts} hosts learned in {learn_time} sec\n')
+                error(
+                    "verified %u hosts learned in %u sec\n" % (learn_hosts, learn_time)
+                )
                 successful_learn_hosts = learn_hosts
                 learn_hosts = min(learn_hosts * 2, max_hosts)
             else:
                 break
         self.assertGreaterEqual(successful_learn_hosts, min_hosts)
 
-    def verify_vlan_flood_limited(self, vlan_first_host, vlan_second_host,
-                                  other_vlan_host):
+    def verify_vlan_flood_limited(
+        self, vlan_first_host, vlan_second_host, other_vlan_host
+    ):
         """Verify that flooding doesn't cross VLANs."""
         for first_host, second_host in (
-                (vlan_first_host, vlan_second_host),
-                (vlan_second_host, vlan_first_host)):
-            tcpdump_filter = f'ether host {first_host.MAC()} or ether host {second_host.MAC()}'
+            (vlan_first_host, vlan_second_host),
+            (vlan_second_host, vlan_first_host),
+        ):
+            tcpdump_filter = "ether host %s or ether host %s" % (
+                first_host.MAC(),
+                second_host.MAC(),
+            )
             tcpdump_txt = self.tcpdump_helper(
-                other_vlan_host, tcpdump_filter, [
-                    partial(first_host.cmd, f'arp -d {second_host.IP()}'),
-                    partial(first_host.cmd, ' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-                packets=1)
+                other_vlan_host,
+                tcpdump_filter,
+                [
+                    partial(first_host.cmd, "arp -d %s" % second_host.IP()),
+                    partial(
+                        first_host.cmd,
+                        " ".join((self.FPINGS_ARGS_ONE, second_host.IP())),
+                    ),
+                ],
+                packets=1,
+            )
             self.verify_no_packets(tcpdump_txt)
 
-    def verify_ping_mirrored(self, first_host, second_host, mirror_host, both_mirrored=False):
+    def verify_ping_mirrored(
+        self, first_host, second_host, mirror_host, both_mirrored=False
+    ):
         """Verify that unicast traffic to and from a mirrored port is mirrored."""
         self.ping((first_host, second_host))
         for host in (first_host, second_host):
             self.require_host_learned(host)
         self.retry_net_ping(hosts=(first_host, second_host))
         tcpdump_filter = (
-            f'(ether src {first_host.MAC()} or ether src {second_host.MAC()}) and '
-            '(icmp[icmptype] == 8 or icmp[icmptype] == 0)')
-        first_ping_second = ' '.join((self.FPINGS_ARGS_ONE, second_host.IP()))
+            "(ether src %s or ether src %s) and "
+            "(icmp[icmptype] == 8 or icmp[icmptype] == 0)"
+        ) % (first_host.MAC(), second_host.MAC())
+        first_ping_second = " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
         expected_pings = 2
         max_expected_pings = 2
         if both_mirrored:
             max_expected_pings *= 2
         tcpdump_txt = self.tcpdump_helper(
-            mirror_host, tcpdump_filter, [
-                partial(first_host.cmd, first_ping_second)], packets=(max_expected_pings + 1))
-        self.assertTrue(re.search(
-            f'{second_host.IP()}: ICMP echo request', tcpdump_txt),
-            msg=tcpdump_txt)
-        self.assertTrue(re.search(
-            f'{first_host.IP()}: ICMP echo reply', tcpdump_txt),
-            msg=tcpdump_txt)
+            mirror_host,
+            tcpdump_filter,
+            [partial(first_host.cmd, first_ping_second)],
+            packets=(max_expected_pings + 1),
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt),
+            msg=tcpdump_txt,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo reply" % first_host.IP(), tcpdump_txt),
+            msg=tcpdump_txt,
+        )
         received_pings = self.match_tcpdump_rx_packets(tcpdump_txt)
         self.assertGreaterEqual(received_pings, expected_pings)
         self.assertLessEqual(received_pings, max_expected_pings)
 
-    def verify_bcast_ping_mirrored(self, first_host, second_host, mirror_host,
-                                   tagged=False, require_learned=True):
+    def verify_bcast_ping_mirrored(
+        self, first_host, second_host, mirror_host, tagged=False, require_learned=True
+    ):
         """Verify that broadcast to a mirrored port, is mirrored."""
         if require_learned:
             self.ping((first_host, second_host))
             for host in (first_host, second_host):
                 self.require_host_learned(host)
             self.retry_net_ping(hosts=(first_host, second_host))
         tcpdump_filter = (
-            f'ether src {second_host.MAC()} and ether dst ff:ff:ff:ff:ff:ff and '
-            'icmp[icmptype] == 8')
+            "ether src %s and ether dst ff:ff:ff:ff:ff:ff and " "icmp[icmptype] == 8"
+        ) % second_host.MAC()
         if tagged:
-            tcpdump_filter = f'vlan and {tcpdump_filter}'
+            tcpdump_filter = "vlan and %s" % tcpdump_filter
         else:
-            tcpdump_filter = f'{tcpdump_filter} and not vlan'
-        second_ping_bcast = f'ping -c3 -b {self.ipv4_vip_bcast()}'
+            tcpdump_filter = "%s and not vlan" % tcpdump_filter
+        second_ping_bcast = "ping -c3 -b %s" % self.ipv4_vip_bcast()
         tcpdump_txt = self.tcpdump_helper(
-            mirror_host, tcpdump_filter, [
-                partial(second_host.cmd, second_ping_bcast)],
-            packets=1)
-        self.assertTrue(re.search(
-            f'{self.ipv4_vip_bcast()}: ICMP echo request', tcpdump_txt),
-            msg=tcpdump_txt)
+            mirror_host,
+            tcpdump_filter,
+            [partial(second_host.cmd, second_ping_bcast)],
+            packets=1,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % self.ipv4_vip_bcast(), tcpdump_txt),
+            msg=tcpdump_txt,
+        )
 
     def verify_ping_mirrored_multi(self, ping_pairs, mirror_host, both_mirrored=False):
-        """ Verify that mirroring of multiple switchs works. Method
+        """Verify that mirroring of multiple switchs works. Method
         will both perform a one at a time ping mirror check and a
         all at once test where all ping pairs are executed at the
         same time.
 
         Args:
             ping_pairs (list of tuple): Hosts to ping for tests
                 in the format '[(host_a, host_b)]` where host_a
                 will ping host_bs IP.
             mirror_host (FaucetHost): host to check mirroring
         """
         # Verify individual ping works
         for hosts in ping_pairs:
-            self.verify_ping_mirrored(hosts[0], hosts[1], mirror_host, both_mirrored=both_mirrored)
+            self.verify_ping_mirrored(
+                hosts[0], hosts[1], mirror_host, both_mirrored=both_mirrored
+            )
 
         # Prepare our ping pairs
         for hosts in ping_pairs:
             self.ping(hosts)
         for hosts in ping_pairs:
             for host in hosts:
                 self.require_host_learned(host)
         for hosts in ping_pairs:
             self.retry_net_ping(hosts=hosts)
 
         mirror_mac = mirror_host.MAC()
         tcpdump_filter = (
-            f'not ether src {mirror_mac} and '
-            '(icmp[icmptype] == 8 or icmp[icmptype] == 0)')
+            "not ether src %s and " "(icmp[icmptype] == 8 or icmp[icmptype] == 0)"
+        ) % mirror_mac
 
         # Calculate the execpted number of pings we need
         # to capture to validate port mirroring
         expected_pings = len(ping_pairs) * 2
         max_expected_pings = expected_pings
         if both_mirrored:
             max_expected_pings *= 2
 
         # Generate and run the mirror test pings
         ping_commands = []
         for hosts in ping_pairs:
             ping_commands.append(
-                lambda hosts=hosts: hosts[0].cmd(' '.join((self.FPINGS_ARGS_ONE, hosts[1].IP()))))
+                lambda hosts=hosts: hosts[0].cmd(
+                    " ".join((self.FPINGS_ARGS_ONE, hosts[1].IP()))
+                )
+            )
         tcpdump_txt = self.tcpdump_helper(
-            mirror_host, tcpdump_filter, ping_commands, packets=(max_expected_pings + 1))
+            mirror_host, tcpdump_filter, ping_commands, packets=(max_expected_pings + 1)
+        )
 
         for hosts in ping_pairs:
-            self.assertTrue(re.search(
-                f'{hosts[0].IP()} > {hosts[1].IP()}: ICMP echo request', tcpdump_txt),
-                msg=tcpdump_txt)
-            self.assertTrue(re.search(
-                f'{hosts[1].IP()} > {hosts[0].IP()}: ICMP echo reply', tcpdump_txt),
-                msg=tcpdump_txt)
+            self.assertTrue(
+                re.search(
+                    "%s > %s: ICMP echo request" % (hosts[0].IP(), hosts[1].IP()),
+                    tcpdump_txt,
+                ),
+                msg=tcpdump_txt,
+            )
+            self.assertTrue(
+                re.search(
+                    "%s > %s: ICMP echo reply" % (hosts[1].IP(), hosts[0].IP()),
+                    tcpdump_txt,
+                ),
+                msg=tcpdump_txt,
+            )
 
         received_pings = self.match_tcpdump_rx_packets(tcpdump_txt)
         self.assertGreaterEqual(received_pings, expected_pings)
         self.assertLessEqual(received_pings, max_expected_pings)
 
     def match_tcpdump_rx_packets(self, tcpdump_txt):
-        match_re = re.compile(r'.*(\d+) packets* captured.*')
+        match_re = re.compile(r".*(\d+) packets* captured.*")
         match = match_re.match(tcpdump_txt)
         self.assertTrue(match, msg=tcpdump_txt)
         packets = int(match.group(1))
         return packets
 
     def tcpdump_rx_packets(self, tcpdump_txt, packets=0):
         return self.match_tcpdump_rx_packets(tcpdump_txt) == packets
 
     def verify_no_packets(self, tcpdump_txt):
-        self.assertTrue(self.tcpdump_rx_packets(tcpdump_txt, packets=0), msg=tcpdump_txt)
+        self.assertTrue(
+            self.tcpdump_rx_packets(tcpdump_txt, packets=0), msg=tcpdump_txt
+        )
 
     def verify_eapol_mirrored(self, first_host, second_host, mirror_host):
         self.ping((first_host, second_host))
         for host in (first_host, second_host):
             self.require_host_learned(host)
         self.retry_net_ping(hosts=(first_host, second_host))
         mirror_mac = mirror_host.MAC()
-        tmp_eap_conf = os.path.join(self.tmpdir, 'eap.conf')
-        tcpdump_filter = (
-            f'not ether src {mirror_mac} and ether proto 0x888e')
+        tmp_eap_conf = os.path.join(self.tmpdir, "eap.conf")
+        tcpdump_filter = "not ether src %s and ether proto 0x888e" % mirror_mac
         eap_conf_cmd = (
-            f'echo "eapol_version=2\nap_scan=0\nnetwork={{\n'
-            f'key_mgmt=IEEE8021X\neap=MD5\nidentity=\\"login\\"\n'
-            f'password=\\"password\\"\n}}\n" > {tmp_eap_conf}')
+            'echo "eapol_version=2\nap_scan=0\nnetwork={\n'
+            'key_mgmt=IEEE8021X\neap=MD5\nidentity=\\"login\\"\n'
+            'password=\\"password\\"\n}\n" > %s' % tmp_eap_conf
+        )
         wpa_supplicant_cmd = mininet_test_util.timeout_cmd(
-            f'wpa_supplicant -c{tmp_eap_conf} -Dwired -i{first_host.defaultIntf().name} -d', 3)
+            "wpa_supplicant -c%s -Dwired -i%s -d"
+            % (tmp_eap_conf, first_host.defaultIntf().name),
+            3,
+        )
         tcpdump_txt = self.tcpdump_helper(
-            mirror_host, tcpdump_filter, [
+            mirror_host,
+            tcpdump_filter,
+            [
                 partial(first_host.cmd, eap_conf_cmd),
                 partial(first_host.cmd, wpa_supplicant_cmd),
                 partial(first_host.cmd, wpa_supplicant_cmd),
-                partial(first_host.cmd, wpa_supplicant_cmd)],
-            timeout=20, packets=1)
+                partial(first_host.cmd, wpa_supplicant_cmd),
+            ],
+            timeout=20,
+            packets=1,
+        )
         self.assertTrue(
-            re.search('01:80:c2:00:00:03, ethertype EAPOL', tcpdump_txt),
-            msg=tcpdump_txt)
+            re.search("01:80:c2:00:00:03, ethertype EAPOL", tcpdump_txt),
+            msg=tcpdump_txt,
+        )
 
     def bogus_mac_flooded_to_port1(self):
         first_host, second_host, third_host = self.hosts_name_ordered()[0:3]
-        unicast_flood_filter = f'ether host {self.BOGUS_MAC}'
-        static_bogus_arp = f'arp -s {first_host.IP()} {self.BOGUS_MAC}'
-        curl_first_host = f'curl -m 5 http://{first_host.IP()}'
+        unicast_flood_filter = "ether host %s" % self.BOGUS_MAC
+        static_bogus_arp = "arp -s %s %s" % (first_host.IP(), self.BOGUS_MAC)
+        curl_first_host = "curl -m 5 http://%s" % first_host.IP()
         tcpdump_txt = self.tcpdump_helper(
-            first_host, unicast_flood_filter,
-            [lambda: second_host.cmd(static_bogus_arp),
-             lambda: second_host.cmd(curl_first_host),
-             lambda: self.ping(hosts=(second_host, third_host))])
+            first_host,
+            unicast_flood_filter,
+            [
+                lambda: second_host.cmd(static_bogus_arp),
+                lambda: second_host.cmd(curl_first_host),
+                lambda: self.ping(hosts=(second_host, third_host)),
+            ],
+        )
         return not self.tcpdump_rx_packets(tcpdump_txt, 0)
 
     def ladvd_cmd(self, ladvd_args, repeats=1, timeout=3):
-        ladvd_mkdir = 'mkdir -p /var/run/ladvd'
-        ladvd_all_args = [f'{mininet_test_util.timeout_cmd(self.LADVD, timeout)} {ladvd_args}'] * repeats
-        ladvd_cmd = ';'.join([ladvd_mkdir] + ladvd_all_args)
+        ladvd_mkdir = "mkdir -p /var/run/ladvd"
+        ladvd_all_args = [
+            "%s %s" % (mininet_test_util.timeout_cmd(self.LADVD, timeout), ladvd_args)
+        ] * repeats
+        ladvd_cmd = ";".join([ladvd_mkdir] + ladvd_all_args)
         return ladvd_cmd
 
-    def ladvd_noisemaker(self, send_cmd, tcpdump_filter, hosts=None, timeout=3, repeats=3):
+    def ladvd_noisemaker(
+        self, send_cmd, tcpdump_filter, hosts=None, timeout=3, repeats=3
+    ):
         if hosts is None:
             hosts = self.hosts_name_ordered()[:2]
         first_host = hosts[0]
         other_hosts = hosts[1:]
         other_host_cmds = []
         for other_host in other_hosts:
-            other_host_cmds.append(partial(other_host.cmd, self.ladvd_cmd(
-                send_cmd % other_host.defaultIntf(), repeats=3, timeout=timeout)))
+            other_host_cmds.append(
+                partial(
+                    other_host.cmd,
+                    self.ladvd_cmd(
+                        send_cmd % other_host.defaultIntf(), repeats=3, timeout=timeout
+                    ),
+                )
+            )
         tcpdump_txt = self.tcpdump_helper(
-            first_host, tcpdump_filter, other_host_cmds,
-            timeout=(timeout * repeats * len(hosts)), packets=1)
+            first_host,
+            tcpdump_filter,
+            other_host_cmds,
+            timeout=(timeout * repeats * len(hosts)),
+            packets=1,
+        )
         self.verify_no_packets(tcpdump_txt)
 
     def verify_lldp_blocked(self, hosts=None, timeout=3):
-        self.ladvd_noisemaker(
-            '-L -o %s', 'ether proto 0x88cc',
-            hosts, timeout=timeout)
+        self.ladvd_noisemaker("-L -o %s", "ether proto 0x88cc", hosts, timeout=timeout)
 
     def verify_cdp_blocked(self, hosts=None, timeout=3):
         self.ladvd_noisemaker(
-            '-C -o %s', 'ether dst host 01:00:0c:cc:cc:cc and ether[20:2]==0x2000',
-            hosts, timeout=timeout)
+            "-C -o %s",
+            "ether dst host 01:00:0c:cc:cc:cc and ether[20:2]==0x2000",
+            hosts,
+            timeout=timeout,
+        )
         self.wait_nonzero_packet_count_flow(
-            {'dl_dst': '01:00:0c:cc:cc:cc'}, self._FLOOD_TABLE, actions=[], ofa_match=False)
-
-    def verify_faucet_reconf(self, timeout=20,
-                             cold_start=True, change_expected=True,
-                             hup=True, reconf_funcs=None, dpid=True):
+            {"dl_dst": "01:00:0c:cc:cc:cc"},
+            self._FLOOD_TABLE,
+            actions=[],
+            ofa_match=False,
+        )
+
+    def verify_faucet_reconf(
+        self,
+        timeout=20,
+        cold_start=True,
+        change_expected=True,
+        hup=True,
+        reconf_funcs=None,
+        dpid=True,
+    ):
         """HUP and verify the HUP was processed."""
         if reconf_funcs is None:
             reconf_funcs = []
         if hup:
             for controller in self.faucet_controllers:
-                reconf_funcs.append(partial(self.hup_controller, controller=controller.name))
-        var = 'faucet_config_reload_warm_total'
+                reconf_funcs.append(
+                    partial(self.hup_controller, controller=controller.name)
+                )
+        var = "faucet_config_reload_warm_total"
         if cold_start:
-            var = 'faucet_config_reload_cold_total'
+            var = "faucet_config_reload_cold_total"
         old_counts = []
         start_configure_counts = []
         for controller in self.faucet_controllers:
             old_count = int(
-                self.scrape_prometheus_var(var, controller=controller.name, dpid=dpid, default=0))
+                self.scrape_prometheus_var(
+                    var, controller=controller.name, dpid=dpid, default=0
+                )
+            )
             old_counts.append(old_count)
             start_configure_count = self.get_configure_count(controller=controller.name)
             start_configure_counts.append(start_configure_count)
         for reconf_func in reconf_funcs:
             reconf_func()
         for i, controller in enumerate(self.faucet_controllers):
             cont_name = controller.name
             start_configure_count = start_configure_counts[i]
             for _ in range(timeout):
                 configure_count = self.get_configure_count(controller=cont_name)
                 if configure_count > start_configure_count:
                     break
                 time.sleep(1)
             self.assertNotEqual(
-                start_configure_count, configure_count, f'FAUCET {cont_name} did not reconfigure')
+                start_configure_count,
+                configure_count,
+                "FAUCET %s did not reconfigure" % cont_name,
+            )
             if cold_start is not None:
                 old_count = old_counts[i]
                 if change_expected:
                     old_count = old_counts[i]
                     for _ in range(timeout):
                         new_count = int(
-                            self.scrape_prometheus_var(var, controller=cont_name,
-                                                       dpid=dpid, default=0))
+                            self.scrape_prometheus_var(
+                                var, controller=cont_name, dpid=dpid, default=0
+                            )
+                        )
                         if new_count > old_count:
                             break
                         time.sleep(1)
                     self.assertTrue(
                         new_count > old_count,
-                        msg=f'FAUCET {cont_name} {var} did not increment: {new_count}')
+                        msg="FAUCET %s %s did not increment: %u"
+                        % (cont_name, var, new_count),
+                    )
                 else:
                     new_count = int(
-                        self.scrape_prometheus_var(var, controller=cont_name,
-                                                   dpid=dpid, default=0))
+                        self.scrape_prometheus_var(
+                            var, controller=cont_name, dpid=dpid, default=0
+                        )
+                    )
                     self.assertEqual(
-                        old_count, new_count,
-                        msg=f'FAUCET {cont_name} {var} incremented: {new_count}')
-            self.wait_for_prometheus_var('faucet_config_applied', 1,
-                                         controller=cont_name, dpid=None, timeout=30)
+                        old_count,
+                        new_count,
+                        msg="FAUCET %s %s incremented: %u"
+                        % (cont_name, var, new_count),
+                    )
+            self.wait_for_prometheus_var(
+                "faucet_config_applied", 1, controller=cont_name, dpid=None, timeout=30
+            )
             self.wait_dp_status(1, controller=cont_name)
 
     def force_faucet_reload(self, new_config):
         """Force FAUCET to reload."""
-        with open(self.faucet_config_path, 'w', encoding='utf-8') as config_file:
+        with open(self.faucet_config_path, "w", encoding="utf-8") as config_file:
             config_file.write(new_config)
         self.verify_faucet_reconf(change_expected=False)
 
     def get_host_port_stats(self, hosts_switch_ports):
         port_stats = {}
         for host, switch_port in hosts_switch_ports:
             if host not in port_stats:
                 port_stats[host] = {}
-            port_stats[host].update(self.get_port_stats_from_dpid(
-                self.dpid, switch_port))
+            port_stats[host].update(
+                self.get_port_stats_from_dpid(self.dpid, switch_port)
+            )
         return port_stats
 
-    def wait_host_stats_updated(self, hosts_switch_ports, timeout, sync_counters_func=None):
+    def wait_host_stats_updated(
+        self, hosts_switch_ports, timeout, sync_counters_func=None
+    ):
         first = self.get_host_port_stats(hosts_switch_ports)
         for _ in range(timeout):
             if sync_counters_func:
                 sync_counters_func()
             if self.get_host_port_stats(hosts_switch_ports) != first:
                 return
             time.sleep(1)
-        self.fail(f'port stats for {hosts_switch_ports} never updated')
+        self.fail("port stats for %s never updated" % hosts_switch_ports)
 
     def of_bytes_mbps(self, start_port_stats, end_port_stats, var, seconds):
-        return (end_port_stats[var] - start_port_stats[var]) * 8 / seconds / self.ONEMBPS
-
-    def verify_iperf_min(self, hosts_switch_ports, min_mbps, client_ip, server_ip,
-                         seconds=5, prop=0.2, sync_counters_func=None):
+        return (
+            (end_port_stats[var] - start_port_stats[var]) * 8 / seconds / self.ONEMBPS
+        )
+
+    def verify_iperf_min(
+        self,
+        hosts_switch_ports,
+        min_mbps,
+        client_ip,
+        server_ip,
+        seconds=5,
+        prop=0.2,
+        sync_counters_func=None,
+    ):
         """Verify minimum performance and OF counters match iperf approximately."""
         # Attempt loose counter sync before starting.
         self.wait_host_stats_updated(
-            hosts_switch_ports, timeout=seconds * 2, sync_counters_func=sync_counters_func)
+            hosts_switch_ports,
+            timeout=seconds * 2,
+            sync_counters_func=sync_counters_func,
+        )
         start_port_stats = self.get_host_port_stats(hosts_switch_ports)
         hosts = [host for host, _ in hosts_switch_ports]
         client_host, server_host = hosts
-        iperf_mbps = self.iperf(
-            client_host, client_ip, server_host, server_ip, seconds)
+        iperf_mbps = self.iperf(client_host, client_ip, server_host, server_ip, seconds)
         self.assertGreater(iperf_mbps, min_mbps)
         # TODO: account for drops.
         for _ in range(3):
             end_port_stats = self.get_host_port_stats(hosts_switch_ports)
             approx_match = True
             for host in hosts:
                 of_rx_mbps = self.of_bytes_mbps(
-                    start_port_stats[host], end_port_stats[host], 'rx_bytes', seconds)
+                    start_port_stats[host], end_port_stats[host], "rx_bytes", seconds
+                )
                 of_tx_mbps = self.of_bytes_mbps(
-                    start_port_stats[host], end_port_stats[host], 'tx_bytes', seconds)
+                    start_port_stats[host], end_port_stats[host], "tx_bytes", seconds
+                )
                 output(of_rx_mbps, of_tx_mbps)
                 max_of_mbps = float(max(of_rx_mbps, of_tx_mbps))
                 iperf_to_max = 0
                 if max_of_mbps:
                     iperf_to_max = iperf_mbps / max_of_mbps
-                msg = f'iperf: {iperf_mbps}mbps, of: {max_of_mbps}mbps ({iperf_to_max})'
+                msg = "iperf: %fmbps, of: %fmbps (%f)" % (
+                    iperf_mbps,
+                    max_of_mbps,
+                    iperf_to_max,
+                )
                 error(msg)
-                if ((iperf_to_max < (1.0 - prop))
-                        or (iperf_to_max > (1.0 + prop))):
+                if (iperf_to_max < (1.0 - prop)) or (iperf_to_max > (1.0 + prop)):
                     approx_match = False
             if approx_match:
                 return
             time.sleep(1)
         self.fail(msg=msg)
 
     @staticmethod
     def port_labels(port_no):
-        port_name = f'b{port_no}'
-        return {'port': port_name, 'port_description': port_name}
+        port_name = "b%u" % port_no
+        return {"port": port_name, "port_description": port_name}
 
     def set_dpid_names(self, dpid_names):
         self.dpid_names = copy.deepcopy(dpid_names)
 
     def wait_port_status(self, dpid, port_no, status, expected_status, timeout=10):
         for _ in range(timeout):
             port_status = self.scrape_prometheus_var(
-                'port_status', self.port_labels(port_no), default=None, dpid=dpid)
+                "port_status", self.port_labels(port_no), default=None, dpid=dpid
+            )
             if port_status is not None and port_status == expected_status:
                 return
             self._portmod(dpid, port_no, status, ofp.OFPPC_PORT_DOWN)
             time.sleep(1)
-        self.fail('dpid %x port %s status %s != expected %u' % (
-            dpid, port_no, port_status, expected_status))
+        self.fail(
+            "dpid %x port %s status %s != expected %u"
+            % (dpid, port_no, port_status, expected_status)
+        )
 
     def set_port_status(self, dpid, port_no, status, wait):
         if dpid is None:
             dpid = self.dpid
         expected_status = 1
         if status == ofp.OFPPC_PORT_DOWN:
             expected_status = 0
@@ -2384,51 +2915,50 @@
     def set_port_up(self, port_no, dpid=None, wait=True):
         self.set_port_status(dpid, port_no, 0, wait)
 
     def wait_dp_status(self, expected_status, controller=None, timeout=30):
         if controller is None:
             controller = self.faucet_controllers[0].name
         return self.wait_for_prometheus_var(
-            'dp_status', expected_status, any_labels=True, controller=controller,
-            default=None, timeout=timeout)
+            "dp_status",
+            expected_status,
+            any_labels=True,
+            controller=controller,
+            default=None,
+            timeout=timeout,
+        )
 
     def _get_tableid(self, name, retries, default):
         return self.scrape_prometheus_var(
-            'faucet_config_table_names', {'table_name': name},
-            retries=retries, default=default)
+            "faucet_config_table_names",
+            {"table_name": name},
+            retries=retries,
+            default=default,
+        )
 
     def quiet_commands(self, host, commands):
         for command in commands:
             result = host.cmd(command)
-            self.assertEqual('', result, msg=f'{command}: {result}')
+            self.assertEqual("", result, msg="%s: %s" % (command, result))
 
     def _config_tableids(self):
         # Wait for VLAN table to appear, rapidly scrape the rest.
-        self._VLAN_TABLE = self._get_tableid(
-            'vlan', 1, self._VLAN_TABLE)
-        self._COPRO_TABLE = self._get_tableid(
-            'vlan', 1, self._COPRO_TABLE)
-        self._PORT_ACL_TABLE = self._get_tableid(
-            'port_acl', 1, self._PORT_ACL_TABLE)
-        self._VLAN_ACL_TABLE = self._get_tableid(
-            'vlan_acl', 1, self._VLAN_ACL_TABLE)
-        self._ETH_SRC_TABLE = self._get_tableid(
-            'eth_src', 1, self._ETH_SRC_TABLE)
-        self._IPV4_FIB_TABLE = self._get_tableid(
-            'ipv4_fib', 1, self._IPV4_FIB_TABLE)
-        self._IPV6_FIB_TABLE = self._get_tableid(
-            'ipv6_fib', 1, self._IPV6_FIB_TABLE)
-        self._VIP_TABLE = self._get_tableid(
-            'vip', 1, self._VIP_TABLE)
+        self._VLAN_TABLE = self._get_tableid("vlan", 1, self._VLAN_TABLE)
+        self._COPRO_TABLE = self._get_tableid("vlan", 1, self._COPRO_TABLE)
+        self._PORT_ACL_TABLE = self._get_tableid("port_acl", 1, self._PORT_ACL_TABLE)
+        self._VLAN_ACL_TABLE = self._get_tableid("vlan_acl", 1, self._VLAN_ACL_TABLE)
+        self._ETH_SRC_TABLE = self._get_tableid("eth_src", 1, self._ETH_SRC_TABLE)
+        self._IPV4_FIB_TABLE = self._get_tableid("ipv4_fib", 1, self._IPV4_FIB_TABLE)
+        self._IPV6_FIB_TABLE = self._get_tableid("ipv6_fib", 1, self._IPV6_FIB_TABLE)
+        self._VIP_TABLE = self._get_tableid("vip", 1, self._VIP_TABLE)
         self._ETH_DST_HAIRPIN_TABLE = self._get_tableid(
-            'eth_dst_hairpin', 1, self._ETH_DST_HAIRPIN_TABLE)
-        self._ETH_DST_TABLE = self._get_tableid(
-            'eth_dst', 1, self._ETH_DST_TABLE)
-        self._FLOOD_TABLE = self._get_tableid(
-            'flood', 1, self._FLOOD_TABLE)
+            "eth_dst_hairpin", 1, self._ETH_DST_HAIRPIN_TABLE
+        )
+        self._ETH_DST_TABLE = self._get_tableid("eth_dst", 1, self._ETH_DST_TABLE)
+        self._FLOOD_TABLE = self._get_tableid("flood", 1, self._FLOOD_TABLE)
 
     def _dp_ports(self):
         return list(sorted(self.port_map.values()))
 
     def flap_port(self, port_no, flap_time=MIN_FLAP_TIME):
         self.set_port_down(port_no)
         time.sleep(flap_time)
@@ -2438,101 +2968,158 @@
         """Flap all ports on switch."""
         for port_no in self._dp_ports():
             self.flap_port(port_no, flap_time=flap_time)
 
     @staticmethod
     def get_mac_of_intf(intf, host=None):
         """Get MAC address of a port."""
-        address_file_name = f'/sys/class/net/{intf}/address'
+        address_file_name = "/sys/class/net/%s/address" % intf
         if host is None:
-            with open(address_file_name, encoding='utf-8') as address_file:
+            with open(address_file_name, encoding="utf-8") as address_file:
                 address = address_file.read()
         else:
-            address = host.cmd(f'cat {address_file_name}')
+            address = host.cmd("cat %s" % address_file_name)
         return address.strip().lower()
 
-    def add_macvlan(self, host, macvlan_intf, ipa=None, ipm=24, mac=None, mode='vepa'):
+    def add_macvlan(self, host, macvlan_intf, ipa=None, ipm=24, mac=None, mode="vepa"):
         if mac is None:
-            mac = ''
+            mac = ""
         else:
-            mac = f'address {mac}'
+            mac = "address %s" % mac
         add_cmds = [
-            f'ip link add {macvlan_intf} link {host.defaultIntf()} {mac} type macvlan mode {mode}',
-            f'ip link set dev {macvlan_intf} up']
+            "ip link add %s link %s %s type macvlan mode %s"
+            % (macvlan_intf, host.defaultIntf(), mac, mode),
+            "ip link set dev %s up" % macvlan_intf,
+        ]
         if ipa:
             add_cmds.append(
-                f'ip address add {ipa}/{ipm} brd + dev {macvlan_intf}')
+                "ip address add %s/%s brd + dev %s" % (ipa, ipm, macvlan_intf)
+            )
         self.quiet_commands(host, add_cmds)
 
     def del_macvlan(self, host, macvlan_intf):
-        self.quiet_commands(host, [
-            host.cmd(f'ip link del link {host.defaultIntf()} {macvlan_intf}')])
+        self.quiet_commands(
+            host,
+            [host.cmd("ip link del link %s %s" % (host.defaultIntf(), macvlan_intf))],
+        )
 
     def add_host_ipv6_address(self, host, ip_v6, intf=None):
         """Add an IPv6 address to a Mininet host."""
         if intf is None:
             intf = host.intf()
-        self.quiet_commands(host, [
-            host.cmd(f'ip -6 addr add {ip_v6} dev {intf}')])
+        self.quiet_commands(
+            host, [host.cmd("ip -6 addr add %s dev %s" % (ip_v6, intf))]
+        )
 
     def add_host_route(self, host, ip_dst, ip_gw):
         """Add an IP route to a Mininet host."""
-        host.cmd(f'ip -{ip_dst.version} route del {ip_dst.network.with_prefixlen}')
-        add_cmd = f'ip -{ip_dst.version} route add {ip_dst.network.with_prefixlen} via {ip_gw}'
+        host.cmd(
+            "ip -%u route del %s" % (ip_dst.version, ip_dst.network.with_prefixlen)
+        )
+        add_cmd = "ip -%u route add %s via %s" % (
+            ip_dst.version,
+            ip_dst.network.with_prefixlen,
+            ip_gw,
+        )
         self.quiet_commands(host, (add_cmd,))
 
-    def _ip_ping(self, host, dst, retries, timeout=500,
-                 fping_bin='fping', intf=None, expected_result=True, count=1,
-                 require_host_learned=require_host_learned):
+    def _ip_ping(
+        self,
+        host,
+        dst,
+        retries,
+        timeout=500,
+        fping_bin="fping",
+        intf=None,
+        expected_result=True,
+        count=1,
+        require_host_learned=require_host_learned,
+    ):
         """Ping a destination from a host"""
         if intf is None:
             intf = host.defaultIntf()
-        good_ping = r'xmt/rcv/%%loss = %u/%u/0%%' % (count, count)
-        ping_cmd = f'{fping_bin} {self.FPING_ARGS} -c{count} -I{intf} -t{timeout} {dst}'
+        good_ping = r"xmt/rcv/%%loss = %u/%u/0%%" % (count, count)
+        ping_cmd = "%s %s -c%u -I%s -t%u %s" % (
+            fping_bin,
+            self.FPING_ARGS,
+            count,
+            intf,
+            timeout,
+            dst,
+        )
         if require_host_learned:
             self.require_host_learned(host)
         pause = timeout / 1e3
         for _ in range(retries):
             ping_out = host.cmd(ping_cmd)
             ping_result = bool(re.search(good_ping, ping_out))
             if ping_result:
                 break
             time.sleep(pause)
             pause *= 2
-        self.assertEqual(ping_result, expected_result, msg=f'{ping_cmd} {ping_result}: {ping_out}')
-
-    def one_ipv4_ping(self, host, dst, retries=3, timeout=1000, intf=None,
-                      require_host_learned=True, expected_result=True):
+        self.assertEqual(
+            ping_result,
+            expected_result,
+            msg="%s %s: %s" % (ping_cmd, ping_result, ping_out),
+        )
+
+    def one_ipv4_ping(
+        self,
+        host,
+        dst,
+        retries=3,
+        timeout=1000,
+        intf=None,
+        require_host_learned=True,
+        expected_result=True,
+    ):
         """Ping an IPv4 destination from a host."""
         return self._ip_ping(
-            host, dst, retries,
-            timeout=timeout, fping_bin='fping', intf=intf,
+            host,
+            dst,
+            retries,
+            timeout=timeout,
+            fping_bin="fping",
+            intf=intf,
             require_host_learned=require_host_learned,
-            expected_result=expected_result)
+            expected_result=expected_result,
+        )
 
     @staticmethod
     def flush_arp_cache(host):
         """Flush the ARP cache for a host."""
         host.cmd("ip -s neigh flush all")
 
     def one_ipv4_controller_ping(self, host):
         """Ping the controller from a host with IPv4."""
         self.flush_arp_cache(host)
         self.one_ipv4_ping(host, self.FAUCET_VIPV4.ip)
-        self.verify_ipv4_host_learned_mac(
-            host, self.FAUCET_VIPV4.ip, self.FAUCET_MAC)
+        self.verify_ipv4_host_learned_mac(host, self.FAUCET_VIPV4.ip, self.FAUCET_MAC)
 
-    def one_ipv6_ping(self, host, dst, retries=5, timeout=1000, intf=None,
-                      require_host_learned=True, expected_result=True):
+    def one_ipv6_ping(
+        self,
+        host,
+        dst,
+        retries=5,
+        timeout=1000,
+        intf=None,
+        require_host_learned=True,
+        expected_result=True,
+    ):
         """Ping an IPv6 destination from a host."""
         return self._ip_ping(
-            host, dst, retries,
-            timeout=timeout, fping_bin='fping6', intf=intf,
+            host,
+            dst,
+            retries,
+            timeout=timeout,
+            fping_bin="fping6",
+            intf=intf,
             require_host_learned=require_host_learned,
-            expected_result=expected_result)
+            expected_result=expected_result,
+        )
 
     def one_ipv6_controller_ping(self, host):
         """Ping the controller from a host with IPv6."""
         self.one_ipv6_ping(host, self.FAUCET_VIPV6.ip)
         # TODO: VIP might not be in neighbor table if still tentative/ND used
         #       non VIP source address.
         # Make test host source addresses consistent.
@@ -2553,261 +3140,319 @@
             if hosts is None:
                 loss = self.ping_all(timeout=timeout)
             else:
                 loss = self.net.ping(hosts, timeout=timeout)
             if loss <= required_loss:
                 return
             time.sleep(1)
-        self.fail(f'ping {loss} loss > required loss {required_loss}')
+        self.fail("ping %f loss > required loss %f" % (loss, required_loss))
 
     @staticmethod
     def tcp_port_free(host, port, ipv=4):
-        listen_out = host.cmd(
-            mininet_test_util.tcp_listening_cmd(port, ipv))
+        listen_out = host.cmd(mininet_test_util.tcp_listening_cmd(port, ipv))
         if listen_out:
             return listen_out
         return None
 
     def wait_for_tcp_free(self, host, port, timeout=10, ipv=4):
         """Wait for a host to start listening on a port."""
         for _ in range(timeout):
             listen_out = self.tcp_port_free(host, port, ipv)
             if listen_out is None:
                 return
             time.sleep(1)
-        self.fail(f'{host} busy on port {port} ({listen_out})')
+        self.fail("%s busy on port %u (%s)" % (host, port, listen_out))
 
     def wait_for_tcp_listen(self, host, port, timeout=10, ipv=4):
         """Wait for a host to start listening on a port."""
         for _ in range(timeout):
             listen_out = self.tcp_port_free(host, port, ipv)
             if listen_out is not None:
                 return
             time.sleep(1)
-        self.fail(f'{host} never listened on port {port}')
+        self.fail("%s never listened on port %u" % (host, port))
 
-    def serve_str_on_tcp_port(self, host, port, serve_str='hello', timeout=20):
+    def serve_str_on_tcp_port(self, host, port, serve_str="hello", timeout=20):
         """Serve str on a TCP port on a host."""
-        host.cmd(mininet_test_util.timeout_cmd(
-            f'echo {serve_str} | nc -l {host.IP()} {port} &', timeout))
+        host.cmd(
+            mininet_test_util.timeout_cmd(
+                "echo %s | nc -l %s %u &" % (serve_str, host.IP(), port), timeout
+            )
+        )
         self.wait_for_tcp_listen(host, port)
 
-    def wait_nonzero_packet_count_flow(self, match, table_id, timeout=15,
-                                       actions=None, dpid=None, ofa_match=True):
+    def wait_nonzero_packet_count_flow(
+        self, match, table_id, timeout=15, actions=None, dpid=None, ofa_match=True
+    ):
         """Wait for a flow to be present and have a non-zero packet_count."""
         if dpid is None:
             dpid = self.dpid
         for _ in range(timeout):
             flow = self.get_matching_flow_on_dpid(
-                dpid, match, table_id, timeout=1,
-                actions=actions, ofa_match=ofa_match)
-            if flow and flow['packet_count'] > 0:
+                dpid, match, table_id, timeout=1, actions=actions, ofa_match=ofa_match
+            )
+            if flow and flow["packet_count"] > 0:
                 return
             time.sleep(1)
         if flow:
-            self.fail(f'DPID {dpid} flow {flow} matching {match} table ID {table_id} had zero packet count')
+            self.fail(
+                "DPID %s flow %s matching %s table ID %s had zero packet count"
+                % (dpid, flow, match, table_id)
+            )
         else:
-            self.fail(f'no flow matching {match} table ID {table_id}')
+            self.fail("no flow matching %s table ID %s" % (match, table_id))
 
-    def verify_tp_dst_blocked(self, port, first_host, second_host, table_id=0, mask=None):
+    def verify_tp_dst_blocked(
+        self, port, first_host, second_host, table_id=0, mask=None
+    ):
         """Verify that a TCP port on a host is blocked from another host."""
-        client_cmd = mininet_test_util.timeout_cmd(f'nc {second_host.IP()} {port}', 5)
+        client_cmd = mininet_test_util.timeout_cmd(
+            "nc %s %u" % (second_host.IP(), port), 5
+        )
         self.serve_str_on_tcp_port(second_host, port)
         self.quiet_commands(first_host, (client_cmd,))
         if table_id is None:
             return
-        match = {
-            'dl_type': IPV4_ETH, 'ip_proto': 6
-        }
+        match = {"dl_type": IPV4_ETH, "ip_proto": 6}
         match_port = int(port)
         if mask is not None:
-            match_port = '/'.join((str(port), str(mask)))
-        match['tp_dst'] = match_port
+            match_port = "/".join((str(port), str(mask)))
+        match["tp_dst"] = match_port
         self.wait_nonzero_packet_count_flow(match, table_id, ofa_match=False)
         # cleanup listening nc (if any)
         second_host.cmd(client_cmd)
 
     def verify_tp_dst_notblocked(self, port, first_host, second_host, table_id=0):
         """Verify that a TCP port on a host is NOT blocked from another host."""
-        serve_str = ''.join(random.choice(string.ascii_letters) for i in range(8))
+        serve_str = "".join(random.choice(string.ascii_letters) for i in range(8))
         self.serve_str_on_tcp_port(second_host, port, serve_str=serve_str)
-        client_str = first_host.cmd(f'nc -w 10 {second_host.IP()} {port}').strip()
+        client_str = first_host.cmd("nc -w 10 %s %u" % (second_host.IP(), port)).strip()
         self.assertEqual(serve_str, client_str)
         if table_id is None:
             return
         self.wait_nonzero_packet_count_flow(
-            {'tp_dst': int(port), 'dl_type': IPV4_ETH, 'ip_proto': 6}, table_id)
+            {"tp_dst": int(port), "dl_type": IPV4_ETH, "ip_proto": 6}, table_id
+        )
 
-    def bcast_dst_blocked_helper(self, port, first_host, second_host, success_re, retries):
-        tcpdump_filter = f'udp and ether src {first_host.MAC()} and ether dst ff:ff:ff:ff:ff:ff'
+    def bcast_dst_blocked_helper(
+        self, port, first_host, second_host, success_re, retries
+    ):
+        tcpdump_filter = "udp and ether src %s and ether dst %s" % (
+            first_host.MAC(),
+            "ff:ff:ff:ff:ff:ff",
+        )
         target_addr = str(self.FAUCET_VIPV4.network.broadcast_address)
         for _ in range(retries):
             tcpdump_txt = self.tcpdump_helper(
-                second_host, tcpdump_filter, [
-                    partial(first_host.cmd, (
-                        f'date | socat - udp-datagram:{target_addr}:{port},broadcast'))],
-                packets=1)
+                second_host,
+                tcpdump_filter,
+                [
+                    partial(
+                        first_host.cmd,
+                        (
+                            "date | socat - udp-datagram:%s:%d,broadcast"
+                            % (target_addr, port)
+                        ),
+                    )
+                ],
+                packets=1,
+            )
             if re.search(success_re, tcpdump_txt):
                 return True
             time.sleep(1)
         return False
 
     def verify_bcast_dst_blocked(self, port, first_host, second_host):
         """Verify that a UDP port on a host is blocked from broadcast."""
-        self.assertTrue(self.bcast_dst_blocked_helper(
-            port, first_host, second_host, r'0 packets received by filter', 1))
+        self.assertTrue(
+            self.bcast_dst_blocked_helper(
+                port, first_host, second_host, r"0 packets received by filter", 1
+            )
+        )
 
     def verify_bcast_dst_notblocked(self, port, first_host, second_host):
         """Verify that a UDP port on a host is NOT blocked from broadcast."""
-        self.assertTrue(self.bcast_dst_blocked_helper(
-            port, first_host, second_host, r'1 packet received by filter', 3))
+        self.assertTrue(
+            self.bcast_dst_blocked_helper(
+                port, first_host, second_host, r"1 packet received by filter", 3
+            )
+        )
 
     @staticmethod
     def swap_host_macs(first_host, second_host):
         """Swap the MAC addresses of two Mininet hosts."""
         first_host_mac = first_host.MAC()
         second_host_mac = second_host.MAC()
         first_host.setMAC(second_host_mac)
         second_host.setMAC(first_host_mac)
 
-    def start_exabgp(self, exabgp_conf, timeout=30, log_prefix=''):
+    def start_exabgp(self, exabgp_conf, timeout=30, log_prefix=""):
         """Start exabgp process on controller host."""
-        exabgp_conf_file_name = os.path.join(self.tmpdir, f'{log_prefix}exabgp.conf')
-        exabgp_log = os.path.join(self.tmpdir, f'{log_prefix}exabgp.log')
-        exabgp_out = os.path.join(self.tmpdir, f'{log_prefix}exabgp.out')
-        exabgp_env = ' '.join((
-            'exabgp.daemon.user=root',
-            'exabgp.log.all=true',
-            'exabgp.log.level=DEBUG',
-            f'exabgp.log.destination={exabgp_log}',
-        ))
-        bgp_port = self.config_ports['bgp_port']
-        exabgp_conf = exabgp_conf % {'bgp_port': bgp_port}
-        with open(exabgp_conf_file_name, 'w', encoding='utf-8') as exabgp_conf_file:
+        exabgp_conf_file_name = os.path.join(self.tmpdir, "%sexabgp.conf" % log_prefix)
+        exabgp_log = os.path.join(self.tmpdir, "%sexabgp.log" % log_prefix)
+        exabgp_out = os.path.join(self.tmpdir, "%sexabgp.out" % log_prefix)
+        exabgp_env = " ".join(
+            (
+                "exabgp.daemon.user=root",
+                "exabgp.log.all=true",
+                "exabgp.log.level=DEBUG",
+                "exabgp.log.destination=%s" % exabgp_log,
+            )
+        )
+        bgp_port = self.config_ports["bgp_port"]
+        exabgp_conf = exabgp_conf % {"bgp_port": bgp_port}
+        with open(exabgp_conf_file_name, "w", encoding="utf-8") as exabgp_conf_file:
             exabgp_conf_file.write(exabgp_conf)
         controller = self._get_controller()
         # Ensure exabgp only attempts one connection.
         exabgp_cmd = mininet_test_util.timeout_cmd(
-            f'exabgp {exabgp_conf_file_name} --once -d 2>&1 > {exabgp_out} &', 300)
-        exabgp_cli = f'env {exabgp_env} {exabgp_cmd}'
+            "exabgp %s --once -d 2>&1 > %s &" % (exabgp_conf_file_name, exabgp_out), 300
+        )
+        exabgp_cli = "env %s %s" % (exabgp_env, exabgp_cmd)
         controller.cmd(exabgp_cli)
         for _ in range(timeout):
             if os.path.exists(exabgp_log):
                 break
             time.sleep(1)
         self.assertTrue(
-            os.path.exists(exabgp_log), msg=f'exabgp ({exabgp_cli}) did not start')
+            os.path.exists(exabgp_log), msg="exabgp (%s) did not start" % exabgp_cli
+        )
         return (exabgp_log, exabgp_out)
 
     def wait_bgp_up(self, neighbor, vlan, exabgp_log, exabgp_err):
         """Wait for BGP to come up."""
         label_values = {
-            'neighbor': neighbor,
-            'vlan': vlan,
+            "neighbor": neighbor,
+            "vlan": vlan,
         }
         for _ in range(60):
             uptime = self.scrape_prometheus_var(
-                'bgp_neighbor_uptime', label_values, default=0)
+                "bgp_neighbor_uptime", label_values, default=0
+            )
             if uptime > 0:
                 return
             time.sleep(1)
         exabgp_log_content = []
         for log_name in (exabgp_log, exabgp_err):
             if os.path.exists(log_name):
-                with open(log_name, encoding='utf-8') as log:
+                with open(log_name, encoding="utf-8") as log:
                     exabgp_log_content.append(log.read())
-        self.fail('exabgp did not peer with FAUCET: %s' % '\n'.join(exabgp_log_content))
+        self.fail("exabgp did not peer with FAUCET: %s" % "\n".join(exabgp_log_content))
 
     @staticmethod
     def matching_lines_from_file(exp, log_name):
         exp_re = re.compile(exp)
-        with open(log_name, encoding='utf-8') as log_file:
+        with open(log_name, encoding="utf-8") as log_file:
             return [log_line for log_line in log_file if exp_re.match(log_line)]
         return []
 
     def wait_until_matching_lines_from_file(self, exp, log_name, timeout=30, count=1):
         """Require (count) matching lines to be present in file."""
         assert timeout >= 1
         lines = []
         for _ in range(timeout):
             if os.path.exists(log_name):
                 lines = self.matching_lines_from_file(exp, log_name)
                 if len(lines) >= count:
                     return lines
             time.sleep(1)
-        self.fail(f'{exp} not found in {log_name} ({len(lines)}/{count})')
+        self.fail("%s not found in %s (%d/%d)" % (exp, log_name, len(lines), count))
 
-    def wait_until_no_matching_lines_from_file(self, exp, log_name, timeout=30, count=1):
+    def wait_until_no_matching_lines_from_file(
+        self, exp, log_name, timeout=30, count=1
+    ):
         """Require (count) matching lines to be non-existent in file."""
         assert timeout >= 1
         lines = []
         for _ in range(timeout):
             if os.path.exists(log_name):
                 lines = self.matching_lines_from_file(exp, log_name)
                 if len(lines) >= count:
-                    return self.fail(f'{exp} found in {log_name} ({len(lines)}/{count})')
+                    return self.fail(
+                        "%s found in %s (%d/%d)" % (exp, log_name, len(lines), count)
+                    )
             time.sleep(1)
         return lines
 
     def wait_until_matching_lines_from_faucet_log_files(self, exp, timeout=30, count=1):
         """Require (count) matching lines to be present in file"""
         for controller_env in self.env.values():
-            if 'FAUCET_LOG' in controller_env:
-                log_name = controller_env['FAUCET_LOG']
+            if "FAUCET_LOG" in controller_env:
+                log_name = controller_env["FAUCET_LOG"]
                 self.wait_until_matching_lines_from_file(exp, log_name, timeout, count)
 
     def wait_until_matching_lines_from_gauge_log_files(self, exp, timeout=30, count=1):
         """Require (count) matching lines to be present in file"""
         for controller_env in self.env.values():
-            if 'GAUGE_LOG' in controller_env:
-                log_name = controller_env['GAUGE_LOG']
+            if "GAUGE_LOG" in controller_env:
+                log_name = controller_env["GAUGE_LOG"]
                 self.wait_until_matching_lines_from_file(exp, log_name, timeout, count)
 
     def exabgp_updates(self, exabgp_log, timeout=60):
         """Verify that exabgp process has received BGP updates."""
         controller = self._get_controller()
         updates = []
         # exabgp should have received our BGP updates
         for _ in range(timeout):
             updates = controller.cmd(
-                r'grep UPDATE %s |grep -Eo "\S+ next-hop \S+"' % exabgp_log)
+                r'grep UPDATE %s |grep -Eo "\S+ next-hop \S+"' % exabgp_log
+            )
             if updates:
                 break
             time.sleep(1)
-        self.assertTrue(updates, 'exabgp did not receive BGP updates')
+        self.assertTrue(updates, "exabgp did not receive BGP updates")
         return updates
 
     def wait_exabgp_sent_updates(self, exabgp_log_name):
         """Verify that exabgp process has sent BGP updates."""
         self.wait_until_matching_lines_from_file(
-            r'.+>> [1-9]+[0-9]* UPDATE.+', exabgp_log_name, timeout=60)
+            r".+>> [1-9]+[0-9]* UPDATE.+", exabgp_log_name, timeout=60
+        )
 
-    def start_wpasupplicant(self, host, wpasupplicant_conf, timeout=10, log_prefix='',
-                            wpa_ctrl_socket_path=''):
+    def start_wpasupplicant(
+        self,
+        host,
+        wpasupplicant_conf,
+        timeout=10,
+        log_prefix="",
+        wpa_ctrl_socket_path="",
+    ):
         """Start wpasupplicant process on Mininet host."""
         wpasupplicant_conf_file_name = os.path.join(
-            self.tmpdir, f'{log_prefix}wpasupplicant.conf')
+            self.tmpdir, "%swpasupplicant.conf" % log_prefix
+        )
         wpasupplicant_log = os.path.join(
-            self.tmpdir, f'{log_prefix}wpasupplicant.log')
-        with open(wpasupplicant_conf_file_name, 'w', encoding='utf-8') as wpasupplicant_conf_file:
+            self.tmpdir, "%swpasupplicant.log" % log_prefix
+        )
+        with open(
+            wpasupplicant_conf_file_name, "w", encoding="utf-8"
+        ) as wpasupplicant_conf_file:
             wpasupplicant_conf_file.write(wpasupplicant_conf)
-        wpa_ctrl_socket = ''
+        wpa_ctrl_socket = ""
         if wpa_ctrl_socket_path:
-            wpa_ctrl_socket = f'-C {wpa_ctrl_socket_path}'
+            wpa_ctrl_socket = "-C %s" % wpa_ctrl_socket_path
         wpasupplicant_cmd = mininet_test_util.timeout_cmd(
-            f'wpa_supplicant -dd -t -c {wpasupplicant_conf_file_name}'
-            f' -i {host.defaultIntf()} -D wired -f {wpasupplicant_log} {wpa_ctrl_socket} &', 300)
+            "wpa_supplicant -dd -t -c %s -i %s -D wired -f %s %s &"
+            % (
+                wpasupplicant_conf_file_name,
+                host.defaultIntf(),
+                wpasupplicant_log,
+                wpa_ctrl_socket,
+            ),
+            300,
+        )
         host.cmd(wpasupplicant_cmd)
         for _ in range(timeout):
             if os.path.exists(wpasupplicant_log):
                 break
             time.sleep(1)
         self.assertTrue(
             os.path.exists(wpasupplicant_log),
-            msg=f'wpasupplicant ({wpasupplicant_cmd}) did not start')
+            msg="wpasupplicant (%s) did not start" % wpasupplicant_cmd,
+        )
         return wpasupplicant_log
 
     def ping_all_when_learned(self, retries=3, hard_timeout=1):
         """Verify all hosts can ping each other once FAUCET has learned them all."""
         # Cause hosts to send traffic that FAUCET can use to learn them.
         for _ in range(retries):
             loss = self.ping_all()
@@ -2815,63 +3460,75 @@
             for host in self.hosts_name_ordered():
                 self.require_host_learned(host, hard_timeout=hard_timeout)
             if loss == 0:
                 return
         self.assertEqual(0, loss)
 
     def match_table(self, prefix):
-        exp_prefix = f'{prefix.network_address}/{prefix.netmask}'
+        exp_prefix = "%s/%s" % (prefix.network_address, prefix.netmask)
         if prefix.version == 6:
-            nw_dst_match = {'ipv6_dst': exp_prefix, 'dl_type': IPV6_ETH}
+            nw_dst_match = {"ipv6_dst": exp_prefix, "dl_type": IPV6_ETH}
             table_id = self._IPV6_FIB_TABLE
         else:
-            nw_dst_match = {'nw_dst': exp_prefix, 'dl_type': IPV4_ETH}
+            nw_dst_match = {"nw_dst": exp_prefix, "dl_type": IPV4_ETH}
             table_id = self._IPV4_FIB_TABLE
         return (nw_dst_match, table_id)
 
-    def wait_for_route_as_flow(self, nexthop, prefix,
-                               vlan_vid=None, timeout=30,
-                               nonzero_packets=False):
+    def wait_for_route_as_flow(
+        self, nexthop, prefix, vlan_vid=None, timeout=30, nonzero_packets=False
+    ):
         """Verify a route has been added as a flow."""
         nw_dst_match, table_id = self.match_table(prefix)
-        nexthop_action = f'SET_FIELD: {{eth_dst:{nexthop}}}'
+        nexthop_action = "SET_FIELD: {eth_dst:%s}" % nexthop
         if vlan_vid is not None:
-            nw_dst_match['dl_vlan'] = str(vlan_vid)
+            nw_dst_match["dl_vlan"] = str(vlan_vid)
         if nonzero_packets:
             self.wait_nonzero_packet_count_flow(
-                nw_dst_match, table_id, timeout=timeout,
-                actions=[nexthop_action], ofa_match=False)
+                nw_dst_match,
+                table_id,
+                timeout=timeout,
+                actions=[nexthop_action],
+                ofa_match=False,
+            )
         else:
             self.wait_until_matching_flow(
-                nw_dst_match, table_id, timeout=timeout,
-                actions=[nexthop_action], ofa_match=False)
+                nw_dst_match,
+                table_id,
+                timeout=timeout,
+                actions=[nexthop_action],
+                ofa_match=False,
+            )
 
     def host_ipv4_alias(self, host, alias_ip, intf=None):
         """Add an IPv4 alias address to a host."""
         if intf is None:
             intf = host.intf()
-        del_cmd = f'ip addr del {alias_ip.with_prefixlen} dev {intf}'
-        add_cmd = f'ip addr add {alias_ip.with_prefixlen} dev {intf} label {intf}:1'
+        del_cmd = "ip addr del %s dev %s" % (alias_ip.with_prefixlen, intf)
+        add_cmd = "ip addr add %s dev %s label %s:1" % (
+            alias_ip.with_prefixlen,
+            intf,
+            intf,
+        )
         host.cmd(del_cmd)
         self.quiet_commands(host, (add_cmd,))
 
     @staticmethod
     def _ip_neigh(host, ipa, ip_ver):
-        neighbors = host.cmd(f'ip -{ip_ver} neighbor show {ipa}')
+        neighbors = host.cmd("ip -%u neighbor show %s" % (ip_ver, ipa))
         neighbors_fields = neighbors.split()
         if len(neighbors_fields) >= 5:
             return neighbors.split()[4]
         return None
 
     def _verify_host_learned_mac(self, host, ipa, ip_ver, mac, retries):
         for _ in range(retries):
             if self._ip_neigh(host, ipa, ip_ver) == mac:
                 return
             time.sleep(1)
-        self.fail(f'could not verify {ipa} resolved to {mac}')
+        self.fail("could not verify %s resolved to %s" % (ipa, mac))
 
     def verify_ipv4_host_learned_mac(self, host, ipa, mac, retries=3):
         self._verify_host_learned_mac(host, ipa, 4, mac, retries)
 
     def verify_ipv4_host_learned_host(self, host, learned_host):
         learned_ip = ipaddress.ip_interface(self.host_ipv4(learned_host))
         self.verify_ipv4_host_learned_mac(host, learned_ip.ip, learned_host.MAC())
@@ -2881,206 +3538,272 @@
 
     def verify_ipv6_host_learned_host(self, host, learned_host):
         learned_ip6 = ipaddress.ip_interface(self.host_ipv6(learned_host))
         self.verify_ipv6_host_learned_mac(host, learned_ip6.ip, learned_host.MAC())
 
     def iperf_client(self, client_host, iperf_client_cmd):
         iperf_results = client_host.cmd(iperf_client_cmd)
-        iperf_csv = iperf_results.strip().split(',')
+        iperf_csv = iperf_results.strip().split(",")
         if len(iperf_csv) == 9:
             return int(iperf_csv[-1]) / self.ONEMBPS
         return -1
 
     def iperf(self, client_host, client_ip, server_host, server_ip, seconds):
-
         def run_iperf(iperf_server_cmd, server_host, server_start_exp, port):
             server_out = server_host.popen(
                 iperf_server_cmd,
                 stdin=mininet_test_util.DEVNULL,
                 stderr=subprocess.STDOUT,
-                close_fds=True)
+                close_fds=True,
+            )
             popens = {server_host: server_out}
             for host, line in pmonitor(popens):
                 if host != server_host:
                     continue
                 if not re.search(server_start_exp, line):
                     continue
-                self.wait_for_tcp_listen(
-                    server_host, port, ipv=server_ip.version)
-                iperf_mbps = self.iperf_client(
-                    client_host, iperf_client_cmd)
+                self.wait_for_tcp_listen(server_host, port, ipv=server_ip.version)
+                iperf_mbps = self.iperf_client(client_host, iperf_client_cmd)
                 self._signal_proc_on_port(server_host, port, 9)
                 return iperf_mbps
             return None
 
         timeout = (seconds * 3) + 5
         for _ in range(3):
-            port = mininet_test_util.find_free_port(
-                self.ports_sock, self._test_name())
-            iperf_base_cmd = f'iperf -f M -p {port}'
+            port = mininet_test_util.find_free_port(self.ports_sock, self._test_name())
+            iperf_base_cmd = "iperf -f M -p %u" % port
             if server_ip.version == 6:
-                iperf_base_cmd += ' -V'
-            iperf_server_cmd = f'{iperf_base_cmd} -s -B {server_ip}'
-            iperf_server_cmd = mininet_test_util.timeout_cmd(
-                iperf_server_cmd, timeout)
-            server_start_exp = r'Server listening on TCP port %u' % port
+                iperf_base_cmd += " -V"
+            iperf_server_cmd = "%s -s -B %s" % (iperf_base_cmd, server_ip)
+            iperf_server_cmd = mininet_test_util.timeout_cmd(iperf_server_cmd, timeout)
+            server_start_exp = r"Server listening on TCP port %u" % port
             iperf_client_cmd = mininet_test_util.timeout_cmd(
-                f'{iperf_base_cmd} -y c -c {server_ip} -B {client_ip} -t {seconds}', timeout)
-            iperf_mbps = run_iperf(iperf_server_cmd, server_host, server_start_exp, port)
+                "%s -y c -c %s -B %s -t %u"
+                % (iperf_base_cmd, server_ip, client_ip, seconds),
+                timeout,
+            )
+            iperf_mbps = run_iperf(
+                iperf_server_cmd, server_host, server_start_exp, port
+            )
             if iperf_mbps is not None and iperf_mbps > 0:
                 return iperf_mbps
             time.sleep(1)
         if iperf_mbps == -1:
-            self.fail(f'iperf client {iperf_client_cmd} did not connect to server {iperf_server_cmd}')
-        self.fail(f'iperf server {iperf_server_cmd} never started')
-
-    def verify_ipv4_routing(self, first_host, first_host_routed_ip,
-                            second_host, second_host_routed_ip):
+            self.fail(
+                "iperf client %s did not connect to server %s"
+                % (iperf_client_cmd, iperf_server_cmd)
+            )
+        self.fail("iperf server %s never started" % iperf_server_cmd)
+
+    def verify_ipv4_routing(
+        self, first_host, first_host_routed_ip, second_host, second_host_routed_ip
+    ):
         """Verify one host can IPV4 route to another via FAUCET."""
         self.host_ipv4_alias(first_host, first_host_routed_ip)
         self.host_ipv4_alias(second_host, second_host_routed_ip)
-        self.add_host_route(
-            first_host, second_host_routed_ip, self.FAUCET_VIPV4.ip)
-        self.add_host_route(
-            second_host, first_host_routed_ip, self.FAUCET_VIPV4.ip)
+        self.add_host_route(first_host, second_host_routed_ip, self.FAUCET_VIPV4.ip)
+        self.add_host_route(second_host, first_host_routed_ip, self.FAUCET_VIPV4.ip)
         self.net.ping(hosts=(first_host, second_host))
-        self.wait_for_route_as_flow(
-            first_host.MAC(), first_host_routed_ip.network)
-        self.wait_for_route_as_flow(
-            second_host.MAC(), second_host_routed_ip.network)
+        self.wait_for_route_as_flow(first_host.MAC(), first_host_routed_ip.network)
+        self.wait_for_route_as_flow(second_host.MAC(), second_host_routed_ip.network)
         self.one_ipv4_ping(first_host, second_host_routed_ip.ip)
         self.one_ipv4_ping(second_host, first_host_routed_ip.ip)
         self.verify_ipv4_host_learned_host(first_host, second_host)
         self.verify_ipv4_host_learned_host(second_host, first_host)
         # verify at least 1M iperf
         for client_host, client_ip, server_host, server_ip in (
-                (first_host, first_host_routed_ip.ip,
-                 second_host, second_host_routed_ip.ip),
-                (second_host, second_host_routed_ip.ip,
-                 first_host, first_host_routed_ip.ip)):
-            iperf_mbps = self.iperf(
-                client_host, client_ip, server_host, server_ip, 5)
-            error(f'{self._test_name()}: {iperf_mbps} mbps to {server_ip}\n')
+            (
+                first_host,
+                first_host_routed_ip.ip,
+                second_host,
+                second_host_routed_ip.ip,
+            ),
+            (
+                second_host,
+                second_host_routed_ip.ip,
+                first_host,
+                first_host_routed_ip.ip,
+            ),
+        ):
+            iperf_mbps = self.iperf(client_host, client_ip, server_host, server_ip, 5)
+            error("%s: %u mbps to %s\n" % (self._test_name(), iperf_mbps, server_ip))
             self.assertGreater(iperf_mbps, 1)
         # verify packets matched routing flows
         self.wait_for_route_as_flow(
-            first_host.MAC(), first_host_routed_ip.network,
-            nonzero_packets=True)
+            first_host.MAC(), first_host_routed_ip.network, nonzero_packets=True
+        )
         self.wait_for_route_as_flow(
-            second_host.MAC(), second_host_routed_ip.network,
-            nonzero_packets=True)
+            second_host.MAC(), second_host_routed_ip.network, nonzero_packets=True
+        )
 
     def verify_ipv4_routing_mesh(self):
         """Verify hosts can route to each other via FAUCET."""
         host_pair = self.hosts_name_ordered()[:2]
         first_host, second_host = host_pair
-        first_host_routed_ip = ipaddress.ip_interface('10.0.1.1/24')
-        second_host_routed_ip = ipaddress.ip_interface('10.0.2.1/24')
-        second_host_routed_ip2 = ipaddress.ip_interface('10.0.3.1/24')
+        first_host_routed_ip = ipaddress.ip_interface("10.0.1.1/24")
+        second_host_routed_ip = ipaddress.ip_interface("10.0.2.1/24")
+        second_host_routed_ip2 = ipaddress.ip_interface("10.0.3.1/24")
         self.verify_ipv4_routing(
-            first_host, first_host_routed_ip,
-            second_host, second_host_routed_ip)
+            first_host, first_host_routed_ip, second_host, second_host_routed_ip
+        )
         self.verify_ipv4_routing(
-            first_host, first_host_routed_ip,
-            second_host, second_host_routed_ip2)
+            first_host, first_host_routed_ip, second_host, second_host_routed_ip2
+        )
         self.swap_host_macs(first_host, second_host)
         self.verify_ipv4_routing(
-            first_host, first_host_routed_ip,
-            second_host, second_host_routed_ip)
+            first_host, first_host_routed_ip, second_host, second_host_routed_ip
+        )
         self.verify_ipv4_routing(
-            first_host, first_host_routed_ip,
-            second_host, second_host_routed_ip2)
+            first_host, first_host_routed_ip, second_host, second_host_routed_ip2
+        )
 
     @staticmethod
     def host_drop_all_ips(host):
         for ipv in (4, 6):
-            host.cmd(f'ip -{ipv} addr flush dev {host.defaultIntf()}')
+            host.cmd("ip -%u addr flush dev %s" % (ipv, host.defaultIntf()))
 
-    def setup_ipv6_hosts_addresses(self, first_host, first_host_ip,
-                                   first_host_routed_ip, second_host,
-                                   second_host_ip, second_host_routed_ip):
+    def setup_ipv6_hosts_addresses(
+        self,
+        first_host,
+        first_host_ip,
+        first_host_routed_ip,
+        second_host,
+        second_host_ip,
+        second_host_routed_ip,
+    ):
         """Configure host IPv6 addresses for testing."""
         for host in first_host, second_host:
-            for intf in ('lo', host.intf()):
-                host.cmd(f'ip -6 addr flush dev {intf}')
+            for intf in ("lo", host.intf()):
+                host.cmd("ip -6 addr flush dev %s" % intf)
         self.add_host_ipv6_address(first_host, first_host_ip)
         self.add_host_ipv6_address(second_host, second_host_ip)
-        self.add_host_ipv6_address(first_host, first_host_routed_ip, intf='lo')
-        self.add_host_ipv6_address(second_host, second_host_routed_ip, intf='lo')
+        self.add_host_ipv6_address(first_host, first_host_routed_ip, intf="lo")
+        self.add_host_ipv6_address(second_host, second_host_routed_ip, intf="lo")
         for host in first_host, second_host:
             self.require_host_learned(host)
 
-    def verify_ipv6_routing(self, first_host, first_host_ip,
-                            first_host_routed_ip, second_host,
-                            second_host_ip, second_host_routed_ip):
+    def verify_ipv6_routing(
+        self,
+        first_host,
+        first_host_ip,
+        first_host_routed_ip,
+        second_host,
+        second_host_ip,
+        second_host_routed_ip,
+    ):
         """Verify one host can IPV6 route to another via FAUCET."""
         self.one_ipv6_ping(first_host, second_host_ip.ip)
         self.one_ipv6_ping(second_host, first_host_ip.ip)
-        self.add_host_route(
-            first_host, second_host_routed_ip, self.FAUCET_VIPV6.ip)
-        self.add_host_route(
-            second_host, first_host_routed_ip, self.FAUCET_VIPV6.ip)
-        self.wait_for_route_as_flow(
-            first_host.MAC(), first_host_routed_ip.network)
-        self.wait_for_route_as_flow(
-            second_host.MAC(), second_host_routed_ip.network)
+        self.add_host_route(first_host, second_host_routed_ip, self.FAUCET_VIPV6.ip)
+        self.add_host_route(second_host, first_host_routed_ip, self.FAUCET_VIPV6.ip)
+        self.wait_for_route_as_flow(first_host.MAC(), first_host_routed_ip.network)
+        self.wait_for_route_as_flow(second_host.MAC(), second_host_routed_ip.network)
         self.one_ipv6_controller_ping(first_host)
         self.one_ipv6_controller_ping(second_host)
         self.one_ipv6_ping(first_host, second_host_routed_ip.ip)
         # verify at least 1M iperf
         for client_host, client_ip, server_host, server_ip in (
-                (first_host, first_host_routed_ip.ip,
-                 second_host, second_host_routed_ip.ip),
-                (second_host, second_host_routed_ip.ip,
-                 first_host, first_host_routed_ip.ip)):
-            iperf_mbps = self.iperf(
-                client_host, client_ip, server_host, server_ip, 5)
-            error(f'{self._test_name()}: {iperf_mbps} mbps to {server_ip}\n')
+            (
+                first_host,
+                first_host_routed_ip.ip,
+                second_host,
+                second_host_routed_ip.ip,
+            ),
+            (
+                second_host,
+                second_host_routed_ip.ip,
+                first_host,
+                first_host_routed_ip.ip,
+            ),
+        ):
+            iperf_mbps = self.iperf(client_host, client_ip, server_host, server_ip, 5)
+            error("%s: %u mbps to %s\n" % (self._test_name(), iperf_mbps, server_ip))
             self.assertGreater(iperf_mbps, 1)
         self.one_ipv6_ping(first_host, second_host_ip.ip)
         self.verify_ipv6_host_learned_mac(
-            first_host, second_host_ip.ip, second_host.MAC())
+            first_host, second_host_ip.ip, second_host.MAC()
+        )
         self.one_ipv6_ping(second_host, first_host_ip.ip)
         self.verify_ipv6_host_learned_mac(
-            second_host, first_host_ip.ip, first_host.MAC())
+            second_host, first_host_ip.ip, first_host.MAC()
+        )
 
-    def verify_ipv6_routing_pair(self, first_host, first_host_ip,
-                                 first_host_routed_ip, second_host,
-                                 second_host_ip, second_host_routed_ip):
+    def verify_ipv6_routing_pair(
+        self,
+        first_host,
+        first_host_ip,
+        first_host_routed_ip,
+        second_host,
+        second_host_ip,
+        second_host_routed_ip,
+    ):
         """Verify hosts can route IPv6 to each other via FAUCET."""
         self.setup_ipv6_hosts_addresses(
-            first_host, first_host_ip, first_host_routed_ip,
-            second_host, second_host_ip, second_host_routed_ip)
+            first_host,
+            first_host_ip,
+            first_host_routed_ip,
+            second_host,
+            second_host_ip,
+            second_host_routed_ip,
+        )
         self.verify_ipv6_routing(
-            first_host, first_host_ip, first_host_routed_ip,
-            second_host, second_host_ip, second_host_routed_ip)
+            first_host,
+            first_host_ip,
+            first_host_routed_ip,
+            second_host,
+            second_host_ip,
+            second_host_routed_ip,
+        )
 
     def verify_ipv6_routing_mesh(self):
         """Verify IPv6 routing between hosts and multiple subnets."""
         host_pair = self.hosts_name_ordered()[:2]
         first_host, second_host = host_pair
-        first_host_ip = ipaddress.ip_interface('fc00::1:1/112')
-        second_host_ip = ipaddress.ip_interface('fc00::1:2/112')
-        first_host_routed_ip = ipaddress.ip_interface('fc00::10:1/112')
-        second_host_routed_ip = ipaddress.ip_interface('fc00::20:1/112')
-        second_host_routed_ip2 = ipaddress.ip_interface('fc00::30:1/112')
+        first_host_ip = ipaddress.ip_interface("fc00::1:1/112")
+        second_host_ip = ipaddress.ip_interface("fc00::1:2/112")
+        first_host_routed_ip = ipaddress.ip_interface("fc00::10:1/112")
+        second_host_routed_ip = ipaddress.ip_interface("fc00::20:1/112")
+        second_host_routed_ip2 = ipaddress.ip_interface("fc00::30:1/112")
         self.verify_ipv6_routing_pair(
-            first_host, first_host_ip, first_host_routed_ip,
-            second_host, second_host_ip, second_host_routed_ip)
+            first_host,
+            first_host_ip,
+            first_host_routed_ip,
+            second_host,
+            second_host_ip,
+            second_host_routed_ip,
+        )
         self.verify_ipv6_routing_pair(
-            first_host, first_host_ip, first_host_routed_ip,
-            second_host, second_host_ip, second_host_routed_ip2)
+            first_host,
+            first_host_ip,
+            first_host_routed_ip,
+            second_host,
+            second_host_ip,
+            second_host_routed_ip2,
+        )
         self.swap_host_macs(first_host, second_host)
         self.verify_ipv6_routing_pair(
-            first_host, first_host_ip, first_host_routed_ip,
-            second_host, second_host_ip, second_host_routed_ip)
+            first_host,
+            first_host_ip,
+            first_host_routed_ip,
+            second_host,
+            second_host_ip,
+            second_host_routed_ip,
+        )
         self.verify_ipv6_routing_pair(
-            first_host, first_host_ip, first_host_routed_ip,
-            second_host, second_host_ip, second_host_routed_ip2)
+            first_host,
+            first_host_ip,
+            first_host_routed_ip,
+            second_host,
+            second_host_ip,
+            second_host_routed_ip2,
+        )
 
     def verify_invalid_bgp_route(self, pattern):
         """Check if we see the pattern in Faucet's log."""
         for cont_env in self.env.values():
-            if 'FAUCET_LOG' in cont_env:
-                lines = self.matching_lines_from_file(
-                    pattern, cont_env['FAUCET_LOG'])
-                self.assertGreater(len(lines), 0, msg=f'{pattern} not found in {cont_env["FAUCET_LOG"]}')
+            if "FAUCET_LOG" in cont_env:
+                lines = self.matching_lines_from_file(pattern, cont_env["FAUCET_LOG"])
+                self.assertGreater(
+                    len(lines),
+                    0,
+                    msg="%s not found in %s" % (pattern, cont_env["FAUCET_LOG"]),
+                )
```

### Comparing `c65faucet-1.0.49/clib/mininet_test_base_topo.py` & `c65faucet-1.0.50/clib/mininet_test_base_topo.py`

 * *Files 11% similar despite different names*

```diff
@@ -63,74 +63,97 @@
 
     def _dp_ports(self):
         """Return ports on the first DP"""
         return list(self.topo.ports[self.topo.switches_by_id[0]].keys())
 
     def get_gauge_watcher_config(self):
         """Return gauge watcher config"""
-        return f"""
+        return """
     port_stats:
-        dps: ['{self.topo.switches_by_id[0]}']
+        dps: ['%s']
         type: 'port_stats'
         interval: 5
         db: 'stats_file'
     port_state:
-        dps: ['{self.topo.switches_by_id[0]}']
+        dps: ['%s']
         type: 'port_state'
         interval: 5
         db: 'state_file'
     flow_table:
-        dps: ['{self.topo.switches_by_id[0]}']
+        dps: ['%s']
         type: 'flow_table'
         interval: 5
-        db: 'flow_dir'"""
+        db: 'flow_dir'
+""" % (
+            self.topo.switches_by_id[0],
+            self.topo.switches_by_id[0],
+            self.topo.switches_by_id[0],
+        )
 
     def first_switch(self):
         """Return the first switch"""
         return self.net.get(self.topo.switches_by_id[0])
 
     def port_labels(self, port_no):
         """Return regex for port label"""
-        port_name = f'b{port_no}'
-        return {'port': port_name, 'port_description': r'.+'}
+        port_name = "b%u" % port_no
+        return {"port": port_name, "port_description": r".+"}
 
     @staticmethod
     def acls():
         """Defined configuration ACLs"""
         return {}
 
     def faucet_vip(self, i):
         """Faucet VLAN VIP"""
-        return f'10.{i + 1}.0.254/{self.NETPREFIX}'
+        return "10.%u.0.254/%u" % (i + 1, self.NETPREFIX)
 
     @staticmethod
     def faucet_mac(i):
         """Faucet VLAN MAC"""
-        return f'00:00:00:00:00:{i + 1}{i + 1}'
+        return "00:00:00:00:00:%u%u" % (i + 1, i + 1)
 
     def host_ip_address(self, host_index, vlan_index):
         """Create a string of the host IP address"""
         if isinstance(vlan_index, (list, tuple)):
             vlan_index = vlan_index[0]
-        return f'10.{vlan_index + 1}.0.{host_index + 1}/{self.NETPREFIX}'
+        return "10.%u.0.%u/%u" % (vlan_index + 1, host_index + 1, self.NETPREFIX)
 
     def host_ping(self, src_host, dst_ip, intf=None):
         """Default method to ping from one host to an IP address"""
         self.one_ipv4_ping(
-            src_host, dst_ip, require_host_learned=False, retries=5, timeout=1000, intf=intf)
+            src_host,
+            dst_ip,
+            require_host_learned=False,
+            retries=5,
+            timeout=1000,
+            intf=intf,
+        )
 
     def set_host_ip(self, host, host_ip):
         """Default method for setting a hosts IP address"""
         host.setIP(str(host_ip.ip), prefixLen=self.NETPREFIX)
 
-    def build_net(self, host_links=None, host_vlans=None, switch_links=None,
-                  link_vlans=None, mininet_host_options=None,
-                  n_vlans=1, dp_options=None, host_options=None,
-                  link_options=None, vlan_options=None, routers=None, router_options=None,
-                  include=None, include_optional=None):
+    def build_net(
+        self,
+        host_links=None,
+        host_vlans=None,
+        switch_links=None,
+        link_vlans=None,
+        mininet_host_options=None,
+        n_vlans=1,
+        dp_options=None,
+        host_options=None,
+        link_options=None,
+        vlan_options=None,
+        routers=None,
+        router_options=None,
+        include=None,
+        include_optional=None,
+    ):
         """
         Args:
             host_links (dict): Host index key to list of dp indices
             host_vlans (dict): Host index key to vlan index/indices
             switch_links (list): List of link tuples of switch indices (u, v)
             link_vlans (dict): Link tuple of switch indices (u, v) mapping to vlans
             mininet_host_options (dict): Host index map to additional mininet host options
@@ -154,40 +177,44 @@
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             hw_dpid=self.hw_dpid,
             hw_ports=self.switch_map,
             port_order=self.port_order,
             start_port=self.start_port,
-            host_options=mininet_host_options
+            host_options=mininet_host_options,
         )
         self.dpids = self.topo.get_dpids()
         self.dpid = self.dpids[0]
         # host_port_maps = {host_n: {switch_n: [ports, ...], ...}, ...}
         # link_port_maps = {(switch_n, switch_m): [ports, ...], ...}
-        port_maps, self.host_port_maps, self.link_port_maps = self.topo.create_port_maps()
+        (
+            port_maps,
+            self.host_port_maps,
+            self.link_port_maps,
+        ) = self.topo.create_port_maps()
         self.port_map = port_maps[self.dpid]
         dpid_names = {}
         # pylint: disable=consider-using-dict-items
         for i in self.topo.switches_by_id:
             dpid = self.topo.dpids_by_id[i]
             name = self.topo.switches_by_id[i]
             dpid_names[dpid] = name
         self.set_dpid_names(dpid_names)
         self.configuration_options = {
-            'acl_options': self.acls(),
-            'dp_options': dp_options,
-            'host_options': host_options,
-            'link_options': link_options,
-            'vlan_options': vlan_options,
-            'routers': routers,
-            'router_options': router_options,
-            'include': include,
-            'include_optional': include_optional,
-            'ignored_switches': self.IGNORED_SWITCHES
+            "acl_options": self.acls(),
+            "dp_options": dp_options,
+            "host_options": host_options,
+            "link_options": link_options,
+            "vlan_options": vlan_options,
+            "routers": routers,
+            "router_options": router_options,
+            "include": include,
+            "include_optional": include_optional,
+            "ignored_switches": self.IGNORED_SWITCHES,
         }
         self.CONFIG = self.topo.get_config(
             n_vlans,
             **self.configuration_options,
         )
         self.mininet_host_options = mininet_host_options
         self.n_vlans = n_vlans
@@ -210,92 +237,112 @@
             if vlan is not None:
                 if isinstance(vlan, list):
                     vlan = tuple(vlan)
                 ips_for_vlans.setdefault(vlan, 0)
                 ip_addr = self.host_ip_address(ips_for_vlans[vlan], vlan)
                 ips_for_vlans[vlan] += 1
                 if self.mininet_host_options and host_id in self.mininet_host_options:
-                    mininet_ip = self.mininet_host_options[host_id].get('ip', None)
+                    mininet_ip = self.mininet_host_options[host_id].get("ip", None)
                     if mininet_ip:
                         ip_addr = mininet_ip
                 ip_interface = ipaddress.ip_interface(ip_addr)
                 self.set_host_ip(host, ip_interface)
             self.host_information[host_id] = {
-                'host': host,
-                'ip': ip_interface,
-                'mac': host.MAC(),
-                'vlan': vlan,
-                'bond': None,
-                'ports': self.host_port_maps[host_id]
+                "host": host,
+                "ip": ip_interface,
+                "mac": host.MAC(),
+                "vlan": vlan,
+                "bond": None,
+                "ports": self.host_port_maps[host_id],
             }
         # Store faucet vip interfaces
         self.faucet_vips = {}
         for vlan in range(self.n_vlans):
             self.faucet_vips[vlan] = ipaddress.ip_interface(self.faucet_vip(vlan))
         # Setup the linux bonds for LACP connected hosts
         self.setup_lacp_bonds()
         # Add host routes to hosts for inter vlan routing
         self.setup_intervlan_host_routes()
 
     def setup_lacp_bonds(self):
         """Search through host options for lacp hosts and configure accordingly"""
-        host_options = self.configuration_options['host_options']
+        host_options = self.configuration_options["host_options"]
         if not host_options:
             return
         bond_index = 1
         for host_id, options in host_options.items():
-            if 'lacp' in options:
-                host = self.host_information[host_id]['host']
+            if "lacp" in options:
+                host = self.host_information[host_id]["host"]
                 # LACP must be configured with host ports down
                 for dp_i, ports in self.host_port_maps[host_id].items():
                     for port in ports:
                         self.set_port_down(port, self.topo.dpids_by_id[dp_i])
                 orig_ip = host.IP()
                 lacp_switches = [
                     self.net.get(self.topo.switches_by_id[i])
-                    for i in self.host_port_maps[host_id]]
+                    for i in self.host_port_maps[host_id]
+                ]
                 bond_members = [
-                    pair[0].name for switch in lacp_switches for pair in host.connectionsTo(switch)]
-                bond_name = f'bond{bond_index}'
-                self.host_information[host_id]['bond'] = bond_name
+                    pair[0].name
+                    for switch in lacp_switches
+                    for pair in host.connectionsTo(switch)
+                ]
+                bond_name = "bond%u" % (bond_index)
+                self.host_information[host_id]["bond"] = bond_name
                 for bond_member in bond_members:
                     # Deconfigure bond members
-                    self.quiet_commands(host, (
-                        f'ip link set {bond_member} down',
-                        f'ip address flush dev {bond_member}'))
+                    self.quiet_commands(
+                        host,
+                        (
+                            "ip link set %s down" % bond_member,
+                            "ip address flush dev %s" % bond_member,
+                        ),
+                    )
                 # Configure bond interface
-                self.quiet_commands(host, (
-                    (f'ip link add {bond_name} address 0e:00:00:00:00:99 '
-                     'type bond mode 802.3ad lacp_rate fast miimon 100 '
-                     'xmit_hash_policy layer2+3'),
-                    f'ip add add {orig_ip}/{self.NETPREFIX} dev {bond_name}',
-                    f'ip link set {bond_name} up'))
+                self.quiet_commands(
+                    host,
+                    (
+                        (
+                            "ip link add %s address 0e:00:00:00:00:99 "
+                            "type bond mode 802.3ad lacp_rate fast miimon 100 "
+                            "xmit_hash_policy layer2+3"
+                        )
+                        % (bond_name),
+                        "ip add add %s/%s dev %s"
+                        % (orig_ip, self.NETPREFIX, bond_name),
+                        "ip link set %s up" % bond_name,
+                    ),
+                )
                 # Add bond members
                 for bond_member in bond_members:
-                    self.quiet_commands(host, (
-                        f'ip link set dev {bond_member} master {bond_name}',))
+                    self.quiet_commands(
+                        host,
+                        ("ip link set dev %s master %s" % (bond_member, bond_name),),
+                    )
                 bond_index += 1
                 # Return the ports to UP
                 for dp_i, ports in self.host_port_maps[host_id].items():
                     for port in ports:
                         self.set_port_up(port, self.topo.dpids_by_id[dp_i])
 
     def setup_intervlan_host_routes(self):
         """Configure host routes between hosts that belong on routed VLANs"""
-        if self.configuration_options['routers']:
+        if self.configuration_options["routers"]:
             for src_name, src in self.host_information.items():
-                src_host = src['host']
-                src_vlan = src['vlan']
-                src_ip = src['ip']
+                src_host = src["host"]
+                src_vlan = src["vlan"]
+                src_ip = src["ip"]
                 for dst_name, dst in self.host_information.items():
                     if src_name != dst_name:
-                        dst_host = dst['host']
-                        dst_vlan = dst['vlan']
-                        dst_ip = dst['ip']
-                        if src_vlan != dst_vlan and self.is_routed_vlans(src_vlan, dst_vlan):
+                        dst_host = dst["host"]
+                        dst_vlan = dst["vlan"]
+                        dst_ip = dst["ip"]
+                        if src_vlan != dst_vlan and self.is_routed_vlans(
+                            src_vlan, dst_vlan
+                        ):
                             src_faucet_vip = self.faucet_vips[src_vlan]
                             dst_faucet_vip = self.faucet_vips[dst_vlan]
                             self.add_host_route(src_host, dst_ip, src_faucet_vip.ip)
                             self.add_host_route(dst_host, src_ip, dst_faucet_vip.ip)
 
     def debug(self):
         """Print additional information when debugging"""
@@ -305,60 +352,80 @@
             pprint.pprint(self.host_information)
             raise
 
     def verify_no_cable_errors(self):
         """Check that prometheus does not detect any stack cabling errors on all DPs"""
         for i, name in self.topo.switches_by_id.items():
             dpid = self.dpids[i]
-            labels = {'dp_id': '0x%x' % int(dpid), 'dp_name': name}
+            labels = {"dp_id": "0x%x" % int(dpid), "dp_name": name}
             self.assertEqual(
-                0, self.scrape_prometheus_var(
-                    var='stack_cabling_errors_total', labels=labels, default=None))
+                0,
+                self.scrape_prometheus_var(
+                    var="stack_cabling_errors_total", labels=labels, default=None
+                ),
+            )
             self.assertGreater(
                 self.scrape_prometheus_var(
-                    var='stack_probes_received_total', labels=labels), 0)
+                    var="stack_probes_received_total", labels=labels
+                ),
+                0,
+            )
 
     def verify_stack_hosts(self, verify_bridge_local_rule=True, retries=3):
         """Verify hosts with stack LLDP messages"""
         lldp_cap_files = []
         for host in self.hosts_name_ordered():
-            lldp_cap_file = os.path.join(self.tmpdir, f'{host}-lldp.cap')
+            lldp_cap_file = os.path.join(self.tmpdir, "%s-lldp.cap" % host)
             lldp_cap_files.append(lldp_cap_file)
-            host.cmd(timeout_cmd(
-                f'tcpdump -U -n -c 1 -i {host.defaultIntf()} -w {host.MAC()}'
-                f' ether proto 0x88CC and not ether src {lldp_cap_file} &', 60))
+            host.cmd(
+                timeout_cmd(
+                    "tcpdump -U -n -c 1 -i %s -w %s ether proto 0x88CC and not ether src %s &"
+                    % (host.defaultIntf(), host.MAC(), lldp_cap_file),
+                    60,
+                )
+            )
         # should not flood LLDP from hosts
         self.verify_lldp_blocked(self.hosts_name_ordered())
         # hosts should see no LLDP probes
         self.verify_empty_caps(lldp_cap_files)
         if verify_bridge_local_rule:
             # Verify 802.1x flood block triggered.
             for dpid in self.dpids:
                 self.wait_nonzero_packet_count_flow(
-                    {'dl_dst': '01:80:c2:00:00:00/ff:ff:ff:ff:ff:f0'},
-                    dpid=dpid, table_id=self._FLOOD_TABLE, ofa_match=False)
+                    {"dl_dst": "01:80:c2:00:00:00/ff:ff:ff:ff:ff:f0"},
+                    dpid=dpid,
+                    table_id=self._FLOOD_TABLE,
+                    ofa_match=False,
+                )
         self.retry_net_ping(retries=retries)
 
     def stack_port_status(self, dpid, dp_name, port_no):
         """Return the status of a stack port from prometheus"""
         labels = self.port_labels(port_no)
-        labels.update({'dp_id': '0x%x' % int(dpid), 'dp_name': dp_name})
+        labels.update({"dp_id": "0x%x" % int(dpid), "dp_name": dp_name})
         return self.scrape_prometheus_var(
-            'port_stack_state', labels=labels,
-            default=None, dpid=dpid)
+            "port_stack_state", labels=labels, default=None, dpid=dpid
+        )
 
     def wait_for_stack_port_status(self, dpid, dp_name, port_no, status, timeout=25):
         """Wait until prometheus detects a stack port has a certain status"""
         labels = self.port_labels(port_no)
-        labels.update({'dp_id': '0x%x' % int(dpid), 'dp_name': dp_name})
+        labels.update({"dp_id": "0x%x" % int(dpid), "dp_name": dp_name})
         if not self.wait_for_prometheus_var(
-                'port_stack_state', status, labels=labels,
-                default=None, dpid=False, timeout=timeout):
-            self.fail('did not get expected dpid %x port %u port_stack_state %u' % (
-                int(dpid), port_no, status))
+            "port_stack_state",
+            status,
+            labels=labels,
+            default=None,
+            dpid=False,
+            timeout=timeout,
+        ):
+            self.fail(
+                "did not get expected dpid %x port %u port_stack_state %u"
+                % (int(dpid), port_no, status)
+            )
 
     def one_stack_port_down(self, dpid, dp_name, port):
         """Set a stack port down and wait for prometheus to detect the change"""
         self.set_port_down(port, dpid, wait=False)
         self.wait_for_stack_port_status(dpid, dp_name, port, 4)
 
     def one_stack_port_up(self, dpid, dp_name, port):
@@ -380,29 +447,29 @@
                     status = self.stack_port_status(dpid, name, port)
                     links += 1
                     if status == 3:  # STACK_STATE_UP
                         links_up += 1
             prop_up = links_up / links
             if prop_up >= prop:
                 return
-        self.fail(f'not enough links up: {links_up} / {links}')
+        self.fail("not enough links up: %f / %f" % (links_up, links))
 
     def verify_stack_down(self):
         """Verify all stack ports are down"""
         links = 0
         links_down = 0
         for link, ports in self.link_port_maps.items():
             for port in ports:
                 dpid = self.topo.dpids_by_id[link[0]]
                 name = self.topo.switches_by_id[link[0]]
                 status = self.stack_port_status(dpid, name, port)
                 links += 1
                 if status != 3:
                     links_down += 1
-        self.assertEqual(links, links_down, 'Not all links DOWN')
+        self.assertEqual(links, links_down, "Not all links DOWN")
 
     def verify_one_stack_down(self, stack_offset_port, coldstart=False):
         """Test conditions when one stack port is down"""
         self.retry_net_ping()
         stack_link = None
         count = 0
         for sport, link in self.topo.ports[self.topo.switches_by_id[0]].items():
@@ -417,103 +484,128 @@
         self.set_port_down(remote_stack_port, self.dpids[1], wait=False)
         # test case where one link is down when coldstarted.
         if coldstart:
             self.coldstart_conf()
         self.verify_stack_up(prop=0.75)
         self.verify_stack_hosts(verify_bridge_local_rule=False)
         # Broadcast works, and first switch doesn't see broadcast packet ins from stack.
-        packet_in_before_broadcast = self.scrape_prometheus_var('of_vlan_packet_ins')
+        packet_in_before_broadcast = self.scrape_prometheus_var("of_vlan_packet_ins")
         self.verify_broadcast()
-        packet_in_after_broadcast = self.scrape_prometheus_var('of_vlan_packet_ins')
-        self.assertEqual(
-            packet_in_before_broadcast,
-            packet_in_after_broadcast)
+        packet_in_after_broadcast = self.scrape_prometheus_var("of_vlan_packet_ins")
+        self.assertEqual(packet_in_before_broadcast, packet_in_after_broadcast)
         self.verify_no_cable_errors()
 
     def verify_no_arp_storm(self, ping_host, tcpdump_host):
         """Check that there is no excess ARP packets in the network"""
         switch_to_switch_links = 0
         for link in self.topo.links():
             src_node, dst_node = link
             if self.topo.isSwitch(src_node):
                 if self.topo.isSwitch(dst_node):
                     switch_to_switch_links += 1
         num_arp_expected = switch_to_switch_links * 2
-        tcpdump_filter = f'arp and ether src {ping_host.MAC()}'
+        tcpdump_filter = "arp and ether src %s" % ping_host.MAC()
         tcpdump_txt = self.tcpdump_helper(
-            tcpdump_host, tcpdump_filter, [
-                lambda: ping_host.cmd(f'arp -d {tcpdump_host.IP()}'),
-                lambda: ping_host.cmd(f'ping -c1 {tcpdump_host.IP()}')],
-            packets=(num_arp_expected + 1))
-        num_arp_received = len(re.findall(
-            f'who-has {tcpdump_host.IP()} tell {ping_host.IP()}', tcpdump_txt))
+            tcpdump_host,
+            tcpdump_filter,
+            [
+                lambda: ping_host.cmd("arp -d %s" % tcpdump_host.IP()),
+                lambda: ping_host.cmd("ping -c1 %s" % tcpdump_host.IP()),
+            ],
+            packets=(num_arp_expected + 1),
+        )
+        num_arp_received = len(
+            re.findall(
+                "who-has %s tell %s" % (tcpdump_host.IP(), ping_host.IP()), tcpdump_txt
+            )
+        )
         self.assertTrue(num_arp_received)
         self.assertLessEqual(num_arp_received, num_arp_expected)
 
     def verify_stack_has_no_loop(self):
         """Ping between first and last hosts (by name) then verify there is no broadcast storm"""
         for ping_host, tcpdump_host in (
-                (self.hosts_name_ordered()[0], self.hosts_name_ordered()[-1]),
-                (self.hosts_name_ordered()[-1], self.hosts_name_ordered()[0])):
+            (self.hosts_name_ordered()[0], self.hosts_name_ordered()[-1]),
+            (self.hosts_name_ordered()[-1], self.hosts_name_ordered()[0]),
+        ):
             self.verify_no_arp_storm(ping_host, tcpdump_host)
 
     def verify_all_stack_hosts(self):
         """Test conditions for stack hosts"""
         for _ in range(2):
             self.verify_stack_up()
             self.verify_no_cable_errors()
             self.verify_stack_hosts()
             self.verify_traveling_dhcp_mac()
             self.verify_unicast_not_looped()
             self.verify_no_bcast_to_self()
             self.verify_stack_has_no_loop()
             self.flap_all_switch_ports()
 
-    def verify_tunnel_established(self, src_host, dst_host, other_host, packets=3, dpid=None):
+    def verify_tunnel_established(
+        self, src_host, dst_host, other_host, packets=3, dpid=None
+    ):
         """Verify ICMP packets tunnelled from src to dst."""
-        icmp_match = {'eth_type': IPV4_ETH, 'ip_proto': 1}
+        icmp_match = {"eth_type": IPV4_ETH, "ip_proto": 1}
         self.wait_until_matching_flow(
-            icmp_match, table_id=self._PORT_ACL_TABLE, ofa_match=False, dpid=dpid)
+            icmp_match, table_id=self._PORT_ACL_TABLE, ofa_match=False, dpid=dpid
+        )
         tcpdump_text = self.tcpdump_helper(
-            dst_host, 'icmp[icmptype] == 8', [
+            dst_host,
+            "icmp[icmptype] == 8",
+            [
                 # need to set static ARP as only ICMP is tunnelled.
-                lambda: src_host.cmd(f'arp -s {other_host.IP()} {other_host.MAC()}'),
-                lambda: src_host.cmd(f'ping -c{packets} -t1 {other_host.IP()}')
+                lambda: src_host.cmd(
+                    "arp -s %s %s" % (other_host.IP(), other_host.MAC())
+                ),
+                lambda: src_host.cmd("ping -c%u -t1 %s" % (packets, other_host.IP())),
             ],
-            packets=1, timeout=(packets + 1),
+            packets=1,
+            timeout=(packets + 1),
         )
         self.wait_nonzero_packet_count_flow(
-            icmp_match, table_id=self._PORT_ACL_TABLE, ofa_match=False, dpid=dpid)
-        self.assertTrue(re.search(
-            f'{other_host.IP()}: ICMP echo request', tcpdump_text
-        ), 'Tunnel was not established')
+            icmp_match, table_id=self._PORT_ACL_TABLE, ofa_match=False, dpid=dpid
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % other_host.IP(), tcpdump_text),
+            "Tunnel was not established",
+        )
 
     def verify_one_broadcast(self, from_host, to_hosts):
         """Verify host connectivity via broadcast"""
-        self.assertGreater(len(to_hosts), 1, 'Testing only one ext host is not useful')
+        self.assertGreater(len(to_hosts), 1, "Testing only one ext host is not useful")
         received_broadcasts = []
         for to_host in to_hosts:
-            if self.verify_broadcast(hosts=(from_host, to_host), broadcast_expected=None):
+            if self.verify_broadcast(
+                hosts=(from_host, to_host), broadcast_expected=None
+            ):
                 received_broadcasts.append(to_host)
         received_names = {host.name: host for host in received_broadcasts}
-        self.assertEqual(len(received_broadcasts), 1,
-                         f'Received not exactly one broadcast from {from_host.name}: {received_names}')
+        self.assertEqual(
+            len(received_broadcasts),
+            1,
+            "Received not exactly one broadcast from %s: %s"
+            % (from_host.name, received_names),
+        )
 
     def map_int_ext_hosts(self):
         """
         Obtains a list of the interal hosts, the external hosts and a dictionary
             of the internal and external hosts for each DP by DP name
         Returns int_hosts, ext_hosts, dp_hosts
         """
         int_hosts = []
         ext_hosts = []
-        dp_hosts = {self.topo.switches_by_id[dp_index]: ([], []) for dp_index in range(self.NUM_DPS)}
-        for host_id, options in self.configuration_options['host_options'].items():
-            host = self.host_information[host_id]['host']
-            if options.get('loop_protect_external', False):
+        dp_hosts = {
+            self.topo.switches_by_id[dp_index]: ([], [])
+            for dp_index in range(self.NUM_DPS)
+        }
+        for host_id, options in self.configuration_options["host_options"].items():
+            host = self.host_information[host_id]["host"]
+            if options.get("loop_protect_external", False):
                 ext_hosts.append(host)
                 int_or_ext = 1
             else:
                 int_hosts.append(host)
                 int_or_ext = 0
             for dp_i in self.host_port_maps[host_id].keys():
                 switch = self.topo.switches_by_id[dp_i]
@@ -531,39 +623,45 @@
         """
         self.verify_stack_up()
         int_hosts, ext_hosts, _ = self.map_int_ext_hosts()
 
         for int_host in int_hosts:
             # All internal hosts can reach other internal hosts.
             for other_int_host in int_hosts - {int_host}:
-                self.verify_broadcast(hosts=(int_host, other_int_host), broadcast_expected=True)
+                self.verify_broadcast(
+                    hosts=(int_host, other_int_host), broadcast_expected=True
+                )
                 self.one_ipv4_ping(int_host, other_int_host.IP())
 
             # All internal hosts can reach exactly one external host.
             self.verify_one_broadcast(int_host, ext_hosts)
 
         for ext_host in ext_hosts:
             # All external hosts can reach internal hosts.
             for int_host in int_hosts:
-                self.verify_broadcast(hosts=(ext_host, int_host), broadcast_expected=True)
+                self.verify_broadcast(
+                    hosts=(ext_host, int_host), broadcast_expected=True
+                )
                 self.one_ipv4_ping(ext_host, int_host.IP())
 
             # All external hosts cannot flood to each other.
             for other_ext_host in ext_hosts - {ext_host}:
-                self.verify_broadcast(hosts=(ext_host, other_ext_host), broadcast_expected=False)
+                self.verify_broadcast(
+                    hosts=(ext_host, other_ext_host), broadcast_expected=False
+                )
 
     def set_externals_state(self, dp_name, externals_up):
         """Set the port up/down state of all external ports on a switch"""
-        dp_conf = self._get_faucet_conf()['dps'][dp_name]
-        for port_num, port_conf in dp_conf['interfaces'].items():
-            if port_conf.get('loop_protect_external'):
+        dp_conf = self._get_faucet_conf()["dps"][dp_name]
+        for port_num, port_conf in dp_conf["interfaces"].items():
+            if port_conf.get("loop_protect_external"):
                 if externals_up:
-                    self.set_port_up(port_num, dp_conf.get('dp_id'))
+                    self.set_port_up(port_num, dp_conf.get("dp_id"))
                 else:
-                    self.set_port_down(port_num, dp_conf.get('dp_id'))
+                    self.set_port_down(port_num, dp_conf.get("dp_id"))
 
     def validate_with_externals_down(self, dp_name):
         """Check situation when all externals on a given dp are down"""
         self.set_externals_state(dp_name, False)
         self.verify_protected_connectivity()
         self.set_externals_state(dp_name, True)
 
@@ -571,66 +669,89 @@
         """Faucet code is not currently correct, so expect to fail."""
         # TODO: Fix faucet so the test inversion is no longer required.
         asserted = False
         try:
             self.validate_with_externals_down(dp_name)
         except AssertionError:
             asserted = True
-        self.assertTrue(asserted, f'Did not fail as expected for {dp_name}')
+        self.assertTrue(asserted, "Did not fail as expected for %s" % dp_name)
 
     def verify_intervlan_routing(self):
         """Verify intervlan routing but for LAG host use bond interface"""
         for src in self.host_information:
             for dst in self.host_information:
                 if dst > src:
                     self.check_host_connectivity_by_id(src, dst)
 
     def check_host_connectivity_by_id(self, src_id, dst_id):
         """Ping from src to dst with host_id parameters if they should be able to"""
-        src_host, src_ip, _, src_vlan, src_bond, _ = self.host_information[src_id].values()
-        dst_host, dst_ip, _, dst_vlan, dst_bond, _ = self.host_information[dst_id].values()
+        src_host, src_ip, _, src_vlan, src_bond, _ = self.host_information[
+            src_id
+        ].values()
+        dst_host, dst_ip, _, dst_vlan, dst_bond, _ = self.host_information[
+            dst_id
+        ].values()
         connectivity = src_vlan == dst_vlan or self.is_routed_vlans(src_vlan, dst_vlan)
         if self.is_routed_vlans(src_vlan, dst_vlan):
             src_vip = self.faucet_vips[src_vlan]
             dst_vip = self.faucet_vips[dst_vlan]
-            self.host_ping(src_host, src_vip.ip, src_bond)  # pytype: disable=attribute-error
-            self.host_ping(dst_host, dst_vip.ip, dst_bond)  # pytype: disable=attribute-error
+            # pytype: disable=attribute-error
+            self.host_ping(src_host, src_vip.ip, src_bond)
+            self.host_ping(dst_host, dst_vip.ip, dst_bond)
         if connectivity:
-            self.host_ping(src_host, dst_ip.ip, src_bond)  # pytype: disable=attribute-error
-            self.host_ping(dst_host, src_ip.ip, dst_bond)  # pytype: disable=attribute-error
+            # pytype: disable=attribute-error
+            self.host_ping(src_host, dst_ip.ip, src_bond)
+            self.host_ping(dst_host, src_ip.ip, dst_bond)
 
     def is_routed_vlans(self, vlan_a, vlan_b):
         """Return true if the two vlans share a router"""
-        if self.configuration_options['routers']:
-            for vlans in self.configuration_options['routers'].values():
-                if (vlan_a in vlans and vlan_b in vlans):
+        if self.configuration_options["routers"]:
+            for vlans in self.configuration_options["routers"].values():
+                if vlan_a in vlans and vlan_b in vlans:
                     return True
         return False
 
-    def bcast_dst_blocked_helper(self, port, first_host, second_host, success_re, retries):
+    def bcast_dst_blocked_helper(
+        self, port, first_host, second_host, success_re, retries
+    ):
         """Helper for checking broadcast destination has been blocked"""
-        tcpdump_filter = f'udp and ether src {first_host.MAC()} and ether dst ff:ff:ff:ff:ff:ff'
+        tcpdump_filter = "udp and ether src %s and ether dst %s" % (
+            first_host.MAC(),
+            "ff:ff:ff:ff:ff:ff",
+        )
         target_addr = str(self.faucet_vips[0].network.broadcast_address)
         for _ in range(retries):
             tcpdump_txt = self.tcpdump_helper(
-                second_host, tcpdump_filter, [
-                    partial(first_host.cmd, (
-                        f'date | socat - udp-datagram:{target_addr}:{port},broadcast'))],
-                packets=1)
+                second_host,
+                tcpdump_filter,
+                [
+                    partial(
+                        first_host.cmd,
+                        (
+                            "date | socat - udp-datagram:%s:%d,broadcast"
+                            % (target_addr, port)
+                        ),
+                    )
+                ],
+                packets=1,
+            )
             if re.search(success_re, tcpdump_txt):
                 return True
             time.sleep(1)
         return False
 
     def get_expected_synced_states(self, host_id):
         """Return the list of regex string for the expected sync state of a LACP LAG connection"""
         synced_state_list = []
-        oper_key = self.configuration_options['host_options'][host_id]['lacp']
+        oper_key = self.configuration_options["host_options"][host_id]["lacp"]
         lacp_ports = [
-            port for ports in self.host_information[host_id]['ports'].values() for port in ports]
+            port
+            for ports in self.host_information[host_id]["ports"].values()
+            for port in ports
+        ]
         for port in lacp_ports:
             synced_state_txt = r"""
 Slave Interface: \S+
 MII Status: up
 Speed: \d+ Mbps
 Duplex: full
 Link Failure Count: \d+
@@ -651,34 +772,38 @@
 details partner lacp pdu:
     system priority: 65535
     system mac address: 0e:00:00:00:00:01
     oper key: %d
     port priority: 255
     port number: %d
     port state: 62
-""".strip() % (oper_key, port)
+""".strip() % (
+                oper_key,
+                port,
+            )
             synced_state_list.append(synced_state_txt)
         return synced_state_list
 
     def prom_lacp_up_ports(self, dpid):
         """Get the number of up LAG ports according to Prometheus for a dpid"""
         lacp_up_ports = 0
-        for host_id, options in self.configuration_options['host_options'].items():
+        for host_id, options in self.configuration_options["host_options"].items():
             # Find LACP hosts
             for key in options.keys():
-                if key == 'lacp':
+                if key == "lacp":
                     # Is LACP host
                     for dp_i, ports in self.host_port_maps[host_id].items():
                         if dpid == self.topo.dpids_by_id[dp_i]:
                             # Host has links to dpid
                             for port in ports:
                                 # Obtain up LACP ports for that dpid
                                 port_labels = self.port_labels(port)
                                 lacp_state = self.scrape_prometheus_var(
-                                    'port_lacp_state', port_labels, default=0, dpid=dpid)
+                                    "port_lacp_state", port_labels, default=0, dpid=dpid
+                                )
                                 lacp_up_ports += 1 if lacp_state == 3 else 0
         return lacp_up_ports
 
     def verify_num_lag_up_ports(self, expected_up_ports, dpid):
         """Checks to see if Prometheus has the expected number of up LAG ports
         on the specified DP"""
         for _ in range(self.LACP_TIMEOUT * 10):
@@ -686,20 +811,22 @@
                 return
             time.sleep(1)
         self.assertEqual(self.prom_lacp_up_ports(dpid), expected_up_ports)
 
     def require_linux_bond_up(self, host_id):
         """Checks to see if the host has properly formed into a bonded state"""
         synced_state_list = self.get_expected_synced_states(host_id)
-        host = self.host_information[host_id]['host']
-        bond_name = self.host_information[host_id]['bond']
+        host = self.host_information[host_id]["host"]
+        bond_name = self.host_information[host_id]["bond"]
         for _ in range(self.LACP_TIMEOUT * 2):
-            result = host.cmd(f'cat /proc/net/bonding/{bond_name}|sed "s/[ \t]*$//g"')
-            result = '\n'.join([line.rstrip() for line in result.splitlines()])
-            with open(os.path.join(self.tmpdir, 'bonding-state.txt'), 'w', encoding='utf-8') as state_file:
+            result = host.cmd('cat /proc/net/bonding/%s|sed "s/[ \t]*$//g"' % bond_name)
+            result = "\n".join([line.rstrip() for line in result.splitlines()])
+            with open(
+                os.path.join(self.tmpdir, "bonding-state.txt"), "w", encoding="utf-8"
+            ) as state_file:
                 state_file.write(result)
             matched_all = True
             for state_txt in synced_state_list:
                 if not re.search(state_txt, result):
                     matched_all = False
                     break
             if matched_all:
@@ -707,15 +834,17 @@
             time.sleep(1)
         synced_state_txt = r""""""
         for state_txt in synced_state_list:
             synced_state_txt += state_txt + "\n\n"
         synced_state_txt.strip()
         self.assertFalse(
             re.search(synced_state_txt, result),
-            msg=f'LACP did not synchronize: {result}\n\nexpected:\n\n{synced_state_txt}')
+            msg="LACP did not synchronize: %s\n\nexpected:\n\n%s"
+            % (result, synced_state_txt),
+        )
 
     def verify_lag_connectivity(self, host_id):
         """Verify LAG connectivity"""
         lacp_ports = self.host_port_maps[host_id]
         # All ports down
         for dp_i, ports in lacp_ports.items():
             dpid = self.topo.dpids_by_id[dp_i]
@@ -744,15 +873,15 @@
         self.verify_num_lag_up_ports(len(lacp_ports[up_dp]) - 1, up_dpid)
         # Ensure connectivity with new ports only
         self.verify_lag_host_connectivity()
 
     def verify_lag_host_connectivity(self):
         """Verify LAG hosts can connect to any other host using the interface"""
         # Find all LACP hosts
-        for lacp_id, host_options in self.configuration_options['host_options'].items():
-            if 'lacp' in host_options:
+        for lacp_id, host_options in self.configuration_options["host_options"].items():
+            if "lacp" in host_options:
                 # Found LACP host
                 for dst_id in self.host_information:
                     if lacp_id == dst_id:
                         continue
                     # Test connectivity to any other host (might be another LAG host)
                     self.check_host_connectivity_by_id(lacp_id, dst_id)
```

### Comparing `c65faucet-1.0.49/clib/mininet_test_topo.py` & `c65faucet-1.0.50/clib/mininet_test_topo.py`

 * *Files 12% similar despite different names*

```diff
@@ -27,89 +27,116 @@
 
 
 class FaucetIntf(TCIntf):
     """TCIntf that doesn't complain unnecessarily"""
 
     def delete(self):
         """Ignore interface deletion failure;
-           this is common after a veth pair has been deleted
-           on the other side."""
-        self.cmd('ip link del', self.name, '|| true')
+        this is common after a veth pair has been deleted
+        on the other side."""
+        self.cmd("ip link del", self.name, "|| true")
         self.node.delIntf(self)
         self.link = None
 
 
 class FaucetLink(Link):
     """Link using FaucetIntfs"""
 
-    def __init__(self, node1, node2, port1=None, port2=None,
-                 intf_name1=None, intf_name2=None,
-                 addr1=None, addr2=None, **params):
-        Link.__init__(self, node1, node2, port1=port1, port2=port2,
-                      intfName1=intf_name1, intfName2=intf_name2,
-                      cls1=FaucetIntf, cls2=FaucetIntf,
-                      addr1=addr1, addr2=addr2,
-                      params1=params, params2=params)
+    def __init__(
+        self,
+        node1,
+        node2,
+        port1=None,
+        port2=None,
+        intf_name1=None,
+        intf_name2=None,
+        addr1=None,
+        addr2=None,
+        **params
+    ):
+        Link.__init__(
+            self,
+            node1,
+            node2,
+            port1=port1,
+            port2=port2,
+            intfName1=intf_name1,
+            intfName2=intf_name2,
+            cls1=FaucetIntf,
+            cls2=FaucetIntf,
+            addr1=addr1,
+            addr2=addr2,
+            params1=params,
+            params2=params,
+        )
 
 
 class FaucetHost(CPULimitedHost):
     """Base Mininet Host class, for Mininet-based tests."""
 
     def __init__(self, *args, **kwargs):
         self.pid_files = []
         super().__init__(*args, **kwargs)
 
     def terminate(self):
         # If any 'dnsmasq' processes were started, terminate them now
         for pid_file in self.pid_files:
-            with open(pid_file, 'r', encoding='utf-8') as pf:
+            with open(pid_file, "r", encoding="utf-8") as pf:
                 for _, pid in enumerate(pf):
                     os.kill(int(pid), 15)
         super().terminate()
 
     def create_dnsmasq(self, tmpdir, iprange, router, vlan, interface=None):
         """Start dnsmasq instance inside dnsmasq namespace"""
         if interface is None:
             interface = self.defaultIntf()
-        dhcp_leasefile = os.path.join(tmpdir, f'nfv-dhcp-{self.name}-{iprange}-vlan{vlan}.leases')
-        log_facility = os.path.join(tmpdir, f'nfv-dhcp-{self.name}-{iprange}-vlan{vlan}.log')
-        pid_file = os.path.join(tmpdir, f'dnsmasq-{self.name}-{iprange}-vlan{vlan}.pid')
+        dhcp_leasefile = os.path.join(
+            tmpdir, "nfv-dhcp-%s-%s-vlan%u.leases" % (self.name, iprange, vlan)
+        )
+        log_facility = os.path.join(
+            tmpdir, "nfv-dhcp-%s-%s-vlan%u.log" % (self.name, iprange, vlan)
+        )
+        pid_file = os.path.join(
+            tmpdir, "dnsmasq-%s-%s-vlan%u.pid" % (self.name, iprange, vlan)
+        )
         self.pid_files.append(pid_file)
-        cmd = 'dnsmasq'
-        opts = ''
-        opts += f' --dhcp-range={iprange},255.255.255.0'
-        opts += ' --dhcp-sequential-ip'
-        opts += f' --dhcp-option=option:router,{router}'
-        opts += ' --no-resolv --txt-record=does.it.work,yes'
-        opts += ' --bind-interfaces'
-        opts += ' --except-interface=lo'
-        opts += f' --interface={interface}'
-        opts += f' --dhcp-leasefile={dhcp_leasefile}'
-        opts += f' --log-facility={log_facility}'
-        opts += f' --pid-file={pid_file}'
-        opts += ' --conf-file='
+        cmd = "dnsmasq"
+        opts = ""
+        opts += " --dhcp-range=%s,255.255.255.0" % iprange
+        opts += " --dhcp-sequential-ip"
+        opts += " --dhcp-option=option:router,%s" % router
+        opts += " --no-resolv --txt-record=does.it.work,yes"
+        opts += " --bind-interfaces"
+        opts += " --except-interface=lo"
+        opts += " --interface=%s" % interface
+        opts += " --dhcp-leasefile=%s" % dhcp_leasefile
+        opts += " --log-facility=%s" % log_facility
+        opts += " --pid-file=%s" % pid_file
+        opts += " --conf-file="
         return self.cmd(cmd + opts)
 
     def run_dhclient(self, tmpdir, interface=None, timeout=10):
         """Run DHCLIENT to obtain ip address via DHCP"""
         if interface is None:
             interface = self.defaultIntf()
-        cmd = 'dhclient'
-        opts = ''
-        opts += ' -1'
-        opts += ' -d'
-        opts += f' -pf {tmpdir}/dhclient-{self.name}.pid'
-        opts += f' -lf {tmpdir}/dhclient-{self.name}.leases'
-        opts += f' {interface}'
+        cmd = "dhclient"
+        opts = ""
+        opts += " -1"
+        opts += " -d"
+        opts += " -pf %s/dhclient-%s.pid" % (tmpdir, self.name)
+        opts += " -lf %s/dhclient-%s.leases" % (tmpdir, self.name)
+        opts += " %s" % interface
         dhclient_cmd = cmd + opts
-        return self.cmd(mininet_test_util.timeout_cmd(dhclient_cmd, timeout), verbose=True)
+        return self.cmd(
+            mininet_test_util.timeout_cmd(dhclient_cmd, timeout), verbose=True
+        )
 
     def return_ip(self):
         """Return host IP as a string"""
-        return self.cmd('hostname -I')
+        return self.cmd("hostname -I")
 
 
 class VLANHost(FaucetHost):
     """Implementation of a Mininet host on a tagged VLAN."""
 
     intf_root_name = None
 
@@ -127,164 +154,183 @@
         if vlans is None:
             vlans = [100]
         self.vlans = vlans
         self.vlan_intfs = {}
         cmds = []
         intf = self.defaultIntf()
         self.intf_root_name = intf.name
-        if 'vlan_intfs' in params:
-            vlan_intfs = params.get('vlan_intfs', {})
+        if "vlan_intfs" in params:
+            vlan_intfs = params.get("vlan_intfs", {})
             for vlan_id, ip_addr in vlan_intfs.items():
                 if isinstance(vlan_id, tuple):
                     # Interface will take multiply VLAN tagged packets
-                    intf_name = f'{intf.name}'
+                    intf_name = "%s" % intf.name
                     for vlan_i in vlan_id:
                         prev_name = intf_name
                         # Cannot have intf name tu0xy-eth0.VID1.VID2 as that takes up too many bytes
-                        intf_name += f'.{vlan_i}'
-                        cmds.extend([
-                            f'ip link add link {prev_name} name {intf_name} type vlan id {vlans[vlan_i]}',
-                            f'ip link set dev {intf_name} up'
-                        ])
+                        intf_name += ".%s" % vlan_i
+                        cmds.extend(
+                            [
+                                "ip link add link %s name %s type vlan id %s"
+                                % (prev_name, intf_name, vlans[vlan_i]),
+                                "ip link set dev %s up" % (intf_name),
+                            ]
+                        )
                         self.nameToIntf[intf_name] = intf
                         self.vlan_intfs.setdefault(vlan_id, [])
                         self.vlan_intfs[vlan_id].append(intf_name)
-                    cmds.append(f'ip -4 addr add {ip_addr} dev {intf_name}')
+                    cmds.append("ip -4 addr add %s dev %s" % (ip_addr, intf_name))
                 else:
-                    intf_name = f'{intf}.{vlans[vlan_id]}'
-                    cmds.extend([
-                        f'vconfig add {intf.name} {vlans[vlan_id]}',
-                        f'ip -4 addr add {ip_addr} dev {intf_name}',
-                        f'ip link set dev {intf_name} up'])
+                    intf_name = "%s.%s" % (intf, vlans[vlan_id])
+                    cmds.extend(
+                        [
+                            "vconfig add %s %d" % (intf.name, vlans[vlan_id]),
+                            "ip -4 addr add %s dev %s" % (ip_addr, intf_name),
+                            "ip link set dev %s up" % intf_name,
+                        ]
+                    )
                     self.nameToIntf[intf_name] = intf
                     self.vlan_intfs[vlan_id] = intf_name
         else:
-            vlan_intf_name = f'{intf}.{".".join(str(v) for v in vlans)}'
-            cmds.extend([
-                f'ip link set dev {vlan_intf_name} up',
-                f'ip -4 addr add {params["ip"]} dev {vlan_intf_name}'])
+            vlan_intf_name = "%s.%s" % (intf, ".".join(str(v) for v in vlans))
+            cmds.extend(
+                [
+                    "ip link set dev %s up" % vlan_intf_name,
+                    "ip -4 addr add %s dev %s" % (params["ip"], vlan_intf_name),
+                ]
+            )
             for vlan in vlans:
-                cmds.append(f'vconfig add {intf} {vlan}')
+                cmds.append("vconfig add %s %d" % (intf, vlan))
             intf.name = vlan_intf_name
             self.nameToIntf[vlan_intf_name] = intf
-        cmds.extend([
-            f'ip -4 addr flush dev {intf}',
-            f'ip -6 addr flush dev {intf}'])
+        cmds.extend(
+            ["ip -4 addr flush dev %s" % intf, "ip -6 addr flush dev %s" % intf]
+        )
         for cmd in cmds:
             self.cmd(cmd)
         return super_config
 
 
 class FaucetSwitch(OVSSwitch):
     """Switch that will be used by all tests (netdev based OVS)."""
 
     clist = None
 
     controller_params = {
-        'controller_burst_limit': 25,
-        'controller_rate_limit': 100,
+        "controller_burst_limit": 25,
+        "controller_rate_limit": 100,
     }
 
     def __init__(self, name, **params):
         self.clist = []
-        super().__init__(
-            name=name, reconnectms=8000, **params)
+        super().__init__(name=name, reconnectms=8000, **params)
 
     @staticmethod
     def _workaround(args):
         """Workarounds/hacks for errors resulting from
-           cmd() calls within Mininet"""
+        cmd() calls within Mininet"""
         # Workaround: ignore ethtool errors on tap interfaces
         # This allows us to use tap tunnels as cables to switch ports,
         # for example to test against OvS in a VM.
-        if (len(args) > 1 and args[0] == 'ethtool -K'
-                and getattr(args[1], 'name', '').startswith('tap')):
+        if (
+            len(args) > 1
+            and args[0] == "ethtool -K"
+            and getattr(args[1], "name", "").startswith("tap")
+        ):
             return True
         return False
 
     def cmd(self, *args, success=0, **kwargs):
         """Commands typically must succeed for proper switch operation,
-           so we check the exit code of the last command in *args.
-           success: desired exit code (or None to skip check)"""
+        so we check the exit code of the last command in *args.
+        success: desired exit code (or None to skip check)"""
         # pylint: disable=arguments-differ
         cmd_output = super().cmd(*args, **kwargs)
-        exit_code = int(super().cmd('echo $?'))
+        exit_code = int(super().cmd("echo $?"))
         if success is not None and exit_code != success:
-            msg = f"{args} exited with ({exit_code}):'{cmd_output}'"
+            msg = "%s exited with (%d):'%s'" % (args, exit_code, cmd_output)
             if self._workaround(args):
-                warn('Ignoring:', msg, '\n')
+                warn("Ignoring:", msg, "\n")
             else:
                 raise RuntimeError(msg)
         return cmd_output
 
     def attach(self, intf):
         "Attach an interface and set its port"
         super().attach(intf)
         # This should be done in Mininet, but we do it for now
         port = self.ports[intf]
-        self.cmd('ovs-vsctl set Interface', intf, f'ofport_request={port}')
+        self.cmd("ovs-vsctl set Interface", intf, "ofport_request=%s" % port)
 
     def add_controller(self, controller):
-        self.clist.append((
-            self.name + controller.name,
-            f'{controller.protocol}:{controller.IP()}:{controller.port}'))
+        self.clist.append(
+            (
+                self.name + controller.name,
+                "%s:%s:%d" % (controller.protocol, controller.IP(), controller.port),
+            )
+        )
         if self.listenPort:
-            self.clist.append((self.name + '-listen',
-                               f'ptcp:{self.listenPort}'))
+            self.clist.append((self.name + "-listen", "ptcp:%s" % self.listenPort))
         ccmd = '-- --id=@%s create Controller target=\\"%s\\"'
         if self.reconnectms:
-            ccmd += f' max_backoff={self.reconnectms}'
+            ccmd += " max_backoff=%d" % self.reconnectms
         for param, value in self.controller_params.items():
-            ccmd += f' {param}={value}'
-        cargs = ' '.join(ccmd % (name, target)
-                         for name, target in self.clist)
+            ccmd += " %s=%s" % (param, value)
+        cargs = " ".join(ccmd % (name, target) for name, target in self.clist)
         # Controller ID list
-        cids = ','.join('@%s' % name for name, _target in self.clist)
+        cids = ",".join("@%s" % name for name, _target in self.clist)
         # One ovs-vsctl command to rule them all!
-        self.vsctl(cargs
-                   + f' -- set bridge {self} controller=[{cids}]')
+        self.vsctl(cargs + " -- set bridge %s controller=[%s]" % (self, cids))
 
     def start(self, controllers):
         # Transcluded from Mininet source, since need to insert
         # controller parameters at switch creation time.
         int(self.dpid, 16)  # DPID must be a hex string
-        switch_intfs = [intf for intf in self.intfList() if self.ports[intf] and not intf.IP()]
+        switch_intfs = [
+            intf for intf in self.intfList() if self.ports[intf] and not intf.IP()
+        ]
         # Command to add interfaces
-        intfs = ' '.join(f' -- add-port {self} {intf}'
-                         + self.intfOpts(intf)
-                         for intf in switch_intfs)
+        intfs = " ".join(
+            " -- add-port %s %s" % (self, intf) + self.intfOpts(intf)
+            for intf in switch_intfs
+        )
         # Command to create controller entries
-        self.clist = [(self.name + c.name, f'{c.protocol}:{c.IP()}:{c.port}')
-                      for c in controllers]
+        self.clist = [
+            (self.name + c.name, "%s:%s:%d" % (c.protocol, c.IP(), c.port))
+            for c in controllers
+        ]
         if self.listenPort:
-            self.clist.append((self.name + '-listen',
-                               f'ptcp:{self.listenPort}'))
+            self.clist.append((self.name + "-listen", "ptcp:%s" % self.listenPort))
         ccmd = '-- --id=@%s create Controller target=\\"%s\\"'
         if self.reconnectms:
-            ccmd += f' max_backoff={self.reconnectms}'
+            ccmd += " max_backoff=%d" % self.reconnectms
         for param, value in self.controller_params.items():
-            ccmd += f' {param}={value}'
-        cargs = ' '.join(ccmd % (name, target)
-                         for name, target in self.clist)
+            ccmd += " %s=%s" % (param, value)
+        cargs = " ".join(ccmd % (name, target) for name, target in self.clist)
         # Controller ID list
-        cids = ','.join('@%s' % name for name, _target in self.clist)
+        cids = ",".join("@%s" % name for name, _target in self.clist)
         # Try to delete any existing bridges with the same name
         if not self.isOldOVS():
-            cargs += f' -- --if-exists del-br {self}'
+            cargs += " -- --if-exists del-br %s" % self
         # One ovs-vsctl command to rule them all!
-        self.vsctl(cargs
-                   + f' -- add-br {self}'
-                   + f' -- set bridge {self} controller=[{cids}]'
-                   + self.bridgeOpts()
-                   + intfs)
+        self.vsctl(
+            cargs
+            + " -- add-br %s" % self
+            + " -- set bridge %s controller=[%s]" % (self, cids)
+            + self.bridgeOpts()
+            + intfs
+        )
         # switch interfaces on mininet host, must have no IP config.
         for intf in switch_intfs:
             for ipv in (4, 6):
-                self.cmd(f'ip -{ipv} addr flush dev {intf}')
-            assert self.cmd(f'echo 1 > /proc/sys/net/ipv6/conf/{intf}/disable_ipv6') == ''
+                self.cmd("ip -%u addr flush dev %s" % (ipv, intf))
+            assert (
+                self.cmd("echo 1 > /proc/sys/net/ipv6/conf/%s/disable_ipv6" % intf)
+                == ""
+            )
         # If necessary, restore TC config overwritten by OVS
         if not self.batch:
             for intf in self.intfList():
                 self.TCReapply(intf)
 
 
 class NoControllerFaucetSwitch(FaucetSwitch):
@@ -294,65 +340,73 @@
         super().start(controllers=[])
 
 
 class FaucetSwitchTopo(Topo):
     """FAUCET switch topology that contains a software switch."""
 
     CPUF = 0.5
-    DELAY = '1ms'
+    DELAY = "1ms"
 
     def __init__(self, *args, **kwargs):
         self.dpid_names = {}  # maps dpids to switch names
         self.switch_dpids = {}  # maps switch names to dpids
         self.switch_ports = {}  # maps switch names to port lists
         self.dpid_port_host = {}  # maps switch hosts to ports
         super().__init__(*args, **kwargs)
 
     @staticmethod
     def _get_sid_prefix(ports_served):
         """Return a unique switch/host prefix for a test."""
         # Linux tools require short interface names.
-        id_chars = ''.join(sorted(string.ascii_letters + string.digits))  # pytype: disable=module-attr
+        id_chars = "".join(
+            sorted(string.ascii_letters + string.digits)
+        )  # pytype: disable=module-attr
         id_a = int(ports_served / len(id_chars))
         id_b = ports_served - (id_a * len(id_chars))
-        return f'{id_chars[id_a]}{id_chars[id_b]}'
+        return "%s%s" % (id_chars[id_a], id_chars[id_b])
 
     def _add_tagged_host(self, sid_prefix, tagged_vids, host_n):
         """Add a single tagged test host."""
-        host_name = 't%s%1.1u' % (sid_prefix, host_n + 1)
+        host_name = "t%s%1.1u" % (sid_prefix, host_n + 1)
         return self.addHost(
-            name=host_name, cls=VLANHost, vlans=tagged_vids, cpu=self.CPUF)
+            name=host_name, cls=VLANHost, vlans=tagged_vids, cpu=self.CPUF
+        )
 
     def _add_untagged_host(self, sid_prefix, host_n, in_namespace=True):
         """Add a single untagged test host."""
-        host_name = 'u%s%1.1u' % (sid_prefix, host_n + 1)
-        return self.addHost(name=host_name, cls=FaucetHost, cpu=self.CPUF, inNamespace=in_namespace)
+        host_name = "u%s%1.1u" % (sid_prefix, host_n + 1)
+        return self.addHost(
+            name=host_name, cls=FaucetHost, cpu=self.CPUF, inNamespace=in_namespace
+        )
 
     def _add_extended_host(self, sid_prefix, host_n, e_cls, tmpdir):
         """Add a single extended test host."""
-        host_name = 'e%s%1.1u' % (sid_prefix, host_n + 1)
+        host_name = "e%s%1.1u" % (sid_prefix, host_n + 1)
         return self.addHost(name=host_name, cls=e_cls, host_n=host_n, tmpdir=tmpdir)
 
     def _add_faucet_switch(self, sid_prefix, dpid, hw_dpid, ovs_type):
         """Add a FAUCET switch."""
         switch_cls = FaucetSwitch
-        switch_name = f's{sid_prefix}'
+        switch_name = "s%s" % sid_prefix
         self.switch_dpids[switch_name] = dpid
         self.dpid_names[dpid] = switch_name
         if hw_dpid and hw_dpid == dpid:
             remap_dpid = str(int(dpid) + 1)
-            output('bridging hardware switch DPID %s (%x) dataplane via OVS DPID %s (%x)\n' % (
-                dpid, int(dpid), remap_dpid, int(remap_dpid)))
+            output(
+                "bridging hardware switch DPID %s (%x) dataplane via OVS DPID %s (%x)\n"
+                % (dpid, int(dpid), remap_dpid, int(remap_dpid))
+            )
             dpid = remap_dpid
             switch_cls = NoControllerFaucetSwitch
         return self.addSwitch(
             name=switch_name,
             cls=switch_cls,
             datapath=ovs_type,
-            dpid=mininet_test_util.mininet_dpid(dpid))
+            dpid=mininet_test_util.mininet_dpid(dpid),
+        )
 
     # Hardware switch port virtualization through
     # transparent OVS attachment bridge/patch panel
     #
     # Since FAUCET is talking to the hardware switch, it needs
     # to use the hardware switch's OpenFlow ports, rather than
     # the OpenFlow ports of the (transparent) OVS attachment bridge.
@@ -360,15 +414,15 @@
     def hw_remap_port(self, dpid, port):
         """Map OVS attachment bridge port number -> HW port number if necessary"""
         if dpid != self.hw_dpid:
             return port
         assert self.hw_ports
         return self.hw_ports[port - self.start_port]
 
-    peer_link = namedtuple('peer_link', 'port peer_dpid peer_port')
+    peer_link = namedtuple("peer_link", "port peer_dpid peer_port")
 
     def hw_remap_peer_link(self, dpid, link):
         """Remap HW port numbers -> OVS port numbers in link if necessary"""
         port = self.hw_remap_port(dpid, link.port)
         peer_port = self.hw_remap_port(link.peer_dpid, link.peer_port)
         return self.peer_link(port, link.peer_dpid, peer_port)
 
@@ -398,37 +452,58 @@
                 self.switch_ports.setdefault(switch, [])
                 self.switch_ports[switch].append(port)
                 self.dpid_port_host[int(dpid)][port] = host
                 index += 1
         return index
 
     # pylint: disable=too-many-locals,arguments-differ
-    def build(self, ovs_type, ports_sock, test_name, dpids,
-              n_tagged=0, tagged_vid=100, n_untagged=0, links_per_host=0,
-              n_extended=0, e_cls=None, tmpdir=None, hw_dpid=None, switch_map=None,
-              host_namespace=None, start_port=SWITCH_START_PORT, port_order=None,
-              get_serialno=mininet_test_util.get_serialno):
+    def build(
+        self,
+        ovs_type,
+        ports_sock,
+        test_name,
+        dpids,
+        n_tagged=0,
+        tagged_vid=100,
+        n_untagged=0,
+        links_per_host=0,
+        n_extended=0,
+        e_cls=None,
+        tmpdir=None,
+        hw_dpid=None,
+        switch_map=None,
+        host_namespace=None,
+        start_port=SWITCH_START_PORT,
+        port_order=None,
+        get_serialno=mininet_test_util.get_serialno,
+    ):
         if not host_namespace:
             host_namespace = {}
         self.hw_dpid = hw_dpid
         self.hw_ports = sorted(switch_map) if switch_map else []
         self.start_port = start_port
         maxlength = n_tagged + n_untagged + n_extended
-        self.port_order = self.extend_port_order(
-            port_order, maxlength)
+        self.port_order = self.extend_port_order(port_order, maxlength)
         for dpid in dpids:
             serialno = get_serialno(ports_sock, test_name)
             sid_prefix = self._get_sid_prefix(serialno)
-            tagged = [self._add_tagged_host(sid_prefix, [tagged_vid], host_n)
-                      for host_n in range(n_tagged)]
-            untagged = [self._add_untagged_host(
-                sid_prefix, host_n, host_namespace.get(host_n, True))
-                for host_n in range(n_untagged)]
-            extended = [self._add_extended_host(sid_prefix, host_n, e_cls, tmpdir)
-                        for host_n in range(n_extended)]
+            tagged = [
+                self._add_tagged_host(sid_prefix, [tagged_vid], host_n)
+                for host_n in range(n_tagged)
+            ]
+            untagged = [
+                self._add_untagged_host(
+                    sid_prefix, host_n, host_namespace.get(host_n, True)
+                )
+                for host_n in range(n_untagged)
+            ]
+            extended = [
+                self._add_extended_host(sid_prefix, host_n, e_cls, tmpdir)
+                for host_n in range(n_extended)
+            ]
             switch = self._add_faucet_switch(sid_prefix, dpid, hw_dpid, ovs_type)
             self._add_links(switch, dpid, tagged + untagged + extended, links_per_host)
 
 
 class BaseFAUCET(Controller):
     """Base class for FAUCET and Gauge controllers."""
 
@@ -439,246 +514,300 @@
     controller_ip = None
     pid_file = None
     tmpdir = None
     ofcap = None
     MAX_OF_PKTS = 5000
     MAX_CTL_TIME = 600
 
-    BASE_CARGS = ' '.join((
-        '--verbose',
-        '--use-stderr',
-        '--ryu-ofp-tcp-listen-port=%s'))
+    BASE_CARGS = " ".join(("--verbose", "--use-stderr", "--ryu-ofp-tcp-listen-port=%s"))
 
     RYU_CONF = """
 [DEFAULT]
 echo_request_interval=10
 maximum_unreplied_echo_requests=5
 socket_timeout=15
 """
 
-    def __init__(self, name, tmpdir, controller_intf=None, controller_ipv6=False,
-                 cargs='', **kwargs):
+    def __init__(
+        self,
+        name,
+        tmpdir,
+        controller_intf=None,
+        controller_ipv6=False,
+        cargs="",
+        **kwargs
+    ):
         self.name_no_pid = name
-        name_with_pid = f'{name}-{os.getpid()}'
+        name_with_pid = "%s-%u" % (name, os.getpid())
         self.tmpdir = tmpdir
         self.controller_intf = controller_intf
         self.controller_ipv6 = controller_ipv6
         super().__init__(
-            name_with_pid, cargs=self._add_cargs(cargs, name_with_pid), **kwargs)
+            name_with_pid, cargs=self._add_cargs(cargs, name_with_pid), **kwargs
+        )
 
     def _add_cargs(self, cargs, name):
-        ofp_listen_host_arg = ''
+        ofp_listen_host_arg = ""
         if self.controller_intf is not None:
             socket_type = socket.AF_INET
             if self.controller_ipv6:
                 socket_type = socket.AF_INET6
-            self.controller_ip = netifaces.ifaddresses(  # pylint: disable=c-extension-no-member
-                self.controller_intf)[socket_type][0]['addr']
-            ofp_listen_host_arg = f'--ryu-ofp-listen-host={self.controller_ip}'
-        self.pid_file = os.path.join(self.tmpdir, name + '.pid')
-        pid_file_arg = f'--ryu-pid-file={self.pid_file}'
-        ryu_conf_file = os.path.join(self.tmpdir, 'ryu.conf')
-        with open(ryu_conf_file, 'w', encoding='utf-8') as ryu_conf:
+            self.controller_ip = (
+                netifaces.ifaddresses(  # pylint: disable=c-extension-no-member
+                    self.controller_intf
+                )[socket_type][0]["addr"]
+            )
+            ofp_listen_host_arg = "--ryu-ofp-listen-host=%s" % self.controller_ip
+        self.pid_file = os.path.join(self.tmpdir, name + ".pid")
+        pid_file_arg = "--ryu-pid-file=%s" % self.pid_file
+        ryu_conf_file = os.path.join(self.tmpdir, "ryu.conf")
+        with open(ryu_conf_file, "w", encoding="utf-8") as ryu_conf:
             ryu_conf.write(self.RYU_CONF)
-        ryu_conf_arg = f'--ryu-config-file={ryu_conf_file}'
-        return ' '.join((
-            self.BASE_CARGS, pid_file_arg, ryu_conf_arg, ofp_listen_host_arg, cargs))
+        ryu_conf_arg = "--ryu-config-file=%s" % ryu_conf_file
+        return " ".join(
+            (self.BASE_CARGS, pid_file_arg, ryu_conf_arg, ofp_listen_host_arg, cargs)
+        )
 
     def IP(self):  # pylint: disable=invalid-name,arguments-differ
         if self.controller_intf is not None:
             return self.controller_ip
         return super().IP()
 
     def _start_tcpdump(self):
         """Start a tcpdump for OF port."""
-        self.ofcap = os.path.join(self.tmpdir, '-'.join((self.name, 'of.cap')))
-        tcpdump_args = ' '.join((
-            '-s 0',
-            '-e',
-            '-n',
-            '-U',
-            '-q',
-            '-W 1',  # max files 1
-            f'-G {self.MAX_CTL_TIME - 1}',
-            f'-c {self.MAX_OF_PKTS}',
-            f'-i {self.controller_intf}',
-            f'-w {self.ofcap}',
-            f'tcp and port {self.port}',
-            '>/dev/null',
-            '2>/dev/null',
-        ))
-        self.cmd(f'timeout {self.MAX_CTL_TIME} tcpdump {tcpdump_args} &')
+        self.ofcap = os.path.join(self.tmpdir, "-".join((self.name, "of.cap")))
+        tcpdump_args = " ".join(
+            (
+                "-s 0",
+                "-e",
+                "-n",
+                "-U",
+                "-q",
+                "-W 1",  # max files 1
+                "-G %u" % (self.MAX_CTL_TIME - 1),
+                "-c %u" % (self.MAX_OF_PKTS),
+                "-i %s" % self.controller_intf,
+                "-w %s" % self.ofcap,
+                "tcp and port %u" % self.port,
+                ">/dev/null",
+                "2>/dev/null",
+            )
+        )
+        self.cmd("timeout %s tcpdump %s &" % (self.MAX_CTL_TIME, tcpdump_args))
         for _ in range(5):
             if os.path.exists(self.ofcap):
                 return
             time.sleep(1)
-        assert False, 'tcpdump of OF channel did not start'
+        assert False, "tcpdump of OF channel did not start"
 
     @staticmethod
     def _tls_cargs(ofctl_port, ctl_privkey, ctl_cert, ca_certs):
         """Add TLS/cert parameters to Ryu."""
         tls_cargs = []
-        for carg_val, carg_key in ((ctl_privkey, 'ryu-ctl-privkey'),
-                                   (ctl_cert, 'ryu-ctl-cert'),
-                                   (ca_certs, 'ryu-ca-certs')):
+        for carg_val, carg_key in (
+            (ctl_privkey, "ryu-ctl-privkey"),
+            (ctl_cert, "ryu-ctl-cert"),
+            (ca_certs, "ryu-ca-certs"),
+        ):
             if carg_val:
-                tls_cargs.append((f'--{carg_key}={carg_val}'))
+                tls_cargs.append(("--%s=%s" % (carg_key, carg_val)))
         if tls_cargs:
-            tls_cargs.append((f'--ryu-ofp-ssl-listen-port={ofctl_port}'))
-        return ' '.join(tls_cargs)
+            tls_cargs.append(("--ryu-ofp-ssl-listen-port=%u" % ofctl_port))
+        return " ".join(tls_cargs)
 
     def _command(self, env, tmpdir, name, args):
         """Wrap controller startup command in shell script with environment."""
         env_vars = []
         for var, val in sorted(env.items()):
-            env_vars.append('='.join((var, val)))
-        script_wrapper_name = os.path.join(tmpdir, f'start-{name}.sh')
-        cprofile_args = ''
+            env_vars.append("=".join((var, val)))
+        script_wrapper_name = os.path.join(tmpdir, "start-%s.sh" % name)
+        cprofile_args = ""
         if self.CPROFILE:
-            cprofile_args = 'python3 -m cProfile -s time'
+            cprofile_args = "python3 -m cProfile -s time"
         full_faucet_dir = os.path.abspath(mininet_test_util.FAUCET_DIR)
-        with open(script_wrapper_name, 'w', encoding='utf-8') as script_wrapper:
-            faucet_cli = (
-                'PYTHONPATH=%s %s exec timeout %u %s %s %s $*\n' % (
-                    os.path.dirname(full_faucet_dir),
-                    ' '.join(env_vars),
-                    self.MAX_CTL_TIME,
-                    os.path.join(full_faucet_dir, '__main__.py'),
-                    cprofile_args,
-                    args))
+        with open(script_wrapper_name, "w", encoding="utf-8") as script_wrapper:
+            faucet_cli = "PYTHONPATH=%s %s exec timeout %u %s %s %s $*\n" % (
+                os.path.dirname(full_faucet_dir),
+                " ".join(env_vars),
+                self.MAX_CTL_TIME,
+                os.path.join(full_faucet_dir, "__main__.py"),
+                cprofile_args,
+                args,
+            )
             script_wrapper.write(faucet_cli)
-        return f'/bin/sh {script_wrapper_name}'
+        return "/bin/sh %s" % script_wrapper_name
 
     def ryu_pid(self):
         """Return PID of ryu-manager process."""
         if os.path.exists(self.pid_file) and os.path.getsize(self.pid_file) > 0:
-            with open(self.pid_file, encoding='utf-8') as pid_file:
-                return int(pid_file.read())
+            pid = None
+            with open(self.pid_file, encoding="utf-8") as pid_file:
+                pid = int(pid_file.read())
+            return pid
         return None
 
-    def listen_port(self, port, state='LISTEN'):
+    def listen_port(self, port, state="LISTEN"):
         """Return True if port in specified TCP state."""
         for ipv in (4, 6):
             listening_out = self.cmd(
-                mininet_test_util.tcp_listening_cmd(port, ipv=ipv, state=state)).split()
+                mininet_test_util.tcp_listening_cmd(port, ipv=ipv, state=state)
+            ).split()
             for pid in listening_out:
                 if int(pid) == self.ryu_pid():
                     return True
         return False
 
     # pylint: disable=invalid-name
     @staticmethod
     def checkListening():
         """Mininet's checkListening() causes occasional false positives (with
-           exceptions we can't catch), and we handle port conflicts ourselves anyway."""
+        exceptions we can't catch), and we handle port conflicts ourselves anyway."""
         return
 
     def listening(self):
         """Return True if controller listening on required ports."""
         return self.listen_port(self.port)
 
     def connected(self):
         """Return True if at least one switch connected and controller healthy."""
-        return self.healthy() and self.listen_port(self.port, state='ESTABLISHED')
+        return self.healthy() and self.listen_port(self.port, state="ESTABLISHED")
 
     def logname(self):
         """Return log file for controller."""
-        return os.path.join('/tmp', self.name + '.log')
+        return os.path.join("/tmp", self.name + ".log")
 
     def healthy(self):
         """Return True if controller logging and listening on required ports."""
-        if (os.path.exists(self.logname())
-                and os.path.getsize(self.logname())
-                and self.listening()):
+        if (
+            os.path.exists(self.logname())
+            and os.path.getsize(self.logname())
+            and self.listening()
+        ):
             return True
         return False
 
     def start(self):
         """Start tcpdump for OF port and then start controller."""
         self._start_tcpdump()
         super().start()
 
     def _stop_cap(self):
         """Stop tcpdump for OF port and run tshark to decode it."""
         if os.path.exists(self.ofcap):
-            self.cmd(' '.join(['fuser', '-15', '-k', self.ofcap]))
-            text_ofcap_log = f'{self.ofcap}.txt'
-            with open(text_ofcap_log, 'w', encoding='utf-8') as text_ofcap:
+            self.cmd(" ".join(["fuser", "-15", "-k", self.ofcap]))
+            text_ofcap_log = "%s.txt" % self.ofcap
+            with open(text_ofcap_log, "w", encoding="utf-8") as text_ofcap:
                 subprocess.call(
-                    ['timeout', str(self.MAX_CTL_TIME),
-                     'tshark', '-l', '-n', '-Q',
-                     '-d', f'tcp.port=={self.port},openflow',
-                     '-O', 'openflow_v4',
-                     '-Y', 'openflow_v4',
-                     '-r', self.ofcap],
+                    [
+                        "timeout",
+                        str(self.MAX_CTL_TIME),
+                        "tshark",
+                        "-l",
+                        "-n",
+                        "-Q",
+                        "-d",
+                        "tcp.port==%u,openflow" % self.port,
+                        "-O",
+                        "openflow_v4",
+                        "-Y",
+                        "openflow_v4",
+                        "-r",
+                        self.ofcap,
+                    ],
                     stdout=text_ofcap,
                     stdin=mininet_test_util.DEVNULL,
                     stderr=mininet_test_util.DEVNULL,
-                    close_fds=True)
+                    close_fds=True,
+                )
 
     def stop(self):  # pylint: disable=arguments-differ
         """Stop controller."""
         try:
             if self.CPROFILE:
                 os.kill(self.ryu_pid(), 2)
             else:
                 os.kill(self.ryu_pid(), 15)
-        except (ProcessLookupError, TypeError):
+        except ProcessLookupError:
             pass
         self._stop_cap()
         super().stop()
         if os.path.exists(self.logname()):
-            tmpdir_logname = os.path.join(
-                self.tmpdir, self.name + '-stdout-stderr.log')
+            tmpdir_logname = os.path.join(self.tmpdir, self.name + "-stdout-stderr.log")
             if os.path.exists(tmpdir_logname):
                 os.remove(tmpdir_logname)
             shutil.move(self.logname(), tmpdir_logname)
 
 
 class FAUCET(BaseFAUCET):
     """Start a FAUCET controller."""
 
-    START_ARGS = ['--ryu-app-lists=%s' % (os.path.dirname(os.path.realpath(__file__)) +
-        '/../ofctl_rest/ofctl_rest.py')]
-
-
-    def __init__(self, name, tmpdir, controller_intf, controller_ipv6, env,
-                 ctl_privkey, ctl_cert, ca_certs,
-                 ports_sock, prom_port, port, test_name, **kwargs):
+    START_ARGS = [
+        "--ryu-app-lists=%s"
+        % (os.path.dirname(os.path.realpath(__file__)) + "/../ofctl_rest/ofctl_rest.py")
+    ]
+
+    def __init__(
+        self,
+        name,
+        tmpdir,
+        controller_intf,
+        controller_ipv6,
+        env,
+        ctl_privkey,
+        ctl_cert,
+        ca_certs,
+        ports_sock,
+        prom_port,
+        port,
+        test_name,
+        **kwargs
+    ):
         self.prom_port = prom_port
-        self.ofctl_port = mininet_test_util.find_free_port(
-            ports_sock, test_name)
-        env['OFCTL_PORT'] = str(self.ofctl_port)
-        env['OFCTL_HOST'] = mininet_test_util.LOCALHOSTV6
-        cargs = ' '.join(
-            self._tls_cargs(port, ctl_privkey, ctl_cert, ca_certs))
+        self.ofctl_port = mininet_test_util.find_free_port(ports_sock, test_name)
+        env["OFCTL_PORT"] = str(self.ofctl_port)
+        env["OFCTL_HOST"] = mininet_test_util.LOCALHOSTV6
+        cargs = " ".join(self._tls_cargs(port, ctl_privkey, ctl_cert, ca_certs))
         super().__init__(
             name,
             tmpdir,
             controller_intf,
             controller_ipv6,
             cargs=cargs,
-            command=self._command(env, tmpdir, name, ' '.join(self.START_ARGS)),
+            command=self._command(env, tmpdir, name, " ".join(self.START_ARGS)),
             port=port,
-            **kwargs)
+            **kwargs
+        )
 
     def listening(self):
         return (
             self.listen_port(self.ofctl_port)
             and self.listen_port(self.prom_port)
-            and super().listening())
+            and super().listening()
+        )
 
 
 class Gauge(BaseFAUCET):
     """Start a Gauge controller."""
 
-    def __init__(self, name, tmpdir, controller_intf, controller_ipv6, env,
-                 ctl_privkey, ctl_cert, ca_certs,
-                 port, **kwargs):
+    def __init__(
+        self,
+        name,
+        tmpdir,
+        controller_intf,
+        controller_ipv6,
+        env,
+        ctl_privkey,
+        ctl_cert,
+        ca_certs,
+        port,
+        **kwargs
+    ):
         super().__init__(
             name,
             tmpdir,
-            controller_intf, controller_ipv6,
+            controller_intf,
+            controller_ipv6,
             cargs=self._tls_cargs(port, ctl_privkey, ctl_cert, ca_certs),
-            command=self._command(env, tmpdir, name, '--gauge'),
+            command=self._command(env, tmpdir, name, "--gauge"),
             port=port,
-            **kwargs)
+            **kwargs
+        )
```

### Comparing `c65faucet-1.0.49/clib/mininet_test_util.py` & `c65faucet-1.0.50/clib/mininet_test_util.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,136 +8,151 @@
 import subprocess
 import time
 
 # pylint: disable=import-error
 from mininet.log import error, output
 
 
-DEVNULL = open(os.devnull, 'wb', encoding=None)  # pylint: disable=consider-using-with
-GETPORT = 'GETPORT'
-PUTPORTS = 'PUTPORTS'
-GETSERIAL = 'GETSERIAL'
-LISTPORTS = 'LISTPORTS'
-LOCALHOST = '127.0.0.1'
-LOCALHOSTV6 = '::1'
-FAUCET_DIR = os.getenv('FAUCET_DIR', '../faucet')
+DEVNULL = open(os.devnull, "wb", encoding=None)  # pylint: disable=consider-using-with
+GETPORT = "GETPORT"
+PUTPORTS = "PUTPORTS"
+GETSERIAL = "GETSERIAL"
+LISTPORTS = "LISTPORTS"
+LOCALHOST = "127.0.0.1"
+LOCALHOSTV6 = "::1"
+FAUCET_DIR = os.getenv("FAUCET_DIR", "../faucet")
 RESERVED_FOR_TESTS_PORTS = (179, 5001, 5002, 6633, 6653)
-with open('/proc/sys/net/netfilter/nf_conntrack_tcp_timeout_time_wait', encoding='utf-8') as pf:
+with open(
+    "/proc/sys/net/netfilter/nf_conntrack_tcp_timeout_time_wait", encoding="utf-8"
+) as pf:
     MIN_PORT_AGE = max(int(pf.read()) / 2, 10)
 
 
 def flat_test_name(_id):
     """Return short form test name from TestCase ID."""
-    return '-'.join(_id.split('.')[1:])
+    return "-".join(_id.split(".")[1:])
 
 
 def lsof_tcp_listening_cmd(port, ipv, state, terse):
     """Return a command line for lsof for processes with specified TCP state."""
-    terse_arg = ''
+    terse_arg = ""
     if terse:
-        terse_arg = '-t'
-    return f'lsof -b -P -n {terse_arg} -sTCP:{state} -i {ipv} -a -i tcp:{port}'
+        terse_arg = "-t"
+    return "lsof -b -P -n %s -sTCP:%s -i %u -a -i tcp:%u" % (
+        terse_arg,
+        state,
+        ipv,
+        port,
+    )
 
 
 def lsof_udp_listening_cmd(port, terse):
     """Return a command line for lsof for processes with specified TCP state."""
-    terse_arg = ''
+    terse_arg = ""
     if terse:
-        terse_arg = '-t'
-    return f'lsof -b -P -n {terse_arg} -i udp:{port} -a'
+        terse_arg = "-t"
+    return "lsof -b -P -n %s -i udp:%u -a" % (terse_arg, port)
 
 
-def tcp_listening_cmd(port, ipv=4, state='LISTEN', terse=True):
+def tcp_listening_cmd(port, ipv=4, state="LISTEN", terse=True):
     """Call lsof_tcp_listening_cmd() with default args."""
     return lsof_tcp_listening_cmd(port, ipv, state, terse)
 
 
 def udp_listening_cmd(port, terse=True):
     """Call lsof_tcp_listening_cmd() with default args."""
     return lsof_udp_listening_cmd(port, terse)
 
 
 def mininet_dpid(int_dpid):
     """Return stringified hex version, of int DPID for mininet."""
-    return str('%x' % int(int_dpid))
+    return str("%x" % int(int_dpid))
 
 
 def str_int_dpid(str_dpid):
     """Return stringified int version, of int or hex DPID from YAML."""
     str_dpid = str(str_dpid)
-    if str_dpid.startswith('0x'):
+    if str_dpid.startswith("0x"):
         return str(int(str_dpid, 16))
     return str(int(str_dpid))
 
 
 def receive_sock_line(sock):
     """Receive a \n terminated line from a socket."""
-    buf = ''
-    while buf.find('\n') <= -1:
+    buf = ""
+    while buf.find("\n") <= -1:
         buf += sock.recv(2**10).decode()
     return buf.strip()
 
 
 def tcp_listening(port):
     """Return True if any process listening on a port."""
-    return subprocess.call(
-        tcp_listening_cmd(port).split(),
-        stdin=DEVNULL,
-        stdout=DEVNULL,
-        stderr=DEVNULL,
-        close_fds=True) == 0
+    return (
+        subprocess.call(
+            tcp_listening_cmd(port).split(),
+            stdin=DEVNULL,
+            stdout=DEVNULL,
+            stderr=DEVNULL,
+            close_fds=True,
+        )
+        == 0
+    )
 
 
 def udp_listening(port):
     """Return True if any process listening on a port."""
-    return subprocess.call(
-        udp_listening_cmd(port).split(),
-        stdin=DEVNULL,
-        stdout=DEVNULL,
-        stderr=DEVNULL,
-        close_fds=True) == 0
+    return (
+        subprocess.call(
+            udp_listening_cmd(port).split(),
+            stdin=DEVNULL,
+            stdout=DEVNULL,
+            stderr=DEVNULL,
+            close_fds=True,
+        )
+        == 0
+    )
 
 
 def test_server_request(ports_socket, name, command):
     assert name is not None
     sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
     sock.connect(ports_socket)
-    sock.sendall((f'{command},{name}\n').encode())
-    output(f'{name} {command}\n')
+    sock.sendall(("%s,%s\n" % (command, name)).encode())
+    output("%s %s\n" % (name, command))
     buf = receive_sock_line(sock)
-    responses = [int(i) for i in buf.split('\n')]
+    responses = [int(i) for i in buf.split("\n")]
     sock.close()
     if len(responses) == 1:
         responses = responses[0]
-    output(f'{name} {command}: {responses}\n')
+    output("%s %s: %u\n" % (name, command, responses))
     return responses
 
 
 def get_serialno(ports_socket, name):
     """Retrieve serial number from test server."""
     return test_server_request(ports_socket, name, GETSERIAL)
 
 
 def find_free_port(ports_socket, name):
     """Retrieve a free TCP port from test server."""
-    request_name = '-'.join((name, str(os.getpid())))
+    request_name = "-".join((name, str(os.getpid())))
     while True:
         port = test_server_request(ports_socket, request_name, GETPORT)
         if not tcp_listening(port):
             return port
-        error(f'port {port} is busy, try another')
+        error("port %u is busy, try another" % port)
 
 
 def find_free_udp_port(ports_socket, name):
-    request_name = '-'.join((name, str(os.getpid())))
+    request_name = "-".join((name, str(os.getpid())))
     while True:
         port = test_server_request(ports_socket, request_name, GETPORT)
         if not udp_listening(port):
             return port
-        error(f'port {port} is busy, try another')
+        error("port %u is busy, try another" % port)
 
 
 def return_free_ports(ports_socket, name):
     """Notify test server that all ports under name are released."""
     return test_server_request(ports_socket, name, PUTPORTS)
 
 
@@ -147,15 +162,15 @@
     free_ports = set()
     port_age = {}
     serialno = 0
 
     def get_port():
         while True:
             free_socket = socket.socket()
-            free_socket.bind(('', 0))
+            free_socket.bind(("", 0))
             free_port = free_socket.getsockname()[1]
             free_socket.close()
             if free_port < 1024:
                 continue
             if free_port in RESERVED_FOR_TESTS_PORTS:
                 continue
             if free_port in free_ports:
@@ -176,15 +191,15 @@
     sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
     sock.bind(ports_socket)
     sock.listen(1)
     cold_start = True
 
     while True:
         connection, _ = sock.accept()
-        command, name = receive_sock_line(connection).split(',')
+        command, name = receive_sock_line(connection).split(",")
         response = None
         if command == GETSERIAL:
             serialno += 1
             response = serialno
         elif command == PUTPORTS:
             ports_returned = 0
             for port in ports_by_name[name]:
@@ -204,23 +219,23 @@
                 time.sleep(1)
             ports_by_name[name].add(port)
             response = port
             queue_free_ports(min_free_ports)
         elif command == LISTPORTS:
             response = list(ports_by_name[name])
         if response is not None:
-            response_str = ''
+            response_str = ""
             if isinstance(response, int):
                 response = [response]
-            response_str = ''.join(['%u\n' % i for i in response])
+            response_str = "".join(["%u\n" % i for i in response])
             connection.sendall(response_str.encode())  # pylint: disable=no-member
         connection.close()
 
 
 def timeout_cmd(cmd, timeout):
     """Return a command line prefaced with a timeout wrappers and stdout/err unbuffered."""
-    return f'timeout -sKILL {timeout}s stdbuf -o0 -e0 {cmd}'
+    return "timeout -sKILL %us stdbuf -o0 -e0 %s" % (timeout, cmd)
 
 
 def timeout_soft_cmd(cmd, timeout):
     """Same as timeout_cmd buf using SIGTERM on timeout."""
-    return f'timeout {timeout}s stdbuf -o0 -e0 {cmd}'
+    return "timeout %us stdbuf -o0 -e0 %s" % (timeout, cmd)
```

### Comparing `c65faucet-1.0.49/clib/mininet_test_watcher.py` & `c65faucet-1.0.50/clib/mininet_test_watcher.py`

 * *Files 3% similar despite different names*

```diff
@@ -61,29 +61,30 @@
             host_graph.remove_edge(*edge)
         self.host_graph = host_graph
 
     def add_fault(self, name):
         """
         Add a general/controller fault
         """
-        error('FAULT: %s\n' % name)
+        error("FAULT: %s\n" % name)
         self.fault_list.append(name)
 
     def add_link_fault(self, src_i, dst_i, name):
         """
         Adds a link fault, i.e: removes a switch-switch edge from the predicted graph
 
         Args:
             src_i (int): Source index of the switch link
             dst_i (int): Destination index of the switch link
             name (str): Fault event name
         """
         try:
             self.switch_graph.remove_edge(
-                self.topo.switches_by_id[src_i], self.topo.switches_by_id[dst_i])
+                self.topo.switches_by_id[src_i], self.topo.switches_by_id[dst_i]
+            )
             self.add_fault(name)
         except networkx.exception.NetworkXError:
             pass
 
     def add_switch_fault(self, i, name):
         """
         Add a switch fault, i.e: removes a switch node from the predicted graph
@@ -135,40 +136,47 @@
                 if not symmetric:
                     connection_graph.add_edge(longest_path[i + 1], longest_path[i])
             # Find and add remaining nodes
             for node in self.switch_graph:
                 if node not in connection_graph:
                     # Add remaining nodes to connection graph
                     path = networkx.shortest_paths.shortest_path(
-                        self.switch_graph, node, list(connection_graph.nodes())[0])
+                        self.switch_graph, node, list(connection_graph.nodes())[0]
+                    )
                     for i in range(0, len(path) - 1):
                         # Add path until we have reached a point that is completely inside
                         # the original simple graph
-                        if path[i] in connection_graph and path[i + 1] in connection_graph:
+                        if (
+                            path[i] in connection_graph
+                            and path[i + 1] in connection_graph
+                        ):
                             break
                     connection_graph.add_edge(path[i], path[i + 1])
                     if not symmetric:
                         connection_graph.add_edge(path[i + 1], path[i])
         return connection_graph
 
-    def get_connected_hosts(self, symmetric=False, transitive=False, intervlan_only=False):
+    def get_connected_hosts(
+        self, symmetric=False, transitive=False, intervlan_only=False
+    ):
         """
         Construct an expected connected host graph
 
         Args:
             symmetric (bool): Assume symmetric pings
             transitive (bool): Assume transitive pings
             intervlan_only (bool): Test hosts only inter-VLAN
 
         Returns:
             networkx.MultiDiGraph: expected host connection graph
         """
         # Generate switch connectivity graph
         switch_connection_graph = self._get_switch_connectivity_graph(
-            symmetric=symmetric, transitive=transitive)
+            symmetric=symmetric, transitive=transitive
+        )
         # Convert switch connections to host connections
         host_connection_graph = networkx.MultiDiGraph()
         for src, dst in switch_connection_graph.edges():
             src_hosts = self.host_graph.neighbors(src)
             dst_hosts = self.host_graph.neighbors(dst)
             for src_host in src_hosts:
                 for dst_host in dst_hosts:
@@ -179,16 +187,16 @@
                             host_connection_graph.add_edge(src_host, dst_host)
                     else:
                         host_connection_graph.add_edge(src_host, dst_host)
         return host_connection_graph
 
     def _routed_vlans(self, src_host, dst_host):
         """Return true only if src_host, dst_host vlans share a router"""
-        src_vlan = self.host_information[self.topo.nodeInfo(src_host)['host_n']]['vlan']
-        dst_vlan = self.host_information[self.topo.nodeInfo(dst_host)['host_n']]['vlan']
+        src_vlan = self.host_information[self.topo.nodeInfo(src_host)["host_n"]]["vlan"]
+        dst_vlan = self.host_information[self.topo.nodeInfo(dst_host)["host_n"]]["vlan"]
         for vlans in self.routers.values():
             if src_vlan in vlans and dst_vlan in vlans:
                 return True
         return False
 
     def get_eligable_link_events(self):
         """Return list of available stack links to take down"""
@@ -221,13 +229,13 @@
 
     def continue_faults(self):
         """Returns true whether there are more faults to occur"""
         return self.get_eligable_link_events() or self.get_eligable_switch_events()
 
     def dump_info(self, tmpdir):
         """Dump topology watcher info into test directory"""
-        sw_graph_fn = os.path.join(tmpdir, 'final_switch_graph.txt')
+        sw_graph_fn = os.path.join(tmpdir, "final_switch_graph.txt")
         networkx.write_edgelist(self.switch_graph, sw_graph_fn)
-        fault_list_fn = os.path.join(tmpdir, 'fault-list.txt')
-        with open(fault_list_fn, 'w', encoding='utf-8') as fl_file:
+        fault_list_fn = os.path.join(tmpdir, "fault-list.txt")
+        with open(fault_list_fn, "w", encoding="utf-8") as fl_file:
             for fault_name in self.fault_list:
-                fl_file.write(fault_name + '\n')
+                fl_file.write(fault_name + "\n")
```

### Comparing `c65faucet-1.0.49/clib/tcpdump_helper.py` & `c65faucet-1.0.50/clib/tcpdump_helper.py`

 * *Files 8% similar despite different names*

```diff
@@ -18,48 +18,60 @@
     started = False
     last_line = None
     funcs = None
     readbuf = None
     blocking = True
 
     # pylint: disable=too-many-arguments
-    def __init__(self, tcpdump_host, tcpdump_filter, funcs=None,
-                 vflags='-v', timeout=10, packets=2, root_intf=False,
-                 pcap_out=None, intf_name=None, blocking=True):
+    def __init__(
+        self,
+        tcpdump_host,
+        tcpdump_filter,
+        funcs=None,
+        vflags="-v",
+        timeout=10,
+        packets=2,
+        root_intf=False,
+        pcap_out=None,
+        intf_name=None,
+        blocking=True,
+    ):
         self.intf_name = intf_name if intf_name else tcpdump_host.intf().name
         self.funcs = funcs
         if root_intf:
-            self.intf_name = self.intf_name.split('.')[0]
+            self.intf_name = self.intf_name.split(".")[0]
 
         tcpdump_flags = vflags
-        # pylint: disable=consider-using-f-string
-        tcpdump_flags += ' -Z root'
-        tcpdump_flags += ' -c %u' % packets if packets else ''
-        tcpdump_flags += ' -w %s' % pcap_out if pcap_out else ''
-        tcpdump_cmd = 'tcpdump -i %s %s --immediate-mode -e -n -U %s' % (
-            self.intf_name, tcpdump_flags, tcpdump_filter)
+        tcpdump_flags += " -Z root"
+        tcpdump_flags += " -c %u" % packets if packets else ""
+        tcpdump_flags += " -w %s" % pcap_out if pcap_out else ""
+        tcpdump_cmd = "tcpdump -i %s %s --immediate-mode -e -n -U %s" % (
+            self.intf_name,
+            tcpdump_flags,
+            tcpdump_filter,
+        )
         pipe_cmd = tcpdump_cmd
         if timeout:
             pipe_cmd = mininet_test_util.timeout_soft_cmd(tcpdump_cmd, timeout)
 
         debug(pipe_cmd)
         self.pipe = tcpdump_host.popen(
             pipe_cmd,
             stdin=mininet_test_util.DEVNULL,
             stdout=subprocess.PIPE,
             stderr=subprocess.STDOUT,
             close_fds=True,
-            shell=False)
+            shell=False,
+        )
 
         stream = self.stream()
         if stream:
-            debug('tcpdump_helper stream fd %s %s' % (
-                stream.fileno(), self.intf_name))
+            debug("tcpdump_helper stream fd %s %s" % (stream.fileno(), self.intf_name))
 
-        self.readbuf = ''
+        self.readbuf = ""
         self.set_blocking(blocking)
 
     def stream(self):
         """Return pipe's STDOUT, or None."""
         if self.pipe:
             return self.pipe.stdout
         return None
@@ -73,15 +85,15 @@
             flags = flags & ~os.O_NONBLOCK
         else:
             flags = flags | os.O_NONBLOCK
         fcntl.fcntl(stdout_fd, fcntl.F_SETFL, flags)
 
     def execute(self):
         """Run the helper and accumulate tcpdump output."""
-        tcpdump_txt = ''
+        tcpdump_txt = ""
         stream = self.stream()
         if stream:
             while True:
                 line = self.next_line()
                 if not line:
                     break
                 debug('tcpdump_helper fd %d line "%s"' % (stream.fileno(), line))
@@ -91,60 +103,65 @@
     def terminate(self):
         """Terminate the helper."""
         stream = self.stream()
         if not self.pipe or not stream:
             return -1
 
         try:
-            debug('tcpdump_helper terminate fd %s' % stream.fileno())
+            debug("tcpdump_helper terminate fd %s" % stream.fileno())
             self.pipe.terminate()
             result = self.pipe.wait()
             if result == 124:
                 # Mask valid result from timeout command.
                 result = 0
             self.pipe.stdout.close()
             self.pipe = None
             return result
         except EnvironmentError as err:
-            error(f'Error closing tcpdump_helper fd {self.pipe.stdout.fileno()}: {err}')
+            error(
+                "Error closing tcpdump_helper fd %d: %s"
+                % (self.pipe.stdout.fileno(), err)
+            )
             return -2
 
     def readline(self):
         """Replacement readline() because built-in doesn't work with non-blocking IO"""
         fileno = self.pipe.stdout.fileno()
-        while '\n' not in self.readbuf:
+        while "\n" not in self.readbuf:
             try:
                 read = os.read(fileno, 2**10)
             except OSError as err:
                 if err.errno != errno.EAGAIN or not self.blocking:
                     raise
                 continue
             if not read:
                 line = self.readbuf
-                self.readbuf = ''
+                self.readbuf = ""
                 return line
             self.readbuf += read.decode()
-        pos = self.readbuf.find('\n') + 1
+        pos = self.readbuf.find("\n") + 1
         line = self.readbuf[0:pos]
         self.readbuf = self.readbuf[pos:]
         return line
 
     def next_line(self):
         """Retrieve next line from helper."""
         while True:
             try:
                 line = self.readline()
             except OSError as err:
-                if err.errno in (errno.EWOULDBLOCK, errno.EAGAIN):
-                    return ''
+                if err.errno == errno.EWOULDBLOCK or err.errno == errno.EAGAIN:
+                    return ""
                 raise
-            assert line or self.started, f'tcpdump did not start: {self.last_line.strip()}'
+            assert line or self.started, (
+                "tcpdump did not start: %s" % self.last_line.strip()
+            )
             if self.started:
                 return line
-            if re.search(f'listening on {self.intf_name}', line):
+            if re.search("listening on %s" % self.intf_name, line):
                 self.started = True
                 # When we see tcpdump start, then call provided functions.
                 if self.funcs is not None:
                     for func in self.funcs:
                         func()
             else:
                 self.last_line = line
```

### Comparing `c65faucet-1.0.49/clib/valve_test_lib.py` & `c65faucet-1.0.50/clib/valve_test_lib.py`

 * *Files 15% similar despite different names*

```diff
@@ -31,15 +31,25 @@
 import shutil
 import tempfile
 
 import unittest
 
 from os_ken.lib import mac
 from os_ken.lib.packet import (
-    arp, ethernet, icmp, icmpv6, ipv4, ipv6, lldp, slow, packet, vlan)
+    arp,
+    ethernet,
+    icmp,
+    icmpv6,
+    ipv4,
+    ipv6,
+    lldp,
+    slow,
+    packet,
+    vlan,
+)
 from os_ken.ofproto import ether, inet
 from os_ken.ofproto import ofproto_v1_3 as ofp
 from os_ken.ofproto import ofproto_v1_3_parser as parser
 from prometheus_client import CollectorRegistry
 from beka.route import RouteAddition, RouteRemoval
 from beka.ip import IPAddress, IPPrefix
 
@@ -62,87 +72,94 @@
     Build and return a dictionary from a pkt
     This function is supposed to be in duality with build_pkt
     i.e. build_dict(build_pkt(dict)) == dict && build_pkt(build_dict(pkt)) == pkt
     """
     pkt_dict = {}
     arp_pkt = pkt.get_protocol(arp.arp)
     if arp_pkt:
-        pkt_dict['arp_source_ip'] = arp_pkt.src_ip
-        pkt_dict['arp_target_ip'] = arp_pkt.dst_ip
-        pkt_dict['opcode'] = arp_pkt.opcode
+        pkt_dict["arp_source_ip"] = arp_pkt.src_ip
+        pkt_dict["arp_target_ip"] = arp_pkt.dst_ip
+        pkt_dict["opcode"] = arp_pkt.opcode
     ipv6_pkt = pkt.get_protocol(ipv6.ipv6)
     if ipv6_pkt:
-        pkt_dict['ipv6_src'] = ipv6_pkt.src
-        pkt_dict['ipv6_dst'] = ipv6_pkt.dst
+        pkt_dict["ipv6_src"] = ipv6_pkt.src
+        pkt_dict["ipv6_dst"] = ipv6_pkt.dst
     icmpv6_pkt = pkt.get_protocol(icmpv6.icmpv6)
     if icmpv6_pkt:
         type_ = icmpv6_pkt.type_
         if type_ == icmpv6.ND_ROUTER_ADVERT:
             for option in icmpv6_pkt.data.options:
-                if hasattr(option, 'hw_src'):
-                    pkt_dict['eth_src'] = option.hw_src
-                if hasattr(option, 'prefix'):
-                    pkt_dict['router_advert_ip'] = option.prefix
+                if hasattr(option, "hw_src"):
+                    pkt_dict["eth_src"] = option.hw_src
+                if hasattr(option, "prefix"):
+                    pkt_dict["router_advert_ip"] = option.prefix
         elif type_ == icmpv6.ND_ROUTER_SOLICIT:
-            pkt_dict['router_solicit_ip'] = None
+            pkt_dict["router_solicit_ip"] = None
         elif type_ == icmpv6.ND_NEIGHBOR_ADVERT:
-            pkt_dict['neighbor_advert_ip'] = icmpv6_pkt.data.dst
-            pkt_dict['eth_src'] = icmpv6_pkt.data.option.hw_src
+            pkt_dict["neighbor_advert_ip"] = icmpv6_pkt.data.dst
+            pkt_dict["eth_src"] = icmpv6_pkt.data.option.hw_src
         elif type_ == icmpv6.ND_NEIGHBOR_SOLICIT:
-            pkt_dict['neighbor_solicit_ip'] = icmpv6_pkt.data.dst
-            pkt_dict['eth_src'] = icmpv6_pkt.data.option.hw_src
+            pkt_dict["neighbor_solicit_ip"] = icmpv6_pkt.data.dst
+            pkt_dict["eth_src"] = icmpv6_pkt.data.option.hw_src
         elif type_ == icmpv6.ICMPV6_ECHO_REQUEST:
-            pkt_dict['echo_request_data'] = icmpv6_pkt.data.data
+            pkt_dict["echo_request_data"] = icmpv6_pkt.data.data
         else:
-            raise NotImplementedError(f'Unknown packet type {icmpv6_pkt} \n')
+            raise NotImplementedError("Unknown packet type %s \n" % icmpv6_pkt)
     ipv4_pkt = pkt.get_protocol(ipv4.ipv4)
     if ipv4_pkt:
-        pkt_dict['ipv4_src'] = ipv4_pkt.src
-        pkt_dict['ipv4_dst'] = ipv4_pkt.dst
+        pkt_dict["ipv4_src"] = ipv4_pkt.src
+        pkt_dict["ipv4_dst"] = ipv4_pkt.dst
     icmp_pkt = pkt.get_protocol(icmp.icmp)
     if icmp_pkt:
         type_ = icmp_pkt.type_
         if type_ == icmp.ICMP_ECHO_REQUEST:
-            pkt_dict['echo_request_data'] = icmp_pkt.data.data
+            pkt_dict["echo_request_data"] = icmp_pkt.data.data
         else:
-            raise NotImplementedError(f'Unknown packet type {icmp_pkt} \n')
+            raise NotImplementedError("Unknown packet type %s \n" % icmp_pkt)
     lacp_pkt = pkt.get_protocol(slow.lacp)
     if lacp_pkt:
-        pkt_dict['actor_system'] = lacp_pkt.actor_system
-        pkt_dict['partner_system'] = lacp_pkt.partner_system
-        pkt_dict['actor_state_synchronization'] = lacp_pkt.actor_state_synchronization
+        pkt_dict["actor_system"] = lacp_pkt.actor_system
+        pkt_dict["partner_system"] = lacp_pkt.partner_system
+        pkt_dict["actor_state_synchronization"] = lacp_pkt.actor_state_synchronization
     lldp_pkt = pkt.get_protocol(lldp.lldp)
     if lldp_pkt:
+
         def faucet_lldp_tlvs(dp_mac, tlv_type, value):
             oui = valve_packet.faucet_oui(dp_mac)
-            value = str(value).encode('utf-8')
+            value = str(value).encode("utf-8")
             return (oui, tlv_type, value)
-        chassis_tlv = valve_packet.tlvs_by_type(lldp_pkt.tlvs, lldp.LLDP_TLV_CHASSIS_ID)[0]
+
+        chassis_tlv = valve_packet.tlvs_by_type(
+            lldp_pkt.tlvs, lldp.LLDP_TLV_CHASSIS_ID
+        )[0]
         chassis_id = valve_packet.addrconv.mac.bin_to_text(chassis_tlv.chassis_id)
-        pkt_dict['chassis_id'] = chassis_id
+        pkt_dict["chassis_id"] = chassis_id
         faucet_tlvs = tuple(valve_packet.parse_faucet_lldp(lldp_pkt, chassis_id))
         remote_dp_id, remote_dp_name, remote_port_id, remote_port_state = faucet_tlvs
-        pkt_dict['system_name'] = remote_dp_name
-        pkt_dict['port_id'] = remote_port_id
-        pkt_dict['eth_dst'] = lldp.LLDP_MAC_NEAREST_BRIDGE
+        pkt_dict["system_name"] = remote_dp_name
+        pkt_dict["port_id"] = remote_port_id
+        pkt_dict["eth_dst"] = lldp.LLDP_MAC_NEAREST_BRIDGE
         tlvs = [
             faucet_lldp_tlvs(chassis_id, valve_packet.LLDP_FAUCET_DP_ID, remote_dp_id),
-            faucet_lldp_tlvs(chassis_id, valve_packet.LLDP_FAUCET_STACK_STATE, remote_port_state)
+            faucet_lldp_tlvs(
+                chassis_id, valve_packet.LLDP_FAUCET_STACK_STATE, remote_port_state
+            ),
         ]
-        pkt_dict['tlvs'] = tlvs
+        pkt_dict["tlvs"] = tlvs
     vlan_pkt = pkt.get_protocol(vlan.vlan)
     if vlan_pkt:
-        pkt_dict['vid'] = vlan_pkt.vid
+        pkt_dict["vid"] = vlan_pkt.vid
     eth_pkt = pkt.get_protocol(ethernet.ethernet)
     if eth_pkt:
-        pkt_dict['eth_src'] = eth_pkt.src
-        if 'eth_dst' in pkt_dict and pkt_dict['eth_dst'] != eth_pkt.dst:
+        pkt_dict["eth_src"] = eth_pkt.src
+        if "eth_dst" in pkt_dict and pkt_dict["eth_dst"] != eth_pkt.dst:
             raise NotImplementedError(
-                'Previous allocation of eth_dst does not match ethernet dst\n')
-        pkt_dict['eth_dst'] = eth_pkt.dst
+                "Previous allocation of eth_dst does not match ethernet dst\n"
+            )
+        pkt_dict["eth_dst"] = eth_pkt.dst
     return pkt_dict
 
 
 def build_pkt(pkt):
     """Build and return a packet and eth type from a dict."""
 
     def serialize(layers):
@@ -150,158 +167,193 @@
         result = packet.Packet()
         for layer in reversed(layers):
             result.add_protocol(layer)
         result.serialize()
         return result
 
     layers = []
-    assert 'eth_dst' in pkt and 'eth_src' in pkt
+    assert "eth_dst" in pkt and "eth_src" in pkt
     ethertype = None
-    if 'arp_source_ip' in pkt and 'arp_target_ip' in pkt:
+    if "arp_source_ip" in pkt and "arp_target_ip" in pkt:
         ethertype = ether.ETH_TYPE_ARP
-        arp_code = pkt.get('arp_code', arp.ARP_REQUEST)
-        layers.append(arp.arp(
-            src_ip=pkt['arp_source_ip'],
-            dst_ip=pkt['arp_target_ip'],
-            opcode=arp_code))
-    elif 'ipv6_src' in pkt and 'ipv6_dst' in pkt:
+        arp_code = pkt.get("arp_code", arp.ARP_REQUEST)
+        layers.append(
+            arp.arp(
+                src_ip=pkt["arp_source_ip"],
+                dst_ip=pkt["arp_target_ip"],
+                opcode=arp_code,
+            )
+        )
+    elif "ipv6_src" in pkt and "ipv6_dst" in pkt:
         ethertype = ether.ETH_TYPE_IPV6
-        if 'router_solicit_ip' in pkt:
-            layers.append(icmpv6.icmpv6(
-                type_=icmpv6.ND_ROUTER_SOLICIT))
-        elif 'neighbor_advert_ip' in pkt:
-            layers.append(icmpv6.icmpv6(
-                type_=icmpv6.ND_NEIGHBOR_ADVERT,
-                data=icmpv6.nd_neighbor(
-                    dst=pkt['neighbor_advert_ip'],
-                    option=icmpv6.nd_option_sla(hw_src=pkt['eth_src']))))
-        elif 'neighbor_solicit_ip' in pkt:
-            layers.append(icmpv6.icmpv6(
-                type_=icmpv6.ND_NEIGHBOR_SOLICIT,
-                data=icmpv6.nd_neighbor(
-                    dst=pkt['neighbor_solicit_ip'],
-                    option=icmpv6.nd_option_sla(hw_src=pkt['eth_src']))))
-        elif 'echo_request_data' in pkt:
-            layers.append(icmpv6.icmpv6(
-                type_=icmpv6.ICMPV6_ECHO_REQUEST,
-                data=icmpv6.echo(id_=1, seq=1, data=pkt['echo_request_data'])))
-        layers.append(ipv6.ipv6(
-            src=pkt['ipv6_src'],
-            dst=pkt['ipv6_dst'],
-            nxt=inet.IPPROTO_ICMPV6))
-    elif 'ipv4_src' in pkt and 'ipv4_dst' in pkt:
+        if "router_solicit_ip" in pkt:
+            layers.append(icmpv6.icmpv6(type_=icmpv6.ND_ROUTER_SOLICIT))
+        elif "neighbor_advert_ip" in pkt:
+            layers.append(
+                icmpv6.icmpv6(
+                    type_=icmpv6.ND_NEIGHBOR_ADVERT,
+                    data=icmpv6.nd_neighbor(
+                        dst=pkt["neighbor_advert_ip"],
+                        option=icmpv6.nd_option_sla(hw_src=pkt["eth_src"]),
+                    ),
+                )
+            )
+        elif "neighbor_solicit_ip" in pkt:
+            layers.append(
+                icmpv6.icmpv6(
+                    type_=icmpv6.ND_NEIGHBOR_SOLICIT,
+                    data=icmpv6.nd_neighbor(
+                        dst=pkt["neighbor_solicit_ip"],
+                        option=icmpv6.nd_option_sla(hw_src=pkt["eth_src"]),
+                    ),
+                )
+            )
+        elif "echo_request_data" in pkt:
+            layers.append(
+                icmpv6.icmpv6(
+                    type_=icmpv6.ICMPV6_ECHO_REQUEST,
+                    data=icmpv6.echo(id_=1, seq=1, data=pkt["echo_request_data"]),
+                )
+            )
+        layers.append(
+            ipv6.ipv6(src=pkt["ipv6_src"], dst=pkt["ipv6_dst"], nxt=inet.IPPROTO_ICMPV6)
+        )
+    elif "ipv4_src" in pkt and "ipv4_dst" in pkt:
         ethertype = ether.ETH_TYPE_IP
         proto = inet.IPPROTO_IP
-        if 'echo_request_data' in pkt:
-            echo = icmp.echo(id_=1, seq=1, data=pkt['echo_request_data'])
+        if "echo_request_data" in pkt:
+            echo = icmp.echo(id_=1, seq=1, data=pkt["echo_request_data"])
             layers.append(icmp.icmp(type_=icmp.ICMP_ECHO_REQUEST, data=echo))
             proto = inet.IPPROTO_ICMP
-        net = ipv4.ipv4(src=pkt['ipv4_src'], dst=pkt['ipv4_dst'], proto=proto)
+        net = ipv4.ipv4(src=pkt["ipv4_src"], dst=pkt["ipv4_dst"], proto=proto)
         layers.append(net)
-    elif 'actor_system' in pkt and 'partner_system' in pkt:
+    elif "actor_system" in pkt and "partner_system" in pkt:
         ethertype = ether.ETH_TYPE_SLOW
-        layers.append(slow.lacp(
-            version=1,
-            actor_system=pkt['actor_system'],
-            actor_port=1,
-            partner_system=pkt['partner_system'],
-            partner_port=1,
-            actor_key=1,
-            partner_key=1,
-            actor_system_priority=65535,
-            partner_system_priority=1,
-            actor_port_priority=255,
-            partner_port_priority=255,
-            actor_state_defaulted=0,
-            partner_state_defaulted=0,
-            actor_state_expired=0,
-            partner_state_expired=0,
-            actor_state_timeout=1,
-            partner_state_timeout=1,
-            actor_state_collecting=1,
-            partner_state_collecting=1,
-            actor_state_distributing=1,
-            partner_state_distributing=1,
-            actor_state_aggregation=1,
-            partner_state_aggregation=1,
-            actor_state_synchronization=pkt['actor_state_synchronization'],
-            partner_state_synchronization=1,
-            actor_state_activity=0,
-            partner_state_activity=0))
-    elif 'chassis_id' in pkt and 'port_id' in pkt:
+        layers.append(
+            slow.lacp(
+                version=1,
+                actor_system=pkt["actor_system"],
+                actor_port=1,
+                partner_system=pkt["partner_system"],
+                partner_port=1,
+                actor_key=1,
+                partner_key=1,
+                actor_system_priority=65535,
+                partner_system_priority=1,
+                actor_port_priority=255,
+                partner_port_priority=255,
+                actor_state_defaulted=0,
+                partner_state_defaulted=0,
+                actor_state_expired=0,
+                partner_state_expired=0,
+                actor_state_timeout=1,
+                partner_state_timeout=1,
+                actor_state_collecting=1,
+                partner_state_collecting=1,
+                actor_state_distributing=1,
+                partner_state_distributing=1,
+                actor_state_aggregation=1,
+                partner_state_aggregation=1,
+                actor_state_synchronization=pkt["actor_state_synchronization"],
+                partner_state_synchronization=1,
+                actor_state_activity=0,
+                partner_state_activity=0,
+            )
+        )
+    elif "chassis_id" in pkt and "port_id" in pkt:
         ethertype = ether.ETH_TYPE_LLDP
         return valve_packet.lldp_beacon(
-            pkt['eth_src'], pkt['chassis_id'], str(pkt['port_id']), 1,
-            org_tlvs=pkt.get('org_tlvs', None),
-            system_name=pkt.get('system_name', None))
+            pkt["eth_src"],
+            pkt["chassis_id"],
+            str(pkt["port_id"]),
+            1,
+            org_tlvs=pkt.get("org_tlvs", None),
+            system_name=pkt.get("system_name", None),
+        )
     assert ethertype is not None, pkt
-    if 'vid' in pkt:
+    if "vid" in pkt:
         tpid = ether.ETH_TYPE_8021Q
-        layers.append(vlan.vlan(vid=pkt['vid'], ethertype=ethertype))
+        layers.append(vlan.vlan(vid=pkt["vid"], ethertype=ethertype))
     else:
         tpid = ethertype
-    eth = ethernet.ethernet(
-        dst=pkt['eth_dst'],
-        src=pkt['eth_src'],
-        ethertype=tpid)
+    eth = ethernet.ethernet(dst=pkt["eth_dst"], src=pkt["eth_src"], ethertype=tpid)
     layers.append(eth)
     result = serialize(layers)
     return result
 
 
-FAUCET_MAC = '0e:00:00:00:00:01'
+FAUCET_MAC = "0e:00:00:00:00:01"
 
 BASE_DP_CONFIG = """
         hardware: 'GenericTFM'
         ignore_learn_ins: 100
         ofchannel_log: '/dev/null'
         packetin_pps: 99
         slowpath_pps: 99
         lldp_beacon:
             send_interval: 1
             max_per_interval: 1
 """
 
-BASE_DP1_CONFIG = """
+BASE_DP1_CONFIG = (
+    """
         dp_id: 1
-""" + BASE_DP_CONFIG
+"""
+    + BASE_DP_CONFIG
+)
 
-DP1_CONFIG = """
+DP1_CONFIG = (
+    """
         combinatorial_port_flood: True
-""" + BASE_DP1_CONFIG
+"""
+    + BASE_DP1_CONFIG
+)
 
-IDLE_DP1_CONFIG = """
+IDLE_DP1_CONFIG = (
+    """
         use_idle_timeout: True
-""" + DP1_CONFIG
+"""
+    + DP1_CONFIG
+)
 
-GROUP_DP1_CONFIG = """
+GROUP_DP1_CONFIG = (
+    """
         group_table: True
-""" + BASE_DP1_CONFIG
+"""
+    + BASE_DP1_CONFIG
+)
 
-DOT1X_CONFIG = """
+DOT1X_CONFIG = (
+    """
         dot1x:
             nfv_intf: lo
             nfv_sw_port: 2
             radius_ip: 127.0.0.1
             radius_port: 1234
             radius_secret: SECRET
-""" + BASE_DP1_CONFIG
+"""
+    + BASE_DP1_CONFIG
+)
 
-DOT1X_ACL_CONFIG = """
+DOT1X_ACL_CONFIG = (
+    """
         dot1x:
             nfv_intf: lo
             nfv_sw_port: 2
             radius_ip: 127.0.0.1
             radius_port: 1234
             radius_secret: SECRET
             auth_acl: auth_acl
             noauth_acl: noauth_acl
-""" + BASE_DP1_CONFIG
+"""
+    + BASE_DP1_CONFIG
+)
 
-CONFIG = """
+CONFIG = (
+    """
 dps:
     s1:
 %s
         interfaces:
             p1:
                 number: 1
                 native_vlan: v100
@@ -406,18 +458,21 @@
             - route:
                 ip_dst: 'fc00::20:0/112'
                 ip_gw: 'fc00::1:99'
     v300:
         vid: 0x300
     v400:
         vid: 0x400
-""" % DP1_CONFIG
+"""
+    % DP1_CONFIG
+)
 
 
-STACK_CONFIG = """
+STACK_CONFIG = (
+    """
 dps:
     s1:
 %s
         stack:
             priority: 1
         interfaces:
             1:
@@ -468,15 +523,17 @@
                 description: p2
                 stack:
                     dp: s2
                     port: 3
 vlans:
     v100:
         vid: 0x100
-    """ % DP1_CONFIG
+    """
+    % DP1_CONFIG
+)
 
 STACK_LOOP_CONFIG = """
 dps:
     s1:
 %s
         interfaces:
             1:
@@ -529,84 +586,90 @@
                     port: 2
             3:
                 description: p3
                 native_vlan: v100
 vlans:
     v100:
         vid: 0x100
-""" % (BASE_DP1_CONFIG, BASE_DP_CONFIG, BASE_DP_CONFIG)
+""" % (
+    BASE_DP1_CONFIG,
+    BASE_DP_CONFIG,
+    BASE_DP_CONFIG,
+)
 
 
 class ValveTestBases:
     """Insulate test base classes from unittest so we can reuse base clases."""
 
     @staticmethod
     def packet_outs_from_flows(flows):
         """Return flows that are packetout actions."""
-        return [flow for flow in flows if isinstance(flow, valve_of.parser.OFPPacketOut)]
+        return [
+            flow for flow in flows if isinstance(flow, valve_of.parser.OFPPacketOut)
+        ]
 
     @staticmethod
     def flowmods_from_flows(flows):
         """Return flows that are flowmods actions."""
         return [flow for flow in flows if isinstance(flow, valve_of.parser.OFPFlowMod)]
 
     class ValveTestNetwork(unittest.TestCase):
         """Base class for tests that require multiple DPs with their own FakeOFTables"""
 
         @staticmethod
-        def profile(func, sortby='cumulative', amount=20, count=1):
+        def profile(func, sortby="cumulative", amount=20, count=1):
             """Convenience method to profile a function call."""
             prof = cProfile.Profile()
             prof.enable()
             for _ in range(count):
                 func()
             prof.disable()
             prof_stream = io.StringIO()
             prof_stats = pstats.Stats(prof, stream=prof_stream).sort_stats(sortby)
             prof_stats.print_stats(amount)
             return (prof_stats, prof_stream.getvalue())
 
         @staticmethod
         def create_mac_str(i, j):
             """Create a host MAC string"""
-            return '00:00:00:%02x:00:%02x' % (i, j)
+            return "00:00:00:%02x:00:%02x" % (i, j)
 
         @staticmethod
         def create_vid(i):
             """Create a vid with VID_PRESENT"""
             return 0x100 * i | ofp.OFPVID_PRESENT
 
         # Default DP name
-        DP_NAME = 's1'
+        DP_NAME = "s1"
 
         # Default DP ID
         DP_ID = 1
 
-        P1_V100_MAC = '00:00:00:01:00:01'
-        P2_V100_MAC = '00:00:00:01:00:02'
-        P3_V100_MAC = '00:00:00:01:00:03'
-        P1_V200_MAC = '00:00:00:02:00:01'
-        P2_V200_MAC = '00:00:00:02:00:02'
-        P3_V200_MAC = '00:00:00:02:00:03'
-        P1_V300_MAC = '00:00:00:03:00:01'
-        UNKNOWN_MAC = '00:00:00:04:00:04'
+        P1_V100_MAC = "00:00:00:01:00:01"
+        P2_V100_MAC = "00:00:00:01:00:02"
+        P3_V100_MAC = "00:00:00:01:00:03"
+        P1_V200_MAC = "00:00:00:02:00:01"
+        P2_V200_MAC = "00:00:00:02:00:02"
+        P3_V200_MAC = "00:00:00:02:00:03"
+        P1_V300_MAC = "00:00:00:03:00:01"
+        UNKNOWN_MAC = "00:00:00:04:00:04"
 
-        BROADCAST_MAC = 'ff:ff:ff:ff:ff:ff'
-        FAUCET_MAC = '0e:00:00:00:00:01'
+        BROADCAST_MAC = "ff:ff:ff:ff:ff:ff"
+        FAUCET_MAC = "0e:00:00:00:00:01"
 
         V100 = 0x100 | ofp.OFPVID_PRESENT
         V200 = 0x200 | ofp.OFPVID_PRESENT
         V300 = 0x300 | ofp.OFPVID_PRESENT
 
         # Number of tables to configure in the FakeOFTable
         NUM_TABLES = 10
         NUM_PORTS = 5
 
-        LOGNAME = 'faucet'
-        ICMP_PAYLOAD = bytes('A' * 64, encoding='UTF-8')
+        LOGNAME = "faucet"
+        ICMP_PAYLOAD = bytes("A" * 64, encoding="UTF-8")
         REQUIRE_TFM = True
         CONFIG_AUTO_REVERT = False
         CONFIG = None
 
         def __init__(self, *args, **kwargs):
             self.dot1x = None
             self.valves_manager = None
@@ -637,40 +700,58 @@
                 increment__sec (int): Amount to increment the current mock time
             Returns:
                 current mock time
             """
             self.mock_now_sec += increment_sec
             return self.mock_now_sec
 
-        def setup_valves(self, config, error_expected=0, log_stdout=False, ports_up=None):
+        def setup_valves(
+            self, config, error_expected=0, log_stdout=False, ports_up=None
+        ):
             """
             Set up test with config
             Args:
                 config (str): The Faucet config file
                 error_expected (int): The error expected, if any
                 log_stdout: Whether to log to stdout or not
             """
             self.tmpdir = tempfile.mkdtemp()
-            self.config_file = os.path.join(self.tmpdir, 'valve_unit.yaml')
-            logfile = 'STDOUT' if log_stdout else os.path.join(self.tmpdir, 'faucet.log')
+            self.config_file = os.path.join(self.tmpdir, "valve_unit.yaml")
+            logfile = (
+                "STDOUT" if log_stdout else os.path.join(self.tmpdir, "faucet.log")
+            )
             self.logger = valve_util.get_logger(self.LOGNAME, logfile, logging.DEBUG, 0)
             self.registry = CollectorRegistry()
             self.metrics = faucet_metrics.FaucetMetrics(reg=self.registry)
-            self.notifier = faucet_event.FaucetEventNotifier(None, self.metrics, self.logger)
+            self.notifier = faucet_event.FaucetEventNotifier(
+                None, self.metrics, self.logger
+            )
             self.bgp = faucet_bgp.FaucetBgp(
-                self.logger, logfile, self.metrics, self.send_flows_to_dp_by_id)
+                self.logger, logfile, self.metrics, self.send_flows_to_dp_by_id
+            )
             self.dot1x = faucet_dot1x.FaucetDot1x(
-                self.logger, logfile, self.metrics, self.send_flows_to_dp_by_id)
+                self.logger, logfile, self.metrics, self.send_flows_to_dp_by_id
+            )
             self.valves_manager = valves_manager.ValvesManager(
-                self.LOGNAME, self.logger, self.metrics, self.notifier,
-                self.bgp, self.dot1x, self.CONFIG_AUTO_REVERT, self.send_flows_to_dp_by_id)
+                self.LOGNAME,
+                self.logger,
+                self.metrics,
+                self.notifier,
+                self.bgp,
+                self.dot1x,
+                self.CONFIG_AUTO_REVERT,
+                self.send_flows_to_dp_by_id,
+            )
             self.last_flows_to_dp[self.DP_ID] = []
             initial_ofmsgs = self.update_config(
-                config, reload_expected=False,
-                error_expected=error_expected, configure_network=True)
+                config,
+                reload_expected=False,
+                error_expected=error_expected,
+                configure_network=True,
+            )
             if not error_expected:
                 for dp_id in self.valves_manager.valves:
                     self.connect_dp(dp_id, ports_up)
             return initial_ofmsgs
 
         def teardown_valves(self):
             """Tear down test valves"""
@@ -694,17 +775,21 @@
                 before_str (str): String representation of the table before changes
                 dp_id (int): DP ID of the table to test difference
             """
             after_hash = self.network.hash_table(int(dp_id))
             if before_hash != after_hash:
                 after_str = str(self.network.tables[dp_id])
                 diff = difflib.unified_diff(
-                    before_str.splitlines(), after_str.splitlines())
-                self.assertEqual(before_hash, after_hash,
-                                 msg='%s != %s\n'.join(diff) % (before_hash, after_hash))
+                    before_str.splitlines(), after_str.splitlines()
+                )
+                self.assertEqual(
+                    before_hash,
+                    after_hash,
+                    msg="%s != %s\n".join(diff) % (before_hash, after_hash),
+                )
 
         def _verify_redundant_safe_offset_ofmsgs(self, ofmsgs, dp_id, offset=1):
             """
             Verify that a copy of the ofmsgs applied to the FakeOFTable with an offset
             will converge to the original table state. This ensures that a redundant
             controller with a delayed ofmsgs application will still result in
             a consistent table structure.
@@ -741,141 +826,180 @@
             if dp_id is None:
                 dp_id = self.DP_ID
             valve = self.valves_manager.valves[dp_id]
             final_ofmsgs = valve.prepare_send_flows(ofmsgs)
             self.network.apply_ofmsgs(int(dp_id), final_ofmsgs)
             if all_offsets:
                 for offset_iter in range(len(ofmsgs)):
-                    self._verify_redundant_safe_offset_ofmsgs(ofmsgs, dp_id, offset_iter)
+                    self._verify_redundant_safe_offset_ofmsgs(
+                        ofmsgs, dp_id, offset_iter
+                    )
             elif offset:
                 self._verify_redundant_safe_offset_ofmsgs(ofmsgs, dp_id, offset)
             return final_ofmsgs
 
         def send_flows_to_dp_by_id(self, valve, flows):
             """Callback function for ValvesManager to simulate sending flows to a DP"""
             flows = valve.prepare_send_flows(flows)
             self.last_flows_to_dp[valve.dp.dp_id] = flows
 
         def configure_network(self):
             """Creates the FakeOFNetwork"""
             for dp_id in self.valves_manager.valves:
                 self.last_flows_to_dp[dp_id] = []
-            self.network = FakeOFNetwork(self.valves_manager, self.NUM_TABLES, self.REQUIRE_TFM)
+            self.network = FakeOFNetwork(
+                self.valves_manager, self.NUM_TABLES, self.REQUIRE_TFM
+            )
 
         def get_events(self):
             events = []
             while True:
                 event = self.valves_manager.notifier.get_event()
                 if not event:
                     return events
                 events.append(event)
 
-        def update_config(self, config, table_dpid=None, reload_type='cold',
-                          reload_expected=True, error_expected=0,
-                          no_reload_no_table_change=True,
-                          configure_network=False):
+        def update_config(
+            self,
+            config,
+            table_dpid=None,
+            reload_type="cold",
+            reload_expected=True,
+            error_expected=0,
+            no_reload_no_table_change=True,
+            configure_network=False,
+        ):
             """
             Updates the Faucet config and reloads Faucet
             Args:
                 config (str): The configuraation that will be loaded
                 dp_id (int): DP ID of the expected reload type
                 reload_type ('warm' or 'cold'): Expected reload increment type
                 reload_expected (bool): Whether the reload type is expected to increment
                 error_expected (int): The error number that is expected from the config
             """
             before_table_states = None
             if self.network is not None:
                 before_table_states = {
-                    dp_id: table.table_state() for dp_id, table in self.network.tables.items()}
-            before_dp_status = int(self.get_prom('dp_status'))
+                    dp_id: table.table_state()
+                    for dp_id, table in self.network.tables.items()
+                }
+            before_dp_status = int(self.get_prom("dp_status"))
             existing_config = None
             if os.path.exists(self.config_file):
-                with open(self.config_file, encoding='utf-8') as config_file:
+                with open(self.config_file, encoding="utf-8") as config_file:
                     existing_config = config_file.read()
-            with open(self.config_file, 'w', encoding='utf-8') as config_file:
+            with open(self.config_file, "w", encoding="utf-8") as config_file:
                 config_file.write(config)
             content_change_expected = config != existing_config
             self.assertEqual(
                 content_change_expected,
-                self.valves_manager.config_watcher.content_changed(self.config_file))
+                self.valves_manager.config_watcher.content_changed(self.config_file),
+            )
             for dp_id in self.valves_manager.valves:
                 self.last_flows_to_dp[dp_id] = []
             reload_ofmsgs = []
             all_ofmsgs = {}
             reload_func = partial(
                 self.valves_manager.request_reload_configs,
-                self.mock_time(10), self.config_file)
+                self.mock_time(10),
+                self.config_file,
+            )
             if error_expected:
                 reload_func()
                 if configure_network:
                     self.configure_network()
             else:
                 if reload_type is not None:
-                    var = f'faucet_config_reload_{reload_type}_total'
+                    var = "faucet_config_reload_%s_total" % reload_type
                     self.prom_inc(
-                        reload_func, var=var, inc_expected=reload_expected, dp_id=table_dpid)
+                        reload_func,
+                        var=var,
+                        inc_expected=reload_expected,
+                        dp_id=table_dpid,
+                    )
                 else:
                     reload_func()
                 if configure_network:
                     self.configure_network()
                 for dp_id in self.valves_manager.valves:
                     reload_ofmsgs = self.last_flows_to_dp.get(dp_id, [])
                     # When cold starting, we must either request a disconnect
                     # from the switch or have flows to send.
-                    if (dp_id == self.DP_ID
-                            and before_dp_status
-                            and reload_type == 'cold'
-                            and reload_expected):
-                        self.assertTrue(reload_ofmsgs is None or reload_ofmsgs, reload_ofmsgs)
+                    if (
+                        dp_id == self.DP_ID
+                        and before_dp_status
+                        and reload_type == "cold"
+                        and reload_expected
+                    ):
+                        self.assertTrue(
+                            reload_ofmsgs is None or reload_ofmsgs, reload_ofmsgs
+                        )
                     if reload_ofmsgs is None:
                         reload_ofmsgs = self.connect_dp(dp_id)
                     else:
                         self._verify_wildcard_deletes(reload_type, reload_ofmsgs)
                         self.apply_ofmsgs(reload_ofmsgs, dp_id)
                     all_ofmsgs[dp_id] = reload_ofmsgs
-                    if (not reload_expected and no_reload_no_table_change
-                            and before_table_states is not None and dp_id in before_table_states):
+                    if (
+                        not reload_expected
+                        and no_reload_no_table_change
+                        and before_table_states is not None
+                        and dp_id in before_table_states
+                    ):
                         before_hash, before_str = before_table_states[dp_id]
                         self._check_table_difference(before_hash, before_str, dp_id)
-            self.assertEqual(before_dp_status, int(self.get_prom('dp_status')))
-            config_status = self.get_prom('faucet_config_load_error', bare=True)
+            self.assertEqual(before_dp_status, int(self.get_prom("dp_status")))
+            config_status = self.get_prom("faucet_config_load_error", bare=True)
             self.assertEqual(error_expected, config_status)
             return all_ofmsgs
 
         def _verify_wildcard_deletes(self, reload_type, reload_ofmsgs):
             """Verify the only wildcard delete usage when warm starting, is for in_port."""
-            if reload_type != 'warm':
+            if reload_type != "warm":
                 return
             for ofmsg in reload_ofmsgs:
                 if not valve_of.is_flowdel(ofmsg):
                     continue
                 self.assertNotEqual(ofmsg.table_id, valve_of.ofp.OFPTT_ALL, ofmsg)
 
-        def update_and_revert_config(self, orig_config, new_config, reload_type,
-                                     verify_func=None, before_table_states=None,
-                                     table_dpid=None):
+        def update_and_revert_config(
+            self,
+            orig_config,
+            new_config,
+            reload_type,
+            verify_func=None,
+            before_table_states=None,
+            table_dpid=None,
+        ):
             """
             Updates to the new config then reverts back to the original config to
                 ensure restarting properly dismantles/keep appropriate flow rules
             Args:
                 orig_config (str): The original configuration file
                 new_config (str): The new configuration file
                 cold_starts (dict): Dictionary of dp_id that is expecting cold
                     starts or warm starts
                 verify_func (func): Function to verify state changes
                 before_table_states (dict): Dict of string state by dp_id of the
                     table before reloading
             """
             if before_table_states is None:
                 before_table_states = {
-                    dp_id: table.table_state() for dp_id, table in self.network.tables.items()}
-            self.update_config(new_config, reload_type=reload_type, table_dpid=table_dpid)
+                    dp_id: table.table_state()
+                    for dp_id, table in self.network.tables.items()
+                }
+            self.update_config(
+                new_config, reload_type=reload_type, table_dpid=table_dpid
+            )
             if verify_func is not None:
                 verify_func()
-            self.update_config(orig_config, reload_type=reload_type, table_dpid=table_dpid)
+            self.update_config(
+                orig_config, reload_type=reload_type, table_dpid=table_dpid
+            )
             for dp_id, states in before_table_states.items():
                 before_hash, before_str = states
                 self._check_table_difference(before_hash, before_str, dp_id)
 
         def connect_dp(self, dp_id=None, ports_up=None):
             """
             Call to connect DP with all (or selected) ports link-up
@@ -888,32 +1012,32 @@
             if dp_id is None:
                 dp_id = self.DP_ID
             valve = self.valves_manager.valves[dp_id]
             if ports_up is None:
                 discovered_up_ports = set(valve.dp.ports.keys())
             else:
                 discovered_up_ports = set(ports_up)
-            connect_msgs = (
-                valve.switch_features(None)
-                + valve.datapath_connect(self.mock_time(10), discovered_up_ports))
+            connect_msgs = valve.switch_features(None) + valve.datapath_connect(
+                self.mock_time(10), discovered_up_ports
+            )
             connect_msgs = self.apply_ofmsgs(connect_msgs, dp_id)
             self.valves_manager.update_config_applied(sent={dp_id: True})
-            self.assertEqual(1, int(self.get_prom('dp_status', dp_id=dp_id)))
+            self.assertEqual(1, int(self.get_prom("dp_status", dp_id=dp_id)))
             self.assertTrue(valve.dp.to_conf())
             return connect_msgs
 
         def disconnect_dp(self):
             valve = self.valves_manager.valves[self.DP_ID]
             valve.datapath_disconnect(self.mock_time())
 
         def migrate_stack_root(self, new_root_name):
             now = self.mock_time()
             self.valves_manager.set_stack_root(now, new_root_name)
             self.valves_manager.reload_stack_root_config(now)
-            self.valves_manager.valve_flow_services(now, 'fast_state_expire')
+            self.valves_manager.valve_flow_services(now, "fast_state_expire")
             self.trigger_all_ports()
 
         def cold_start(self, dp_id=None):
             """
             Cold start a DP
             Args:
                 dp_id: ID for the DP to cold start
@@ -935,28 +1059,26 @@
                     dp_id = self.DP_ID
                 if dp_id not in self.valves_manager.valves:
                     dp_id = self.DP_ID
                     dp_name = self.DP_NAME
                 else:
                     valve = self.valves_manager.valves[dp_id]
                     dp_name = valve.dp.name
-                labels.update({
-                    'dp_name': dp_name,
-                    'dp_id': '0x%x' % dp_id})
+                labels.update({"dp_name": dp_name, "dp_id": "0x%x" % dp_id})
             val = self.registry.get_sample_value(var, labels)
             if val is None:
                 val = 0
             return val
 
         def prom_inc(self, func, var, labels=None, inc_expected=True, dp_id=None):
             """Check Prometheus variable increments by 1 after calling a function."""
             before = self.get_prom(var, labels, dp_id)
             func()
             after = self.get_prom(var, labels, dp_id)
-            msg = f'{var} {labels} before {before} after {after}'
+            msg = "%s %s before %f after %f" % (var, labels, before, after)
             if inc_expected:
                 self.assertEqual(before + 1, after, msg=msg)
             else:
                 self.assertEqual(before, after, msg=msg)
 
         def rcv_packet(self, port, vid, match, dp_id=None):
             """
@@ -973,37 +1095,49 @@
             if dp_id is None:
                 dp_id = self.DP_ID
             valve = self.valves_manager.valves[dp_id]
             pkt = build_pkt(match)
             vlan_pkt = pkt
             if vid and vid not in match:
                 vlan_match = match
-                vlan_match['vid'] = vid
+                vlan_match["vid"] = vid
                 vlan_pkt = build_pkt(match)
             msg = namedtuple(
-                'null_msg',
-                ('match', 'in_port', 'data', 'total_len', 'cookie', 'reason'))(
-                    {'in_port': port}, port, vlan_pkt.data, len(vlan_pkt.data),
-                    valve.dp.cookie, valve_of.ofp.OFPR_ACTION)
+                "null_msg",
+                ("match", "in_port", "data", "total_len", "cookie", "reason"),
+            )(
+                {"in_port": port},
+                port,
+                vlan_pkt.data,
+                len(vlan_pkt.data),
+                valve.dp.cookie,
+                valve_of.ofp.OFPR_ACTION,
+            )
             for i in self.valves_manager.valves:
                 self.last_flows_to_dp[i] = []
             now = self.mock_time(0)
-            packet_in_func = partial(self.valves_manager.valve_packet_in, now, valve, msg)
+            packet_in_func = partial(
+                self.valves_manager.valve_packet_in, now, valve, msg
+            )
             if dp_id == self.DP_ID:
-                self.prom_inc(packet_in_func, 'of_packet_ins_total')
+                self.prom_inc(packet_in_func, "of_packet_ins_total")
             else:
                 packet_in_func()
             all_ofmsgs = {}
             for i in self.valves_manager.valves:
                 rcv_packet_ofmsgs = self.last_flows_to_dp[i]
                 all_ofmsgs[i] = rcv_packet_ofmsgs
                 self.last_flows_to_dp[i] = []
                 self.apply_ofmsgs(rcv_packet_ofmsgs, i)
             for valve_service in (
-                    'resolve_gateways', 'advertise', 'fast_advertise', 'state_expire'):
+                "resolve_gateways",
+                "advertise",
+                "fast_advertise",
+                "state_expire",
+            ):
                 self.valves_manager.valve_flow_services(now, valve_service)
             self.valves_manager.update_metrics(now)
             return all_ofmsgs
 
         def rcv_lldp(self, port, other_dp, other_port, dp_id=None):
             """
             Receives an LLDP packet
@@ -1017,30 +1151,36 @@
             """
             if dp_id is None:
                 dp_id = self.DP_ID
             tlvs = []
             tlvs.extend(valve_packet.faucet_lldp_tlvs(other_dp))
             tlvs.extend(valve_packet.faucet_lldp_stack_state_tlvs(other_dp, other_port))
             dp_mac = other_dp.faucet_dp_mac if other_dp.faucet_dp_mac else FAUCET_MAC
-            rcv_ofmsgs = self.rcv_packet(port.number, 0, {
-                'eth_src': dp_mac,
-                'eth_dst': lldp.LLDP_MAC_NEAREST_BRIDGE,
-                'port_id': other_port.number,
-                'chassis_id': dp_mac,
-                'system_name': other_dp.name,
-                'org_tlvs': tlvs}, dp_id=dp_id)
+            rcv_ofmsgs = self.rcv_packet(
+                port.number,
+                0,
+                {
+                    "eth_src": dp_mac,
+                    "eth_dst": lldp.LLDP_MAC_NEAREST_BRIDGE,
+                    "port_id": other_port.number,
+                    "chassis_id": dp_mac,
+                    "system_name": other_dp.name,
+                    "org_tlvs": tlvs,
+                },
+                dp_id=dp_id,
+            )
             return rcv_ofmsgs
 
         def port_labels(self, port_no, dp_id=None):
             """Get port labels"""
             if dp_id is None:
                 dp_id = self.DP_ID
             valve = self.valves_manager.valves[dp_id]
             port = valve.dp.ports[port_no]
-            return {'port': port.name, 'port_description': port.description}
+            return {"port": port.name, "port_description": port.description}
 
         def port_expected_status(self, port_no, exp_status, dp_id=None):
             """
             Verify port has expected status
             Args:
                 port_no (int): Port number of the port on the DP
                 exp_status (int): Expected status for the port
@@ -1048,18 +1188,21 @@
             """
             if dp_id is None:
                 dp_id = self.DP_ID
             valve = self.valves_manager.valves[dp_id]
             if port_no not in valve.dp.ports:
                 return
             labels = self.port_labels(port_no, dp_id)
-            status = int(self.get_prom('port_status', labels=labels, dp_id=dp_id))
+            status = int(self.get_prom("port_status", labels=labels, dp_id=dp_id))
             self.assertEqual(
-                status, exp_status,
-                msg=f'status {status} != expected {exp_status} for port {labels}')
+                status,
+                exp_status,
+                msg="status %u != expected %u for port %s"
+                % (status, exp_status, labels),
+            )
 
         def get_other_valves(self, valve):
             """Return other running valves"""
             return self.valves_manager._other_running_valves(valve)
 
         def add_port(self, port_no, link_up=True, dp_id=None):
             """
@@ -1068,48 +1211,66 @@
                 port_no (int): Port number to set to UP
                 link_up (bool): Port initially link up ?
                 dp_id (int): DP ID containing the port number
             """
             if dp_id is None:
                 dp_id = self.DP_ID
             valve = self.valves_manager.valves[dp_id]
-            self.apply_ofmsgs(valve.port_status_handler(
-                port_no, ofp.OFPPR_ADD, 0 if link_up else ofp.OFPPS_LINK_DOWN,
-                [], self.mock_time(0)).get(valve, []))
+            self.apply_ofmsgs(
+                valve.port_status_handler(
+                    port_no,
+                    ofp.OFPPR_ADD,
+                    0 if link_up else ofp.OFPPS_LINK_DOWN,
+                    [],
+                    self.mock_time(0),
+                ).get(valve, [])
+            )
             self.port_expected_status(port_no, 1 if link_up else 0)
 
         def delete_port(self, port_no, dp_id=None):
             """
             Delete a port
             Args:
                 port_no (int): Port number to set to UP
                 dp_id (int): DP ID containing the port number
             """
             if dp_id is None:
                 dp_id = self.DP_ID
             valve = self.valves_manager.valves[dp_id]
-            self.apply_ofmsgs(valve.port_status_handler(
-                port_no, ofp.OFPPR_DELETE, ofp.OFPPS_LINK_DOWN, [],
-                self.mock_time(0)).get(valve, []))
+            self.apply_ofmsgs(
+                valve.port_status_handler(
+                    port_no,
+                    ofp.OFPPR_DELETE,
+                    ofp.OFPPS_LINK_DOWN,
+                    [],
+                    self.mock_time(0),
+                ).get(valve, [])
+            )
             self.port_expected_status(port_no, 0)
 
         def set_port_state(self, port_no, link_up, dp_id=None):
             """
             Set the link up/down state of a port
             Args:
                 port_no (int): Port number to set to UP
                 link_up (bool): Port now link up ?
                 dp_id (int): DP ID containing the port number
             """
             if dp_id is None:
                 dp_id = self.DP_ID
             valve = self.valves_manager.valves[dp_id]
-            self.apply_ofmsgs(valve.port_status_handler(
-                port_no, ofp.OFPPR_MODIFY, 0 if link_up else ofp.OFPPS_LINK_DOWN,
-                [], self.mock_time(0)).get(valve, []))
+            self.apply_ofmsgs(
+                valve.port_status_handler(
+                    port_no,
+                    ofp.OFPPR_MODIFY,
+                    0 if link_up else ofp.OFPPS_LINK_DOWN,
+                    [],
+                    self.mock_time(0),
+                ).get(valve, [])
+            )
             self.port_expected_status(port_no, 1 if link_up else 0)
 
         def set_port_link_up(self, port_no, dp_id=None):
             """
             Set a port link up
             """
             self.set_port_state(port_no, True, dp_id=dp_id)
@@ -1139,50 +1300,60 @@
             Trigger a stack port by receiving an LLDP packet
             Args:
                 ignore_ports (list): List of port objects to ignore when sending LLDP
                     packets, this effectively takes the stack port down
             """
             # Expire all of the stack ports
             if ignore_ports:
-                valves = [self.valves_manager.valves[port.dp_id] for port in ignore_ports]
-                max_interval = max([valve.dp.lldp_beacon['send_interval'] for valve in valves])
+                valves = [
+                    self.valves_manager.valves[port.dp_id] for port in ignore_ports
+                ]
+                max_interval = max(
+                    [valve.dp.lldp_beacon["send_interval"] for valve in valves]
+                )
                 max_lost = max([port.max_lldp_lost for port in ignore_ports])
                 now = self.mock_time((max_interval * max_lost) + 1)
                 for dp_id in self.valves_manager.valves:
                     self.last_flows_to_dp[dp_id] = []
-                self.valves_manager.valve_flow_services(now, 'fast_state_expire')
+                self.valves_manager.valve_flow_services(now, "fast_state_expire")
                 for dp_id in self.valves_manager.valves:
                     self.apply_ofmsgs(self.last_flows_to_dp[dp_id], dp_id)
                     self.last_flows_to_dp[dp_id] = []
                 for valve in self.valves_manager.valves.values():
                     for port in valve.dp.ports.values():
                         if port.stack:
                             exp_state = 4
                             self.assertEqual(
-                                port.dyn_stack_current_state, exp_state,
-                                f'{port} stack state {port.dyn_stack_current_state} != {exp_state}')
+                                port.dyn_stack_current_state,
+                                exp_state,
+                                "%s stack state %s != %s"
+                                % (port, port.dyn_stack_current_state, exp_state),
+                            )
             # Send LLDP packets to reset the stack ports that we want to be up
             for dp_id, valve in self.valves_manager.valves.items():
                 for port in valve.dp.ports.values():
                     if ignore_ports and port in ignore_ports:
                         continue
                     if port.stack:
-                        peer_dp = port.stack['dp']
-                        peer_port = port.stack['port']
+                        peer_dp = port.stack["dp"]
+                        peer_port = port.stack["port"]
                         self.rcv_lldp(port, peer_dp, peer_port, dp_id)
             # Verify stack ports are in the correct state
             for valve in self.valves_manager.valves.values():
                 for port in valve.dp.ports.values():
                     if port.stack:
                         exp_state = 3
                         if ignore_ports and port in ignore_ports:
                             exp_state = 4
                         self.assertEqual(
-                            port.dyn_stack_current_state, exp_state,
-                            f'{port} stack state {port.dyn_stack_current_state} != {exp_state}')
+                            port.dyn_stack_current_state,
+                            exp_state,
+                            "%s stack state %s != %s"
+                            % (port, port.dyn_stack_current_state, exp_state),
+                        )
 
         def flap_port(self, port_no):
             """Flap op status on a port."""
             self.set_port_down(port_no)
             self.set_port_up(port_no)
 
         def all_stack_up(self):
@@ -1190,39 +1361,37 @@
             for valve in self.valves_manager.valves.values():
                 valve.dp.dyn_running = True
                 for port in valve.dp.stack_ports():
                     port.stack_up()
 
         def up_stack_port(self, port, dp_id=None):
             """Bring up a single stack port"""
-            peer_dp = port.stack['dp']
-            peer_port = port.stack['port']
+            peer_dp = port.stack["dp"]
+            peer_port = port.stack["port"]
             for state_func in [peer_port.stack_init, peer_port.stack_up]:
                 state_func()
                 self.rcv_lldp(port, peer_dp, peer_port, dp_id)
             self.assertTrue(port.is_stack_up())
 
         def down_stack_port(self, port):
             """Bring down a single stack port"""
             self.up_stack_port(port)
-            peer_port = port.stack['port']
+            peer_port = port.stack["port"]
             peer_port.stack_gone()
             now = self.mock_time(600)
-            self.valves_manager.valve_flow_services(
-                now,
-                'fast_state_expire')
+            self.valves_manager.valve_flow_services(now, "fast_state_expire")
             self.assertTrue(port.is_stack_gone())
 
         def _update_port_map(self, port, add_else_remove):
             this_dp = port.dp_id
             this_num = port.number
-            this_key = f'{this_dp}:{this_num}'
-            peer_dp = port.stack['dp'].dp_id
-            peer_num = port.stack['port'].number
-            peer_key = f'{peer_dp}:{peer_num}'
+            this_key = "%s:%s" % (this_dp, this_num)
+            peer_dp = port.stack["dp"].dp_id
+            peer_num = port.stack["port"].number
+            peer_key = "%s:%s" % (peer_dp, peer_num)
             key_array = [this_key, peer_key]
             key_array.sort()
             key = key_array[0]
             if add_else_remove:
                 self.up_ports[key] = port
             else:
                 del self.up_ports[key]
@@ -1237,27 +1406,26 @@
                     self.up_stack_port(port, dp_id=valve.dp.dp_id)
                     self._update_port_map(port, True)
             self.trigger_all_ports(packets=packets)
 
         def trigger_all_ports(self, packets=10):
             """Do the needful to trigger any pending state changes"""
             valve = self.valves_manager.valves[self.DP_ID]
-            interval = valve.dp.lldp_beacon['send_interval']
+            interval = valve.dp.lldp_beacon["send_interval"]
             for _ in range(0, packets):
                 for port in self.up_ports.values():
                     dp_id = port.dp_id
                     this_dp = self.valves_manager.valves[dp_id].dp
-                    peer_dp = port.stack['dp']
-                    peer_port = port.stack['port']
+                    peer_dp = port.stack["dp"]
+                    peer_port = port.stack["port"]
                     self.rcv_lldp(port, peer_dp, peer_port, dp_id)
                     self.rcv_lldp(peer_port, this_dp, port, peer_dp.dp_id)
                 self.last_flows_to_dp[self.DP_ID] = []
                 now = self.mock_time(interval)
-                self.valves_manager.valve_flow_services(
-                    now, 'fast_state_expire')
+                self.valves_manager.valve_flow_services(now, "fast_state_expire")
                 flows = self.last_flows_to_dp[self.DP_ID]
                 self.apply_ofmsgs(flows, dp_id=self.DP_ID)
 
         def deactivate_stack_port(self, port, packets=10):
             """Deactivate a given stack port"""
             self._update_port_map(port, False)
             self.trigger_all_ports(packets=packets)
@@ -1284,74 +1452,107 @@
 
         def set_stack_port_down(self, port_no, valve=None):
             """Set stack port up recalculating topology as necessary."""
             self.set_stack_port_status(port_no, 2, valve)
 
         def validate_flood(self, in_port, vlan_vid, out_port, expected, msg):
             bcast_match = {
-                'in_port': in_port,
-                'eth_dst': mac.BROADCAST_STR,
-                'vlan_vid': vlan_vid,
-                'eth_type': 0x800,
+                "in_port": in_port,
+                "eth_dst": mac.BROADCAST_STR,
+                "vlan_vid": vlan_vid,
+                "eth_type": 0x800,
             }
             if expected:
                 self.assertTrue(
-                    self.network.tables[self.DP_ID].is_output(bcast_match, port=out_port), msg=msg)
+                    self.network.tables[self.DP_ID].is_output(
+                        bcast_match, port=out_port
+                    ),
+                    msg=msg,
+                )
             else:
                 self.assertFalse(
-                    self.network.tables[self.DP_ID].is_output(bcast_match, port=out_port), msg=msg)
+                    self.network.tables[self.DP_ID].is_output(
+                        bcast_match, port=out_port
+                    ),
+                    msg=msg,
+                )
 
         def pkt_match(self, src, dst):
             """Make a unicast packet match dict for the given src & dst"""
             return {
-                'eth_src': '00:00:00:01:00:%02x' % src,
-                'eth_dst': '00:00:00:01:00:%02x' % dst,
-                'ipv4_src': f'10.0.0.{src}',
-                'ipv4_dst': f'10.0.0.{dst}',
-                'vid': self.V100
+                "eth_src": "00:00:00:01:00:%02x" % src,
+                "eth_dst": "00:00:00:01:00:%02x" % dst,
+                "ipv4_src": "10.0.0.%d" % src,
+                "ipv4_dst": "10.0.0.%d" % dst,
+                "vid": self.V100,
             }
 
         def _config_edge_learn_stack_root(self, new_value):
             config = yaml_load(self.CONFIG)
-            config['vlans']['v100']['edge_learn_stack_root'] = new_value
+            config["vlans"]["v100"]["edge_learn_stack_root"] = new_value
             return yaml_dump(config)
 
         def learn_hosts(self):
             """Learn some hosts."""
             # TODO: verify learn caching.
             for _ in range(2):
-                self.rcv_packet(1, 0x100, {
-                    'eth_src': self.P1_V100_MAC,
-                    'eth_dst': self.UNKNOWN_MAC,
-                    'ipv4_src': '10.0.0.1',
-                    'ipv4_dst': '10.0.0.4'})
+                self.rcv_packet(
+                    1,
+                    0x100,
+                    {
+                        "eth_src": self.P1_V100_MAC,
+                        "eth_dst": self.UNKNOWN_MAC,
+                        "ipv4_src": "10.0.0.1",
+                        "ipv4_dst": "10.0.0.4",
+                    },
+                )
                 # TODO: verify host learning banned
-                self.rcv_packet(1, 0x100, {
-                    'eth_src': self.UNKNOWN_MAC,
-                    'eth_dst': self.P1_V100_MAC,
-                    'ipv4_src': '10.0.0.4',
-                    'ipv4_dst': '10.0.0.1'})
-                self.rcv_packet(3, 0x100, {
-                    'eth_src': self.P3_V100_MAC,
-                    'eth_dst': self.P2_V100_MAC,
-                    'ipv4_src': '10.0.0.3',
-                    'ipv4_dst': '10.0.0.2',
-                    'vid': 0x100})
-                self.rcv_packet(2, 0x200, {
-                    'eth_src': self.P2_V200_MAC,
-                    'eth_dst': self.P3_V200_MAC,
-                    'ipv4_src': '10.0.0.2',
-                    'ipv4_dst': '10.0.0.3',
-                    'vid': 0x200})
-                self.rcv_packet(3, 0x200, {
-                    'eth_src': self.P3_V200_MAC,
-                    'eth_dst': self.P2_V200_MAC,
-                    'ipv4_src': '10.0.0.3',
-                    'ipv4_dst': '10.0.0.2',
-                    'vid': 0x200})
+                self.rcv_packet(
+                    1,
+                    0x100,
+                    {
+                        "eth_src": self.UNKNOWN_MAC,
+                        "eth_dst": self.P1_V100_MAC,
+                        "ipv4_src": "10.0.0.4",
+                        "ipv4_dst": "10.0.0.1",
+                    },
+                )
+                self.rcv_packet(
+                    3,
+                    0x100,
+                    {
+                        "eth_src": self.P3_V100_MAC,
+                        "eth_dst": self.P2_V100_MAC,
+                        "ipv4_src": "10.0.0.3",
+                        "ipv4_dst": "10.0.0.2",
+                        "vid": 0x100,
+                    },
+                )
+                self.rcv_packet(
+                    2,
+                    0x200,
+                    {
+                        "eth_src": self.P2_V200_MAC,
+                        "eth_dst": self.P3_V200_MAC,
+                        "ipv4_src": "10.0.0.2",
+                        "ipv4_dst": "10.0.0.3",
+                        "vid": 0x200,
+                    },
+                )
+                self.rcv_packet(
+                    3,
+                    0x200,
+                    {
+                        "eth_src": self.P3_V200_MAC,
+                        "eth_dst": self.P2_V200_MAC,
+                        "ipv4_src": "10.0.0.3",
+                        "ipv4_dst": "10.0.0.2",
+                        "vid": 0x200,
+                    },
+                )
 
         def verify_expiry(self):
             """Verify FIB resolution attempts expire."""
             valve = self.valves_manager.valves[self.DP_ID]
 
             def expire():
                 now = self.mock_time(valve.dp.timeout * 2)
@@ -1383,86 +1584,114 @@
             def _verify_flood_to_port(match, port, valve_vlan, port_number=None):
                 if valve_vlan.port_is_tagged(port):
                     vid = valve_vlan.vid | ofp.OFPVID_PRESENT
                 else:
                     vid = 0
                 if port_number is None:
                     port_number = port.number
-                return self.network.tables[dp_id].is_output(match, port=port_number, vid=vid)
+                return self.network.tables[dp_id].is_output(
+                    match, port=port_number, vid=vid
+                )
 
             for match in matches:
-                in_port_number = match['in_port']
+                in_port_number = match["in_port"]
                 in_port = valve.dp.ports[in_port_number]
 
-                if ('vlan_vid' in match
-                        and match['vlan_vid'] & ofp.OFPVID_PRESENT != 0):
-                    valve_vlan = valve.dp.vlans[match['vlan_vid'] & ~ofp.OFPVID_PRESENT]
+                if "vlan_vid" in match and match["vlan_vid"] & ofp.OFPVID_PRESENT != 0:
+                    valve_vlan = valve.dp.vlans[match["vlan_vid"] & ~ofp.OFPVID_PRESENT]
                 else:
                     valve_vlan = in_port.native_vlan
 
-                all_ports = {
-                    port for port in valve.dp.ports.values() if port.running()}
+                all_ports = {port for port in valve.dp.ports.values() if port.running()}
                 remaining_ports = all_ports - {
-                    port for port in valve_vlan.get_ports() if port.running}
+                    port for port in valve_vlan.get_ports() if port.running
+                }
 
                 hairpin_output = _verify_flood_to_port(
-                    match, in_port, valve_vlan, ofp.OFPP_IN_PORT)
+                    match, in_port, valve_vlan, ofp.OFPP_IN_PORT
+                )
                 self.assertEqual(
-                    in_port.hairpin, hairpin_output,
-                    msg=f'hairpin flooding incorrect (expected {in_port.hairpin} got {hairpin_output})')
+                    in_port.hairpin,
+                    hairpin_output,
+                    msg="hairpin flooding incorrect (expected %s got %s)"
+                    % (in_port.hairpin, hairpin_output),
+                )
 
                 for port in valve_vlan.get_ports():
                     output = _verify_flood_to_port(match, port, valve_vlan)
                     if valve.floods_to_root():
                         # Packet should only be flooded to root.
-                        self.assertEqual(False, output, 'unexpected non-root flood')
+                        self.assertEqual(False, output, "unexpected non-root flood")
                     else:
                         # Packet must be flooded to all ports on the VLAN.
                         if port == in_port:
-                            self.assertEqual(port.hairpin, output,
-                                             f'unexpected hairpin flood {match} {port.number}')
+                            self.assertEqual(
+                                port.hairpin,
+                                output,
+                                "unexpected hairpin flood %s %u" % (match, port.number),
+                            )
                         else:
                             self.assertTrue(
                                 output,
                                 msg=(
-                                    f'{match} with unknown eth_dst not flooded'
-                                    f' on VLAN {valve_vlan.vid} to port {port.number}\n{self.network.tables[dp_id]}'))
+                                    "%s with unknown eth_dst not flooded"
+                                    " on VLAN %u to port %u\n%s"
+                                    % (
+                                        match,
+                                        valve_vlan.vid,
+                                        port.number,
+                                        self.network.tables[dp_id],
+                                    )
+                                ),
+                            )
 
                 # Packet must not be flooded to ports not on the VLAN.
                 for port in remaining_ports:
                     if port.stack:
                         self.assertTrue(
-                            self.network.tables[dp_id].is_output(match, port=port.number),
-                            msg=(f'Unknown eth_dst not flooded to stack port {port}'))
+                            self.network.tables[dp_id].is_output(
+                                match, port=port.number
+                            ),
+                            msg=("Unknown eth_dst not flooded to stack port %s" % port),
+                        )
                     elif not port.mirror:
                         self.assertFalse(
-                            self.network.tables[dp_id].is_output(match, port=port.number),
-                            msg=(f'Unknown eth_dst flooded to non-VLAN/stack/mirror {port}'))
+                            self.network.tables[dp_id].is_output(
+                                match, port=port.number
+                            ),
+                            msg=(
+                                "Unknown eth_dst flooded to non-VLAN/stack/mirror %s"
+                                % port
+                            ),
+                        )
 
         def verify_pkt(self, pkt, expected_pkt):
             """
             Verifies that a packet contains the matches with correct values
 
             Args:
                 pkt (packet.Packet): The packet object to build into the dictionary
                 expected_pkt (dict): The expected values to be contained in the packet directory
             """
             pkt_dict = build_dict(pkt)
             for key in expected_pkt:
                 self.assertTrue(
-                    key in pkt_dict,
-                    f'key {key} not in pkt {pkt_dict}')
+                    key in pkt_dict, "key %s not in pkt %s" % (key, pkt_dict)
+                )
                 if expected_pkt[key] is None:
                     # Sometimes we may not know that correct value but
                     #   want to ensure that there exists a value so use the None
                     #   value for a packet key
                     continue
                 self.assertEqual(
-                    expected_pkt[key], pkt_dict[key],
-                    f'key: {key} not matching ({expected_pkt[key]} != {pkt_dict[key]})')
+                    expected_pkt[key],
+                    pkt_dict[key],
+                    "key: %s not matching (%s != %s)"
+                    % (key, expected_pkt[key], pkt_dict[key]),
+                )
 
         def verify_route_add_del(self, dp_id, vlan_vid, ip_gw, ip_dst):
             """
             Verifies that adding then deleting routes maintains consistent
                 flow rules in the FakeOFTable
 
             Args:
@@ -1478,52 +1707,61 @@
             self.assertTrue(route_add_replies)
             self.apply_ofmsgs(route_add_replies, dp_id)
             route_del_replies = valve.del_route(valve_vlan, ip_dst)
             self.assertTrue(route_del_replies)
             self.apply_ofmsgs(route_del_replies, dp_id)
             after_table_state = str(self.network.tables[dp_id])
             diff = difflib.unified_diff(
-                before_table_state.splitlines(), after_table_state.splitlines())
-            self.assertEqual(
-                before_table_state, after_table_state, msg='\n'.join(diff))
+                before_table_state.splitlines(), after_table_state.splitlines()
+            )
+            self.assertEqual(before_table_state, after_table_state, msg="\n".join(diff))
 
     class ValveTestBig(ValveTestNetwork):
         """Test basic switching/L2/L3 functions."""
 
         def setUp(self):
             self.CONFIG = CONFIG
             self.setup_valves(CONFIG)
 
         def test_notifier_socket_path(self):
             """Test notifier socket path checker."""
-            new_path = os.path.join(self.tmpdir, 'new_path/new_socket')
+            new_path = os.path.join(self.tmpdir, "new_path/new_socket")
             self.assertEqual(self.notifier.check_path(new_path), new_path)
-            stale_socket = os.path.join(self.tmpdir, 'stale_socket')
-            with open(stale_socket, 'w', encoding='utf-8') as stale_socket_file:
-                stale_socket_file.write('')
+            stale_socket = os.path.join(self.tmpdir, "stale_socket")
+            with open(stale_socket, "w", encoding="utf-8") as stale_socket_file:
+                stale_socket_file.write("")
             self.assertEqual(self.notifier.check_path(stale_socket), stale_socket)
 
         def test_disconnect(self):
             """Test disconnection of DP from controller."""
             valve = self.valves_manager.valves[self.DP_ID]
-            self.assertEqual(1, int(self.get_prom('dp_status')))
-            self.prom_inc(partial(valve.datapath_disconnect, self.mock_time()),
-                          'of_dp_disconnections_total')
-            self.assertEqual(0, int(self.get_prom('dp_status')))
+            self.assertEqual(1, int(self.get_prom("dp_status")))
+            self.prom_inc(
+                partial(valve.datapath_disconnect, self.mock_time()),
+                "of_dp_disconnections_total",
+            )
+            self.assertEqual(0, int(self.get_prom("dp_status")))
 
         def test_unexpected_port(self):
             """Test packet in from unexpected port."""
             self.prom_inc(
-                partial(self.rcv_packet, 999, 0x100, {
-                    'eth_src': self.P1_V300_MAC,
-                    'eth_dst': self.UNKNOWN_MAC,
-                    'ipv4_src': '10.0.0.1',
-                    'ipv4_dst': '10.0.0.2'}),
-                'of_unexpected_packet_ins_total',
-                inc_expected=True)
+                partial(
+                    self.rcv_packet,
+                    999,
+                    0x100,
+                    {
+                        "eth_src": self.P1_V300_MAC,
+                        "eth_dst": self.UNKNOWN_MAC,
+                        "ipv4_src": "10.0.0.1",
+                        "ipv4_dst": "10.0.0.2",
+                    },
+                ),
+                "of_unexpected_packet_ins_total",
+                inc_expected=True,
+            )
 
         def test_oferror(self):
             """Test OFError handler."""
             valve = self.valves_manager.valves[self.DP_ID]
             datapath = None
             msg = valve_of.parser.OFPFlowMod(datapath=datapath)
             msg.xid = 123
@@ -1531,32 +1769,34 @@
             test_error = valve_of.parser.OFPErrorMsg(datapath=datapath, msg=msg)
             valve.oferror(test_error)
 
         def test_tfm(self):
             """Test TFM is sent."""
             valve = self.valves_manager.valves[self.DP_ID]
             network_table = self.network.tables[self.DP_ID]
-            self.assertTrue(
-                isinstance(valve, TfmValve),
-                msg=type(valve))
+            self.assertTrue(isinstance(valve, TfmValve), msg=type(valve))
             discovered_up_ports = set(range(1, self.NUM_PORTS + 1))
             flows = valve.datapath_connect(self.mock_time(10), discovered_up_ports)
             self.apply_ofmsgs(flows)
             tfm_flows = [
-                flow for flow in flows if isinstance(
-                    flow, valve_of.parser.OFPTableFeaturesStatsRequest)]
+                flow
+                for flow in flows
+                if isinstance(flow, valve_of.parser.OFPTableFeaturesStatsRequest)
+            ]
             self.assertTrue(tfm_flows)
             for _, table in valve.dp.tables.items():
                 # Ensure the TFM generated for each table has the correct values
                 table_id = table.table_id
                 self.assertIn(table_id, network_table.tfm)
                 tfm_body = network_table.tfm[table_id]
                 tfm_oxm = [
-                    tfm for tfm in tfm_body.properties
-                    if isinstance(tfm, valve_of.parser.OFPTableFeaturePropOxm)]
+                    tfm
+                    for tfm in tfm_body.properties
+                    if isinstance(tfm, valve_of.parser.OFPTableFeaturePropOxm)
+                ]
                 tfm_setfields = []
                 tfm_matchtypes = []
                 tfm_exactmatch = []
                 for oxm in tfm_oxm:
                     if oxm.type == valve_of.ofp.OFPTFPT_MATCH:
                         tfm_matchtypes.extend(oxm.oxm_ids)
                     elif oxm.type == valve_of.ofp.OFPTFPT_WILDCARDS:
@@ -1569,16 +1809,18 @@
                 for oxm_id in tfm_exactmatch:
                     self.assertIn(oxm_id.type, table.match_types)
                     self.assertEqual(oxm_id.hasmask, table.match_types[oxm_id.type])
                 for oxm_id in tfm_setfields:
                     self.assertIn(oxm_id.type, table.set_fields)
                     self.assertFalse(oxm_id.hasmask)
                 tfm_nexttables = [
-                    tfm for tfm in tfm_body.properties
-                    if isinstance(tfm, valve_of.parser.OFPTableFeaturePropNextTables)]
+                    tfm
+                    for tfm in tfm_body.properties
+                    if isinstance(tfm, valve_of.parser.OFPTableFeaturePropNextTables)
+                ]
                 tfm_nexttable = []
                 tfm_misstable = []
                 for tfm_nt in tfm_nexttables:
                     if tfm_nt.type == valve_of.ofp.OFPTFPT_NEXT_TABLES:
                         tfm_nexttable.append(tfm_nt)
                     elif tfm_nt.type == valve_of.ofp.OFPTFPT_NEXT_TABLES_MISS:
                         tfm_misstable.append(tfm_nt)
@@ -1597,557 +1839,673 @@
             self.assertEqual(None, valve.parse_pkt_meta(msg))
             msg.cookie = valve.dp.cookie
             self.assertEqual(None, valve.parse_pkt_meta(msg))
             msg.reason = valve_of.ofp.OFPR_ACTION
             self.assertEqual(None, valve.parse_pkt_meta(msg))
             msg.match = parser.OFPMatch(in_port=1)
             self.assertEqual(None, valve.parse_pkt_meta(msg))
-            msg.data = b'1234'
+            msg.data = b"1234"
             self.assertEqual(None, valve.parse_pkt_meta(msg))
 
         def test_loop_protect(self):
             """Learn loop protection."""
             for _ in range(2):
-                self.rcv_packet(1, 0x100, {
-                    'eth_src': self.P1_V100_MAC,
-                    'eth_dst': self.UNKNOWN_MAC,
-                    'ipv4_src': '10.0.0.1',
-                    'ipv4_dst': '10.0.0.2'})
-                self.rcv_packet(2, 0x100, {
-                    'eth_src': self.P1_V100_MAC,
-                    'eth_dst': self.UNKNOWN_MAC,
-                    'ipv4_src': '10.0.0.1',
-                    'ipv4_dst': '10.0.0.2',
-                    'vid': 0x100})
+                self.rcv_packet(
+                    1,
+                    0x100,
+                    {
+                        "eth_src": self.P1_V100_MAC,
+                        "eth_dst": self.UNKNOWN_MAC,
+                        "ipv4_src": "10.0.0.1",
+                        "ipv4_dst": "10.0.0.2",
+                    },
+                )
+                self.rcv_packet(
+                    2,
+                    0x100,
+                    {
+                        "eth_src": self.P1_V100_MAC,
+                        "eth_dst": self.UNKNOWN_MAC,
+                        "ipv4_src": "10.0.0.1",
+                        "ipv4_dst": "10.0.0.2",
+                        "vid": 0x100,
+                    },
+                )
 
         def test_lldp(self):
             """Test LLDP reception."""
-            self.assertFalse(self.rcv_packet(1, 0, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': lldp.LLDP_MAC_NEAREST_BRIDGE,
-                'chassis_id': self.P1_V100_MAC,
-                'port_id': 1})[self.DP_ID])
+            self.assertFalse(
+                self.rcv_packet(
+                    1,
+                    0,
+                    {
+                        "eth_src": self.P1_V100_MAC,
+                        "eth_dst": lldp.LLDP_MAC_NEAREST_BRIDGE,
+                        "chassis_id": self.P1_V100_MAC,
+                        "port_id": 1,
+                    },
+                )[self.DP_ID]
+            )
 
         def test_bogon_arp_for_controller(self):
             """Bogon ARP request for controller VIP."""
-            replies = self.rcv_packet(1, 0x100, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': mac.BROADCAST_STR,
-                'arp_code': arp.ARP_REQUEST,
-                'arp_source_ip': '8.8.8.8',
-                'arp_target_ip': '10.0.0.254'})[self.DP_ID]
+            replies = self.rcv_packet(
+                1,
+                0x100,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": mac.BROADCAST_STR,
+                    "arp_code": arp.ARP_REQUEST,
+                    "arp_source_ip": "8.8.8.8",
+                    "arp_target_ip": "10.0.0.254",
+                },
+            )[self.DP_ID]
             # Must be no ARP reply to an ARP request not in our subnet.
             self.assertFalse(ValveTestBases.packet_outs_from_flows(replies))
 
         def test_arp_for_controller(self):
             """ARP request for controller VIP."""
             valve = self.valves_manager.valves[self.DP_ID]
             for _retries in range(3):
                 for arp_mac in (mac.BROADCAST_STR, valve.dp.vlans[0x100].faucet_mac):
-                    arp_replies = self.rcv_packet(1, 0x100, {
-                        'eth_src': self.P1_V100_MAC,
-                        'eth_dst': arp_mac,
-                        'arp_code': arp.ARP_REQUEST,
-                        'arp_source_ip': '10.0.0.1',
-                        'arp_target_ip': '10.0.0.254'})[self.DP_ID]
+                    arp_replies = self.rcv_packet(
+                        1,
+                        0x100,
+                        {
+                            "eth_src": self.P1_V100_MAC,
+                            "eth_dst": arp_mac,
+                            "arp_code": arp.ARP_REQUEST,
+                            "arp_source_ip": "10.0.0.1",
+                            "arp_target_ip": "10.0.0.254",
+                        },
+                    )[self.DP_ID]
                     packet_outs = ValveTestBases.packet_outs_from_flows(arp_replies)
                     self.assertTrue(packet_outs)
                     for arp_pktout in packet_outs:
                         pkt = packet.Packet(arp_pktout.data)
                         exp_pkt = {
-                            'opcode': 2,
-                            'arp_source_ip': '10.0.0.254',
-                            'arp_target_ip': '10.0.0.1',
-                            'eth_src': FAUCET_MAC,
-                            'eth_dst': self.P1_V100_MAC}
+                            "opcode": 2,
+                            "arp_source_ip": "10.0.0.254",
+                            "arp_target_ip": "10.0.0.1",
+                            "eth_src": FAUCET_MAC,
+                            "eth_dst": self.P1_V100_MAC,
+                        }
                         self.verify_pkt(pkt, exp_pkt)
 
         def test_arp_reply_from_host(self):
             """ARP reply for host."""
-            arp_replies = self.rcv_packet(1, 0x100, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': FAUCET_MAC,
-                'arp_code': arp.ARP_REPLY,
-                'arp_source_ip': '10.0.0.1',
-                'arp_target_ip': '10.0.0.254'})[self.DP_ID]
+            arp_replies = self.rcv_packet(
+                1,
+                0x100,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": FAUCET_MAC,
+                    "arp_code": arp.ARP_REPLY,
+                    "arp_source_ip": "10.0.0.1",
+                    "arp_target_ip": "10.0.0.254",
+                },
+            )[self.DP_ID]
             self.assertTrue(arp_replies)
             self.assertFalse(ValveTestBases.packet_outs_from_flows(arp_replies))
 
         def test_nd_for_controller(self):
             """IPv6 ND for controller VIP."""
             for dst_ip in (
-                    ipaddress.IPv6Address('fe80::1:254'),
-                    ipaddress.IPv6Address('fc00::1:254')):
+                ipaddress.IPv6Address("fe80::1:254"),
+                ipaddress.IPv6Address("fc00::1:254"),
+            ):
                 nd_mac = valve_packet.ipv6_link_eth_mcast(dst_ip)
                 ip_gw_mcast = valve_packet.ipv6_solicited_node_from_ucast(dst_ip)
                 for _retries in range(3):
-                    nd_replies = self.rcv_packet(2, 0x200, {
-                        'eth_src': self.P2_V200_MAC,
-                        'eth_dst': nd_mac,
-                        'vid': 0x200,
-                        'ipv6_src': 'fc00::1:1',
-                        'ipv6_dst': str(ip_gw_mcast),
-                        'neighbor_solicit_ip': str(dst_ip)})[self.DP_ID]
+                    nd_replies = self.rcv_packet(
+                        2,
+                        0x200,
+                        {
+                            "eth_src": self.P2_V200_MAC,
+                            "eth_dst": nd_mac,
+                            "vid": 0x200,
+                            "ipv6_src": "fc00::1:1",
+                            "ipv6_dst": str(ip_gw_mcast),
+                            "neighbor_solicit_ip": str(dst_ip),
+                        },
+                    )[self.DP_ID]
                     packet_outs = ValveTestBases.packet_outs_from_flows(nd_replies)
                     self.assertTrue(packet_outs)
                     for nd_pktout in packet_outs:
                         pkt = packet.Packet(nd_pktout.data)
                         exp_pkt = {
-                            'eth_src': FAUCET_MAC,
-                            'eth_dst': self.P2_V200_MAC,
-                            'ipv6_src': str(dst_ip),
-                            'ipv6_dst': 'fc00::1:1',
-                            'neighbor_advert_ip': str(dst_ip)}
+                            "eth_src": FAUCET_MAC,
+                            "eth_dst": self.P2_V200_MAC,
+                            "ipv6_src": str(dst_ip),
+                            "ipv6_dst": "fc00::1:1",
+                            "neighbor_advert_ip": str(dst_ip),
+                        }
                         self.verify_pkt(pkt, exp_pkt)
 
         def test_nd_from_host(self):
             """IPv6 NA from host."""
-            na_replies = self.rcv_packet(2, 0x200, {
-                'eth_src': self.P2_V200_MAC,
-                'eth_dst': FAUCET_MAC,
-                'vid': 0x200,
-                'ipv6_src': 'fc00::1:1',
-                'ipv6_dst': 'fc00::1:254',
-                'neighbor_advert_ip': 'fc00::1:1'})[self.DP_ID]
+            na_replies = self.rcv_packet(
+                2,
+                0x200,
+                {
+                    "eth_src": self.P2_V200_MAC,
+                    "eth_dst": FAUCET_MAC,
+                    "vid": 0x200,
+                    "ipv6_src": "fc00::1:1",
+                    "ipv6_dst": "fc00::1:254",
+                    "neighbor_advert_ip": "fc00::1:1",
+                },
+            )[self.DP_ID]
             self.assertTrue(na_replies)
             self.assertFalse(ValveTestBases.packet_outs_from_flows(na_replies))
 
         def test_ra_for_controller(self):
             """IPv6 RA for controller."""
-            router_solicit_ip = 'ff02::2'
-            ra_replies = self.rcv_packet(2, 0x200, {
-                'eth_src': self.P2_V200_MAC,
-                'eth_dst': '33:33:00:00:00:02',
-                'vid': 0x200,
-                'ipv6_src': 'fe80::1:1',
-                'ipv6_dst': router_solicit_ip,
-                'router_solicit_ip': router_solicit_ip})[self.DP_ID]
+            router_solicit_ip = "ff02::2"
+            ra_replies = self.rcv_packet(
+                2,
+                0x200,
+                {
+                    "eth_src": self.P2_V200_MAC,
+                    "eth_dst": "33:33:00:00:00:02",
+                    "vid": 0x200,
+                    "ipv6_src": "fe80::1:1",
+                    "ipv6_dst": router_solicit_ip,
+                    "router_solicit_ip": router_solicit_ip,
+                },
+            )[self.DP_ID]
             packet_outs = ValveTestBases.packet_outs_from_flows(ra_replies)
             self.assertTrue(packet_outs)
             for ra_pktout in packet_outs:
                 pkt = packet.Packet(ra_pktout.data)
                 exp_pkt = {
-                    'ipv6_src': 'fe80::1:254',
-                    'ipv6_dst': 'fe80::1:1',
-                    'eth_src': FAUCET_MAC,
-                    'eth_dst': self.P2_V200_MAC,
-                    'router_advert_ip': 'fc00::1:0'}
+                    "ipv6_src": "fe80::1:254",
+                    "ipv6_dst": "fe80::1:1",
+                    "eth_src": FAUCET_MAC,
+                    "eth_dst": self.P2_V200_MAC,
+                    "router_advert_ip": "fc00::1:0",
+                }
                 self.verify_pkt(pkt, exp_pkt)
 
         def test_icmp_ping_controller(self):
             """IPv4 ping controller VIP."""
-            echo_replies = self.rcv_packet(1, 0x100, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': FAUCET_MAC,
-                'vid': 0x100,
-                'ipv4_src': '10.0.0.1',
-                'ipv4_dst': '10.0.0.254',
-                'echo_request_data': self.ICMP_PAYLOAD})[self.DP_ID]
+            echo_replies = self.rcv_packet(
+                1,
+                0x100,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": FAUCET_MAC,
+                    "vid": 0x100,
+                    "ipv4_src": "10.0.0.1",
+                    "ipv4_dst": "10.0.0.254",
+                    "echo_request_data": self.ICMP_PAYLOAD,
+                },
+            )[self.DP_ID]
             packet_outs = ValveTestBases.packet_outs_from_flows(echo_replies)
             self.assertTrue(packet_outs)
             data = packet_outs[0].data
             self.assertTrue(data.endswith(self.ICMP_PAYLOAD), msg=data)
 
         def test_unresolved_route(self):
             """Test unresolved route tries to resolve."""
-            ip_dst = ipaddress.IPv4Network('10.100.100.0/24')
-            ip_gw = ipaddress.IPv4Address('10.0.0.1')
+            ip_dst = ipaddress.IPv4Network("10.100.100.0/24")
+            ip_gw = ipaddress.IPv4Address("10.0.0.1")
             valve = self.valves_manager.valves[self.DP_ID]
             valve_vlan = valve.dp.vlans[0x100]
-            route_add_replies = valve.add_route(
-                valve_vlan, ip_gw, ip_dst)
+            route_add_replies = valve.add_route(valve_vlan, ip_gw, ip_dst)
             self.assertFalse(route_add_replies)
-            resolve_replies = valve.resolve_gateways(
-                self.mock_time(10), None)
+            resolve_replies = valve.resolve_gateways(self.mock_time(10), None)
             self.assertFalse(resolve_replies)
-            resolve_replies = valve.resolve_gateways(
-                self.mock_time(99), None)
+            resolve_replies = valve.resolve_gateways(self.mock_time(99), None)
             self.assertTrue(resolve_replies)
 
         def test_add_del_route(self):
             """IPv4 add/del of a route."""
-            arp_replies = self.rcv_packet(1, 0x100, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': mac.BROADCAST_STR,
-                'arp_code': arp.ARP_REQUEST,
-                'arp_source_ip': '10.0.0.1',
-                'arp_target_ip': '10.0.0.254'})[self.DP_ID]
+            arp_replies = self.rcv_packet(
+                1,
+                0x100,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": mac.BROADCAST_STR,
+                    "arp_code": arp.ARP_REQUEST,
+                    "arp_source_ip": "10.0.0.1",
+                    "arp_target_ip": "10.0.0.254",
+                },
+            )[self.DP_ID]
             pkt_outs = ValveTestBases.packet_outs_from_flows(arp_replies)
             self.assertTrue(pkt_outs)
             for arp_pktout in pkt_outs:
                 pkt = packet.Packet(arp_pktout.data)
                 exp_pkt = {
-                    'arp_source_ip': '10.0.0.254',
-                    'arp_target_ip': '10.0.0.1',
-                    'eth_src': FAUCET_MAC,
-                    'eth_dst': self.P1_V100_MAC}
+                    "arp_source_ip": "10.0.0.254",
+                    "arp_target_ip": "10.0.0.1",
+                    "eth_src": FAUCET_MAC,
+                    "eth_dst": self.P1_V100_MAC,
+                }
                 self.verify_pkt(pkt, exp_pkt)
-            ip_gw = ipaddress.IPv4Address('10.0.0.1')
-            ip_dst = ipaddress.IPv4Network('10.100.100.0/24')
+            ip_gw = ipaddress.IPv4Address("10.0.0.1")
+            ip_dst = ipaddress.IPv4Network("10.100.100.0/24")
             self.verify_route_add_del(self.DP_ID, 0x100, ip_gw, ip_dst)
 
         def test_host_ipv4_fib_route(self):
             """Test learning a FIB rule for an IPv4 host."""
-            fib_route_replies = self.rcv_packet(1, 0x100, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': self.UNKNOWN_MAC,
-                'vid': 0x100,
-                'ipv4_src': '10.0.0.2',
-                'ipv4_dst': '10.0.0.4',
-                'echo_request_data': bytes(
-                    'A' * 8, encoding='UTF-8')})  # pytype: disable=wrong-keyword-args
+            fib_route_replies = self.rcv_packet(
+                1,
+                0x100,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": self.UNKNOWN_MAC,
+                    "vid": 0x100,
+                    "ipv4_src": "10.0.0.2",
+                    "ipv4_dst": "10.0.0.4",
+                    "echo_request_data": bytes("A" * 8, encoding="UTF-8"),
+                },
+            )  # pytype: disable=wrong-keyword-args
             fib_route_replies = fib_route_replies[self.DP_ID]
             self.assertTrue(fib_route_replies)
             self.assertFalse(ValveTestBases.packet_outs_from_flows(fib_route_replies))
             valve = self.valves_manager.valves[self.DP_ID]
             route_add_replies = valve.add_route(
                 valve.dp.vlans[0x100],
-                ipaddress.IPv4Address('10.0.0.2'),
-                ipaddress.IPv4Network('0.0.0.0/0'))
+                ipaddress.IPv4Address("10.0.0.2"),
+                ipaddress.IPv4Network("0.0.0.0/0"),
+            )
             self.assertTrue(route_add_replies)
             self.verify_expiry()
 
         def test_host_ipv6_fib_route(self):
             """Test learning a FIB rule for an IPv6 host."""
-            fib_route_replies = self.rcv_packet(2, 0x200, {
-                'eth_src': self.P2_V200_MAC,
-                'eth_dst': self.UNKNOWN_MAC,
-                'vid': 0x200,
-                'ipv6_src': 'fc00::1:2',
-                'ipv6_dst': 'fc00::1:4',
-                'echo_request_data': self.ICMP_PAYLOAD})[self.DP_ID]
+            fib_route_replies = self.rcv_packet(
+                2,
+                0x200,
+                {
+                    "eth_src": self.P2_V200_MAC,
+                    "eth_dst": self.UNKNOWN_MAC,
+                    "vid": 0x200,
+                    "ipv6_src": "fc00::1:2",
+                    "ipv6_dst": "fc00::1:4",
+                    "echo_request_data": self.ICMP_PAYLOAD,
+                },
+            )[self.DP_ID]
             # We want to know this host was learned we did not get packet outs.
             self.assertTrue(fib_route_replies)
             self.assertFalse(ValveTestBases.packet_outs_from_flows(fib_route_replies))
             self.verify_expiry()
 
         def test_ping_unknown_neighbor(self):
             """IPv4 ping unknown host on same subnet, causing proactive learning."""
-            echo_replies = self.rcv_packet(1, 0x100, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': FAUCET_MAC,
-                'vid': 0x100,
-                'ipv4_src': '10.0.0.1',
-                'ipv4_dst': '10.0.0.99',
-                'echo_request_data': self.ICMP_PAYLOAD})[self.DP_ID]
+            echo_replies = self.rcv_packet(
+                1,
+                0x100,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": FAUCET_MAC,
+                    "vid": 0x100,
+                    "ipv4_src": "10.0.0.1",
+                    "ipv4_dst": "10.0.0.99",
+                    "echo_request_data": self.ICMP_PAYLOAD,
+                },
+            )[self.DP_ID]
             self.assertTrue(echo_replies)
             out_pkts = ValveTestBases.packet_outs_from_flows(echo_replies)
             self.assertTrue(out_pkts)
             for out_pkt in out_pkts:
                 pkt = packet.Packet(out_pkt.data)
                 exp_pkt = {
-                    'arp_source_ip': '10.0.0.254',
-                    'arp_target_ip': '10.0.0.99',
-                    'opcode': 1,
-                    'eth_src': FAUCET_MAC,
-                    'eth_dst': self.BROADCAST_MAC
+                    "arp_source_ip": "10.0.0.254",
+                    "arp_target_ip": "10.0.0.99",
+                    "opcode": 1,
+                    "eth_src": FAUCET_MAC,
+                    "eth_dst": self.BROADCAST_MAC,
                 }
                 self.verify_pkt(pkt, exp_pkt)
 
         def test_ping6_unknown_neighbor(self):
             """IPv6 ping unknown host on same subnet, causing proactive learning."""
-            echo_replies = self.rcv_packet(2, 0x200, {
-                'eth_src': self.P2_V200_MAC,
-                'eth_dst': FAUCET_MAC,
-                'vid': 0x200,
-                'ipv6_src': 'fc00::1:2',
-                'ipv6_dst': 'fc00::1:4',
-                'echo_request_data': self.ICMP_PAYLOAD})[self.DP_ID]
+            echo_replies = self.rcv_packet(
+                2,
+                0x200,
+                {
+                    "eth_src": self.P2_V200_MAC,
+                    "eth_dst": FAUCET_MAC,
+                    "vid": 0x200,
+                    "ipv6_src": "fc00::1:2",
+                    "ipv6_dst": "fc00::1:4",
+                    "echo_request_data": self.ICMP_PAYLOAD,
+                },
+            )[self.DP_ID]
             self.assertTrue(echo_replies)
             out_pkts = ValveTestBases.packet_outs_from_flows(echo_replies)
             self.assertTrue(out_pkts)
             for out_pkt in out_pkts:
                 pkt = packet.Packet(out_pkt.data)
                 exp_pkt = {
-                    'ipv6_src': 'fc00::1:254',
-                    'ipv6_dst': 'ff02::1:ff01:4',
-                    'neighbor_solicit_ip': 'fc00::1:4',
-                    'eth_src': FAUCET_MAC,
-                    'eth_dst': '33:33:ff:01:00:04'
+                    "ipv6_src": "fc00::1:254",
+                    "ipv6_dst": "ff02::1:ff01:4",
+                    "neighbor_solicit_ip": "fc00::1:4",
+                    "eth_src": FAUCET_MAC,
+                    "eth_dst": "33:33:ff:01:00:04",
                 }
                 self.verify_pkt(pkt, exp_pkt)
 
         def test_icmpv6_ping_controller(self):
             """IPv6 ping controller VIP."""
-            echo_replies = self.rcv_packet(2, 0x200, {
-                'eth_src': self.P2_V200_MAC,
-                'eth_dst': FAUCET_MAC,
-                'vid': 0x200,
-                'ipv6_src': 'fc00::1:1',
-                'ipv6_dst': 'fc00::1:254',
-                'echo_request_data': self.ICMP_PAYLOAD})[self.DP_ID]
+            echo_replies = self.rcv_packet(
+                2,
+                0x200,
+                {
+                    "eth_src": self.P2_V200_MAC,
+                    "eth_dst": FAUCET_MAC,
+                    "vid": 0x200,
+                    "ipv6_src": "fc00::1:1",
+                    "ipv6_dst": "fc00::1:254",
+                    "echo_request_data": self.ICMP_PAYLOAD,
+                },
+            )[self.DP_ID]
             packet_outs = ValveTestBases.packet_outs_from_flows(echo_replies)
             self.assertTrue(packet_outs)
             data = packet_outs[0].data
             self.assertTrue(data.endswith(self.ICMP_PAYLOAD), msg=data)
 
         def test_invalid_vlan(self):
             """Test that packets with incorrect vlan tagging get dropped."""
 
             matches = [
-                {'in_port': 1, 'vlan_vid': 18 | ofp.OFPVID_PRESENT},
-                {'in_port': 1, 'vlan_vid': self.V100},
-                {'in_port': 3, 'vlan_vid': 0}]
+                {"in_port": 1, "vlan_vid": 18 | ofp.OFPVID_PRESENT},
+                {"in_port": 1, "vlan_vid": self.V100},
+                {"in_port": 3, "vlan_vid": 0},
+            ]
             for match in matches:
                 self.assertFalse(
                     self.network.tables[self.DP_ID].is_output(match),
-                    msg='Packets with incorrect vlan tags are output')
+                    msg="Packets with incorrect vlan tags are output",
+                )
 
         def test_unknown_eth_src(self):
             """Test that packets from unknown macs are sent to controller.
 
             Untagged packets should have VLAN tags pushed before they are sent to
             the controller.
             """
             valve = self.valves_manager.valves[self.DP_ID]
             matches = [
-                {'in_port': 1, 'vlan_vid': 0},
-                {'in_port': 1, 'vlan_vid': 0, 'eth_src': self.UNKNOWN_MAC},
-                {
-                    'in_port': 1,
-                    'vlan_vid': 0,
-                    'eth_src': self.P2_V200_MAC
-                },
-                {'in_port': 2, 'vlan_vid': 0, 'eth_dst': self.UNKNOWN_MAC},
-                {'in_port': 2, 'vlan_vid': 0},
-                {
-                    'in_port': 2,
-                    'vlan_vid': self.V100,
-                    'eth_src': self.P2_V200_MAC
-                },
+                {"in_port": 1, "vlan_vid": 0},
+                {"in_port": 1, "vlan_vid": 0, "eth_src": self.UNKNOWN_MAC},
+                {"in_port": 1, "vlan_vid": 0, "eth_src": self.P2_V200_MAC},
+                {"in_port": 2, "vlan_vid": 0, "eth_dst": self.UNKNOWN_MAC},
+                {"in_port": 2, "vlan_vid": 0},
+                {"in_port": 2, "vlan_vid": self.V100, "eth_src": self.P2_V200_MAC},
                 {
-                    'in_port': 2,
-                    'vlan_vid': self.V100,
-                    'eth_src': self.UNKNOWN_MAC,
-                    'eth_dst': self.P1_V100_MAC
+                    "in_port": 2,
+                    "vlan_vid": self.V100,
+                    "eth_src": self.UNKNOWN_MAC,
+                    "eth_dst": self.P1_V100_MAC,
                 },
             ]
             for match in matches:
-                if match['vlan_vid'] != 0:
-                    vid = match['vlan_vid']
+                if match["vlan_vid"] != 0:
+                    vid = match["vlan_vid"]
                 else:
-                    vid = valve.dp.get_native_vlan(match['in_port']).vid
+                    vid = valve.dp.get_native_vlan(match["in_port"]).vid
                     vid = vid | ofp.OFPVID_PRESENT
                 self.assertTrue(
-                    self.network.tables[self.DP_ID].is_output(match, ofp.OFPP_CONTROLLER, vid=vid),
-                    msg=f"Packet with unknown ethernet src not sent to controller: {match}")
+                    self.network.tables[self.DP_ID].is_output(
+                        match, ofp.OFPP_CONTROLLER, vid=vid
+                    ),
+                    msg="Packet with unknown ethernet src not sent to controller: "
+                    "{0}".format(match),
+                )
 
         def test_unknown_eth_dst_rule(self):
             """Test that packets with unkown eth dst addrs get flooded correctly.
 
             They must be output to each port on the associated vlan, with the
             correct vlan tagging. And they must not be forwarded to a port not
             on the associated vlan
             """
             self.learn_hosts()
             matches = [
                 {
-                    'in_port': 3,
-                    'vlan_vid': self.V100,
-                },
-                {
-                    'in_port': 2,
-                    'vlan_vid': 0,
-                    'eth_dst': self.P1_V100_MAC
+                    "in_port": 3,
+                    "vlan_vid": self.V100,
                 },
+                {"in_port": 2, "vlan_vid": 0, "eth_dst": self.P1_V100_MAC},
+                {"in_port": 1, "vlan_vid": 0, "eth_src": self.P1_V100_MAC},
                 {
-                    'in_port': 1,
-                    'vlan_vid': 0,
-                    'eth_src': self.P1_V100_MAC
+                    "in_port": 3,
+                    "vlan_vid": self.V200,
+                    "eth_src": self.P2_V200_MAC,
                 },
-                {
-                    'in_port': 3,
-                    'vlan_vid': self.V200,
-                    'eth_src': self.P2_V200_MAC,
-                }
             ]
             self.verify_flooding(matches)
 
         def test_known_eth_src_rule(self):
             """Test that packets with known eth src addrs are not sent to controller."""
             self.learn_hosts()
             matches = [
+                {"in_port": 1, "vlan_vid": 0, "eth_src": self.P1_V100_MAC},
+                {"in_port": 2, "vlan_vid": self.V200, "eth_src": self.P2_V200_MAC},
                 {
-                    'in_port': 1,
-                    'vlan_vid': 0,
-                    'eth_src': self.P1_V100_MAC
+                    "in_port": 3,
+                    "vlan_vid": self.V200,
+                    "eth_src": self.P3_V200_MAC,
+                    "eth_dst": self.P2_V200_MAC,
                 },
-                {
-                    'in_port': 2,
-                    'vlan_vid': self.V200,
-                    'eth_src': self.P2_V200_MAC
-                },
-                {
-                    'in_port': 3,
-                    'vlan_vid': self.V200,
-                    'eth_src': self.P3_V200_MAC,
-                    'eth_dst': self.P2_V200_MAC
-                }
             ]
             for match in matches:
                 self.assertFalse(
-                    self.network.tables[self.DP_ID].is_output(match, port=ofp.OFPP_CONTROLLER),
-                    msg=f"Packet ({match}) output to controller when eth_src address is known")
+                    self.network.tables[self.DP_ID].is_output(
+                        match, port=ofp.OFPP_CONTROLLER
+                    ),
+                    msg="Packet ({0}) output to controller when eth_src address"
+                    " is known".format(match),
+                )
 
         def test_known_eth_src_deletion(self):
             """Verify that when a mac changes port the old rules get deleted.
 
             If a mac address is seen on one port, then seen on a different port on
             the same vlan the rules associated with that mac address on previous
             port need to be deleted. IE packets with that mac address arriving on
             the old port should be output to the controller."""
 
-            self.rcv_packet(3, 0x200, {
-                'eth_src': self.P2_V200_MAC,
-                'eth_dst': self.UNKNOWN_MAC,
-                'vlan_vid': 0x200,
-                'ipv4_src': '10.0.0.3',
-                'ipv4_dst': '10.0.0.3'})
-            match = {'in_port': 2, 'vlan_vid': 0, 'eth_src': self.P2_V200_MAC}
+            self.rcv_packet(
+                3,
+                0x200,
+                {
+                    "eth_src": self.P2_V200_MAC,
+                    "eth_dst": self.UNKNOWN_MAC,
+                    "vlan_vid": 0x200,
+                    "ipv4_src": "10.0.0.3",
+                    "ipv4_dst": "10.0.0.3",
+                },
+            )
+            match = {"in_port": 2, "vlan_vid": 0, "eth_src": self.P2_V200_MAC}
             self.assertTrue(
-                self.network.tables[self.DP_ID].is_output(match, port=ofp.OFPP_CONTROLLER),
-                msg='eth src rule not deleted when mac seen on another port')
+                self.network.tables[self.DP_ID].is_output(
+                    match, port=ofp.OFPP_CONTROLLER
+                ),
+                msg="eth src rule not deleted when mac seen on another port",
+            )
 
         def test_known_eth_dst_rule(self):
             """Test that packets with known eth dst addrs are output correctly.
 
             Output to the correct port with the correct vlan tagging."""
             self.learn_hosts()
             match_results = [
-                ({
-                    'in_port': 2,
-                    'vlan_vid': self.V100,
-                    'eth_dst': self.P1_V100_MAC
-                }, {
-                    'out_port': 1,
-                    'vlan_vid': 0
-                }),
-                ({
-                    'in_port': 3,
-                    'vlan_vid': self.V200,
-                    'eth_dst': self.P2_V200_MAC,
-                    'eth_src': self.P3_V200_MAC
-                }, {
-                    'out_port': 2,
-                    'vlan_vid': 0,
-                })
+                (
+                    {"in_port": 2, "vlan_vid": self.V100, "eth_dst": self.P1_V100_MAC},
+                    {"out_port": 1, "vlan_vid": 0},
+                ),
+                (
+                    {
+                        "in_port": 3,
+                        "vlan_vid": self.V200,
+                        "eth_dst": self.P2_V200_MAC,
+                        "eth_src": self.P3_V200_MAC,
+                    },
+                    {
+                        "out_port": 2,
+                        "vlan_vid": 0,
+                    },
+                ),
             ]
             for match, result in match_results:
                 self.assertTrue(
                     self.network.tables[self.DP_ID].is_output(
-                        match, result['out_port'], vid=result['vlan_vid']),
-                    msg='packet not output to port correctly when eth dst is known')
+                        match, result["out_port"], vid=result["vlan_vid"]
+                    ),
+                    msg="packet not output to port correctly when eth dst is known",
+                )
                 incorrect_ports = set(range(1, self.NUM_PORTS + 1))
-                incorrect_ports.remove(result['out_port'])
+                incorrect_ports.remove(result["out_port"])
                 for port in incorrect_ports:
                     self.assertFalse(
                         self.network.tables[self.DP_ID].is_output(match, port=port),
-                        msg=f'packet {match} output to incorrect port {port} when eth_dst is known')
+                        msg=(
+                            "packet %s output to incorrect port %u when eth_dst "
+                            "is known" % (match, port)
+                        ),
+                    )
             self.verify_expiry()
 
         def test_mac_vlan_separation(self):
             """Test that when a mac is seen on a second vlan the original vlan
             rules are unaffected."""
             self.learn_hosts()
-            self.rcv_packet(2, 0x200, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': self.UNKNOWN_MAC,
-                'vlan_vid': 0x200,
-                'ipv4_src': '10.0.0.2',
-                'ipv4_dst': '10.0.0.3'})
+            self.rcv_packet(
+                2,
+                0x200,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": self.UNKNOWN_MAC,
+                    "vlan_vid": 0x200,
+                    "ipv4_src": "10.0.0.2",
+                    "ipv4_dst": "10.0.0.3",
+                },
+            )
 
             # check eth_src rule
-            match1 = {'in_port': 1, 'vlan_vid': 0, 'eth_src': self.P1_V100_MAC}
+            match1 = {"in_port": 1, "vlan_vid": 0, "eth_src": self.P1_V100_MAC}
             self.assertFalse(
                 self.network.tables[self.DP_ID].is_output(match1, ofp.OFPP_CONTROLLER),
-                msg=('mac address being seen on a vlan affects eth_src rule on '
-                     'other vlan'))
+                msg=(
+                    "mac address being seen on a vlan affects eth_src rule on "
+                    "other vlan"
+                ),
+            )
 
             # check eth_dst rule
-            match2 = {'in_port': 3, 'vlan_vid': self.V100, 'eth_dst': self.P1_V100_MAC}
+            match2 = {"in_port": 3, "vlan_vid": self.V100, "eth_dst": self.P1_V100_MAC}
             self.assertTrue(
                 self.network.tables[self.DP_ID].is_output(match2, port=1, vid=0),
-                msg=('mac address being seen on a vlan affects eth_dst rule on '
-                     'other vlan'))
+                msg=(
+                    "mac address being seen on a vlan affects eth_dst rule on "
+                    "other vlan"
+                ),
+            )
             for port in (2, 4):
                 self.assertFalse(
                     self.network.tables[self.DP_ID].is_output(match2, port=port),
-                    msg=('mac address being seen on a vlan affects eth_dst rule on '
-                         'other vlan'))
+                    msg=(
+                        "mac address being seen on a vlan affects eth_dst rule on "
+                        "other vlan"
+                    ),
+                )
 
         def test_known_eth_dst_deletion(self):
             """Test that eth_dst rules are deleted when the mac is learned on
             another port.
 
             This should only occur when the mac is seen on the same vlan."""
-            self.rcv_packet(2, 0x100, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': self.UNKNOWN_MAC,
-                'ipv4_src': '10.0.0.2',
-                'ipv4_dst': '10.0.0.3'})
-            match = {'in_port': 3, 'vlan_vid': self.V100, 'eth_dst': self.P1_V100_MAC}
+            self.rcv_packet(
+                2,
+                0x100,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": self.UNKNOWN_MAC,
+                    "ipv4_src": "10.0.0.2",
+                    "ipv4_dst": "10.0.0.3",
+                },
+            )
+            match = {"in_port": 3, "vlan_vid": self.V100, "eth_dst": self.P1_V100_MAC}
             self.assertTrue(
                 self.network.tables[self.DP_ID].is_output(match, port=2, vid=self.V100),
-                msg='Packet not output correctly after mac is learnt on new port')
+                msg="Packet not output correctly after mac is learnt on new port",
+            )
             self.assertFalse(
                 self.network.tables[self.DP_ID].is_output(match, port=1),
-                msg='Packet output on old port after mac is learnt on new port')
+                msg="Packet output on old port after mac is learnt on new port",
+            )
 
         def test_port_delete_eth_dst(self):
-            """Test that when a port is disabled packets are correctly output. """
+            """Test that when a port is disabled packets are correctly output."""
             valve = self.valves_manager.valves[self.DP_ID]
-            match = {'in_port': 2, 'vlan_vid': self.V100, 'eth_dst': self.P1_V100_MAC}
+            match = {"in_port": 2, "vlan_vid": self.V100, "eth_dst": self.P1_V100_MAC}
 
-            valve_vlan = valve.dp.vlans[match['vlan_vid'] & ~ofp.OFPVID_PRESENT]
+            valve_vlan = valve.dp.vlans[match["vlan_vid"] & ~ofp.OFPVID_PRESENT]
             ofmsgs = valve.port_delete(port_num=1)
             self.apply_ofmsgs(ofmsgs)
 
             # Check packets are output to each port on vlan
             for port in valve_vlan.get_ports():
-                if port.number != match['in_port'] and port.running():
+                if port.number != match["in_port"] and port.running():
                     if valve_vlan.port_is_tagged(port):
                         vid = valve_vlan.vid | ofp.OFPVID_PRESENT
                     else:
                         vid = 0
                     self.assertTrue(
-                        self.network.tables[self.DP_ID].is_output(match, port=port.number, vid=vid),
-                        msg=(f'packet {match} with eth dst learnt on deleted port not output '
-                             'correctly on vlan {valve_vlan.vid} to port {port.number}'))
+                        self.network.tables[self.DP_ID].is_output(
+                            match, port=port.number, vid=vid
+                        ),
+                        msg=(
+                            "packet %s with eth dst learnt on deleted port not output "
+                            "correctly on vlan %u to port %u"
+                            % (match, valve_vlan.vid, port.number)
+                        ),
+                    )
 
         def test_port_down_eth_src_removal(self):
             """Test that when a port goes down and comes back up learnt mac
             addresses are deleted."""
 
-            match = {'in_port': 1, 'vlan_vid': 0, 'eth_src': self.P1_V100_MAC}
+            match = {"in_port": 1, "vlan_vid": 0, "eth_src": self.P1_V100_MAC}
             self.flap_port(1)
             self.assertTrue(
-                self.network.tables[self.DP_ID].is_output(match, port=ofp.OFPP_CONTROLLER),
-                msg='Packet not output to controller after port bounce')
+                self.network.tables[self.DP_ID].is_output(
+                    match, port=ofp.OFPP_CONTROLLER
+                ),
+                msg="Packet not output to controller after port bounce",
+            )
 
         def test_port_add_input(self):
             """Test that when a port is enabled packets are input correctly."""
             _ = self.valves_manager.valves[self.DP_ID]
 
-            match = {'in_port': 1, 'vlan_vid': 0}
+            match = {"in_port": 1, "vlan_vid": 0}
             orig_config = yaml_load(self.CONFIG)
             deletedport1_config = copy.copy(orig_config)
-            del deletedport1_config['dps'][self.DP_NAME]['interfaces']['p1']
+            del deletedport1_config["dps"][self.DP_NAME]["interfaces"]["p1"]
             self.update_config(yaml_dump(deletedport1_config))
             self.assertFalse(
                 self.network.tables[self.DP_ID].is_output(match, port=2, vid=self.V100),
-                msg='Packet output after port delete')
+                msg="Packet output after port delete",
+            )
 
             self.update_config(self.CONFIG)
             self.assertTrue(
                 self.network.tables[self.DP_ID].is_output(match, port=2, vid=self.V100),
-                msg='Packet not output after port add')
+                msg="Packet not output after port add",
+            )
 
         def test_dp_acl_deny(self):
             """Test DP acl denies forwarding"""
-            acl_config = """
+            acl_config = (
+                """
 dps:
     s1:
         dp_acls: [drop_non_ospf_ipv4]
 %s
         interfaces:
             p2:
                 number: 2
@@ -2181,38 +2539,47 @@
             bands:
                 [
                     {
                         type: "DROP",
                         rate: 1
                     }
                 ]
-""" % DP1_CONFIG
+"""
+                % DP1_CONFIG
+            )
 
             drop_match = {
-                'in_port': 2,
-                'vlan_vid': 0,
-                'eth_type': 0x800,
-                'ipv4_dst': '192.0.2.1'}
+                "in_port": 2,
+                "vlan_vid": 0,
+                "eth_type": 0x800,
+                "ipv4_dst": "192.0.2.1",
+            }
             accept_match = {
-                'in_port': 2,
-                'vlan_vid': 0,
-                'eth_type': 0x800,
-                'ipv4_dst': '224.0.0.5'}
+                "in_port": 2,
+                "vlan_vid": 0,
+                "eth_type": 0x800,
+                "ipv4_dst": "224.0.0.5",
+            }
             self.update_config(acl_config)
             self.flap_port(2)
             self.assertFalse(
                 self.network.tables[self.DP_ID].is_output(drop_match),
-                msg='packet not blocked by ACL')
+                msg="packet not blocked by ACL",
+            )
             self.assertTrue(
-                self.network.tables[self.DP_ID].is_output(accept_match, port=3, vid=self.V200),
-                msg='packet not allowed by ACL')
+                self.network.tables[self.DP_ID].is_output(
+                    accept_match, port=3, vid=self.V200
+                ),
+                msg="packet not allowed by ACL",
+            )
 
         def test_dp_acl_deny_ordered(self):
             """Test DP acl denies forwarding"""
-            acl_config = """
+            acl_config = (
+                """
 dps:
     s1:
         dp_acls: [drop_non_ospf_ipv4]
 %s
         interfaces:
             p2:
                 number: 2
@@ -2246,38 +2613,47 @@
             bands:
                 [
                     {
                         type: "DROP",
                         rate: 1
                     }
                 ]
-""" % DP1_CONFIG
+"""
+                % DP1_CONFIG
+            )
 
             drop_match = {
-                'in_port': 2,
-                'vlan_vid': 0,
-                'eth_type': 0x800,
-                'ipv4_dst': '192.0.2.1'}
+                "in_port": 2,
+                "vlan_vid": 0,
+                "eth_type": 0x800,
+                "ipv4_dst": "192.0.2.1",
+            }
             accept_match = {
-                'in_port': 2,
-                'vlan_vid': 0,
-                'eth_type': 0x800,
-                'ipv4_dst': '224.0.0.5'}
+                "in_port": 2,
+                "vlan_vid": 0,
+                "eth_type": 0x800,
+                "ipv4_dst": "224.0.0.5",
+            }
             self.update_config(acl_config)
             self.flap_port(2)
             self.assertFalse(
                 self.network.tables[self.DP_ID].is_output(drop_match),
-                msg='packet not blocked by ACL')
+                msg="packet not blocked by ACL",
+            )
             self.assertTrue(
-                self.network.tables[self.DP_ID].is_output(accept_match, port=3, vid=self.V200),
-                msg='packet not allowed by ACL')
+                self.network.tables[self.DP_ID].is_output(
+                    accept_match, port=3, vid=self.V200
+                ),
+                msg="packet not allowed by ACL",
+            )
 
         def test_port_acl_deny(self):
             """Test that port ACL denies forwarding."""
-            acl_config = """
+            acl_config = (
+                """
 dps:
     s1:
 %s
         interfaces:
             p2:
                 number: 2
                 native_vlan: v200
@@ -2308,141 +2684,172 @@
             bands:
                 [
                     {
                         type: "DROP",
                         rate: 1
                     }
                 ]
-""" % DP1_CONFIG
+"""
+                % DP1_CONFIG
+            )
 
             drop_match = {
-                'in_port': 2,
-                'vlan_vid': 0,
-                'eth_type': 0x800,
-                'ipv4_dst': '192.0.2.1'}
+                "in_port": 2,
+                "vlan_vid": 0,
+                "eth_type": 0x800,
+                "ipv4_dst": "192.0.2.1",
+            }
             accept_match = {
-                'in_port': 2,
-                'vlan_vid': 0,
-                'eth_type': 0x800,
-                'ipv4_dst': '224.0.0.5'}
+                "in_port": 2,
+                "vlan_vid": 0,
+                "eth_type": 0x800,
+                "ipv4_dst": "224.0.0.5",
+            }
             # base case
             for match in (drop_match, accept_match):
                 self.assertTrue(
-                    self.network.tables[self.DP_ID].is_output(match, port=3, vid=self.V200),
-                    msg='Packet not output before adding ACL')
+                    self.network.tables[self.DP_ID].is_output(
+                        match, port=3, vid=self.V200
+                    ),
+                    msg="Packet not output before adding ACL",
+                )
 
             self.update_config(acl_config)
             self.assertFalse(
                 self.network.tables[self.DP_ID].is_output(drop_match),
-                msg='packet not blocked by ACL')
+                msg="packet not blocked by ACL",
+            )
             self.assertTrue(
-                self.network.tables[self.DP_ID].is_output(accept_match, port=3, vid=self.V200),
-                msg='packet not allowed by ACL')
+                self.network.tables[self.DP_ID].is_output(
+                    accept_match, port=3, vid=self.V200
+                ),
+                msg="packet not allowed by ACL",
+            )
 
         def test_lldp_beacon(self):
             """Test LLDP beacon service."""
             valve = self.valves_manager.valves[self.DP_ID]
             lldp_pkts = valve.fast_advertise(self.mock_time(10), None)
             self.assertTrue(lldp_pkts)
             out_pkts = ValveTestBases.packet_outs_from_flows(lldp_pkts[valve])
             self.assertTrue(out_pkts)
             for out_pkt in out_pkts:
                 pkt = packet.Packet(out_pkt.data)
                 exp_pkt = {
-                    'chassis_id': FAUCET_MAC,
-                    'system_name': 'faucet',
-                    'port_id': None,
-                    'eth_src': FAUCET_MAC,
-                    'eth_dst': lldp.LLDP_MAC_NEAREST_BRIDGE,
-                    'tlvs': None
+                    "chassis_id": FAUCET_MAC,
+                    "system_name": "faucet",
+                    "port_id": None,
+                    "eth_src": FAUCET_MAC,
+                    "eth_dst": lldp.LLDP_MAC_NEAREST_BRIDGE,
+                    "tlvs": None,
                 }
                 self.verify_pkt(pkt, exp_pkt)
 
         def test_unknown_port(self):
             """Test port status change for unknown port handled."""
             self.set_port_up(99)
 
         def test_port_modify(self):
             """Set port status modify."""
             valve = self.valves_manager.valves[self.DP_ID]
             for port_status in (0, 1):
-                self.apply_ofmsgs(valve.port_status_handler(
-                    1, ofp.OFPPR_MODIFY, port_status, [], self.mock_time())[valve])
+                self.apply_ofmsgs(
+                    valve.port_status_handler(
+                        1, ofp.OFPPR_MODIFY, port_status, [], self.mock_time()
+                    )[valve]
+                )
 
         def test_unknown_port_status(self):
             """Test unknown port status message."""
             valve = self.valves_manager.valves[self.DP_ID]
             known_messages = set([ofp.OFPPR_MODIFY, ofp.OFPPR_ADD, ofp.OFPPR_DELETE])
-            unknown_messages = list(set(range(0, len(known_messages) + 1)) - known_messages)
+            unknown_messages = list(
+                set(range(0, len(known_messages) + 1)) - known_messages
+            )
             self.assertTrue(unknown_messages)
-            self.assertFalse(valve.port_status_handler(
-                1, unknown_messages[0], 1, [], self.mock_time()).get(valve, []))
+            self.assertFalse(
+                valve.port_status_handler(
+                    1, unknown_messages[0], 1, [], self.mock_time()
+                ).get(valve, [])
+            )
 
         def test_move_port(self):
             """Test host moves a port."""
-            self.rcv_packet(2, 0x200, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': self.UNKNOWN_MAC,
-                'vlan_vid': 0x200,
-                'ipv4_src': '10.0.0.2',
-                'ipv4_dst': '10.0.0.3'})
-            self.rcv_packet(4, 0x200, {
-                'eth_src': self.P1_V100_MAC,
-                'eth_dst': self.UNKNOWN_MAC,
-                'vlan_vid': 0x200,
-                'ipv4_src': '10.0.0.2',
-                'ipv4_dst': '10.0.0.3'})
+            self.rcv_packet(
+                2,
+                0x200,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": self.UNKNOWN_MAC,
+                    "vlan_vid": 0x200,
+                    "ipv4_src": "10.0.0.2",
+                    "ipv4_dst": "10.0.0.3",
+                },
+            )
+            self.rcv_packet(
+                4,
+                0x200,
+                {
+                    "eth_src": self.P1_V100_MAC,
+                    "eth_dst": self.UNKNOWN_MAC,
+                    "vlan_vid": 0x200,
+                    "ipv4_src": "10.0.0.2",
+                    "ipv4_dst": "10.0.0.3",
+                },
+            )
 
         def test_bgp_route_change(self):
             """Test BGP route change handler."""
-            nexthop = '10.0.0.1'
-            prefix = '192.168.1.1/32'
+            nexthop = "10.0.0.1"
+            prefix = "192.168.1.1/32"
             add_event = RouteAddition(
                 IPPrefix.from_string(prefix),
                 IPAddress.from_string(nexthop),
-                '65001',
-                'IGP'
+                "65001",
+                "IGP",
             )
             del_event = RouteRemoval(
                 IPPrefix.from_string(prefix),
             )
             self.bgp._bgp_route_handler(
-                add_event,
-                faucet_bgp.BgpSpeakerKey(self.DP_ID, 0x100, 4))
+                add_event, faucet_bgp.BgpSpeakerKey(self.DP_ID, 0x100, 4)
+            )
             self.bgp._bgp_route_handler(
-                del_event,
-                faucet_bgp.BgpSpeakerKey(self.DP_ID, 0x100, 4))
+                del_event, faucet_bgp.BgpSpeakerKey(self.DP_ID, 0x100, 4)
+            )
             self.bgp._bgp_up_handler(nexthop, 65001)
             self.bgp._bgp_down_handler(nexthop, 65001)
 
         def test_packet_in_rate(self):
             """Test packet in rate limit triggers."""
             valve = self.valves_manager.valves[self.DP_ID]
             now = self.mock_time(10)
             for _ in range(valve.dp.ignore_learn_ins * 2 + 1):
                 if valve.rate_limit_packet_ins(now):
                     return
-            self.fail('packet in rate limit not triggered')
+            self.fail("packet in rate limit not triggered")
 
         def test_ofdescstats_handler(self):
             """Test OFDescStatsReply handler."""
             valve = self.valves_manager.valves[self.DP_ID]
             body = parser.OFPDescStats(
-                mfr_desc=u'test_mfr_desc'.encode(),
-                hw_desc=u'test_hw_desc'.encode(),
-                sw_desc=u'test_sw_desc'.encode(),
-                serial_num=u'99'.encode(),
-                dp_desc=u'test_dp_desc'.encode())
+                mfr_desc="test_mfr_desc".encode(),
+                hw_desc="test_hw_desc".encode(),
+                sw_desc="test_sw_desc".encode(),
+                serial_num="99".encode(),
+                dp_desc="test_dp_desc".encode(),
+            )
             valve.ofdescstats_handler(body)
             invalid_body = parser.OFPDescStats(
-                mfr_desc=b'\x80',
-                hw_desc=b'test_hw_desc',
-                sw_desc=b'test_sw_desc',
-                serial_num=b'99',
-                dp_desc=b'test_dp_desc')
+                mfr_desc=b"\x80",
+                hw_desc=b"test_hw_desc",
+                sw_desc=b"test_sw_desc",
+                serial_num=b"99",
+                dp_desc=b"test_dp_desc",
+            )
             valve.ofdescstats_handler(invalid_body)
 
         def test_dp_disconnect_cleanup(self):
             """Test port varz cleanup post dp disconnect"""
             valve = self.valves_manager.valves[self.DP_ID]
             port_num = list(valve.dp.ports.keys())[0]
             self.port_expected_status(port_num, 1)
@@ -2457,17 +2864,30 @@
             self.setup_valves(self.CONFIG)
             self.activate_all_ports()
             for valve in self.valves_manager.valves.values():
                 for port in valve.dp.ports.values():
                     if port.stack:
                         self.set_stack_port_up(port.number, valve)
 
-        def validate_tunnel(self, src_dpid, dst_dpid, in_port, in_vid, out_port,
-                            out_vid, expected, msg, pcp=False, packet_match=None,
-                            eth_type=0x0800, ip_proto=1, trace=False):
+        def validate_tunnel(
+            self,
+            src_dpid,
+            dst_dpid,
+            in_port,
+            in_vid,
+            out_port,
+            out_vid,
+            expected,
+            msg,
+            pcp=False,
+            packet_match=None,
+            eth_type=0x0800,
+            ip_proto=1,
+            trace=False,
+        ):
             """
             Validate correct tunnel output by constructing a test packet and
                 inputting it into the network and measure for the state of the
                 packet and where it is output
 
             Args:
                 src_dpid (int): DPID for the source of the test packet
@@ -2482,55 +2902,71 @@
                 packet_match (dict): Additional packet headers
                 eth_type (int): Eth type for the test packet
                 ip_proto (int): IP proto for the test packet
                 trace (bool): Whether to print the trace of the packet through
                     the network (for debugging)
             """
             bcast_match = {
-                'in_port': in_port,
-                'eth_dst': mac.BROADCAST_STR,
-                'eth_type': eth_type,
-                'ip_proto': ip_proto
+                "in_port": in_port,
+                "eth_dst": mac.BROADCAST_STR,
+                "eth_type": eth_type,
+                "ip_proto": ip_proto,
             }
             if in_vid:
                 if isinstance(in_vid, list):
-                    bcast_match['vlan_vid'] = in_vid[0] | ofp.OFPVID_PRESENT
-                    bcast_match['encap_vid'] = in_vid[1] | ofp.OFPVID_PRESENT
+                    bcast_match["vlan_vid"] = in_vid[0] | ofp.OFPVID_PRESENT
+                    bcast_match["encap_vid"] = in_vid[1] | ofp.OFPVID_PRESENT
                 else:
                     in_vid = in_vid | ofp.OFPVID_PRESENT
-                    bcast_match['vlan_vid'] = in_vid
+                    bcast_match["vlan_vid"] = in_vid
             if pcp:
                 bcast_match[valve_of.TUNNEL_INDICATOR_FIELD] = pcp
             if packet_match:
                 for key, value in packet_match.items():
                     bcast_match[key] = value
             if out_vid:
                 if not isinstance(out_vid, list):
                     out_vid = out_vid | ofp.OFPVID_PRESENT
             if expected:
-                self.assertTrue(self.network.is_output(
-                    bcast_match, src_dpid, dst_dpid, port=out_port, vid=out_vid,
-                    trace=trace), msg=msg)
+                self.assertTrue(
+                    self.network.is_output(
+                        bcast_match,
+                        src_dpid,
+                        dst_dpid,
+                        port=out_port,
+                        vid=out_vid,
+                        trace=trace,
+                    ),
+                    msg=msg,
+                )
             else:
-                self.assertFalse(self.network.is_output(
-                    bcast_match, src_dpid, dst_dpid, port=out_port, vid=out_vid,
-                    trace=trace), msg=msg)
+                self.assertFalse(
+                    self.network.is_output(
+                        bcast_match,
+                        src_dpid,
+                        dst_dpid,
+                        port=out_port,
+                        vid=out_vid,
+                        trace=trace,
+                    ),
+                    msg=msg,
+                )
 
     class ValveTestStackedRouting(ValveTestNetwork):
         """Test inter-vlan routing with stacking capabilities in an IPV4 network"""
 
         V100 = 0x100
         V200 = 0x200
-        VLAN100_FAUCET_MAC = '00:00:00:00:00:11'
-        VLAN200_FAUCET_MAC = '00:00:00:00:00:22'
+        VLAN100_FAUCET_MAC = "00:00:00:00:00:11"
+        VLAN200_FAUCET_MAC = "00:00:00:00:00:22"
 
-        VLAN100_FAUCET_VIPS = ''
-        VLAN100_FAUCET_VIP_SPACE = ''
-        VLAN200_FAUCET_VIPS = ''
-        VLAN200_FAUCET_VIP_SPACE = ''
+        VLAN100_FAUCET_VIPS = ""
+        VLAN100_FAUCET_VIP_SPACE = ""
+        VLAN200_FAUCET_VIPS = ""
+        VLAN200_FAUCET_VIP_SPACE = ""
 
         V100_HOSTS = []
         V200_HOSTS = []
 
         def base_config(self):
             """Create the base config"""
             self.V100_HOSTS = [1, 2, 3, 4]
@@ -2612,80 +3048,113 @@
             faucet_mac: '%s'
             faucet_vips: ['%s']
         vlan200:
             vid: 0x200
             faucet_mac: '%s'
             faucet_vips: ['%s']
     %s
-           """ % (self.VLAN100_FAUCET_MAC, self.VLAN100_FAUCET_VIP_SPACE,
-                  self.VLAN200_FAUCET_MAC, self.VLAN200_FAUCET_VIP_SPACE,
-                  self.base_config())
+           """ % (
+                self.VLAN100_FAUCET_MAC,
+                self.VLAN100_FAUCET_VIP_SPACE,
+                self.VLAN200_FAUCET_MAC,
+                self.VLAN200_FAUCET_VIP_SPACE,
+                self.base_config(),
+            )
 
         def setup_stack_routing(self):
             """Create a stacking config file."""
             self.create_config()
             self.setup_valves(self.CONFIG)
             self.trigger_stack_ports()
 
         @staticmethod
         def create_mac(vindex, host):
             """Create a MAC address string"""
-            return f'00:00:00:0{vindex}:00:0{host}'
+            return "00:00:00:0%u:00:0%u" % (vindex, host)
 
         @staticmethod
         def create_ip(vindex, host):
             """Create a IP address string"""
-            return f'10.0.{vindex}.{host}'
+            return "10.0.%u.%u" % (vindex, host)
 
         @staticmethod
         def get_eth_type():
             """Returns IPV4 ether type"""
             return valve_of.ether.ETH_TYPE_IP
 
         def create_match(self, vindex, host, faucet_mac, faucet_vip, code):
             """Create an ARP reply message"""
             return {
-                'eth_src': self.create_mac(vindex, host),
-                'eth_dst': faucet_mac,
-                'arp_code': code,
-                'arp_source_ip': self.create_ip(vindex, host),
-                'arp_target_ip': faucet_vip
+                "eth_src": self.create_mac(vindex, host),
+                "eth_dst": faucet_mac,
+                "arp_code": code,
+                "arp_source_ip": self.create_ip(vindex, host),
+                "arp_target_ip": faucet_vip,
             }
 
         def verify_router_cache(self, ip_match, eth_match, vid, dp_id):
             """Verify router nexthop cache stores correct values"""
             host_valve = self.valves_manager.valves[dp_id]
             for valve in self.valves_manager.valves.values():
                 valve_vlan = valve.dp.vlans[vid]
                 route_manager = valve._route_manager_by_eth_type.get(
-                    self.get_eth_type(), None)
+                    self.get_eth_type(), None
+                )
                 vlan_nexthop_cache = route_manager._vlan_nexthop_cache(valve_vlan)
                 self.assertTrue(vlan_nexthop_cache)
                 host_ip = ipaddress.ip_address(ip_match)
                 # Check IP address is properly cached
                 self.assertIn(host_ip, vlan_nexthop_cache)
                 nexthop = vlan_nexthop_cache[host_ip]
                 # Check MAC address is properly cached
                 self.assertEqual(eth_match, nexthop.eth_src)
                 if host_valve != valve:
                     # Check the proper nexthop port is cached
-                    expected_port = valve.stack_manager.relative_port_towards(host_valve.dp.name)
+                    expected_port = valve.stack_manager.relative_port_towards(
+                        host_valve.dp.name
+                    )
                     self.assertEqual(expected_port, nexthop.port)
 
         def test_router_cache_learn_hosts(self):
             """Have all router caches contain proper host nexthops"""
             # Learn Vlan100 hosts
             for host_id in self.V100_HOSTS:
                 dp_id = host_id
-                self.rcv_packet(1, self.V100, self.create_match(
-                    1, host_id, self.VLAN100_FAUCET_MAC,
-                    self.VLAN100_FAUCET_VIPS, arp.ARP_REPLY), dp_id=dp_id)
+                self.rcv_packet(
+                    1,
+                    self.V100,
+                    self.create_match(
+                        1,
+                        host_id,
+                        self.VLAN100_FAUCET_MAC,
+                        self.VLAN100_FAUCET_VIPS,
+                        arp.ARP_REPLY,
+                    ),
+                    dp_id=dp_id,
+                )
                 self.verify_router_cache(
-                    self.create_ip(1, host_id), self.create_mac(1, host_id), self.V100, dp_id)
+                    self.create_ip(1, host_id),
+                    self.create_mac(1, host_id),
+                    self.V100,
+                    dp_id,
+                )
             # Learn Vlan200 hosts
             for host_id in self.V200_HOSTS:
                 dp_id = host_id
-                self.rcv_packet(2, self.V200, self.create_match(
-                    2, host_id, self.VLAN200_FAUCET_MAC,
-                    self.VLAN200_FAUCET_VIPS, arp.ARP_REPLY), dp_id=dp_id)
+                self.rcv_packet(
+                    2,
+                    self.V200,
+                    self.create_match(
+                        2,
+                        host_id,
+                        self.VLAN200_FAUCET_MAC,
+                        self.VLAN200_FAUCET_VIPS,
+                        arp.ARP_REPLY,
+                    ),
+                    dp_id=dp_id,
+                )
                 self.verify_router_cache(
-                    self.create_ip(2, host_id), self.create_mac(2, host_id), self.V200, dp_id)
+                    self.create_ip(2, host_id),
+                    self.create_mac(2, host_id),
+                    self.V200,
+                    dp_id,
+                )
```

### Comparing `c65faucet-1.0.49/debian/control` & `c65faucet-1.0.50/debian/control`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/debian/copyright` & `c65faucet-1.0.50/debian/copyright`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/debian/faucet.postinst` & `c65faucet-1.0.50/debian/faucet.postinst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/debian/gauge.postinst` & `c65faucet-1.0.50/debian/gauge.postinst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/debian/rules` & `c65faucet-1.0.50/debian/rules`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docker/fuzz_config.sh` & `c65faucet-1.0.50/docker/fuzz_config.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docker/fuzz_packet.sh` & `c65faucet-1.0.50/docker/fuzz_packet.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docker/install-faucet.sh` & `c65faucet-1.0.50/docker/install-faucet.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docker/localtest.sh` & `c65faucet-1.0.50/docker/localtest.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docker/pip_deps.sh` & `c65faucet-1.0.50/docker/pip_deps.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docker/runtests.sh` & `c65faucet-1.0.50/docker/runtests.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docker/shard_tests.sh` & `c65faucet-1.0.50/docker/shard_tests.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docker-compose.yaml` & `c65faucet-1.0.50/docker-compose.yaml`

 * *Files 0% similar despite different names*

```diff
@@ -27,15 +27,15 @@
             - './etc/prometheus/faucet.rules.yml:/etc/prometheus/faucet.rules.yml'
         links:
             - faucet
             - gauge
 
     grafana:
         restart: always
-        image: 'grafana/grafana:9.4.7'
+        image: 'grafana/grafana:9.5.1'
         user: 'root'
         ports:
             - '3000:3000'
         volumes:
             - '${FAUCET_PREFIX}/opt/grafana:/var/lib/grafana'
         links:
             - influxdb
```

### Comparing `c65faucet-1.0.49/docs/Makefile` & `c65faucet-1.0.50/docs/Makefile`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/deployments/ONF_Faucet_deploy1.png` & `c65faucet-1.0.50/docs/_static/deployments/ONF_Faucet_deploy1.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/deployments/nznog17-physical-network.jpg` & `c65faucet-1.0.50/docs/_static/deployments/nznog17-physical-network.jpg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/deployments/nznog17-virtual-network.jpg` & `c65faucet-1.0.50/docs/_static/deployments/nznog17-virtual-network.jpg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/grafana-dashboards/faucet_instrumentation.json` & `c65faucet-1.0.50/docs/_static/grafana-dashboards/faucet_instrumentation.json`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/grafana-dashboards/faucet_inventory.json` & `c65faucet-1.0.50/docs/_static/grafana-dashboards/faucet_inventory.json`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/grafana-dashboards/faucet_port_statistics.json` & `c65faucet-1.0.50/docs/_static/grafana-dashboards/faucet_port_statistics.json`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/8021X-conf-diagram.svg` & `c65faucet-1.0.50/docs/_static/images/8021X-conf-diagram.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/faucet-architecture.svg` & `c65faucet-1.0.50/docs/_static/images/faucet-architecture.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/faucet-pipeline.png` & `c65faucet-1.0.50/docs/_static/images/faucet-pipeline.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/faucet-pipeline.svg` & `c65faucet-1.0.50/docs/_static/images/faucet-pipeline.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/faucet-pipeline.txt` & `c65faucet-1.0.50/docs/_static/images/faucet-pipeline.txt`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/gauge-nznog17.png` & `c65faucet-1.0.50/docs/_static/images/gauge-nznog17.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/gauge-snapshot1.png` & `c65faucet-1.0.50/docs/_static/images/gauge-snapshot1.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/gauge-snapshot2.png` & `c65faucet-1.0.50/docs/_static/images/gauge-snapshot2.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/gauge-snapshot3.png` & `c65faucet-1.0.50/docs/_static/images/gauge-snapshot3.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-acls.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-acls.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-bgp-routing.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-bgp-routing.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-ivr.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-ivr.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-multi-root-stack.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-multi-root-stack.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-nfv-services.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-nfv-services.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-stack-loop.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-stack-loop.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-stack-tunnel.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-stack-tunnel.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-stack.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-stack.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-stackwithivr.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-stackwithivr.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-static-routing.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-static-routing.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/images/tutorial-vlans.svg` & `c65faucet-1.0.50/docs/_static/images/tutorial-vlans.svg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/tutorial/cleanup` & `c65faucet-1.0.50/docs/_static/tutorial/cleanup`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/_static/tutorial/inter_switch_link` & `c65faucet-1.0.50/docs/_static/tutorial/inter_switch_link`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/architecture.rst` & `c65faucet-1.0.50/docs/architecture.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/conf.py` & `c65faucet-1.0.50/docs/conf.py`

 * *Files 18% similar despite different names*

```diff
@@ -15,229 +15,238 @@
 # serve to show the default.
 
 # If extensions (or modules to document with autodoc) are in another directory,
 # add these directories to sys.path here. If the directory is relative to the
 # documentation root, use os.path.abspath to make it absolute, like shown here.
 #
 
-'''
+"""
 Faucet's Sphinx configuration
-'''
+"""
 
 import os
 import sys
 
-sys.path.insert(0, os.path.abspath('../'))
+sys.path.insert(0, os.path.abspath("../"))
 
 autodoc_default_options = {
-    'members': True,
-    'show-inheritance': True,
-    'undoc-members': True,
+    "members": True,
+    "show-inheritance": True,
+    "undoc-members": True,
 }
 
 # -- General configuration ------------------------------------------------
 
 # If your documentation needs a minimal Sphinx version, state it here.
 #
-needs_sphinx = '1.8'
+needs_sphinx = "1.8"
 
 # Add any Sphinx extension module names here, as strings. They can be
 # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
 # ones.
-extensions = ['sphinx.ext.autodoc',
-              'sphinx.ext.coverage',
-              'sphinx.ext.doctest',
-              'sphinx.ext.githubpages',
-              'sphinx.ext.napoleon',
-              'sphinx.ext.viewcode',
-              'sphinxcontrib.rsvgconverter'
-              ]
+extensions = [
+    "sphinx.ext.autodoc",
+    "sphinx.ext.coverage",
+    "sphinx.ext.doctest",
+    "sphinx.ext.githubpages",
+    "sphinx.ext.napoleon",
+    "sphinx.ext.viewcode",
+    "sphinxcontrib.rsvgconverter",
+]
 
 # Add any paths that contain templates here, relative to this directory.
-templates_path = ['_templates']
+templates_path = ["_templates"]
 
 # The suffix(es) of source filenames.
 # You can specify multiple suffix as a list of string:
 #
 # source_suffix = ['.rst', '.md']
-source_suffix = '.rst'
+source_suffix = ".rst"
 
 # The master toctree document.
-master_doc = 'index'
+master_doc = "index"
 
 # General information about the project.
-copyright = '2018-2022, Faucet Developers'  # pylint: disable=redefined-builtin
-author = 'Faucet Developers'
+copyright = "2018-2022, Faucet Developers"  # pylint: disable=redefined-builtin
+author = "Faucet Developers"
 
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
 #
 # This is also used if you do content translation via gettext catalogs.
 # Usually you set "language" from the command line for these cases.
-language = 'en'
+language = "en"
 
 # List of patterns, relative to source directory, that match files and
 # directories to ignore when looking for source files.
 # This patterns also effect to html_static_path and html_extra_path
-exclude_patterns = ['_build', 'README.rst', 'Thumbs.db', '.DS_Store']
+exclude_patterns = ["_build", "README.rst", "Thumbs.db", ".DS_Store"]
 
 # The name of the Pygments (syntax highlighting) style to use.
-pygments_style = 'sphinx'
+pygments_style = "sphinx"
 
 # If true, `todo` and `todoList` produce output, else they produce nothing.
 todo_include_todos = False
 
 
 # -- Options for HTML output ----------------------------------------------
 
 # The theme to use for HTML and HTML Help pages.  See the documentation for
 # a list of builtin themes.
 #
-html_theme = 'sphinx_rtd_theme'
+html_theme = "sphinx_rtd_theme"
 
 # Theme options are theme-specific and customize the look and feel of a theme
 # further.  For a list of options available for each theme, see the
 # documentation.
 #
 # html_theme_options = {}
 
 # Add any paths that contain custom static files (such as style sheets) here,
 # relative to this directory. They are copied after the builtin static files,
 # so a file named "default.css" will overwrite the builtin "default.css".
-html_static_path = ['_static']
+html_static_path = ["_static"]
 
 # Custom sidebar templates, must be a dictionary that maps document names
 # to template names.
 #
 # This is required for the alabaster theme
 # refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars
 html_sidebars = {
-    '**': [
-        'relations.html',  # needs 'show_related': True theme option to display
-        'searchbox.html',
+    "**": [
+        "relations.html",  # needs 'show_related': True theme option to display
+        "searchbox.html",
     ]
 }
 
 
 # -- Options for HTMLHelp output ------------------------------------------
 
 # Output file base name for HTML help builder.
-htmlhelp_basename = 'faucetdoc'
+htmlhelp_basename = "faucetdoc"
 
 
 # -- Options for LaTeX output ---------------------------------------------
 
 latex_elements = {
     # The paper size ('letterpaper' or 'a4paper').
     #
     # 'papersize': 'letterpaper',
-
     # The font size ('10pt', '11pt' or '12pt').
     #
     # 'pointsize': '10pt',
-
     # Additional stuff for the LaTeX preamble.
     #
     # 'preamble': '',
-
     # Latex figure (float) alignment
     #
     # 'figure_align': 'htbp',
 }
 
 # Grouping the document tree into LaTeX files. List of tuples
 # (source start file, target name, title,
 #  author, documentclass [howto, manual, or own class]).
 latex_documents = [
-    (master_doc, 'faucet.tex', 'Faucet Documentation',
-     'Faucet Developers', 'manual'),
+    (master_doc, "faucet.tex", "Faucet Documentation", "Faucet Developers", "manual"),
 ]
 
 
 # -- Options for manual page output ---------------------------------------
 
 # One entry per manual page. List of tuples
 # (source start file, name, description, authors, manual section).
-man_pages = [
-    (master_doc, 'faucet', 'Faucet Documentation',
-     [author], 1)
-]
+man_pages = [(master_doc, "faucet", "Faucet Documentation", [author], 1)]
 
 
 # -- Options for Texinfo output -------------------------------------------
 
 # Grouping the document tree into Texinfo files. List of tuples
 # (source start file, target name, title, author,
 #  dir menu entry, description, category)
 texinfo_documents = [
-    (master_doc, 'faucet', 'Faucet Documentation',
-     author, 'faucet', '',
-     'Miscellaneous'),
+    (
+        master_doc,
+        "faucet",
+        "Faucet Documentation",
+        author,
+        "faucet",
+        "",
+        "Miscellaneous",
+    ),
 ]
 
 # -- Magic to run sphinx-apidoc automatically -----------------------------
 
 # See https://github.com/rtfd/readthedocs.org/issues/1139
 # on which this is based.
 
 
 def run_apidoc(_):
     """Call sphinx-apidoc on faucet module"""
 
     from sphinx.ext.apidoc import main as apidoc_main
-    apidoc_main(['-e', '-o', 'source/apidoc', '../faucet'])
+
+    apidoc_main(["-e", "-o", "source/apidoc", "../faucet"])
 
 
 def generate_prometheus_metric_table(_):
     """Autogen prometheus metrics documentation"""
 
     import faucet.faucet_metrics
     import faucet.gauge_prom
     from prometheus_client import CollectorRegistry
 
     block_text = {}
     output_path = {
-        'faucet': 'autogen/faucet_prometheus_metric_table.rst',
-        'gauge': 'autogen/gauge_prometheus_metric_table.rst'
+        "faucet": "autogen/faucet_prometheus_metric_table.rst",
+        "gauge": "autogen/gauge_prometheus_metric_table.rst",
     }
 
     metrics = {
-        'faucet': faucet.faucet_metrics.FaucetMetrics(reg=CollectorRegistry()),
-        'gauge': faucet.gauge_prom.GaugePrometheusClient(reg=CollectorRegistry())
+        "faucet": faucet.faucet_metrics.FaucetMetrics(reg=CollectorRegistry()),
+        "gauge": faucet.gauge_prom.GaugePrometheusClient(reg=CollectorRegistry()),
     }
 
     for module in ["faucet", "gauge"]:
-        block_text[module] = f"""
-.. list-table:: {module.title()} prometheus metrics
+        block_text[
+            module
+        ] = """\
+.. list-table:: {} prometheus metrics
     :widths: 40 10 55
     :header-rows: 1
 
     * - Metric
       - Type
       - Description
-"""
+""".format(
+            module.title()
+        )
 
         # pylint: disable=protected-access
         for metric in metrics[module]._reg.collect():
             if metric.type == "counter":
-                metric_name = f"{metric.name}_total"
+                metric_name = "{}_total".format(metric.name)
             else:
                 metric_name = metric.name
 
-            block_text[module] += f"""
-    * - {metric_name}
-      - {metric.type}
-      - {metric.documentation}
-"""
+            block_text[
+                module
+            ] += """\
+    * - {}
+      - {}
+      - {}
+""".format(
+                metric_name, metric.type, metric.documentation
+            )
 
-        with open(output_path[module], 'w', encoding='utf-8') as output_file:
+        with open(output_path[module], "w", encoding="utf-8") as output_file:
             output_file.write(block_text[module])
 
 
 def setup(app):
-    """ Add hooks into Sphinx to change behaviour and autogen documentation """
+    """Add hooks into Sphinx to change behaviour and autogen documentation"""
 
     # Add custom css
     app.add_css_file("css/responsive-tables.css")
     # Override Sphinx setup to trigger sphinx-apidoc.
-    app.connect('builder-inited', run_apidoc)
-    app.connect('builder-inited', generate_prometheus_metric_table)
+    app.connect("builder-inited", run_apidoc)
+    app.connect("builder-inited", generate_prometheus_metric_table)
```

### Comparing `c65faucet-1.0.49/docs/configuration.rst` & `c65faucet-1.0.50/docs/configuration.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/developer_guide.rst` & `c65faucet-1.0.50/docs/developer_guide.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/external_resources.rst` & `c65faucet-1.0.50/docs/external_resources.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/fuzzing.rst` & `c65faucet-1.0.50/docs/fuzzing.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/installation.rst` & `c65faucet-1.0.50/docs/installation.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/intro.rst` & `c65faucet-1.0.50/docs/intro.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/monitoring.rst` & `c65faucet-1.0.50/docs/monitoring.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/release_notes/1.7.0.rst` & `c65faucet-1.0.50/docs/release_notes/1.7.0.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/release_notes/1.9.0.rst` & `c65faucet-1.0.50/docs/release_notes/1.9.0.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/testing.rst` & `c65faucet-1.0.50/docs/testing.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/tutorials/acls.rst` & `c65faucet-1.0.50/docs/tutorials/acls.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/tutorials/conntrack.rst` & `c65faucet-1.0.50/docs/tutorials/conntrack.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/tutorials/first_time.rst` & `c65faucet-1.0.50/docs/tutorials/first_time.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/tutorials/nfv_services.rst` & `c65faucet-1.0.50/docs/tutorials/nfv_services.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/tutorials/routing.rst` & `c65faucet-1.0.50/docs/tutorials/routing.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/tutorials/stacking.rst` & `c65faucet-1.0.50/docs/tutorials/stacking.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/tutorials/vlans.rst` & `c65faucet-1.0.50/docs/tutorials/vlans.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/allied-telesis/README_Allied_Telesis.rst` & `c65faucet-1.0.50/docs/vendors/allied-telesis/README_Allied_Telesis.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/cisco/README_Cisco.rst` & `c65faucet-1.0.50/docs/vendors/cisco/README_Cisco.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/hpe/README_Aruba.rst` & `c65faucet-1.0.50/docs/vendors/hpe/README_Aruba.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/lagopus/README_Lagopus.rst` & `c65faucet-1.0.50/docs/vendors/lagopus/README_Lagopus.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/northboundnetworks/README_ZodiacFX.rst` & `c65faucet-1.0.50/docs/vendors/northboundnetworks/README_ZodiacFX.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/northboundnetworks/README_ZodiacGX.rst` & `c65faucet-1.0.50/docs/vendors/northboundnetworks/README_ZodiacGX.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/northboundnetworks/conf-zodiac.sh` & `c65faucet-1.0.50/docs/vendors/northboundnetworks/conf-zodiac.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/noviflow/README_noviflow.rst` & `c65faucet-1.0.50/docs/vendors/noviflow/README_noviflow.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/ovs/README_OVS-DPDK.rst` & `c65faucet-1.0.50/docs/vendors/ovs/README_OVS-DPDK.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/ovs/faucet_ovs_test.png` & `c65faucet-1.0.50/docs/vendors/ovs/faucet_ovs_test.png`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/docs/vendors/ovs/faucet_testing_with_OVS_on_hardware.rst` & `c65faucet-1.0.50/docs/vendors/ovs/faucet_testing_with_OVS_on_hardware.rst`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/etc/faucet/acls.yaml` & `c65faucet-1.0.50/etc/faucet/acls.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/etc/faucet/faucet.yaml` & `c65faucet-1.0.50/etc/faucet/faucet.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/etc/faucet/gauge.yaml` & `c65faucet-1.0.50/etc/faucet/gauge.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/etc/prometheus/faucet.rules.yml` & `c65faucet-1.0.50/etc/prometheus/faucet.rules.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/etc/prometheus/prometheus.yml` & `c65faucet-1.0.50/etc/prometheus/prometheus.yml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/__main__.py` & `c65faucet-1.0.50/faucet/__main__.py`

 * *Files 22% similar despite different names*

```diff
@@ -21,91 +21,104 @@
 import argparse
 import os
 import sys
 
 from pbr.version import VersionInfo
 
 if sys.version_info < (3,) or sys.version_info < (3, 5):
-    raise ImportError("""You are trying to run faucet on python {py}
+    raise ImportError(
+        """You are trying to run faucet on python {py}
 
-Faucet is not compatible with python {py}, please upgrade to python 3.5 or newer."""
-                      .format(py='.'.join([str(v) for v in sys.version_info[:3]])))
+Faucet is not compatible with python {py}, please upgrade to python 3.5 or newer.""".format(
+            py=".".join([str(v) for v in sys.version_info[:3]])
+        )
+    )
 
 RYU_OPTIONAL_ARGS = [
-    ('ca-certs', 'CA certificates'),
-    ('config-dir', """Path to a config directory to pull `*.conf` files
+    ("ca-certs", "CA certificates"),
+    (
+        "config-dir",
+        """Path to a config directory to pull `*.conf` files
                       from. This file set is sorted, so as to provide a
                       predictable parse order if individual options are
                       over-ridden. The set is parsed after the file(s)
                       specified via previous --config-file, arguments hence
-                      over-ridden options in the directory take precedence."""),
-    ('config-file', """Path to a config file to use. Multiple config files
+                      over-ridden options in the directory take precedence.""",
+    ),
+    (
+        "config-file",
+        """Path to a config file to use. Multiple config files
                        can be specified, with values in later files taking
-                       precedence. Defaults to None.""", "/etc/faucet/ryu.conf"),
-    ('ctl-cert', 'controller certificate'),
-    ('ctl-privkey', 'controller private key'),
-    ('default-log-level', 'default log level'),
-    ('log-config-file', 'Path to a logging config file to use'),
-    ('log-dir', 'log file directory'),
-    ('log-file', 'log file name'),
-    ('log-file-mode', 'default log file permission'),
-    ('observe-links', 'observe link discovery events'),
-    ('ofp-listen-host', 'openflow listen host (default 0.0.0.0)'),
-    ('ofp-ssl-listen-port', 'openflow ssl listen port (default: 6653)'),
-    ('ofp-switch-address-list', """list of IP address and port pairs (default empty).
-                                   e.g., "127.0.0.1:6653,[::1]:6653"""),
-    ('ofp-switch-connect-interval', 'interval in seconds to connect to switches (default 1)'),
-    ('ofp-tcp-listen-port', 'openflow tcp listen port (default: 6653)'),
-    ('pid-file', 'pid file name'),
-    ('user-flags', 'Additional flags file for user applications'),
+                       precedence. Defaults to None.""",
+        "/etc/faucet/ryu.conf",
+    ),
+    ("ctl-cert", "controller certificate"),
+    ("ctl-privkey", "controller private key"),
+    ("default-log-level", "default log level"),
+    ("log-config-file", "Path to a logging config file to use"),
+    ("log-dir", "log file directory"),
+    ("log-file", "log file name"),
+    ("log-file-mode", "default log file permission"),
+    ("observe-links", "observe link discovery events"),
+    ("ofp-listen-host", "openflow listen host (default 0.0.0.0)"),
+    ("ofp-ssl-listen-port", "openflow ssl listen port (default: 6653)"),
+    (
+        "ofp-switch-address-list",
+        """list of IP address and port pairs (default empty).
+                                   e.g., "127.0.0.1:6653,[::1]:6653""",
+    ),
+    (
+        "ofp-switch-connect-interval",
+        "interval in seconds to connect to switches (default 1)",
+    ),
+    ("ofp-tcp-listen-port", "openflow tcp listen port (default: 6653)"),
+    ("pid-file", "pid file name"),
+    ("user-flags", "Additional flags file for user applications"),
 ]
 
 
 def parse_args(sys_args):
     """Parse Faucet/Gauge arguments.
 
     Returns:
         argparse.Namespace: command line arguments
     """
 
-    args = argparse.ArgumentParser(
-        prog='faucet', description='Faucet SDN Controller')
-    args.add_argument('--gauge', action='store_true', help='run Gauge instead')
+    args = argparse.ArgumentParser(prog="faucet", description="Faucet SDN Controller")
+    args.add_argument("--gauge", action="store_true", help="run Gauge instead")
     args.add_argument(
-        '-v', '--verbose', action='store_true', help='produce verbose output')
+        "-v", "--verbose", action="store_true", help="produce verbose output"
+    )
     args.add_argument(
-        '-V', '--version', action='store_true', help='print version and exit')
+        "-V", "--version", action="store_true", help="print version and exit"
+    )
+    args.add_argument("--use-stderr", action="store_true", help="log to standard error")
+    args.add_argument("--use-syslog", action="store_true", help="output to syslog")
     args.add_argument(
-        '--use-stderr', action='store_true', help='log to standard error')
-    args.add_argument(
-        '--use-syslog', action='store_true', help='output to syslog')
-    args.add_argument(
-        '--ryu-app-lists',
-        action='append',
-        help='add Ryu app (can be specified multiple times)',
-        metavar='APP')
+        "--ryu-app-lists",
+        action="append",
+        help="add Ryu app (can be specified multiple times)",
+        metavar="APP",
+    )
 
     for ryu_arg in RYU_OPTIONAL_ARGS:
         if len(ryu_arg) >= 3:
             args.add_argument(
-                '--ryu-%s' % ryu_arg[0],
-                help=ryu_arg[1],
-                default=ryu_arg[2])
+                "--ryu-%s" % ryu_arg[0], help=ryu_arg[1], default=ryu_arg[2]
+            )
         else:
-            args.add_argument(
-                '--ryu-%s' % ryu_arg[0],
-                help=ryu_arg[1])
+            args.add_argument("--ryu-%s" % ryu_arg[0], help=ryu_arg[1])
 
     return args.parse_args(sys_args)
 
 
 def print_version():
     """Print version number and exit."""
-    version = VersionInfo('c65faucet').semantic_version().release_string()
-    message = 'c65faucet %s' % version
+    version = VersionInfo("c65faucet").semantic_version().release_string()
+    message = "c65faucet %s" % version
     print(message)
 
 
 def build_ryu_args(argv):
     args = parse_args(argv[1:])
 
     # Checking version number?
@@ -114,49 +127,49 @@
         return []
 
     prog = os.path.basename(argv[0])
     ryu_args = []
 
     # Handle log location
     if args.use_stderr:
-        ryu_args.append('--use-stderr')
+        ryu_args.append("--use-stderr")
     if args.use_syslog:
-        ryu_args.append('--use-syslog')
+        ryu_args.append("--use-syslog")
 
     # Verbose output?
     if args.verbose:
-        ryu_args.append('--verbose')
+        ryu_args.append("--verbose")
 
     for arg, val in vars(args).items():
-        if not val or not arg.startswith('ryu'):
+        if not val or not arg.startswith("ryu"):
             continue
-        if arg == 'ryu_app_lists':
+        if arg == "ryu_app_lists":
             continue
-        if arg == 'ryu_config_file' and not os.path.isfile(val):
+        if arg == "ryu_config_file" and not os.path.isfile(val):
             continue
-        arg_name = arg.replace('ryu_', '').replace('_', '-')
-        ryu_args.append('--%s=%s' % (arg_name, val))
+        arg_name = arg.replace("ryu_", "").replace("_", "-")
+        ryu_args.append("--%s=%s" % (arg_name, val))
 
     # Running Faucet or Gauge?
-    if args.gauge or os.path.basename(prog) == 'gauge':
-        ryu_args.append('faucet.gauge')
+    if args.gauge or os.path.basename(prog) == "gauge":
+        ryu_args.append("faucet.gauge")
     else:
-        ryu_args.append('faucet.faucet')
+        ryu_args.append("faucet.faucet")
 
     # Check for additional Ryu apps.
     if args.ryu_app_lists:
         ryu_args.extend(args.ryu_app_lists)
 
     # Replace current process with ryu-manager from PATH (no PID change).
-    ryu_args.insert(0, 'osken-manager')
+    ryu_args.insert(0, "osken-manager")
     return ryu_args
 
 
 def main():
     """Main program."""
     ryu_args = build_ryu_args(sys.argv)
     if ryu_args:
         os.execvp(ryu_args[0], ryu_args)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `c65faucet-1.0.49/faucet/acl.py` & `c65faucet-1.0.50/faucet/acl.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/check_faucet_config.py` & `c65faucet-1.0.50/faucet/check_faucet_config.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 from faucet.config_parser import dp_parser
 from faucet.conf import InvalidConfigError
 
 
 def check_config(conf_files, debug_level, check_output_file):
     """Return True and successful config dict, if all config can be parsed."""
     logname = os.devnull
-    logger = logging.getLogger('%s.config' % logname)
+    logger = logging.getLogger("%s.config" % logname)
     logger_handler = logging.StreamHandler(stream=sys.stderr)
     logger.addHandler(logger_handler)
     logger.propagate = 0
     logger.setLevel(debug_level)
     check_output = []
 
     if conf_files:
@@ -50,20 +50,20 @@
                     check_result = True
                     continue
             except InvalidConfigError as config_err:
                 check_output = [config_err]
             break
     else:
         check_result = False
-        check_output = ['no files specified']
+        check_output = ["no files specified"]
 
     pprint.pprint(check_output, stream=check_output_file)
     return check_result
 
 
 def main():
     """Mainline."""
     sys.exit(not check_config(sys.argv[1:], logging.DEBUG, sys.stdout))
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `c65faucet-1.0.49/faucet/conf.py` & `c65faucet-1.0.50/faucet/conf.py`

 * *Files 14% similar despite different names*

```diff
@@ -49,30 +49,36 @@
 
     def __init__(self, _id, dp_id, conf=None):
         self._id = _id
         self.dp_id = dp_id
         if conf is None:
             conf = {}
         if self.defaults is not None and self.defaults_types is not None:
-            diff = set(self.defaults.keys()).symmetric_difference(set(self.defaults_types.keys()))
+            diff = set(self.defaults.keys()).symmetric_difference(
+                set(self.defaults_types.keys())
+            )
             assert not diff, diff
         if isinstance(conf, dict):
             self.update(conf)
             self.set_defaults()
         self.check_config()
         self.orig_conf = {k: self.__dict__[k] for k in self.defaults}
         for k, conf_v in self.orig_conf.items():
             if isinstance(conf_v, Conf):
                 self.orig_conf[k] = conf_v.orig_conf
 
     def __setattr__(self, name, value):
-        if not self.dyn_finalized or name.startswith('dyn') or name in self.mutable_attrs:
+        if (
+            not self.dyn_finalized
+            or name.startswith("dyn")
+            or name in self.mutable_attrs
+        ):
             super().__setattr__(name, value)
         else:
-            raise ValueError('cannot update %s on finalized Conf object' % name)
+            raise ValueError("cannot update %s on finalized Conf object" % name)
 
     def _set_default(self, key, value, conf=None):
         if conf is None:
             conf = self.__dict__
         assert key in conf, key
         if conf[key] is None:
             conf[key] = value
@@ -85,32 +91,41 @@
         """Set default values and run any basic sanity checks."""
         self._set_conf_defaults(self.defaults, self.__dict__)
 
     def _check_unknown_conf(self, conf):
         """Check that supplied conf dict doesn't specify keys not defined."""
         sub_conf_names = set(conf.keys())
         unknown_conf_names = sub_conf_names - set(self.defaults.keys())
-        test_config_condition(unknown_conf_names, '%s fields unknown in %s' % (
-            unknown_conf_names, self._id))
+        test_config_condition(
+            unknown_conf_names,
+            "%s fields unknown in %s" % (unknown_conf_names, self._id),
+        )
 
     def _check_conf_types(self, conf, conf_types):
         """Check that conf value is of the correct type."""
-        test_config_condition(not isinstance(conf, dict), (
-            'Conf object %s contents %s must be type %s not %s' % (
-                self._id, conf, dict, type(conf))))
+        test_config_condition(
+            not isinstance(conf, dict),
+            (
+                "Conf object %s contents %s must be type %s not %s"
+                % (self._id, conf, dict, type(conf))
+            ),
+        )
         for conf_key, conf_value in conf.items():
             test_config_condition(
-                conf_key not in conf_types, '%s field unknown in %s (known types %s)' % (
-                    conf_key, self._id, conf_types))
+                conf_key not in conf_types,
+                "%s field unknown in %s (known types %s)"
+                % (conf_key, self._id, conf_types),
+            )
             if conf_value is not None:
                 conf_type = conf_types[conf_key]
                 test_config_condition(
-                    not isinstance(conf_value, conf_type), '%s value %s must be %s not %s' % (
-                        conf_key, conf_value,
-                        conf_type, type(conf_value)))  # pytype: disable=invalid-typevar
+                    not isinstance(conf_value, conf_type),
+                    "%s value %s must be %s not %s"
+                    % (conf_key, conf_value, conf_type, type(conf_value)),
+                )  # pytype: disable=invalid-typevar
 
     @staticmethod
     def _set_unknown_conf(conf, conf_types):
         for conf_key, conf_type in conf_types.items():
             if conf_key not in conf:
                 if conf_type == list:
                     conf[conf_key] = []
@@ -128,116 +143,151 @@
         """Check config at instantiation time for errors, typically via assert."""
         return
 
     def _conf_keys(self, conf, subconf=True, ignore_keys=None):
         """Return a list of key/values of attributes with dyn/Conf attributes/filtered."""
         conf_keys = []
         for key, value in sorted(
-                ((key, value) for key, value in conf.orig_conf.items()
-                    if key in self.defaults)):
+            (
+                (key, value)
+                for key, value in conf.orig_conf.items()
+                if key in self.defaults
+            )
+        ):
             if ignore_keys and key in ignore_keys:
                 continue
             if not subconf and value:
                 if isinstance(value, Conf):
                     continue
                 if isinstance(value, (tuple, list, set)) and isinstance(value[0], Conf):
                     continue
             conf_keys.append((key, self._str_conf(value)))
         return conf_keys
 
     @staticmethod
     def _conf_dyn_keys(conf):
-        return [(key, value) for key, value in conf.__dict__.items() if key.startswith('dyn')]
+        return [
+            (key, value)
+            for key, value in conf.__dict__.items()
+            if key.startswith("dyn")
+        ]
 
     def merge_dyn(self, other_conf):
         """Merge dynamic state from other conf object."""
         self.__dict__.update(self._conf_dyn_keys(other_conf))
 
     def _str_conf(self, conf_v):
         if isinstance(conf_v, (bool, str, int)):
             return conf_v
-        if isinstance(conf_v, (
-                ipaddress.IPv4Address, ipaddress.IPv4Interface, ipaddress.IPv4Network,
-                ipaddress.IPv6Address, ipaddress.IPv6Interface, ipaddress.IPv6Network)):
+        if isinstance(
+            conf_v,
+            (
+                ipaddress.IPv4Address,
+                ipaddress.IPv4Interface,
+                ipaddress.IPv4Network,
+                ipaddress.IPv6Address,
+                ipaddress.IPv6Interface,
+                ipaddress.IPv6Network,
+            ),
+        ):
             return str(conf_v)
         if isinstance(conf_v, (dict, OrderedDict)):
-            return {str(i): self._str_conf(j) for i, j in conf_v.items() if j is not None}
+            return {
+                str(i): self._str_conf(j) for i, j in conf_v.items() if j is not None
+            }
         if isinstance(conf_v, (list, tuple, frozenset)):
             return tuple(self._str_conf(i) for i in conf_v if i is not None)
         if isinstance(conf_v, Conf):
-            for i in ('name', '_id'):
+            for i in ("name", "_id"):
                 if hasattr(conf_v, i):
                     return getattr(conf_v, i)
         return None
 
     def to_conf(self):
         """Return configuration as a dict."""
-        conf = {
-            k: self.orig_conf[str(k)] for k in self.defaults if k != 'name'}
-        return json.dumps(self._str_conf(conf), sort_keys=True, indent=4, separators=(',', ': '))
+        conf = {k: self.orig_conf[str(k)] for k in self.defaults if k != "name"}
+        return json.dumps(
+            self._str_conf(conf), sort_keys=True, indent=4, separators=(",", ": ")
+        )
 
     def conf_diff(self, other):
         """Return text diff between two Confs."""
         differ = difflib.Differ()
-        return '\n'.join(differ.compare(
-            self.to_conf().splitlines(), other.to_conf().splitlines()))
+        return "\n".join(
+            differ.compare(self.to_conf().splitlines(), other.to_conf().splitlines())
+        )
 
     def conf_hash(self, subconf=True, ignore_keys=None):
         """Return hash of keys configurably filtering attributes."""
-        return hash(frozenset(list(map(
-            str, self._conf_keys(self, subconf=subconf, ignore_keys=ignore_keys)))))
+        return hash(
+            frozenset(
+                list(
+                    map(
+                        str,
+                        self._conf_keys(self, subconf=subconf, ignore_keys=ignore_keys),
+                    )
+                )
+            )
+        )
 
     def __hash__(self):
         if self.dyn_hash is not None:
             return self.dyn_hash
         dyn_hash = self.conf_hash(subconf=True)
         if self.dyn_finalized:
             self.dyn_hash = dyn_hash
         return dyn_hash
 
     def _finalize_val(self, val):
         if isinstance(val, list):
-            return tuple(
-                self._finalize_val(v) for v in val)
+            return tuple(self._finalize_val(v) for v in val)
         if isinstance(val, set):
-            return frozenset(
-                [self._finalize_val(v) for v in val])
+            return frozenset([self._finalize_val(v) for v in val])
         if isinstance(val, dict):
-            return OrderedDict([
-                (k, self._finalize_val(v)) for k, v in sorted(val.items(), key=str)])
+            return OrderedDict(
+                [(k, self._finalize_val(v)) for k, v in sorted(val.items(), key=str)]
+            )
         return val
 
     def finalize(self):
         """Configuration parsing marked complete."""
         self.__dict__.update(
-            {k: self._finalize_val(v) for k, v in self.__dict__.items()
-             if not k.startswith('dyn')})
+            {
+                k: self._finalize_val(v)
+                for k, v in self.__dict__.items()
+                if not k.startswith("dyn")
+            }
+        )
         self.dyn_finalized = True
 
     def ignore_subconf(self, other, ignore_keys=None):
         """Return True if this config same as other, ignoring sub config."""
-        return (self.conf_hash(
-            subconf=False, ignore_keys=ignore_keys) == other.conf_hash(
-                subconf=False, ignore_keys=ignore_keys))
+        return self.conf_hash(
+            subconf=False, ignore_keys=ignore_keys
+        ) == other.conf_hash(subconf=False, ignore_keys=ignore_keys)
 
     def __eq__(self, other):
         return self.__hash__() == other.__hash__()
 
     def __ne__(self, other):
         return not self.__eq__(other)
 
     @staticmethod
     def _check_ip_str(ip_str, ip_method=ipaddress.ip_address):
         try:
             # bool type is deprecated by the library ipaddress
             if not isinstance(ip_str, bool):
                 return ip_method(ip_str)
-            raise InvalidConfigError('Invalid IP address %s: IP address of type bool' % (ip_str))
+            raise InvalidConfigError(
+                "Invalid IP address %s: IP address of type bool" % (ip_str)
+            )
         except (ValueError, AttributeError, TypeError) as err:
-            raise InvalidConfigError('Invalid IP address %s: %s' % (ip_str, err)) from err
+            raise InvalidConfigError(
+                "Invalid IP address %s: %s" % (ip_str, err)
+            ) from err
 
     @staticmethod
     def _ipvs(ipas):
         return frozenset([ipa.version for ipa in ipas])
 
     @staticmethod
     def _by_ipv(ipas, ipv):
```

### Comparing `c65faucet-1.0.49/faucet/config_parser.py` & `c65faucet-1.0.50/faucet/config_parser.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/config_parser_util.py` & `c65faucet-1.0.50/faucet/config_parser_util.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/dp.py` & `c65faucet-1.0.50/faucet/dp.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/faucet.py` & `c65faucet-1.0.50/faucet/faucet.py`

 * *Files 7% similar despite different names*

```diff
@@ -16,15 +16,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 # implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 # pylint: disable=using-constant-test,wrong-import-order,wrong-import-position
 import eventlet
-if True:                # A trick to satisfy linting for E402
+
+if True:  # A trick to satisfy linting for E402
     eventlet.monkey_patch()
 
 import time
 
 from functools import partial
 
 from os_ken.controller.handler import CONFIG_DISPATCHER
@@ -41,97 +42,127 @@
 from faucet import faucet_bgp
 from faucet import faucet_dot1x
 from faucet import valves_manager
 from faucet import faucet_metrics
 from faucet import valve_of
 
 
-EXPORT_RYU_CONFIGS = ['echo_request_interval', 'maximum_unreplied_echo_requests']
+EXPORT_RYU_CONFIGS = ["echo_request_interval", "maximum_unreplied_echo_requests"]
 
 
-class EventFaucetMaintainStackRoot(event.EventBase):  # pylint: disable=too-few-public-methods
+class EventFaucetMaintainStackRoot(  # pylint: disable=too-few-public-methods
+    event.EventBase
+):
     """Event used to maintain stack root."""
 
 
-class EventFaucetMetricUpdate(event.EventBase):  # pylint: disable=too-few-public-methods
+class EventFaucetMetricUpdate(  # pylint: disable=too-few-public-methods
+    event.EventBase
+):
     """Event used to trigger update of metrics."""
 
 
-class EventFaucetResolveGateways(event.EventBase):  # pylint: disable=too-few-public-methods
+class EventFaucetResolveGateways(  # pylint: disable=too-few-public-methods
+    event.EventBase
+):
     """Event used to trigger gateway re/resolution."""
 
 
 class EventFaucetStateExpire(event.EventBase):  # pylint: disable=too-few-public-methods
     """Event used to trigger expiration of state in controller."""
 
 
-class EventFaucetFastStateExpire(event.EventBase):  # pylint: disable=too-few-public-methods
+class EventFaucetFastStateExpire(  # pylint: disable=too-few-public-methods
+    event.EventBase
+):
     """Event used to trigger fast expiration of state in controller."""
 
 
 class EventFaucetAdvertise(event.EventBase):  # pylint: disable=too-few-public-methods
     """Event used to trigger periodic network advertisements (eg IPv6 RAs)."""
 
 
-class EventFaucetFastAdvertise(event.EventBase):  # pylint: disable=too-few-public-methods
+class EventFaucetFastAdvertise(  # pylint: disable=too-few-public-methods
+    event.EventBase
+):
     """Event used to trigger periodic fast network advertisements (eg LACP)."""
 
 
-class EventFaucetEventSockHeartbeat(event.EventBase):  # pylint: disable=too-few-public-methods
+class EventFaucetEventSockHeartbeat(  # pylint: disable=too-few-public-methods
+    event.EventBase
+):
     """Event used to trigger periodic events on event sock,
     causing it to raise an exception if conn is broken.
     """
 
 
 class Faucet(OSKenAppBase):
     """A OSKenApp that implements an L2/L3 learning VLAN switch.
 
     Valve provides the switch implementation; this is a shim for the Ryu
     event handling framework to interface with Valve.
     """
+
     _CONTEXTS = {
-        'dpset': dpset.DPSet,
+        "dpset": dpset.DPSet,
     }
     _VALVE_SERVICES = {
         EventFaucetMetricUpdate: (None, 5),
-        EventFaucetResolveGateways: ('resolve_gateways', 2),
-        EventFaucetStateExpire: ('state_expire', 5),
-        EventFaucetFastStateExpire: ('fast_state_expire', 2),
-        EventFaucetAdvertise: ('advertise', 15),
-        EventFaucetFastAdvertise: ('fast_advertise', 5),
+        EventFaucetResolveGateways: ("resolve_gateways", 2),
+        EventFaucetStateExpire: ("state_expire", 5),
+        EventFaucetFastStateExpire: ("fast_state_expire", 2),
+        EventFaucetAdvertise: ("advertise", 15),
+        EventFaucetFastAdvertise: ("fast_advertise", 5),
     }
-    logname = 'faucet'
-    exc_logname = logname + '.exception'
+    logname = "faucet"
+    exc_logname = logname + ".exception"
     bgp = None
     notifier = None
     valves_manager = None
     event_socket_heartbeat_time = 0
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.prom_client = faucet_metrics.FaucetMetrics(reg=self._reg)
         self.bgp = faucet_bgp.FaucetBgp(
-            self.logger, self.exc_logname, self.prom_client, self._send_flow_msgs)
+            self.logger, self.exc_logname, self.prom_client, self._send_flow_msgs
+        )
         self.dot1x = faucet_dot1x.FaucetDot1x(
-            self.logger, self.exc_logname, self.prom_client, self._send_flow_msgs)
+            self.logger, self.exc_logname, self.prom_client, self._send_flow_msgs
+        )
         self.notifier = faucet_event.FaucetEventNotifier(
-            self.get_setting('EVENT_SOCK'), self.prom_client, self.logger)
+            self.get_setting("EVENT_SOCK"), self.prom_client, self.logger
+        )
         self.valves_manager = valves_manager.ValvesManager(
-            self.logname, self.logger, self.prom_client, self.notifier, self.bgp,
-            self.dot1x, self.get_setting('CONFIG_AUTO_REVERT'), self._send_flow_msgs)
+            self.logname,
+            self.logger,
+            self.prom_client,
+            self.notifier,
+            self.bgp,
+            self.dot1x,
+            self.get_setting("CONFIG_AUTO_REVERT"),
+            self._send_flow_msgs,
+        )
         self.thread_managers = (self.bgp, self.dot1x, self.prom_client, self.notifier)
-        self.event_sock_hrtbeat_time = int(self.get_setting('EVENT_SOCK_HEARTBEAT') or 0)
+        self.event_sock_hrtbeat_time = int(
+            self.get_setting("EVENT_SOCK_HEARTBEAT") or 0
+        )
         if self.event_sock_hrtbeat_time > 0:
-            self._VALVE_SERVICES[EventFaucetEventSockHeartbeat] = ('event_sock_heartbeat',
-                                                                   self.event_sock_hrtbeat_time)
+            self._VALVE_SERVICES[EventFaucetEventSockHeartbeat] = (
+                "event_sock_heartbeat",
+                self.event_sock_hrtbeat_time,
+            )
         self.stack_root_state_update_time = int(
-            self.get_setting('STACK_ROOT_STATE_UPDATE_TIME') or 0)
+            self.get_setting("STACK_ROOT_STATE_UPDATE_TIME") or 0
+        )
         if self.stack_root_state_update_time:
-            self._VALVE_SERVICES[EventFaucetMaintainStackRoot] = (None,
-                                                                  self.stack_root_state_update_time)
+            self._VALVE_SERVICES[EventFaucetMaintainStackRoot] = (
+                None,
+                self.stack_root_state_update_time,
+            )
 
     @kill_on_exception(exc_logname)
     def _check_thread_exception(self):
         super()._check_thread_exception()
 
     def _export_ryu_config(self):
         for opt_name in EXPORT_RYU_CONFIGS:
@@ -140,73 +171,75 @@
             self.prom_client.ryu_config.labels(**config_labels).set(value)
 
     @kill_on_exception(exc_logname)
     def start(self):
         super().start()
 
         # Start Prometheus
-        prom_port = int(self.get_setting('PROMETHEUS_PORT'))
-        prom_addr = self.get_setting('PROMETHEUS_ADDR')
+        prom_port = int(self.get_setting("PROMETHEUS_PORT"))
+        prom_addr = self.get_setting("PROMETHEUS_ADDR")
         self.prom_client.start(prom_port, prom_addr)
         self._export_ryu_config()
 
         # Start event notifier
         notifier_thread = self.notifier.start()
         if notifier_thread is not None:
             self.threads.append(notifier_thread)
 
         for service_event, service_pair in self._VALVE_SERVICES.items():
             name, interval = service_pair
             thread = hub.spawn(
-                partial(self._thread_reschedule, service_event(), interval))
+                partial(self._thread_reschedule, service_event(), interval)
+            )
             thread.name = name
             self.threads.append(thread)
 
     def _delete_deconfigured_dp(self, deleted_dpid):
-        self.logger.info(
-            'Deleting de-configured %s', dpid_log(deleted_dpid))
+        self.logger.info("Deleting de-configured %s", dpid_log(deleted_dpid))
         ryu_dp = self.dpset.get(deleted_dpid)
         if ryu_dp is not None:
             ryu_dp.close()
 
     @set_ev_cls(EventReconfigure, MAIN_DISPATCHER)
     @kill_on_exception(exc_logname)
     def reload_config(self, ryu_event):
         """Handle a request to reload configuration."""
         super().reload_config(ryu_event)
         self.valves_manager.request_reload_configs(
-            time.time(), self.config_file, delete_dp=self._delete_deconfigured_dp)
+            time.time(), self.config_file, delete_dp=self._delete_deconfigured_dp
+        )
 
     @kill_on_exception(exc_logname)
     def _send_flow_msgs(self, valve, flow_msgs, ryu_dp=None):
         """Send OpenFlow messages to a connected datapath.
 
         Args:
             Valve instance or None.
             flow_msgs (list): OpenFlow messages to send.
             ryu_dp: Override datapath from DPSet.
         """
         if ryu_dp is None:
             ryu_dp = self.dpset.get(valve.dp.dp_id)
         if not ryu_dp:
-            valve.logger.error('send_flow_msgs: DP not up')
+            valve.logger.error("send_flow_msgs: DP not up")
             return
         valve.send_flows(ryu_dp, flow_msgs, time.time())
 
     def _get_valve(self, ryu_event, require_running=False):
         """Get Valve instance to response to an event.
 
         Args:
             ryu_event (ryu.controller.event.Event): event
             require_running (bool): require DP to be running.
         Returns:
             valve, ryu_dp, msg: tuple of Nones, or datapath object, Ryu datapath, and msg (if any)
         """
         valve, ryu_dp, msg = self._get_datapath_obj(
-            self.valves_manager.valves, ryu_event)
+            self.valves_manager.valves, ryu_event
+        )
         if valve:
             if msg:
                 valve.ofchannel_log([msg])
             if require_running and not valve.dp.dyn_running:
                 valve = None
         return (valve, ryu_dp, msg)
 
@@ -218,15 +251,17 @@
     def metric_update(self, _):
         """Handle a request to update metrics in the controller."""
         self.valves_manager.update_metrics(time.time())
 
     @set_ev_cls(EventFaucetMaintainStackRoot, MAIN_DISPATCHER)
     @kill_on_exception(exc_logname)
     def _maintain_stack_root(self, _):
-        self.valves_manager.maintain_stack_root(time.time(), self.stack_root_state_update_time)
+        self.valves_manager.maintain_stack_root(
+            time.time(), self.stack_root_state_update_time
+        )
 
     @set_ev_cls(EventFaucetEventSockHeartbeat, MAIN_DISPATCHER)
     @kill_on_exception(exc_logname)
     def _event_socket_heartbeat(self, _):
         self.valves_manager.event_socket_heartbeat()
 
     @set_ev_cls(EventFaucetResolveGateways, MAIN_DISPATCHER)
@@ -234,44 +269,50 @@
     @set_ev_cls(EventFaucetFastStateExpire, MAIN_DISPATCHER)
     @set_ev_cls(EventFaucetAdvertise, MAIN_DISPATCHER)
     @set_ev_cls(EventFaucetFastAdvertise, MAIN_DISPATCHER)
     @kill_on_exception(exc_logname)
     def _valve_flow_services(self, ryu_event):
         """Call a method on all Valves and send any resulting flows."""
         self.valves_manager.valve_flow_services(
-            time.time(),
-            self._VALVE_SERVICES[type(ryu_event)][0])
+            time.time(), self._VALVE_SERVICES[type(ryu_event)][0]
+        )
 
-    @set_ev_cls(ofp_event.EventOFPPacketIn, MAIN_DISPATCHER)  # pylint: disable=no-member
+    @set_ev_cls(
+        ofp_event.EventOFPPacketIn, MAIN_DISPATCHER  # pylint: disable=no-member
+    )
     @kill_on_exception(exc_logname)
     def packet_in_handler(self, ryu_event):
         """Handle a packet in event from the dataplane.
 
         Args:
             ryu_event (ryu.controller.event.EventReplyBase): packet in message.
         """
         valve, _, msg = self._get_valve(ryu_event, require_running=True)
         if valve is None:
             return
         self.valves_manager.valve_packet_in(ryu_event.timestamp, valve, msg)
 
-    @set_ev_cls(ofp_event.EventOFPErrorMsg, MAIN_DISPATCHER)  # pylint: disable=no-member
+    @set_ev_cls(
+        ofp_event.EventOFPErrorMsg, MAIN_DISPATCHER  # pylint: disable=no-member
+    )
     @kill_on_exception(exc_logname)
     def error_handler(self, ryu_event):
         """Handle an OFPError from a datapath.
 
         Args:
             ryu_event (ryu.controller.ofp_event.EventOFPErrorMsg): trigger
         """
         valve, _, msg = self._get_valve(ryu_event)
         if valve is None:
             return
         valve.oferror(msg)
 
-    @set_ev_cls(ofp_event.EventOFPSwitchFeatures, CONFIG_DISPATCHER)  # pylint: disable=no-member
+    @set_ev_cls(
+        ofp_event.EventOFPSwitchFeatures, CONFIG_DISPATCHER  # pylint: disable=no-member
+    )
     @kill_on_exception(exc_logname)
     def features_handler(self, ryu_event):
         """Handle receiving a switch features message from a datapath.
 
         Args:
             ryu_event (ryu.controller.ofp_event.EventOFPStateChange): trigger.
         """
@@ -288,78 +329,94 @@
             ryu_event (ryu.controller.ofp_event.Event)
         """
         now = time.time()
         valve, ryu_dp, _ = self._get_valve(ryu_event)
         if valve is None:
             return
         discovered_up_ports = {
-            port.port_no for port in list(ryu_dp.ports.values())
-            if (valve_of.port_status_from_state(port.state)
-                and not valve_of.ignore_port(port.port_no))}
+            port.port_no
+            for port in list(ryu_dp.ports.values())
+            if (
+                valve_of.port_status_from_state(port.state)
+                and not valve_of.ignore_port(port.port_no)
+            )
+        }
         self._send_flow_msgs(
-            valve, self.valves_manager.datapath_connect(now, valve, discovered_up_ports))
+            valve, self.valves_manager.datapath_connect(now, valve, discovered_up_ports)
+        )
         self.valves_manager.update_config_applied({valve.dp.dp_id: True})
 
     @kill_on_exception(exc_logname)
     def _datapath_disconnect(self, ryu_event):
         """Handle any/all disconnection of a datapath.
 
         Args:
             ryu_event (ryu.controller.ofp_event.Event)
         """
         valve, _, _ = self._get_valve(ryu_event)
         if valve is None:
             return
         valve.datapath_disconnect(time.time())
 
-    @set_ev_cls(ofp_event.EventOFPDescStatsReply, MAIN_DISPATCHER)  # pylint: disable=no-member
+    @set_ev_cls(
+        ofp_event.EventOFPDescStatsReply, MAIN_DISPATCHER  # pylint: disable=no-member
+    )
     @kill_on_exception(exc_logname)
     def desc_stats_reply_handler(self, ryu_event):
         """Handle OFPDescStatsReply from datapath.
 
         Args:
             ryu_event (ryu.controller.ofp_event.EventOFPDescStatsReply): trigger.
         """
         valve, _, msg = self._get_valve(ryu_event)
         if valve is None:
             return
         valve.ofdescstats_handler(msg.body)
 
-    @set_ev_cls(ofp_event.EventOFPPortDescStatsReply, CONFIG_DISPATCHER)  # pylint: disable=no-member
+    @set_ev_cls(
+        ofp_event.EventOFPPortDescStatsReply,  # pylint: disable=no-member
+        CONFIG_DISPATCHER,
+    )
     @kill_on_exception(exc_logname)
     def port_desc_stats_reply_handler(self, ryu_event):
         """Handle OFPPortDescStatsReply from datapath.
 
         Args:
             ryu_event (ryu.controller.ofp_event.EventOFPPortDescStatsReply): trigger.
         """
         valve, _, msg = self._get_valve(ryu_event)
         if valve is None:
             return
         self.valves_manager.port_desc_stats_reply_handler(valve, msg, time.time())
 
-    @set_ev_cls(ofp_event.EventOFPPortStatus, MAIN_DISPATCHER)  # pylint: disable=no-member
+    @set_ev_cls(
+        ofp_event.EventOFPPortStatus, MAIN_DISPATCHER  # pylint: disable=no-member
+    )
     @kill_on_exception(exc_logname)
     def port_status_handler(self, ryu_event):
         """Handle a port status change event.
 
         Args:
             ryu_event (ryu.controller.ofp_event.EventOFPPortStatus): trigger.
         """
         valve, _, msg = self._get_valve(ryu_event, require_running=True)
         if valve is None:
             return
         self.valves_manager.port_status_handler(valve, msg, time.time())
 
-    @set_ev_cls(ofp_event.EventOFPFlowRemoved, MAIN_DISPATCHER)  # pylint: disable=no-member
+    @set_ev_cls(
+        ofp_event.EventOFPFlowRemoved, MAIN_DISPATCHER  # pylint: disable=no-member
+    )
     @kill_on_exception(exc_logname)
     def flowremoved_handler(self, ryu_event):
         """Handle a flow removed event.
 
         Args:
             ryu_event (ryu.controller.ofp_event.EventOFPFlowRemoved): trigger.
         """
         valve, ryu_dp, msg = self._get_valve(ryu_event, require_running=True)
         if valve is None:
             return
         if msg.reason == ryu_dp.ofproto.OFPRR_IDLE_TIMEOUT:
-            self._send_flow_msgs(valve, valve.flow_timeout(time.time(), msg.table_id, msg.match))
+            self._send_flow_msgs(
+                valve, valve.flow_timeout(time.time(), msg.table_id, msg.match)
+            )
```

### Comparing `c65faucet-1.0.49/faucet/faucet_bgp.py` & `c65faucet-1.0.50/faucet/faucet_bgp.py`

 * *Files 3% similar despite different names*

```diff
@@ -30,16 +30,19 @@
 
     def __init__(self, dp_id, vlan_vid, ipv):
         self.dp_id = dp_id
         self.vlan_vid = vlan_vid
         self.ipv = ipv
 
     def __str__(self):
-        return 'BGP speaker key DP ID: %u, VLAN VID: %u, IP version: %u' % (
-            self.dp_id, self.vlan_vid, self.ipv)
+        return "BGP speaker key DP ID: %u, VLAN VID: %u, IP version: %u" % (
+            self.dp_id,
+            self.vlan_vid,
+            self.ipv,
+        )
 
     def __repr__(self):
         return self.__str__()
 
     def __hash__(self):
         return hash(self.__str__())
 
@@ -77,19 +80,19 @@
         neighbor_states = []
         if bgp_speaker is not None:
             neighbor_states = bgp_speaker.neighbor_states()
         return neighbor_states
 
     @kill_on_exception(exc_logname)
     def _bgp_up_handler(self, remote_ip, remote_as):
-        self.logger.info('BGP peer router ID %s AS %s up' % (remote_ip, remote_as))
+        self.logger.info("BGP peer router ID %s AS %s up" % (remote_ip, remote_as))
 
     @kill_on_exception(exc_logname)
     def _bgp_down_handler(self, remote_ip, remote_as):
-        self.logger.info('BGP peer router ID %s AS %s down' % (remote_ip, remote_as))
+        self.logger.info("BGP peer router ID %s AS %s down" % (remote_ip, remote_as))
         # TODO: delete RIB routes for down peer.
 
     @kill_on_exception(exc_logname)
     def _bgp_route_handler(self, path_change, bgp_speaker_key):
         """Handle a BGP change event.
 
         Args:
@@ -97,118 +100,138 @@
         """
         dp_id = bgp_speaker_key.dp_id
         vlan_vid = bgp_speaker_key.vlan_vid
         valve, vlan = self._valve_vlan(dp_id, vlan_vid)
         if vlan is None:
             return
         prefix = ipaddress.ip_network(str(path_change.prefix))
-        route_str = 'BGP route %s' % prefix
+        route_str = "BGP route %s" % prefix
 
         if path_change.next_hop:
             nexthop = ipaddress.ip_address(str(path_change.next_hop))
-            route_str = 'BGP route %s nexthop %s' % (prefix, nexthop)
+            route_str = "BGP route %s nexthop %s" % (prefix, nexthop)
 
             if vlan.is_faucet_vip(nexthop):
                 self.logger.error(
-                    'Skipping %s because nexthop cannot be us' % route_str)
+                    "Skipping %s because nexthop cannot be us" % route_str
+                )
                 return
 
             if valve.router_vlan_for_ip_gw(vlan, nexthop) is None:
                 self.logger.info(
-                    'Skipping %s because nexthop not in %s' % (route_str, vlan))
+                    "Skipping %s because nexthop not in %s" % (route_str, vlan)
+                )
                 return
 
         if bgp_speaker_key not in self._dp_bgp_rib:
             self._dp_bgp_rib[bgp_speaker_key] = {}
 
         flowmods = []
         if path_change.is_withdraw:
-            self.logger.info('withdraw %s', route_str)
+            self.logger.info("withdraw %s", route_str)
             if prefix in self._dp_bgp_rib[bgp_speaker_key]:
                 del self._dp_bgp_rib[bgp_speaker_key][prefix]
             flowmods = valve.del_route(vlan, prefix)
         else:
-            self.logger.info('add %s', route_str)
+            self.logger.info("add %s", route_str)
             self._dp_bgp_rib[bgp_speaker_key][prefix] = nexthop
             flowmods = valve.add_route(vlan, nexthop, prefix)
         if flowmods:
             self._send_flow_msgs(valve, flowmods)
 
     @staticmethod
     def _vlan_prefixes_by_ipv(vlan, ipv):
         vlan_prefixes = [
-            (str(faucet_vip), str(faucet_vip.ip)) for faucet_vip in vlan.faucet_vips_by_ipv(ipv)]
-        vlan_prefixes.extend([
-            (str(ip_dst), str(ip_gw)) for ip_dst, ip_gw in vlan.routes_by_ipv(ipv).items()])
+            (str(faucet_vip), str(faucet_vip.ip))
+            for faucet_vip in vlan.faucet_vips_by_ipv(ipv)
+        ]
+        vlan_prefixes.extend(
+            [
+                (str(ip_dst), str(ip_gw))
+                for ip_dst, ip_gw in vlan.routes_by_ipv(ipv).items()
+            ]
+        )
         return vlan_prefixes
 
     def _create_bgp_speaker_for_vlan(self, bgp_speaker_key, bgp_router):
         """Set up BGP speaker for an individual VLAN if required.
 
         Args:
             bgp_speaker_key (BgpSpeakerKey): BGP speaker key.
             bgp_router: Router.
         Returns:
             ryu.services.protocols.bgp.bgpspeaker.BGPSpeaker: BGP speaker.
         """
-        server_address = sorted(bgp_router.bgp_server_addresses_by_ipv(bgp_speaker_key.ipv))[0]
+        server_address = sorted(
+            bgp_router.bgp_server_addresses_by_ipv(bgp_speaker_key.ipv)
+        )[0]
         beka = Beka(
             local_address=str(server_address),
             bgp_port=bgp_router.bgp_port(),
             local_as=bgp_router.bgp_as(),
             router_id=bgp_router.bgp_routerid(),
             peer_up_handler=self._bgp_up_handler,
             peer_down_handler=self._bgp_down_handler,
             route_handler=lambda x: self._bgp_route_handler(x, bgp_speaker_key),
-            error_handler=self.logger.warning)
-        for ip_dst, ip_gw in self._vlan_prefixes_by_ipv(bgp_router.bgp_vlan(), bgp_speaker_key.ipv):
+            error_handler=self.logger.warning,
+        )
+        for ip_dst, ip_gw in self._vlan_prefixes_by_ipv(
+            bgp_router.bgp_vlan(), bgp_speaker_key.ipv
+        ):
             beka.add_route(prefix=str(ip_dst), next_hop=str(ip_gw))
-        for bgp_neighbor_address in bgp_router.bgp_neighbor_addresses_by_ipv(bgp_speaker_key.ipv):
+        for bgp_neighbor_address in bgp_router.bgp_neighbor_addresses_by_ipv(
+            bgp_speaker_key.ipv
+        ):
             beka.add_neighbor(
                 connect_mode=bgp_router.bgp_connect_mode(),
                 peer_ip=str(bgp_neighbor_address),
-                peer_as=bgp_router.bgp_neighbor_as())
+                peer_as=bgp_router.bgp_neighbor_as(),
+            )
         self.thread = hub.spawn(beka.run)
-        self.thread.name = 'beka'
+        self.thread.name = "beka"
         return beka
 
     def shutdown_bgp_speakers(self):
         """Shutdown any active BGP speakers."""
         for bgp_speaker in self._dp_bgp_speakers.values():
             bgp_speaker.shutdown()
         self._dp_bgp_speakers = {}
 
     def _add_bgp_speaker(self, valve, bgp_speaker_key, bgp_router):
         if bgp_speaker_key in self._dp_bgp_speakers:
-            self.logger.info('Skipping re/configuration of existing %s' % bgp_speaker_key)
+            self.logger.info(
+                "Skipping re/configuration of existing %s" % bgp_speaker_key
+            )
             bgp_speaker = self._dp_bgp_speakers[bgp_speaker_key]
             if bgp_speaker_key in self._dp_bgp_rib:
                 # Re-add routes (to avoid flapping BGP even when VLAN cold starts).
                 for prefix, nexthop in self._dp_bgp_rib[bgp_speaker_key].items():
-                    self.logger.info('Re-adding %s via %s' % (prefix, nexthop))
+                    self.logger.info("Re-adding %s via %s" % (prefix, nexthop))
                     bgp_vlan = bgp_router.bgp_vlan()
                     flowmods = valve.add_route(bgp_vlan, nexthop, prefix)
                     if flowmods:
                         self._send_flow_msgs(valve, flowmods)
         else:
-            self.logger.info('Adding %s' % bgp_speaker_key)
+            self.logger.info("Adding %s" % bgp_speaker_key)
             bgp_speaker = self._create_bgp_speaker_for_vlan(bgp_speaker_key, bgp_router)
         return {bgp_speaker_key: bgp_speaker}
 
     def _add_valve_bgp_speakers(self, valve):
         bgp_speakers = {}
         bgp_routers = valve.dp.bgp_routers()
         if bgp_routers:
             dp_id = valve.dp.dp_id
             for bgp_router in bgp_routers:
                 bgp_vlan = bgp_router.bgp_vlan()
                 vlan_vid = bgp_vlan.vid
                 for ipv in bgp_router.bgp_ipvs():
                     bgp_speaker_key = BgpSpeakerKey(dp_id, vlan_vid, ipv)
-                    bgp_speakers.update(self._add_bgp_speaker(valve, bgp_speaker_key, bgp_router))
+                    bgp_speakers.update(
+                        self._add_bgp_speaker(valve, bgp_speaker_key, bgp_router)
+                    )
         return bgp_speakers
 
     def reset(self, valves):
         """Set up a BGP speaker for every VLAN that requires it."""
         # TODO: port status changes should cause us to withdraw a route.
         new_dp_bgp_speakers = {}
         if valves:
@@ -229,12 +252,17 @@
             ipv = bgp_speaker_key.ipv
             valve, vlan = self._valve_vlan(dp_id, vlan_vid)
             if vlan is None:
                 continue
             neighbor_states = self._neighbor_states(bgp_speaker)
             for neighbor, neighbor_state in neighbor_states:
                 neighbor_labels = dict(
-                    valve.dp.base_prom_labels(), vlan=vlan.vid, neighbor=neighbor)
+                    valve.dp.base_prom_labels(), vlan=vlan.vid, neighbor=neighbor
+                )
                 self.metrics.bgp_neighbor_uptime_seconds.labels(  # pylint: disable=no-member
-                    **neighbor_labels).set(neighbor_state['info']['uptime'])
+                    **neighbor_labels
+                ).set(
+                    neighbor_state["info"]["uptime"]
+                )
                 self.metrics.bgp_neighbor_routes.labels(  # pylint: disable=no-member
-                    **dict(neighbor_labels, ipv=ipv)).set(vlan.route_count_by_ipv(ipv))
+                    **dict(neighbor_labels, ipv=ipv)
+                ).set(vlan.route_count_by_ipv(ipv))
```

### Comparing `c65faucet-1.0.49/faucet/faucet_dot1x.py` & `c65faucet-1.0.50/faucet/faucet_dot1x.py`

 * *Files 7% similar despite different names*

```diff
@@ -27,17 +27,17 @@
     Args:
         valve_index (int): The internally used id of the valve.
         port_num (int): port number
 
     Returns:
         str
     """
-    two_byte_port_num = ("%04x" % port_num)
-    two_byte_port_num_formatted = two_byte_port_num[:2] + ':' + two_byte_port_num[2:]
-    return '00:00:00:%02x:%s' % (valve_index, two_byte_port_num_formatted)
+    two_byte_port_num = "%04x" % port_num
+    two_byte_port_num_formatted = two_byte_port_num[:2] + ":" + two_byte_port_num[2:]
+    return "00:00:00:%02x:%s" % (valve_index, two_byte_port_num_formatted)
 
 
 class FaucetDot1x:  # pylint: disable=too-many-instance-attributes
     """Wrapper for experimental Chewie 802.1x authenticator."""
 
     exc_logname = None
 
@@ -51,33 +51,42 @@
 
         self._send_flow_msgs = send_flow_msgs
         self._valves = None
         self._dot1x_speaker = None
         self._auth_acl_name = None
         self._noauth_acl_name = None
 
-    def _create_dot1x_speaker(self, dot1x_intf, chewie_id, radius_ip, radius_port, radius_secret):
+    def _create_dot1x_speaker(
+        self, dot1x_intf, chewie_id, radius_ip, radius_port, radius_secret
+    ):
         """
 
         Args:
             dot1x_intf (str):
             chewie_id (str):
             radius_ip (str):
             radius_port (int):
             radius_secret (str):
 
         Returns:
             Chewie
         """
         _chewie = chewie.Chewie(
-            dot1x_intf, self.logger,
-            self.auth_handler, self.failure_handler, self.logoff_handler,
-            radius_ip, radius_port, radius_secret, chewie_id)
+            dot1x_intf,
+            self.logger,
+            self.auth_handler,
+            self.failure_handler,
+            self.logoff_handler,
+            radius_ip,
+            radius_port,
+            radius_secret,
+            chewie_id,
+        )
         self.thread = hub.spawn(_chewie.run)
-        self.thread.name = 'chewie'
+        self.thread.name = "chewie"
         return _chewie
 
     def _get_valve_and_port(self, port_id):
         """Finds the valve and port that this address corresponds to
         Args:
             port_id: is a macaddress string"""
         valve, port = self.mac_to_port[port_id]
@@ -88,66 +97,87 @@
         auth_acl = datapath.acls.get(self._auth_acl_name)
         noauth_acl = datapath.acls.get(self._noauth_acl_name)
         return (auth_acl, noauth_acl)
 
     # Loggin Methods
     def log_auth_event(self, valve, port_num, mac_str, status):
         """Log an authentication attempt event"""
-        self.metrics.inc_var('dp_dot1x_{}'.format(status), valve.dp.base_prom_labels())
-        self.metrics.inc_var('port_dot1x_{}'.format(status), valve.dp.port_labels(port_num))
+        self.metrics.inc_var("dp_dot1x_{}".format(status), valve.dp.base_prom_labels())
+        self.metrics.inc_var(
+            "port_dot1x_{}".format(status), valve.dp.port_labels(port_num)
+        )
         self.logger.info(
-            '{} from MAC {} on {}'.format(status.capitalize(), mac_str, port_num))
-        valve.dot1x_event({'AUTHENTICATION': {'dp_id': valve.dp.dp_id,
-                                              'port': port_num,
-                                              'eth_src': mac_str,
-                                              'status': status}})
+            "{} from MAC {} on {}".format(status.capitalize(), mac_str, port_num)
+        )
+        valve.dot1x_event(
+            {
+                "AUTHENTICATION": {
+                    "dp_id": valve.dp.dp_id,
+                    "port": port_num,
+                    "eth_src": mac_str,
+                    "status": status,
+                }
+            }
+        )
 
     def log_port_event(self, event_type, port_type, valve, port_num):
         """Log a dot1x port event"""
-        valve.dot1x_event({event_type: {'dp_id': valve.dp.dp_id,
-                                        'port': port_num,
-                                        'port_type': port_type}})
+        valve.dot1x_event(
+            {
+                event_type: {
+                    "dp_id": valve.dp.dp_id,
+                    "port": port_num,
+                    "port_type": port_type,
+                }
+            }
+        )
 
     @kill_on_exception(exc_logname)
-    def auth_handler(self, address, port_id, *args, **kwargs):  # pylint: disable=unused-argument
+    def auth_handler(
+        self, address, port_id, *args, **kwargs
+    ):  # pylint: disable=unused-argument
         """Callback for when a successful auth happens."""
         address_str = str(address)
         valve, dot1x_port = self._get_valve_and_port(port_id)
         port_num = dot1x_port.number
 
-        self.log_auth_event(valve, port_num, address_str, 'success')
-        flowmods = self._get_login_flowmod(dot1x_port, valve, address_str,
-                                           kwargs.get('vlan_name', None),
-                                           kwargs.get('filter_id', None))
+        self.log_auth_event(valve, port_num, address_str, "success")
+        flowmods = self._get_login_flowmod(
+            dot1x_port,
+            valve,
+            address_str,
+            kwargs.get("vlan_name", None),
+            kwargs.get("filter_id", None),
+        )
         if flowmods:
             self._send_flow_msgs(valve, flowmods)
 
     @kill_on_exception(exc_logname)
     def logoff_handler(self, address, port_id):
         """Callback for when an EAP logoff happens."""
         address_str = str(address)
         valve, dot1x_port = self._get_valve_and_port(port_id)
         port_num = dot1x_port.number
 
-        self.log_auth_event(valve, port_num, address_str, 'logoff')
+        self.log_auth_event(valve, port_num, address_str, "logoff")
 
         flowmods = self._get_logoff_flowmod(dot1x_port, valve, address_str)
 
         if flowmods:
             self._send_flow_msgs(valve, flowmods)
 
     @kill_on_exception(exc_logname)
     def failure_handler(self, address, port_id):
         """Callback for when a EAP failure happens."""
         address_str = str(address)
 
         valve, dot1x_port = self._get_valve_and_port(port_id)
         port_num = dot1x_port.number
 
-        self.log_auth_event(valve, port_num, address_str, 'failure')
+        self.log_auth_event(valve, port_num, address_str, "failure")
         flowmods = self._get_logoff_flowmod(dot1x_port, valve, address_str)
 
         if flowmods:
             self._send_flow_msgs(valve, flowmods)
 
     def set_mac_str(self, valve, valve_index, port_num):
         """
@@ -171,23 +201,23 @@
             dot1x_ports (Iterable of Port objects):
             nfv_sw_port (Port):
 
         Returns:
             list of flowmods
         """
         self._dot1x_speaker.port_down(
-            get_mac_str(self.dp_id_to_valve_index[dp_id], nfv_sw_port.number))
+            get_mac_str(self.dp_id_to_valve_index[dp_id], nfv_sw_port.number)
+        )
         valve = self._valves[dp_id]
 
-        self.log_port_event("PORT_UP", 'nfv', valve, nfv_sw_port.number)
+        self.log_port_event("PORT_UP", "nfv", valve, nfv_sw_port.number)
 
         ret = []
         for port in dot1x_ports:
-            ret.extend(self.create_flow_pair(
-                dp_id, port, nfv_sw_port, valve))
+            ret.extend(self.create_flow_pair(dp_id, port, nfv_sw_port, valve))
         return ret
 
     def port_up(self, dp_id, dot1x_port, nfv_sw_port):
         """Setup the dot1x forward port acls.
         Args:
             dp_id (int):
             dot1x_port (Port):
@@ -198,20 +228,19 @@
         """
         port_num = dot1x_port.number
 
         mac_str = get_mac_str(self.dp_id_to_valve_index[dp_id], port_num)
         self._dot1x_speaker.port_up(mac_str)
         valve = self._valves[dp_id]
 
-        self.log_port_event("PORT_UP", 'supplicant', valve, port_num)
+        self.log_port_event("PORT_UP", "supplicant", valve, port_num)
 
         # Dealing with ACLs
         flowmods = []
-        flowmods.extend(self.create_flow_pair(
-            dp_id, dot1x_port, nfv_sw_port, valve))
+        flowmods.extend(self.create_flow_pair(dp_id, dot1x_port, nfv_sw_port, valve))
 
         flowmods.extend(self._add_unauthenticated_flowmod(dot1x_port, valve))
 
         if dot1x_port.dot1x_mab:
             self.logger.info("Port % is using Mac Auth Bypass", dot1x_port.number)
             flowmods.append(self.create_mab_flow(dp_id, dot1x_port, nfv_sw_port, valve))
 
@@ -230,15 +259,17 @@
         Returns:
             list
         """
         acl_manager = valve.acl_manager
         if dot1x_port.running():
             valve_index = self.dp_id_to_valve_index[dp_id]
             mac = get_mac_str(valve_index, dot1x_port.number)
-            return acl_manager.create_mab_flow(dot1x_port.number, nfv_sw_port.number, mac)
+            return acl_manager.create_mab_flow(
+                dot1x_port.number, nfv_sw_port.number, mac
+            )
         return []
 
     def create_flow_pair(self, dp_id, dot1x_port, nfv_sw_port, valve):
         """Creates the pair of flows that redirects the eapol packets to/from
         the supplicant and nfv port
 
         Args:
@@ -251,15 +282,16 @@
             list
         """
         acl_manager = valve.acl_manager
         if dot1x_port.running():
             valve_index = self.dp_id_to_valve_index[dp_id]
             mac = get_mac_str(valve_index, dot1x_port.number)
             return acl_manager.create_dot1x_flow_pair(
-                dot1x_port.number, nfv_sw_port.number, mac)
+                dot1x_port.number, nfv_sw_port.number, mac
+            )
         return []
 
     def port_down(self, dp_id, dot1x_port, nfv_sw_port):
         """
         Remove the acls added by FaucetDot1x.get_port_acls
         Args:
             dp_id (int):
@@ -273,93 +305,127 @@
         port_num = dot1x_port.number
 
         mac = get_mac_str(valve_index, port_num)
         self._dot1x_speaker.port_down(mac)
 
         valve = self._valves[dp_id]
         acl_manager = valve.acl_manager
-        self.log_port_event("PORT_DOWN", 'supplicant', valve, port_num)
+        self.log_port_event("PORT_DOWN", "supplicant", valve, port_num)
 
         flowmods = []
         flowmods.extend(self._del_authenticated_flowmod(dot1x_port, valve, mac))
         flowmods.extend(self._del_unauthenticated_flowmod(dot1x_port, valve))
         # NOTE: The flow_pair are not included in unauthed flowmod
-        flowmods.extend(acl_manager.del_mab_flow(dot1x_port.number, nfv_sw_port.number, mac))
-        flowmods.extend(acl_manager.del_dot1x_flow_pair(dot1x_port.number, nfv_sw_port.number, mac))
+        flowmods.extend(
+            acl_manager.del_mab_flow(dot1x_port.number, nfv_sw_port.number, mac)
+        )
+        flowmods.extend(
+            acl_manager.del_dot1x_flow_pair(dot1x_port.number, nfv_sw_port.number, mac)
+        )
         return flowmods
 
     def reset(self, valves):
         """Set up a dot1x speaker."""
         self._valves = valves
         dot1x_valves = [
-            valve for valve in valves.values() if valve.dp.dot1x and valve.dp.dot1x_ports()]
-        assert len(dot1x_valves) < 255, 'dot1x not supported for > 255 DPs'
+            valve
+            for valve in valves.values()
+            if valve.dp.dot1x and valve.dp.dot1x_ports()
+        ]
+        assert len(dot1x_valves) < 255, "dot1x not supported for > 255 DPs"
         if not dot1x_valves:
             return
 
         first_valve = dot1x_valves[0]
-        dot1x_intf = first_valve.dp.dot1x['nfv_intf']
-        radius_ip = first_valve.dp.dot1x['radius_ip']
-        radius_port = first_valve.dp.dot1x['radius_port']
-        radius_secret = first_valve.dp.dot1x['radius_secret']
+        dot1x_intf = first_valve.dp.dot1x["nfv_intf"]
+        radius_ip = first_valve.dp.dot1x["radius_ip"]
+        radius_port = first_valve.dp.dot1x["radius_port"]
+        radius_secret = first_valve.dp.dot1x["radius_secret"]
 
-        self._auth_acl_name = first_valve.dp.dot1x.get('auth_acl')
-        self._noauth_acl_name = first_valve.dp.dot1x.get('noauth_acl')
+        self._auth_acl_name = first_valve.dp.dot1x.get("auth_acl")
+        self._noauth_acl_name = first_valve.dp.dot1x.get("noauth_acl")
 
         self._dot1x_speaker = self._create_dot1x_speaker(
-            dot1x_intf, first_valve.dp.faucet_dp_mac,
-            radius_ip, radius_port, radius_secret)
+            dot1x_intf,
+            first_valve.dp.faucet_dp_mac,
+            radius_ip,
+            radius_port,
+            radius_secret,
+        )
 
         for valve_index, valve in enumerate(dot1x_valves, start=0):
             self.dp_id_to_valve_index[valve.dp.dp_id] = valve_index
             for dot1x_port in valve.dp.dot1x_ports():
                 self.set_mac_str(valve, valve_index, dot1x_port.number)
                 self.logger.info(
-                    'dot1x enabled on %s (%s) port %s, NFV interface %s' % (
-                        valve.dp, valve_index, dot1x_port, dot1x_intf))
+                    "dot1x enabled on %s (%s) port %s, NFV interface %s"
+                    % (valve.dp, valve_index, dot1x_port, dot1x_intf)
+                )
 
-            valve.dot1x_event({'ENABLED': {'dp_id': valve.dp.dp_id}})
+            valve.dot1x_event({"ENABLED": {"dp_id": valve.dp.dp_id}})
 
     def _get_logoff_flowmod(self, dot1x_port, valve, mac_str):
         """Return flowmods required to logoff port"""
         flowmods = []
-        flowmods.extend(
-            self._del_authenticated_flowmod(dot1x_port, valve, mac_str))
-        flowmods.extend(
-            self._add_unauthenticated_flowmod(dot1x_port, valve))
+        flowmods.extend(self._del_authenticated_flowmod(dot1x_port, valve, mac_str))
+        flowmods.extend(self._add_unauthenticated_flowmod(dot1x_port, valve))
         return flowmods
 
-    def _get_login_flowmod(self, dot1x_port, valve,  # pylint: disable=too-many-arguments
-                           mac_str, vlan_name, acl_name):
+    def _get_login_flowmod(
+        self,
+        dot1x_port,
+        valve,  # pylint: disable=too-many-arguments
+        mac_str,
+        vlan_name,
+        acl_name,
+    ):
         """Return flowmods required to login port"""
         flowmods = []
+        flowmods.extend(self._del_unauthenticated_flowmod(dot1x_port, valve))
         flowmods.extend(
-            self._del_unauthenticated_flowmod(dot1x_port, valve))
-        flowmods.extend(
-            self._add_authenticated_flowmod(dot1x_port, valve, mac_str, vlan_name, acl_name))
+            self._add_authenticated_flowmod(
+                dot1x_port, valve, mac_str, vlan_name, acl_name
+            )
+        )
         return flowmods
 
-    def _add_authenticated_flowmod(self, dot1x_port, valve,  # pylint: disable=too-many-arguments
-                                   mac_str, vlan_name, acl_name):
+    def _add_authenticated_flowmod(
+        self,
+        dot1x_port,
+        valve,  # pylint: disable=too-many-arguments
+        mac_str,
+        vlan_name,
+        acl_name,
+    ):
         """Return flowmods for successful authentication on port"""
         port_num = dot1x_port.number
         flowmods = []
         acl_manager = valve.acl_manager
 
         acl = valve.dp.acls.get(acl_name, None)
         if dot1x_port.dot1x_dyn_acl and acl:
-            self.logger.info("DOT1X_DYN_ACL: Adding ACL '{0}' for port '{1}'".format(
-                acl_name, port_num))
-            self.logger.debug("DOT1X_DYN_ACL: ACL contents: '{0}'".format(str(acl.__dict__)))
+            self.logger.info(
+                "DOT1X_DYN_ACL: Adding ACL '{0}' for port '{1}'".format(
+                    acl_name, port_num
+                )
+            )
+            self.logger.debug(
+                "DOT1X_DYN_ACL: ACL contents: '{0}'".format(str(acl.__dict__))
+            )
             flowmods.extend(acl_manager.add_port_acl(acl, port_num, mac_str))
         elif dot1x_port.dot1x_acl:
             auth_acl, _ = self._get_acls(valve.dp)
-            self.logger.info("DOT1X_PRE_ACL: Adding ACL '{0}' for port '{1}'".format(
-                acl_name, port_num))
-            self.logger.debug("DOT1X_PRE_ACL: ACL contents: '{0}'".format(str(auth_acl.__dict__)))
+            self.logger.info(
+                "DOT1X_PRE_ACL: Adding ACL '{0}' for port '{1}'".format(
+                    acl_name, port_num
+                )
+            )
+            self.logger.debug(
+                "DOT1X_PRE_ACL: ACL contents: '{0}'".format(str(auth_acl.__dict__))
+            )
             flowmods.extend(acl_manager.add_port_acl(auth_acl, port_num, mac_str))
         else:
             flowmods.extend(acl_manager.add_authed_mac(port_num, mac_str))
 
         if vlan_name:
             flowmods.extend(valve.add_dot1x_native_vlan(port_num, vlan_name))
         return flowmods
@@ -385,21 +451,25 @@
     def _add_unauthenticated_flowmod(self, dot1x_port, valve, mac_str=None):
         """Return flowmods default on a port"""
         flowmods = []
         acl_manager = valve.acl_manager
 
         if dot1x_port.dot1x_acl:
             _, noauth_acl = self._get_acls(valve.dp)
-            flowmods.extend(acl_manager.add_port_acl(noauth_acl, dot1x_port.number, mac_str))
+            flowmods.extend(
+                acl_manager.add_port_acl(noauth_acl, dot1x_port.number, mac_str)
+            )
 
         return flowmods
 
     def _del_unauthenticated_flowmod(self, dot1x_port, valve, mac_str=None):
         """Return flowmods for deleting default / unauthenticated flows from a port"""
         flowmods = []
         acl_manager = valve.acl_manager
 
         if dot1x_port.dot1x_acl:
             _, noauth_acl = self._get_acls(valve.dp)
-            flowmods.extend(acl_manager.del_port_acl(noauth_acl, dot1x_port.number, mac_str))
+            flowmods.extend(
+                acl_manager.del_port_acl(noauth_acl, dot1x_port.number, mac_str)
+            )
 
         return flowmods
```

### Comparing `c65faucet-1.0.49/faucet/faucet_event.py` & `c65faucet-1.0.50/faucet/faucet_event.py`

 * *Files 6% similar despite different names*

```diff
@@ -59,81 +59,86 @@
         self.thread = None
         self.lock = NonBlockLock()
         self.event_q = eventlet.queue.Queue(120)
 
     def start(self):
         """Start socket server."""
         if self.socket_path:
-            stream_server = StreamServer((self.socket_path, None), self._loop).serve_forever
+            stream_server = StreamServer(
+                (self.socket_path, None), self._loop
+            ).serve_forever
             self.thread = hub.spawn(stream_server)
-            self.thread.name = 'event'
+            self.thread.name = "event"
         return self.thread
 
     def _loop(self, sock, _addr):
         """Serve events."""
         with self.lock.acquire_nonblock() as result:
             if not result:
-                self.logger.info('multiple event clients not supported')
+                self.logger.info("multiple event clients not supported")
             else:
-                self.logger.info('event client connected')
+                self.logger.info("event client connected")
                 while True:
                     event = self.event_q.get()
                     event_bytes = bytes(
-                        '\n'.join((json.dumps(event, default=str), '')).encode('UTF-8'))
+                        "\n".join((json.dumps(event, default=str), "")).encode("UTF-8")
+                    )
                     try:
                         sock.sendall(event_bytes)
                     except (socket.error, IOError) as err:
-                        self.logger.info('event client disconnected: %s', err)
+                        self.logger.info("event client disconnected: %s", err)
                         break
         try:
             sock.close()
         except (socket.error, IOError):
             pass
 
     def get_event(self):
-        assert self.thread is None, 'not allowed with async _loop'
+        assert self.thread is None, "not allowed with async _loop"
         return None if self.event_q.empty() else self.event_q.get()
 
     def notify(self, dp_id, dp_name, event_dict):
         """Notify of an event."""
         assert isinstance(event_dict, dict)
         self.event_id += 1
         event = {
-            'version': 1,
-            'time': time.time(),
-            'dp_id': dp_id,
-            'dp_name': dp_name,
-            'event_id': self.event_id,
+            "version": 1,
+            "time": time.time(),
+            "dp_id": dp_id,
+            "dp_name": dp_name,
+            "event_id": self.event_id,
         }
         for header_key in list(event):
             assert header_key not in event_dict
         event.update(event_dict)
-        self.metrics.faucet_event_id.set(event['event_id'])
+        self.metrics.faucet_event_id.set(event["event_id"])
         if self.event_q.full():
             self.event_q.get()
         self.event_q.put(event)
 
     def check_path(self, socket_path):
         """Check that socket_path is valid."""
         if not socket_path:
             return None
         socket_path = os.path.abspath(socket_path)
         socket_dir = os.path.dirname(socket_path)
         # Create parent directories that don't exist.
         if not os.path.exists(socket_dir):
             try:
                 os.makedirs(socket_dir)
-            except (PermissionError) as err:  # pytype: disable=name-error
-                self.logger.error('Unable to create event socket directory: %s', err)
+            except PermissionError as err:  # pytype: disable=name-error
+                self.logger.error("Unable to create event socket directory: %s", err)
                 return None
         # Check directory permissions.
         if not os.access(socket_dir, os.R_OK | os.W_OK | os.X_OK):
-            self.logger.error('Incorrect permissions set on socket directory %s', socket_dir)
+            self.logger.error(
+                "Incorrect permissions set on socket directory %s", socket_dir
+            )
             return None
         # Remove stale socket file.
         if os.path.exists(socket_path):
             try:
                 os.remove(socket_path)
-            except (PermissionError) as err:  # pytype: disable=name-error
-                self.logger.error('Unable to remove old socket: %s', err)
+            except PermissionError as err:  # pytype: disable=name-error
+                self.logger.error("Unable to remove old socket: %s", err)
                 return None
         return socket_path
```

### Comparing `c65faucet-1.0.49/faucet/faucet_metrics.py` & `c65faucet-1.0.50/faucet/faucet_metrics.py`

 * *Files 20% similar despite different names*

```diff
@@ -28,197 +28,222 @@
     """Container class for objects that can be exported to Prometheus."""
 
     _dpid_counters = None  # type: dict
     _dpid_gauges = None  # type: dict
 
     def __init__(self, reg=None):
         super().__init__(reg=reg)
-        self.port_required_labels = self.REQUIRED_LABELS + ['port', 'port_description']
+        self.port_required_labels = self.REQUIRED_LABELS + ["port", "port_description"]
         self._dpid_counters = {}
         self._dpid_gauges = {}
         self.ryu_config = self._gauge(
-            'ryu_config',
-            'ryu configuration option', ['param'])
+            "ryu_config", "ryu configuration option", ["param"]
+        )
         self.faucet_stack_root_dpid = self._gauge(
-            'faucet_stack_root_dpid',
-            'set to current stack root DPID', [])
+            "faucet_stack_root_dpid", "set to current stack root DPID", []
+        )
         self.faucet_config_reload_requests = self._counter(
-            'faucet_config_reload_requests',
-            'number of config reload requests', [])
+            "faucet_config_reload_requests", "number of config reload requests", []
+        )
         self.faucet_config_load_error = self._gauge(
-            'faucet_config_load_error',
-            '1 if last attempt to re/load config failed', [])
+            "faucet_config_load_error", "1 if last attempt to re/load config failed", []
+        )
         self.faucet_config_hash = self._info(
-            'faucet_config_hash',
-            'file hashes for last successful config')
+            "faucet_config_hash", "file hashes for last successful config"
+        )
         self.faucet_config_hash_func = self._gauge(
-            'faucet_config_hash_func',
-            'algorithm used to compute config hashes', ['algorithm'])
+            "faucet_config_hash_func",
+            "algorithm used to compute config hashes",
+            ["algorithm"],
+        )
         self.faucet_config_applied = self._gauge(
-            'faucet_config_applied',
-            'fraction of DPs that we have tried to apply config to', [])
+            "faucet_config_applied",
+            "fraction of DPs that we have tried to apply config to",
+            [],
+        )
         self.faucet_event_id = self._gauge(
-            'faucet_event_id',
-            'highest/most recent event ID to be sent', [])
+            "faucet_event_id", "highest/most recent event ID to be sent", []
+        )
         self.faucet_config_reload_warm = self._dpid_counter(
-            'faucet_config_reload_warm',
-            'number of warm, differences only config reloads executed')
+            "faucet_config_reload_warm",
+            "number of warm, differences only config reloads executed",
+        )
         self.faucet_config_reload_cold = self._dpid_counter(
-            'faucet_config_reload_cold',
-            'number of cold, complete reprovision config reloads executed')
+            "faucet_config_reload_cold",
+            "number of cold, complete reprovision config reloads executed",
+        )
         self.of_ignored_packet_ins = self._dpid_counter(
-            'of_ignored_packet_ins',
-            'number of OF packet_ins received but ignored from DP (due to rate limiting)')
+            "of_ignored_packet_ins",
+            "number of OF packet_ins received but ignored from DP (due to rate limiting)",
+        )
         self.of_unexpected_packet_ins = self._dpid_counter(
-            'of_unexpected_packet_ins',
-            'number of OF packet_ins received that are unexpected from DP (e.g. for unknown VLAN)')
+            "of_unexpected_packet_ins",
+            "number of OF packet_ins received that are unexpected from DP (e.g. for unknown VLAN)",
+        )
         self.of_packet_ins = self._dpid_counter(
-            'of_packet_ins',
-            'number of OF packet_ins received from DP')
+            "of_packet_ins", "number of OF packet_ins received from DP"
+        )
         self.of_non_vlan_packet_ins = self._dpid_counter(
-            'of_non_vlan_packet_ins',
-            'number of OF packet_ins received from DP, not associated with a FAUCET VLAN')
+            "of_non_vlan_packet_ins",
+            "number of OF packet_ins received from DP, not associated with a FAUCET VLAN",
+        )
         self.of_vlan_packet_ins = self._dpid_counter(
-            'of_vlan_packet_ins',
-            'number of OF packet_ins received from DP, associated with a FAUCET VLAN')
+            "of_vlan_packet_ins",
+            "number of OF packet_ins received from DP, associated with a FAUCET VLAN",
+        )
         self.of_flowmsgs_sent = self._dpid_counter(
-            'of_flowmsgs_sent',
-            'number of OF flow messages (and packet outs) sent to DP')
+            "of_flowmsgs_sent",
+            "number of OF flow messages (and packet outs) sent to DP",
+        )
         self.of_errors = self._dpid_counter(
-            'of_errors',
-            'number of OF errors received from DP')
+            "of_errors", "number of OF errors received from DP"
+        )
         self.of_dp_connections = self._dpid_counter(
-            'of_dp_connections',
-            'number of OF connections from a DP')
+            "of_dp_connections", "number of OF connections from a DP"
+        )
         self.of_dp_disconnections = self._dpid_counter(
-            'of_dp_disconnections',
-            'number of OF connections from a DP')
+            "of_dp_disconnections", "number of OF connections from a DP"
+        )
         self.vlan_hosts_learned = self._gauge(
-            'vlan_hosts_learned',
-            'number of hosts learned on a VLAN',
-            self.REQUIRED_LABELS + ['vlan'])
+            "vlan_hosts_learned",
+            "number of hosts learned on a VLAN",
+            self.REQUIRED_LABELS + ["vlan"],
+        )
         self.port_vlan_hosts_learned = self._gauge(
-            'port_vlan_hosts_learned',
-            'number of hosts learned on a port and VLAN',
-            self.port_required_labels + ['vlan'])
+            "port_vlan_hosts_learned",
+            "number of hosts learned on a port and VLAN",
+            self.port_required_labels + ["vlan"],
+        )
         self.vlan_neighbors = self._gauge(
-            'vlan_neighbors',
-            'number of L3 neighbors on a VLAN (whether resolved to L2 addresses, or not)',
-            self.REQUIRED_LABELS + ['vlan', 'ipv'])
+            "vlan_neighbors",
+            "number of L3 neighbors on a VLAN (whether resolved to L2 addresses, or not)",
+            self.REQUIRED_LABELS + ["vlan", "ipv"],
+        )
         self.vlan_learn_bans = self._gauge(
-            'vlan_learn_bans',
-            'number of times learning was banned on a VLAN',
-            self.REQUIRED_LABELS + ['vlan'])
+            "vlan_learn_bans",
+            "number of times learning was banned on a VLAN",
+            self.REQUIRED_LABELS + ["vlan"],
+        )
         self.faucet_config_table_names = self._gauge(
-            'faucet_config_table_names',
-            'number to names map of FAUCET pipeline tables',
-            self.REQUIRED_LABELS + ['table_name', 'next_tables'])
+            "faucet_config_table_names",
+            "number to names map of FAUCET pipeline tables",
+            self.REQUIRED_LABELS + ["table_name", "next_tables"],
+        )
         self.faucet_packet_in_secs = self._histogram(
-            'faucet_packet_in_secs',
-            'FAUCET packet in processing time',
+            "faucet_packet_in_secs",
+            "FAUCET packet in processing time",
             self.REQUIRED_LABELS,
-            (0.0001, 0.001, 0.01, 0.1, 1))
+            (0.0001, 0.001, 0.01, 0.1, 1),
+        )
         self.faucet_valve_service_secs = self._histogram(
-            'faucet_valve_service_secs',
-            'FAUCET valve service processing time',
-            self.REQUIRED_LABELS + ['valve_service'],
-            (0.0001, 0.001, 0.01, 0.1, 1))
+            "faucet_valve_service_secs",
+            "FAUCET valve service processing time",
+            self.REQUIRED_LABELS + ["valve_service"],
+            (0.0001, 0.001, 0.01, 0.1, 1),
+        )
         self.bgp_neighbor_uptime_seconds = self._gauge(
-            'bgp_neighbor_uptime',
-            'BGP neighbor uptime in seconds',
-            self.REQUIRED_LABELS + ['vlan', 'neighbor'])
+            "bgp_neighbor_uptime",
+            "BGP neighbor uptime in seconds",
+            self.REQUIRED_LABELS + ["vlan", "neighbor"],
+        )
         self.bgp_neighbor_routes = self._gauge(
-            'bgp_neighbor_routes',
-            'BGP neighbor route count',
-            self.REQUIRED_LABELS + ['vlan', 'neighbor', 'ipv'])
+            "bgp_neighbor_routes",
+            "BGP neighbor route count",
+            self.REQUIRED_LABELS + ["vlan", "neighbor", "ipv"],
+        )
         self.learned_macs = self._gauge(
-            'learned_macs',
-            ('MAC address stored as 64bit number to DP ID, port, VLAN, '
-             'and n (discrete index)'),
-            self.port_required_labels + ['vlan', 'n'])
+            "learned_macs",
+            (
+                "MAC address stored as 64bit number to DP ID, port, VLAN, "
+                "and n (discrete index)"
+            ),
+            self.port_required_labels + ["vlan", "n"],
+        )
         self.port_status = self._gauge(
-            'port_status',
-            'status of switch ports',
-            self.port_required_labels)
+            "port_status", "status of switch ports", self.port_required_labels
+        )
         self.port_stack_state = self._gauge(
-            'port_stack_state',
-            'state of stacking on a port',
-            self.port_required_labels)
+            "port_stack_state", "state of stacking on a port", self.port_required_labels
+        )
         self.port_learn_bans = self._gauge(
-            'port_learn_bans',
-            'number of times learning was banned on a port',
-            self.port_required_labels)
+            "port_learn_bans",
+            "number of times learning was banned on a port",
+            self.port_required_labels,
+        )
         self.learned_l2_port = self._gauge(
-            'learned_l2_port',
-            'learned port of l2 entries',
-            self.REQUIRED_LABELS + ['vid', 'eth_src'])
+            "learned_l2_port",
+            "learned port of l2 entries",
+            self.REQUIRED_LABELS + ["vid", "eth_src"],
+        )
         self.port_lacp_role = self._gauge(
-            'port_lacp_role',
-            'LACP role of a port',
-            self.port_required_labels)
+            "port_lacp_role", "LACP role of a port", self.port_required_labels
+        )
         self.port_lacp_state = self._gauge(
-            'port_lacp_state',
-            'state of LACP on a port',
-            self.port_required_labels)
-        self.dp_status = self._dpid_gauge(
-            'dp_status',
-            'status of datapaths')
+            "port_lacp_state", "state of LACP on a port", self.port_required_labels
+        )
+        self.dp_status = self._dpid_gauge("dp_status", "status of datapaths")
         self.dp_root_hop_port = self._gauge(
-            'dp_root_hop_port',
-            'port that leads to stack root DP',
-            self.REQUIRED_LABELS)
+            "dp_root_hop_port", "port that leads to stack root DP", self.REQUIRED_LABELS
+        )
         self.of_dp_desc_stats = self._gauge(
-            'of_dp_desc_stats',
-            'DP description (OFPDescStatsReply)',
-            self.REQUIRED_LABELS + ['mfr_desc', 'hw_desc', 'sw_desc', 'serial_num', 'dp_desc'])
+            "of_dp_desc_stats",
+            "DP description (OFPDescStatsReply)",
+            self.REQUIRED_LABELS
+            + ["mfr_desc", "hw_desc", "sw_desc", "serial_num", "dp_desc"],
+        )
         self.stack_cabling_errors = self._dpid_counter(
-            'stack_cabling_errors',
-            'number of cabling errors detected in all FAUCET stacks')
+            "stack_cabling_errors",
+            "number of cabling errors detected in all FAUCET stacks",
+        )
         self.stack_probes_received = self._dpid_counter(
-            'stack_probes_received',
-            'number of stacking messages received')
+            "stack_probes_received", "number of stacking messages received"
+        )
         self.is_dp_stack_root = self._dpid_gauge(
-            'is_dp_stack_root',
-            'bool indicating if dp is stack root')
+            "is_dp_stack_root", "bool indicating if dp is stack root"
+        )
         self.dp_dot1x_success = self._dpid_counter(
-            'dp_dot1x_success',
-            'number of successful authentications on dp')
+            "dp_dot1x_success", "number of successful authentications on dp"
+        )
         self.dp_dot1x_failure = self._dpid_counter(
-            'dp_dot1x_failure',
-            'number of authentications attempts failed on dp')
+            "dp_dot1x_failure", "number of authentications attempts failed on dp"
+        )
         self.dp_dot1x_logoff = self._dpid_counter(
-            'dp_dot1x_logoff',
-            'number of eap-logoff events on dp')
+            "dp_dot1x_logoff", "number of eap-logoff events on dp"
+        )
         self.port_dot1x_success = self._counter(
-            'port_dot1x_success',
-            'number of successful authentications on port',
-            self.port_required_labels)
+            "port_dot1x_success",
+            "number of successful authentications on port",
+            self.port_required_labels,
+        )
         self.port_dot1x_failure = self._counter(
-            'port_dot1x_failure',
-            'number of authentications attempts failed on port',
-            self.port_required_labels)
+            "port_dot1x_failure",
+            "number of authentications attempts failed on port",
+            self.port_required_labels,
+        )
         self.port_dot1x_logoff = self._counter(
-            'port_dot1x_logoff',
-            'number of eap-logoff events on port',
-            self.port_required_labels)
+            "port_dot1x_logoff",
+            "number of eap-logoff events on port",
+            self.port_required_labels,
+        )
         self.lacp_port_id = self._gauge(
-            'lacp_port_id',
-            'lacp port ID for for port',
-            self.port_required_labels)
+            "lacp_port_id", "lacp port ID for for port", self.port_required_labels
+        )
         self.port_stack_state_change_count = self._counter(
-            'port_stack_state_change_count',
-            'number of changes in port stack state',
-            self.port_required_labels)
+            "port_stack_state_change_count",
+            "number of changes in port stack state",
+            self.port_required_labels,
+        )
         self.port_lacp_state_change_count = self._counter(
-            'port_lacp_state_change_count',
-            'number of changes in port lacp state',
-            self.port_required_labels)
+            "port_lacp_state_change_count",
+            "number of changes in port lacp state",
+            self.port_required_labels,
+        )
         self.stack_root_change_count = self._counter(
-            'stack_root_change_count',
-            'number of changes in stack root', [])
+            "stack_root_change_count", "number of changes in stack root", []
+        )
 
     def _counter(self, var, var_help, labels):
         return Counter(var, var_help, labels, registry=self._reg)
 
     def _gauge(self, var, var_help, labels):
         return PromGauge(var, var_help, labels, registry=self._reg)
```

### Comparing `c65faucet-1.0.49/faucet/faucet_pipeline.py` & `c65faucet-1.0.50/faucet/faucet_pipeline.py`

 * *Files 15% similar despite different names*

```diff
@@ -18,19 +18,32 @@
 
 from faucet.faucet_metadata import EGRESS_METADATA_MASK
 
 
 class ValveTableConfig:  # pylint: disable=too-many-instance-attributes
     """Configuration for a single table."""
 
-    def __init__(self, name, table_id,  # pylint: disable=too-many-arguments
-                 exact_match=None, meter=None, output=True, miss_goto=None,
-                 size=None, match_types=None, set_fields=None, dec_ttl=None,
-                 vlan_scale=None, vlan_port_scale=None,
-                 next_tables=None, metadata_match=0, metadata_write=0):
+    def __init__(
+        self,
+        name,
+        table_id,  # pylint: disable=too-many-arguments
+        exact_match=None,
+        meter=None,
+        output=True,
+        miss_goto=None,
+        size=None,
+        match_types=None,
+        set_fields=None,
+        dec_ttl=None,
+        vlan_scale=None,
+        vlan_port_scale=None,
+        next_tables=None,
+        metadata_match=0,
+        metadata_write=0,
+    ):
         self.name = name
         self.table_id = table_id
         self.exact_match = exact_match
         self.meter = meter
         self.output = output
         self.miss_goto = miss_goto
         self.size = size
@@ -44,143 +57,158 @@
         if next_tables:
             assert isinstance(next_tables, (list, tuple))
             self.next_tables = next_tables
         else:
             self.next_tables = ()
 
     def __str__(self):
-        field_strs = ' '.join([
-            '%s: %s' % (key, val)
-            for key, val in sorted(self.__dict__.items())
-            if val])
-        return 'table config %s' % field_strs
+        field_strs = " ".join(
+            ["%s: %s" % (key, val) for key, val in sorted(self.__dict__.items()) if val]
+        )
+        return "table config %s" % field_strs
 
     def __repr__(self):
         return self.__str__()
 
     def __hash__(self):
         return hash(self.__str__())
 
     def __eq__(self, other):
         return self.__hash__() == other.__hash__()
 
     def __lt__(self, other):
         return self.__hash__() < other.__hash__()
 
 
-_NEXT_ETH = ('eth_dst_hairpin', 'eth_dst', 'flood')
-_NEXT_VIP = ('vip',) + _NEXT_ETH
+_NEXT_ETH = ("eth_dst_hairpin", "eth_dst", "flood")
+_NEXT_VIP = ("vip",) + _NEXT_ETH
 
 
 def _fib_table(ipv, table_id):
     return ValveTableConfig(
-        'ipv%u_fib' % ipv,
+        "ipv%u_fib" % ipv,
         table_id,
-        match_types=(('eth_type', False), ('ipv%u_dst' % ipv, True), ('vlan_vid', False)),
-        set_fields=('eth_dst', 'eth_src', 'vlan_vid'),
+        match_types=(
+            ("eth_type", False),
+            ("ipv%u_dst" % ipv, True),
+            ("vlan_vid", False),
+        ),
+        set_fields=("eth_dst", "eth_src", "vlan_vid"),
         dec_ttl=True,
         vlan_port_scale=3.1,
-        next_tables=_NEXT_VIP
+        next_tables=_NEXT_VIP,
     )
 
 
 PORT_ACL_DEFAULT_CONFIG = ValveTableConfig(
-    'port_acl',
+    "port_acl",
     0,
-    match_types=(('in_port', False),),
-    next_tables=(('vlan',) + _NEXT_VIP)
+    match_types=(("in_port", False),),
+    next_tables=(("vlan",) + _NEXT_VIP),
 )
 VLAN_DEFAULT_CONFIG = ValveTableConfig(
-    'vlan',
+    "vlan",
     PORT_ACL_DEFAULT_CONFIG.table_id + 1,
-    match_types=(('eth_dst', True), ('eth_type', False),
-                 ('in_port', False), ('vlan_vid', False)),
-    set_fields=('vlan_vid',),
+    match_types=(
+        ("eth_dst", True),
+        ("eth_type", False),
+        ("in_port", False),
+        ("vlan_vid", False),
+    ),
+    set_fields=("vlan_vid",),
     vlan_port_scale=3,
-    next_tables=('copro', 'vlan_acl', 'classification', 'eth_src')
+    next_tables=("copro", "vlan_acl", "classification", "eth_src"),
 )
 COPRO_DEFAULT_CONFIG = ValveTableConfig(
-    'copro',
+    "copro",
     VLAN_DEFAULT_CONFIG.table_id + 1,
-    match_types=(('in_port', False), ('eth_type', False), ('vlan_vid', False)),
+    match_types=(("in_port", False), ("eth_type", False), ("vlan_vid", False)),
     vlan_port_scale=1.5,
-    miss_goto='eth_dst',
-    next_tables=(('eth_dst',)),
+    miss_goto="eth_dst",
+    next_tables=(("eth_dst",)),
 )
 VLAN_ACL_DEFAULT_CONFIG = ValveTableConfig(
-    'vlan_acl',
+    "vlan_acl",
     VLAN_DEFAULT_CONFIG.table_id + 1,
-    next_tables=(('classification', 'eth_src') + _NEXT_ETH))
+    next_tables=(("classification", "eth_src") + _NEXT_ETH),
+)
 CLASSIFICATION_DEFAULT_CONFIG = ValveTableConfig(
-    'classification',
+    "classification",
     VLAN_ACL_DEFAULT_CONFIG.table_id + 1,
-    miss_goto='eth_src',
-    next_tables=(('eth_src', 'ipv4_fib', 'ipv6_fib') + _NEXT_VIP)
+    miss_goto="eth_src",
+    next_tables=(("eth_src", "ipv4_fib", "ipv6_fib") + _NEXT_VIP),
 )
 ETH_SRC_DEFAULT_CONFIG = ValveTableConfig(
-    'eth_src',
+    "eth_src",
     CLASSIFICATION_DEFAULT_CONFIG.table_id + 1,
-    miss_goto='eth_dst',
-    next_tables=(('ipv4_fib', 'ipv6_fib') + _NEXT_VIP),
-    match_types=(('eth_dst', True), ('eth_src', False), ('eth_type', False),
-                 ('in_port', False), ('vlan_vid', False)),
-    set_fields=('vlan_vid', 'eth_dst'),
+    miss_goto="eth_dst",
+    next_tables=(("ipv4_fib", "ipv6_fib") + _NEXT_VIP),
+    match_types=(
+        ("eth_dst", True),
+        ("eth_src", False),
+        ("eth_type", False),
+        ("in_port", False),
+        ("vlan_vid", False),
+    ),
+    set_fields=("vlan_vid", "eth_dst"),
     vlan_port_scale=4.1,
 )
 IPV4_FIB_DEFAULT_CONFIG = _fib_table(4, ETH_SRC_DEFAULT_CONFIG.table_id + 1)
 IPV6_FIB_DEFAULT_CONFIG = _fib_table(6, IPV4_FIB_DEFAULT_CONFIG.table_id + 1)
 VIP_DEFAULT_CONFIG = ValveTableConfig(
-    'vip',
+    "vip",
     IPV6_FIB_DEFAULT_CONFIG.table_id + 1,
-    match_types=(('arp_tpa', False), ('eth_dst', False), ('eth_type', False),
-                 ('icmpv6_type', False), ('ip_proto', False)),
+    match_types=(
+        ("arp_tpa", False),
+        ("eth_dst", False),
+        ("eth_type", False),
+        ("icmpv6_type", False),
+        ("ip_proto", False),
+    ),
     next_tables=_NEXT_ETH,
     vlan_scale=8,
 )
 ETH_DST_HAIRPIN_DEFAULT_CONFIG = ValveTableConfig(
-    'eth_dst_hairpin',
+    "eth_dst_hairpin",
     VIP_DEFAULT_CONFIG.table_id + 1,
-    match_types=(('in_port', False), ('eth_dst', False), ('vlan_vid', False)),
-    miss_goto='eth_dst',
+    match_types=(("in_port", False), ("eth_dst", False), ("vlan_vid", False)),
+    miss_goto="eth_dst",
     exact_match=True,
     vlan_port_scale=4.1,
 )
 ETH_DST_DEFAULT_CONFIG = ValveTableConfig(
-    'eth_dst',
+    "eth_dst",
     ETH_DST_HAIRPIN_DEFAULT_CONFIG.table_id + 1,
     exact_match=True,
-    miss_goto='flood',  # Note: when using egress acls the miss goto will be
-                        # egress acl table
-    match_types=(('eth_dst', False), ('vlan_vid', False)),
-    next_tables=('egress', 'egress_acl'),
+    miss_goto="flood",  # Note: when using egress acls the miss goto will be
+    # egress acl table
+    match_types=(("eth_dst", False), ("vlan_vid", False)),
+    next_tables=("egress", "egress_acl"),
     vlan_port_scale=4.1,
-    metadata_write=EGRESS_METADATA_MASK
+    metadata_write=EGRESS_METADATA_MASK,
 )
 EGRESS_ACL_DEFAULT_CONFIG = ValveTableConfig(
-    'egress_acl',
-    ETH_DST_DEFAULT_CONFIG.table_id + 1,
-    next_tables=('egress',)
+    "egress_acl", ETH_DST_DEFAULT_CONFIG.table_id + 1, next_tables=("egress",)
 )
 EGRESS_DEFAULT_CONFIG = ValveTableConfig(
-    'egress',
+    "egress",
     EGRESS_ACL_DEFAULT_CONFIG.table_id + 1,
-    match_types=(('metadata', True), ('vlan_vid', False)),
+    match_types=(("metadata", True), ("vlan_vid", False)),
     vlan_port_scale=1.5,
-    next_tables=('flood',),
-    miss_goto='flood',
-    metadata_match=EGRESS_METADATA_MASK
+    next_tables=("flood",),
+    miss_goto="flood",
+    metadata_match=EGRESS_METADATA_MASK,
 )
 FLOOD_DEFAULT_CONFIG = ValveTableConfig(
-    'flood',
+    "flood",
     EGRESS_DEFAULT_CONFIG.table_id + 1,
-    match_types=(('eth_dst', True), ('in_port', False), ('vlan_vid', False)),
+    match_types=(("eth_dst", True), ("in_port", False), ("vlan_vid", False)),
     vlan_port_scale=8.0,
 )
-MINIMUM_FAUCET_PIPELINE_TABLES = {
-    'vlan', 'eth_src', 'eth_dst', 'flood'}
+MINIMUM_FAUCET_PIPELINE_TABLES = {"vlan", "eth_src", "eth_dst", "flood"}
 
 # TODO: implement an eth_type table before VLAN. This would enable interception
 # of control protocols and simplify matches in vlan/eth_src, enabling use of
 # exact_match.
 FAUCET_PIPELINE = (
     PORT_ACL_DEFAULT_CONFIG,
     VLAN_DEFAULT_CONFIG,
@@ -195,21 +223,21 @@
     ETH_DST_DEFAULT_CONFIG,
     EGRESS_ACL_DEFAULT_CONFIG,
     EGRESS_DEFAULT_CONFIG,
     FLOOD_DEFAULT_CONFIG,
 )
 
 DEFAULT_CONFIGS = {
-    'port_acl': PORT_ACL_DEFAULT_CONFIG,
-    'vlan': VLAN_DEFAULT_CONFIG,
-    'copro': COPRO_DEFAULT_CONFIG,
-    'vlan_acl': VLAN_ACL_DEFAULT_CONFIG,
-    'eth_src': ETH_SRC_DEFAULT_CONFIG,
-    'ipv4_fib': IPV4_FIB_DEFAULT_CONFIG,
-    'ipv6_fib': IPV6_FIB_DEFAULT_CONFIG,
-    'vip': VIP_DEFAULT_CONFIG,
-    'eth_dst_hairpin': ETH_DST_HAIRPIN_DEFAULT_CONFIG,
-    'eth_dst': ETH_DST_DEFAULT_CONFIG,
-    'egress_acl': EGRESS_ACL_DEFAULT_CONFIG,
-    'egress': EGRESS_DEFAULT_CONFIG,
-    'flood': FLOOD_DEFAULT_CONFIG,
+    "port_acl": PORT_ACL_DEFAULT_CONFIG,
+    "vlan": VLAN_DEFAULT_CONFIG,
+    "copro": COPRO_DEFAULT_CONFIG,
+    "vlan_acl": VLAN_ACL_DEFAULT_CONFIG,
+    "eth_src": ETH_SRC_DEFAULT_CONFIG,
+    "ipv4_fib": IPV4_FIB_DEFAULT_CONFIG,
+    "ipv6_fib": IPV6_FIB_DEFAULT_CONFIG,
+    "vip": VIP_DEFAULT_CONFIG,
+    "eth_dst_hairpin": ETH_DST_HAIRPIN_DEFAULT_CONFIG,
+    "eth_dst": ETH_DST_DEFAULT_CONFIG,
+    "egress_acl": EGRESS_ACL_DEFAULT_CONFIG,
+    "egress": EGRESS_DEFAULT_CONFIG,
+    "flood": FLOOD_DEFAULT_CONFIG,
 }
```

### Comparing `c65faucet-1.0.49/faucet/fctl.py` & `c65faucet-1.0.50/faucet/fctl.py`

 * *Files 6% similar despite different names*

```diff
@@ -30,73 +30,78 @@
 from prometheus_client import parser
 
 
 # TODO: byte/packet counters could be per second (given multiple samples)
 def decode_value(metric_name, value):
     """Convert values to human readible format based on metric name"""
     result = value
-    if metric_name == 'learned_macs':
-        result = ':'.join(
-            format(octet, '02x') for octet in int(value).to_bytes(  # pytype: disable=attribute-error
-                6, byteorder='big')
+    if metric_name == "learned_macs":
+        result = ":".join(
+            format(octet, "02x")
+            for octet in int(value).to_bytes(  # pytype: disable=attribute-error
+                6, byteorder="big"
+            )
         )
     return result
 
 
 def scrape_prometheus(endpoints, retries=3, err_output_file=sys.stdout):
     """Scrape a list of Prometheus/FAUCET/Gauge endpoints and aggregate results."""
     metrics = []
     for endpoint in endpoints:
         content = None
         err = None
         for _ in range(retries):
             try:
-                if endpoint.startswith('http'):
-                    response = requests.get(endpoint)
-                    if response.status_code == requests.status_codes.codes.ok:  # pylint: disable=no-member
-                        content = response.content.decode('utf-8', 'strict')
+                if endpoint.startswith("http"):
+                    response = requests.get(endpoint, timeout=30)
+                    if (
+                        response.status_code
+                        == requests.status_codes.codes.ok  # pylint: disable=no-member
+                    ):
+                        content = response.content.decode("utf-8", "strict")
                         break
                 else:
-                    with urllib.request.urlopen(endpoint) as response:  # pytype: disable=module-attr
-                        content = response.read().decode('utf-8', 'strict')
+                    with urllib.request.urlopen(  # pytype: disable=module-attr
+                        endpoint
+                    ) as response:
+                        content = response.read().decode("utf-8", "strict")
                     break
             except (requests.exceptions.ConnectionError, ValueError) as exception:
                 err = exception
                 time.sleep(1)
         if err is not None:
             err_output_file.write(str(err))
             return None
         try:
-            endpoint_metrics = parser.text_string_to_metric_families(
-                content)
+            endpoint_metrics = parser.text_string_to_metric_families(content)
             metrics.extend(endpoint_metrics)
         except ValueError as err:
             err_output_file.write(str(err))
             return None
     return metrics
 
 
-def _get_samples_from_metrics(metrics, metric_name, label_matches,
-                              nonzero_only=False):
+def _get_samples_from_metrics(metrics, metric_name, label_matches, nonzero_only=False):
     result = []
     for metric in metrics:
         if metric_name is None or metric.name == metric_name:
             for sample in metric.samples:
                 labels = sample.labels
                 value = sample.value
                 if label_matches is None or set(label_matches.items()).issubset(
-                        set(labels.items())):
+                    set(labels.items())
+                ):
                     if nonzero_only and int(value) == 0:
                         continue
                     result.append(sample)
     return result
 
 
-def get_samples(endpoints, metric_name, label_matches, nonzero_only=False,
-                retries=3):
+def get_samples(endpoints, metric_name, label_matches, nonzero_only=False, retries=3):
     """return a list of Prometheus samples for a given metric
 
     Prometheus Sample objects are named tuples with the fields: name, labels,
     value, timestamp, exemplar.
 
     Arguments:
         endpoints (list of strings): the prometheus endpoints to query
@@ -105,110 +110,122 @@
         nonzero_only (bool): only return samples with non-zero values
         retries (int): number of retries when querying
     Returns:
         list of Prometheus Sample objects"""
     metrics = scrape_prometheus(endpoints, retries)
     if metrics is None:
         return None
-    return _get_samples_from_metrics(
-        metrics, metric_name, label_matches, nonzero_only)
+    return _get_samples_from_metrics(metrics, metric_name, label_matches, nonzero_only)
 
 
-def report_label_match_metrics(report_metrics, metrics, display_labels=None,
-                               nonzero_only=False, delim='\t',
-                               label_matches=None):
+def report_label_match_metrics(
+    report_metrics,
+    metrics,
+    display_labels=None,
+    nonzero_only=False,
+    delim="\t",
+    label_matches=None,
+):
     """Text report on a list of Prometheus metrics."""
     report_output = []
     if report_metrics is None:
         report_metrics = [None]
     for metric_name in report_metrics:
         for sample in _get_samples_from_metrics(
-                metrics, metric_name, label_matches, nonzero_only):
+            metrics, metric_name, label_matches, nonzero_only
+        ):
             labels = sample.labels
             value = sample.value
             sorted_labels = [
-                (key, val) for key, val in sorted(labels.items())
-                if not display_labels or key in display_labels]
+                (key, val)
+                for key, val in sorted(labels.items())
+                if not display_labels or key in display_labels
+            ]
             value = decode_value(metric_name, value)
             report_output.append(
-                delim.join((metric_name, str(sorted_labels), str(value))))
-        report_output = '\n'.join(report_output)
+                delim.join((metric_name, str(sorted_labels), str(value)))
+            )
+        report_output = "\n".join(report_output)
     return report_output
 
 
 def parse_args(sys_args):
     """Parse and return CLI args."""
 
     arg_parser = argparse.ArgumentParser(
-        prog='fctl',
-        description='Retrieve FAUCET/Gauge state using Prometheus variables.',
+        prog="fctl",
+        description="Retrieve FAUCET/Gauge state using Prometheus variables.",
         usage="""
     MACs learned on a DP.
 
     {argv0} -n --endpoints=http://172.17.0.1:9302 --metrics=learned_macs --labels=dp_id:0xb827eb608918
 
     Status of all DPs
 
     {argv0} -n --endpoints=http://172.17.0.1:9302 --metrics=dp_status
-""".format(**{'argv0': sys.argv[0]}))
-    arg_parser.add_argument(
-        '-n', '--nonzero', action='store_true', help='nonzero results only')
-    arg_parser.add_argument(
-        '-e', '--endpoints', help='list of endpoint URLs to query')
+""".format(
+            **{"argv0": sys.argv[0]}
+        ),
+    )
     arg_parser.add_argument(
-        '-m', '--metrics', help='list of metrics to query')
+        "-n", "--nonzero", action="store_true", help="nonzero results only"
+    )
+    arg_parser.add_argument("-e", "--endpoints", help="list of endpoint URLs to query")
+    arg_parser.add_argument("-m", "--metrics", help="list of metrics to query")
     arg_parser.add_argument(
-        '-l', '--labels', help='list of labels that must be present')
+        "-l", "--labels", help="list of labels that must be present"
+    )
     arg_parser.add_argument(
-        '--display-labels', help='list of labels to filter display by (default all)')
+        "--display-labels", help="list of labels to filter display by (default all)"
+    )
 
     endpoints = []
     report_metrics = []
     label_matches = None
     display_labels = None
     nonzero_only = False
 
     try:
         args = arg_parser.parse_args(sys_args)
         if args.nonzero:
             nonzero_only = True
         if args.endpoints:
-            endpoints = args.endpoints.split(',')
+            endpoints = args.endpoints.split(",")
         if args.metrics:
-            report_metrics = args.metrics.split(',')
+            report_metrics = args.metrics.split(",")
         if args.labels:
             label_matches = {}
-            for label_value in args.labels.split(','):
-                label, value = label_value.split(':')
+            for label_value in args.labels.split(","):
+                label, value = label_value.split(":")
                 label_matches[label] = value
         if args.display_labels:
-            display_labels = args.display_labels.split(',')
+            display_labels = args.display_labels.split(",")
     except (KeyError, IndexError):
         arg_parser.print_usage()
         sys.exit(-1)
 
     return (endpoints, report_metrics, label_matches, nonzero_only, display_labels)
 
 
 def main():
     (
         endpoints,
         report_metrics,
         label_matches,
         nonzero_only,
-        display_labels
+        display_labels,
     ) = parse_args(sys.argv[1:])
     metrics = scrape_prometheus(endpoints)
     if metrics is None:
         sys.exit(1)
     report = report_label_match_metrics(
         report_metrics,
         metrics,
         nonzero_only=nonzero_only,
         label_matches=label_matches,
-        display_labels=display_labels
+        display_labels=display_labels,
     )
     print(report)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
```

### Comparing `c65faucet-1.0.49/faucet/gauge.py` & `c65faucet-1.0.50/faucet/gauge.py`

 * *Files 2% similar despite different names*

```diff
@@ -38,16 +38,17 @@
     """Ryu app for polling Faucet controlled datapaths for stats/state.
 
     It can poll multiple datapaths. The configuration files for each datapath
     should be listed, one per line, in the file set as the environment variable
     GAUGE_CONFIG. It logs to the file set as the environment variable
     GAUGE_LOG,
     """
-    logname = 'gauge'
-    exc_logname = logname + '.exception'
+
+    logname = "gauge"
+    exc_logname = logname + ".exception"
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.watchers = {}
         self.config_watcher = ConfigWatcher()
         self.faucet_config_watchers = []
         self.prom_client = GaugePrometheusClient(reg=self._reg)
@@ -66,23 +67,30 @@
         """
         return self._get_datapath_obj(self.watchers, ryu_event)
 
     @kill_on_exception(exc_logname)
     def _load_config(self):
         """Load Gauge config."""
         try:
-            conf_hash, _faucet_config_files, faucet_conf_hashes, new_confs = watcher_parser(
-                self.config_file, self.logname, self.prom_client)
+            (
+                conf_hash,
+                _faucet_config_files,
+                faucet_conf_hashes,
+                new_confs,
+            ) = watcher_parser(self.config_file, self.logname, self.prom_client)
             watchers = [
-                watcher_factory(watcher_conf)(watcher_conf, self.logname, self.prom_client)
-                for watcher_conf in new_confs]
+                watcher_factory(watcher_conf)(
+                    watcher_conf, self.logname, self.prom_client
+                )
+                for watcher_conf in new_confs
+            ]
             self.prom_client.reregister_nonflow_vars()
         except InvalidConfigError as err:
             self.config_watcher.update(self.config_file)
-            self.logger.error('invalid config: %s', err)
+            self.logger.error("invalid config: %s", err)
             return
 
         for old_watchers in self.watchers.values():
             self._stop_watchers(old_watchers)
         new_watchers = {}
         for watcher in watchers:
             watcher_dpid = watcher.dp.dp_id
@@ -96,23 +104,22 @@
         timestamp = time.time()
         for watcher_dpid, watchers in new_watchers.items():
             ryu_dp = self.dpset.get(watcher_dpid)
             if ryu_dp:
                 self._start_watchers(ryu_dp, watchers, timestamp)
 
         self.watchers = new_watchers
-        self.config_watcher.update(
-            self.config_file, {self.config_file: conf_hash})
+        self.config_watcher.update(self.config_file, {self.config_file: conf_hash})
         self.faucet_config_watchers = []
         for faucet_config_file, faucet_conf_hash in faucet_conf_hashes.items():
             faucet_config_watcher = ConfigWatcher()
             faucet_config_watcher.update(faucet_config_file, faucet_conf_hash)
             self.faucet_config_watchers.append(faucet_config_watcher)
-            self.logger.info('watching FAUCET config %s', faucet_config_file)
-        self.logger.info('config complete')
+            self.logger.info("watching FAUCET config %s", faucet_config_file)
+        self.logger.info("config complete")
 
     @kill_on_exception(exc_logname)
     def _update_watcher(self, name, ryu_event):
         """Call watcher with event data."""
         watchers, _ryu_dp, msg = self._get_watchers(ryu_event)
         if watchers is None:
             return
@@ -139,28 +146,29 @@
             for i, watcher in enumerate(watchers_by_name):
                 is_active = i == 0
                 watcher.report_dp_status(1)
                 watcher.start(ryu_dp, is_active)
                 if isinstance(watcher, GaugePortStatePoller):
                     for port in ryu_dp.ports.values():
                         msg = parser.OFPPortStatus(
-                            ryu_dp, desc=port, reason=ofp.OFPPR_ADD)
+                            ryu_dp, desc=port, reason=ofp.OFPPR_ADD
+                        )
                         watcher.update(timestamp, msg)
 
     @kill_on_exception(exc_logname)
     def _datapath_connect(self, ryu_event):
         """Handle DP up.
 
         Args:
             ryu_event (ryu.controller.event.EventReplyBase): DP event.
         """
         watchers, ryu_dp, _ = self._get_watchers(ryu_event)
         if watchers is None:
             return
-        self.logger.info('%s up', dpid_log(ryu_dp.id))
+        self.logger.info("%s up", dpid_log(ryu_dp.id))
         ryu_dp.send_msg(valve_of.faucet_config(datapath=ryu_dp))
         ryu_dp.send_msg(valve_of.faucet_async(datapath=ryu_dp, packet_in=False))
         self._start_watchers(ryu_dp, watchers, time.time())
 
     @staticmethod
     def _stop_watchers(watchers):
         """Stop watchers for DP."""
@@ -176,28 +184,36 @@
 
         Args:
            ryu_event (ryu.controller.event.EventReplyBase): DP event.
         """
         watchers, ryu_dp, _ = self._get_watchers(ryu_event)
         if watchers is None:
             return
-        self.logger.info('%s down', dpid_log(ryu_dp.id))
+        self.logger.info("%s down", dpid_log(ryu_dp.id))
         self._stop_watchers(watchers)
 
     _WATCHER_HANDLERS = {
-        ofp_event.EventOFPPortStatus: 'port_state',  # pylint: disable=no-member
-        ofp_event.EventOFPPortStatsReply: 'port_stats',  # pylint: disable=no-member
-        ofp_event.EventOFPFlowStatsReply: 'flow_table',  # pylint: disable=no-member
-        ofp_event.EventOFPMeterStatsReply: 'meter_stats',  # pylint: disable=no-member
+        ofp_event.EventOFPPortStatus: "port_state",  # pylint: disable=no-member
+        ofp_event.EventOFPPortStatsReply: "port_stats",  # pylint: disable=no-member
+        ofp_event.EventOFPFlowStatsReply: "flow_table",  # pylint: disable=no-member
+        ofp_event.EventOFPMeterStatsReply: "meter_stats",  # pylint: disable=no-member
     }
 
-    @set_ev_cls(ofp_event.EventOFPPortStatus, MAIN_DISPATCHER)  # pylint: disable=no-member
-    @set_ev_cls(ofp_event.EventOFPPortStatsReply, MAIN_DISPATCHER)  # pylint: disable=no-member
-    @set_ev_cls(ofp_event.EventOFPFlowStatsReply, MAIN_DISPATCHER)  # pylint: disable=no-member
-    @set_ev_cls(ofp_event.EventOFPMeterStatsReply, MAIN_DISPATCHER)  # pylint: disable=no-member
+    @set_ev_cls(
+        ofp_event.EventOFPPortStatus, MAIN_DISPATCHER  # pylint: disable=no-member
+    )
+    @set_ev_cls(
+        ofp_event.EventOFPPortStatsReply, MAIN_DISPATCHER  # pylint: disable=no-member
+    )
+    @set_ev_cls(
+        ofp_event.EventOFPFlowStatsReply, MAIN_DISPATCHER  # pylint: disable=no-member
+    )
+    @set_ev_cls(
+        ofp_event.EventOFPMeterStatsReply, MAIN_DISPATCHER  # pylint: disable=no-member
+    )
     @kill_on_exception(exc_logname)
     def update_watcher_handler(self, ryu_event):
         """Handle any kind of stats/change event.
 
         Args:
            ryu_event (ryu.controller.event.EventReplyBase): stats/change event.
         """
```

### Comparing `c65faucet-1.0.49/faucet/gauge_influx.py` & `c65faucet-1.0.50/faucet/gauge_influx.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,108 +15,125 @@
 # implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from influxdb import InfluxDBClient
 from influxdb.exceptions import InfluxDBClientError, InfluxDBServerError
 import requests  # pytype: disable=pyi-error
-from faucet.gauge_pollers import GaugePortStatePoller, GaugeFlowTablePoller, GaugePortStatsPoller
+from faucet.gauge_pollers import (
+    GaugePortStatePoller,
+    GaugeFlowTablePoller,
+    GaugePortStatsPoller,
+)
 
 
 class InfluxShipper:
     """Convenience class for shipping values to InfluxDB.
 
     Inheritors must have a WatcherConf object as conf.
     """
+
     conf = None
-    ship_error_prefix = 'error shipping points: '
+    ship_error_prefix = "error shipping points: "
     logger = None
 
     def ship_points(self, points):
         """Make a connection to InfluxDB and ship points."""
 
         if self.conf is not None:
             try:
                 client = InfluxDBClient(
                     host=self.conf.influx_host,
                     port=self.conf.influx_port,
                     username=self.conf.influx_user,
                     password=self.conf.influx_pwd,
                     database=self.conf.influx_db,
-                    timeout=self.conf.influx_timeout)
+                    timeout=self.conf.influx_timeout,
+                )
                 if client:
-                    if client.write_points(points=points, time_precision='s'):
+                    if client.write_points(points=points, time_precision="s"):
                         return True
                     self.logger.warning(
-                        '%s failed to update InfluxDB' % self.ship_error_prefix)
+                        "%s failed to update InfluxDB" % self.ship_error_prefix
+                    )
                 else:
                     self.logger.warning(
-                        '%s error connecting to InfluxDB' % self.ship_error_prefix)
-            except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout,
-                    InfluxDBClientError, InfluxDBServerError) as err:
-                self.logger.warning('%s %s' % (self.ship_error_prefix, err))
+                        "%s error connecting to InfluxDB" % self.ship_error_prefix
+                    )
+            except (
+                requests.exceptions.ConnectionError,
+                requests.exceptions.ReadTimeout,
+                InfluxDBClientError,
+                InfluxDBServerError,
+            ) as err:
+                self.logger.warning("%s %s" % (self.ship_error_prefix, err))
         return False
 
     @staticmethod
     def make_point(tags, rcv_time, stat_name, stat_val):
         """Make an InfluxDB point."""
         # InfluxDB has only one integer type, int64. We are logging OF
         # stats that are uint64. Use float64 to prevent an overflow.
         # q.v. https://docs.influxdata.com/influxdb/v1.2/write_protocols/line_protocol_reference/
         point = {
-            'measurement': stat_name,
-            'tags': tags,
-            'time': int(rcv_time),
+            "measurement": stat_name,
+            "tags": tags,
+            "time": int(rcv_time),
             # pylint: disable=no-member
-            'fields': {'value': float(stat_val)}}
+            "fields": {"value": float(stat_val)},
+        }
         return point
 
-    def make_port_point(self, dp_name, port_name, rcv_time, stat_name, stat_val):  # pylint: disable=too-many-arguments
+    def make_port_point(
+        self, dp_name, port_name, rcv_time, stat_name, stat_val
+    ):  # pylint: disable=too-many-arguments
         """Make an InfluxDB point about a port measurement."""
         port_tags = {
-            'dp_name': dp_name,
-            'port_name': port_name,
+            "dp_name": dp_name,
+            "port_name": port_name,
         }
         return self.make_point(port_tags, rcv_time, stat_name, stat_val)
 
 
 class GaugePortStateInfluxDBLogger(GaugePortStatePoller, InfluxShipper):
     """
 
-Example:
-    ::
+    Example:
+        ::
 
-     > use faucet
-     Using database faucet
-     > precision rfc3339
-     > select * from port_state_reason where port_name = 'port1.0.1' order by time desc limit 10;
-     name: port_state_reason
-     -----------------------
-     time                    dp_name                 port_name       value
-     2017-02-21T02:12:29Z    windscale-faucet-1      port1.0.1       2
-     2017-02-21T02:12:25Z    windscale-faucet-1      port1.0.1       2
-     2016-07-27T22:05:08Z    windscale-faucet-1      port1.0.1       2
-     2016-05-25T04:33:00Z    windscale-faucet-1      port1.0.1       2
-     2016-05-25T04:32:57Z    windscale-faucet-1      port1.0.1       2
-     2016-05-25T04:31:21Z    windscale-faucet-1      port1.0.1       2
-     2016-05-25T04:31:18Z    windscale-faucet-1      port1.0.1       2
-     2016-05-25T04:27:07Z    windscale-faucet-1      port1.0.1       2
-     2016-05-25T04:27:04Z    windscale-faucet-1      port1.0.1       2
-     2016-05-25T04:24:53Z    windscale-faucet-1      port1.0.1       2
+         > use faucet
+         Using database faucet
+         > precision rfc3339
+         > select * from port_state_reason where port_name = 'port1.0.1' order by time desc limit 10;
+         name: port_state_reason
+         -----------------------
+         time                    dp_name                 port_name       value
+         2017-02-21T02:12:29Z    windscale-faucet-1      port1.0.1       2
+         2017-02-21T02:12:25Z    windscale-faucet-1      port1.0.1       2
+         2016-07-27T22:05:08Z    windscale-faucet-1      port1.0.1       2
+         2016-05-25T04:33:00Z    windscale-faucet-1      port1.0.1       2
+         2016-05-25T04:32:57Z    windscale-faucet-1      port1.0.1       2
+         2016-05-25T04:31:21Z    windscale-faucet-1      port1.0.1       2
+         2016-05-25T04:31:18Z    windscale-faucet-1      port1.0.1       2
+         2016-05-25T04:27:07Z    windscale-faucet-1      port1.0.1       2
+         2016-05-25T04:27:04Z    windscale-faucet-1      port1.0.1       2
+         2016-05-25T04:24:53Z    windscale-faucet-1      port1.0.1       2
 
     """
 
     def _update(self, rcv_time, msg):
         reason = msg.reason
         port_no = msg.desc.port_no
         if port_no in self.dp.ports:
             port_name = self.dp.ports[port_no].name
             points = [
                 self.make_port_point(
-                    self.dp.name, port_name, rcv_time, 'port_state_reason', reason)]
+                    self.dp.name, port_name, rcv_time, "port_state_reason", reason
+                )
+            ]
             self.ship_points(points)
 
     def send_req(self):
         """Send a stats request to a datapath."""
         raise NotImplementedError  # pragma: no cover
 
     def no_response(self):
@@ -161,52 +178,53 @@
      2017-03-06T05:20:12Z    windscale-faucet-1      port1.0.1       76063941
     """
 
     def _update(self, rcv_time, msg):
         points = []
         for stat in msg.body:
             port_name = str(stat.port_no)
-            for stat_name, stat_val in self._format_stat_pairs('_', stat):
+            for stat_name, stat_val in self._format_stat_pairs("_", stat):
                 points.append(
                     self.make_port_point(
-                        self.dp.name, port_name, rcv_time, stat_name, stat_val))
+                        self.dp.name, port_name, rcv_time, stat_name, stat_val
+                    )
+                )
         self.ship_points(points)
 
 
 class GaugeFlowTableInfluxDBLogger(GaugeFlowTablePoller, InfluxShipper):
     # pylint: disable=line-too-long
     """
 
-Example:
-    ::
-
-     > use faucet
-     Using database faucet
-     > show series where table_id = '0' and in_port = '2'
-     key
-     ---
-     flow_byte_count,dp_name=windscale-faucet-1,eth_type=2048,in_port=2,ip_proto=17,priority=9099,table_id=0,udp_dst=53
-     flow_byte_count,dp_name=windscale-faucet-1,eth_type=2048,in_port=2,ip_proto=6,priority=9098,table_id=0,tcp_dst=53
-     flow_byte_count,dp_name=windscale-faucet-1,in_port=2,priority=9097,table_id=0
-     flow_packet_count,dp_name=windscale-faucet-1,eth_type=2048,in_port=2,ip_proto=17,priority=9099,table_id=0,udp_dst=53
-     flow_packet_count,dp_name=windscale-faucet-1,eth_type=2048,in_port=2,ip_proto=6,priority=9098,table_id=0,tcp_dst=53
-     flow_packet_count,dp_name=windscale-faucet-1,in_port=2,priority=9097,table_id=0
-     > select * from flow_byte_count where table_id = '0' and in_port = '2' and ip_proto = '17' and time > now() - 5m
-     name: flow_byte_count
-     time                arp_tpa dp_name            eth_dst eth_src eth_type icmpv6_type in_port ip_proto ipv4_dst ipv6_dst priority table_id tcp_dst udp_dst value vlan_vid
-     ----                ------- -------            ------- ------- -------- ----------- ------- -------- -------- -------- -------- -------- ------- ------- ----- --------
-     1501154797000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      9414
-     1501154857000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      10554
-     1501154917000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      10554
-     1501154977000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      12164
-     1501155037000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      12239
+    Example:
+        ::
 
-"""  # noqa: E501
+         > use faucet
+         Using database faucet
+         > show series where table_id = '0' and in_port = '2'
+         key
+         ---
+         flow_byte_count,dp_name=windscale-faucet-1,eth_type=2048,in_port=2,ip_proto=17,priority=9099,table_id=0,udp_dst=53
+         flow_byte_count,dp_name=windscale-faucet-1,eth_type=2048,in_port=2,ip_proto=6,priority=9098,table_id=0,tcp_dst=53
+         flow_byte_count,dp_name=windscale-faucet-1,in_port=2,priority=9097,table_id=0
+         flow_packet_count,dp_name=windscale-faucet-1,eth_type=2048,in_port=2,ip_proto=17,priority=9099,table_id=0,udp_dst=53
+         flow_packet_count,dp_name=windscale-faucet-1,eth_type=2048,in_port=2,ip_proto=6,priority=9098,table_id=0,tcp_dst=53
+         flow_packet_count,dp_name=windscale-faucet-1,in_port=2,priority=9097,table_id=0
+         > select * from flow_byte_count where table_id = '0' and in_port = '2' and ip_proto = '17' and time > now() - 5m
+         name: flow_byte_count
+         time                arp_tpa dp_name            eth_dst eth_src eth_type icmpv6_type in_port ip_proto ipv4_dst ipv6_dst priority table_id tcp_dst udp_dst value vlan_vid
+         ----                ------- -------            ------- ------- -------- ----------- ------- -------- -------- -------- -------- -------- ------- ------- ----- --------
+         1501154797000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      9414
+         1501154857000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      10554
+         1501154917000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      10554
+         1501154977000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      12164
+         1501155037000000000         windscale-faucet-1                 2048                 2       17                         9099     0                53      12239
+    """  # noqa: E501
 
     def _update(self, rcv_time, msg):
         points = []
         jsondict = msg.to_jsondict()
-        for stats_reply in jsondict['OFPFlowStatsReply']['body']:
-            stats = stats_reply['OFPFlowStats']
+        for stats_reply in jsondict["OFPFlowStatsReply"]["body"]:
+            stats = stats_reply["OFPFlowStats"]
             for var, tags, count in self._parse_flow_stats(stats):
                 points.append(self.make_point(tags, rcv_time, var, count))
         self.ship_points(points)
```

### Comparing `c65faucet-1.0.49/faucet/gauge_pollers.py` & `c65faucet-1.0.50/faucet/gauge_pollers.py`

 * *Files 18% similar despite different names*

```diff
@@ -33,36 +33,37 @@
 
     def __init__(self, conf, logname, prom_client):
         self.dp = conf.dp  # pylint: disable=invalid-name
         self.conf = conf
         self.prom_client = prom_client
         self.reply_pending = False
         self.ryudp = None
-        self.logger = logging.getLogger(
-            logname + '.{0}'.format(self.conf.type)
-        )
+        self.logger = logging.getLogger(logname + ".{0}".format(self.conf.type))
         # _running indicates that the watcher is receiving data
         self._running = False
         self.req = None
 
     def report_dp_status(self, dp_status):
         """Report DP status."""
         self.prom_client.dp_status.labels(
-            **dict(dp_id=hex(self.dp.dp_id), dp_name=self.dp.name)).set(dp_status)  # pylint: disable=no-member
+            **dict(dp_id=hex(self.dp.dp_id), dp_name=self.dp.name)
+        ).set(
+            dp_status
+        )  # pylint: disable=no-member
 
     def start(self, ryudp, active):
         """Start the poller."""
         self.ryudp = ryudp
         self._running = True
         if active:
-            self.logger.info('starting')
+            self.logger.info("starting")
 
     def stop(self):
         """Stop the poller."""
-        self.logger.info('stopping')
+        self.logger.info("stopping")
         self._running = False
 
     def running(self):
         """Return True if the poller is running."""
         return self._running
 
     @staticmethod
@@ -73,19 +74,22 @@
     def send_req(self):
         """Send a stats request to a datapath."""
         raise NotImplementedError  # pragma: no cover
 
     def no_response(self):
         """Called when a polling cycle passes without receiving a response."""
 
-        dpid_str = ''
-        if self.req and hasattr(self.req, 'datapath'):
-            dpid_str = 'DPID %s (%s)' % (self.req.datapath.id, hex(self.req.datapath.id))
+        dpid_str = ""
+        if self.req and hasattr(self.req, "datapath"):
+            dpid_str = "DPID %s (%s)" % (
+                self.req.datapath.id,
+                hex(self.req.datapath.id),
+            )
 
-        self.logger.info('%s no response to %s', dpid_str, self.req)
+        self.logger.info("%s no response to %s", dpid_str, self.req)
 
     def update(self, rcv_time, msg):
         """Handle the responses to requests.
 
         Called when a reply to a stats request sent by this object is received
         by the controller.
 
@@ -95,38 +99,40 @@
         Args:
             rcv_time: the time the response was received
             msg: the stats reply message
         """
         # TODO: it may be worth while verifying this is the correct stats
         # response before doing this
         if not self.running():
-            self.logger.debug('update received when not running')
+            self.logger.debug("update received when not running")
             return
         self.reply_pending = False
         self._update(rcv_time, msg)
 
     def _format_stat_pairs(self, _delim, _stat):
         return ()
 
     def _dp_stat_name(self, _stat, _stat_name):
-        return ''
+        return ""
 
     def _rcv_time(self, rcv_time):
-        return time.strftime('%b %d %H:%M:%S', time.localtime(rcv_time))
+        return time.strftime("%b %d %H:%M:%S", time.localtime(rcv_time))
 
     def _update(self, rcv_time, msg):
         if not self.conf.file:
             return
         rcv_time_str = self._rcv_time(rcv_time)
         log_lines = []
         for stat in msg.body:
-            for stat_name, stat_val in self._format_stat_pairs('-', stat):
+            for stat_name, stat_val in self._format_stat_pairs("-", stat):
                 dp_stat_name = self._dp_stat_name(stat, stat_name)
-                log_lines.append(self._update_line(rcv_time_str, dp_stat_name, stat_val))
-        with open(self.conf.file, 'a', encoding='utf-8') as logfile:
+                log_lines.append(
+                    self._update_line(rcv_time_str, dp_stat_name, stat_val)
+                )
+        with open(self.conf.file, "a", encoding="utf-8") as logfile:
             logfile.writelines(log_lines)
 
     @staticmethod
     def _format_stats(delim, stat_pairs):
         formatted_stats = []
         for stat_name_list, stat_val in stat_pairs:
             stat_name = delim.join(stat_name_list)
@@ -134,15 +140,15 @@
             if stat_val == 2**64 - 1:
                 stat_val = 0
             formatted_stats.append((stat_name, stat_val))
         return formatted_stats
 
     @staticmethod
     def _update_line(rcv_time_str, stat_name, stat_val):
-        return '\t'.join((rcv_time_str, stat_name, str(stat_val))) + '\n'
+        return "\t".join((rcv_time_str, stat_name, str(stat_val))) + "\n"
 
 
 class GaugeThreadPoller(GaugePoller):
     """A ryu thread object for sending and receiving OpenFlow stats requests.
 
     The thread runs in a loop sending a request, sleeping then checking a
     response was received before sending another request.
@@ -158,15 +164,15 @@
         self.ryudp = None
 
     def start(self, ryudp, active):
         self.stop()
         super().start(ryudp, active)
         if active:
             self.thread = hub.spawn(self)
-            self.thread.name = 'GaugeThreadPoller'
+            self.thread.name = "GaugeThreadPoller"
 
     def stop(self):
         super().stop()
         if self.is_active():
             hub.kill(self.thread)
             hub.joinall([self.thread])
             self.thread = None
@@ -201,32 +207,33 @@
         if self.ryudp:
             self.req = parser.OFPMeterStatsRequest(self.ryudp, 0, ofp.OFPM_ALL)
             self.ryudp.send_msg(self.req)
 
 
 class GaugePortStatsPoller(GaugeThreadPoller):
     """Periodically sends a port stats request to the datapath and parses
-       and outputs the response.
+    and outputs the response.
     """
 
     def send_req(self):
         if self.ryudp:
             self.req = parser.OFPPortStatsRequest(self.ryudp, 0, ofp.OFPP_ANY)
             self.ryudp.send_msg(self.req)
 
     def _format_stat_pairs(self, delim, stat):
         stat_pairs = (
-            (('packets', 'out'), stat.tx_packets),
-            (('packets', 'in'), stat.rx_packets),
-            (('bytes', 'out'), stat.tx_bytes),
-            (('bytes', 'in'), stat.rx_bytes),
-            (('dropped', 'out'), stat.tx_dropped),
-            (('dropped', 'in'), stat.rx_dropped),
-            (('errors', 'out'), stat.tx_errors),
-            (('errors', 'in'), stat.rx_errors))
+            (("packets", "out"), stat.tx_packets),
+            (("packets", "in"), stat.rx_packets),
+            (("bytes", "out"), stat.tx_bytes),
+            (("bytes", "in"), stat.rx_bytes),
+            (("dropped", "out"), stat.tx_dropped),
+            (("dropped", "in"), stat.rx_dropped),
+            (("errors", "out"), stat.tx_errors),
+            (("errors", "in"), stat.rx_errors),
+        )
         return self._format_stats(delim, stat_pairs)
 
 
 class GaugeFlowTablePoller(GaugeThreadPoller):
     """Periodically dumps the current datapath flow table as a yaml object.
 
     Includes a timestamp and a reference ($DATAPATHNAME-flowtables). The
@@ -234,46 +241,47 @@
     matches all flows.
     """
 
     def send_req(self):
         if self.ryudp:
             match = parser.OFPMatch()
             self.req = parser.OFPFlowStatsRequest(
-                self.ryudp, 0, ofp.OFPTT_ALL, ofp.OFPP_ANY, ofp.OFPG_ANY,
-                0, 0, match)
+                self.ryudp, 0, ofp.OFPTT_ALL, ofp.OFPP_ANY, ofp.OFPG_ANY, 0, 0, match
+            )
             self.ryudp.send_msg(self.req)
 
     def _parse_flow_stats(self, stats):
         """Parse flow stats reply message into tags/labels and byte/packet counts."""
-        packet_count = int(stats['packet_count'])
-        byte_count = int(stats['byte_count'])
-        instructions = stats['instructions']
+        packet_count = int(stats["packet_count"])
+        byte_count = int(stats["byte_count"])
+        instructions = stats["instructions"]
         tags = {
-            'dp_name': self.dp.name,
-            'dp_id': hex(self.dp.dp_id),
-            'table_id': int(stats['table_id']),
-            'priority': int(stats['priority']),
-            'inst_count': len(instructions),
-            'cookie': int(stats['cookie']),
+            "dp_name": self.dp.name,
+            "dp_id": hex(self.dp.dp_id),
+            "table_id": int(stats["table_id"]),
+            "priority": int(stats["priority"]),
+            "inst_count": len(instructions),
+            "cookie": int(stats["cookie"]),
         }
-        oxm_matches = stats['match']['OFPMatch']['oxm_fields']
+        oxm_matches = stats["match"]["OFPMatch"]["oxm_fields"]
         for oxm_match in oxm_matches:
-            oxm_tlv = oxm_match['OXMTlv']
-            mask = oxm_tlv['mask']
-            val = oxm_tlv['value']
-            orig_field = oxm_tlv['field']
+            oxm_tlv = oxm_match["OXMTlv"]
+            mask = oxm_tlv["mask"]
+            val = oxm_tlv["value"]
+            orig_field = oxm_tlv["field"]
             if mask is not None:
-                val = '/'.join((str(val), str(mask)))
+                val = "/".join((str(val), str(mask)))
             field = OLD_MATCH_FIELDS.get(orig_field, orig_field)
             tags[field] = val
-            if field == 'vlan_vid' and mask is None:
-                tags['vlan'] = devid_present(int(val))
+            if field == "vlan_vid" and mask is None:
+                tags["vlan"] = devid_present(int(val))
         return (
-            ('flow_packet_count', tags, packet_count),
-            ('flow_byte_count', tags, byte_count))
+            ("flow_packet_count", tags, packet_count),
+            ("flow_byte_count", tags, byte_count),
+        )
 
 
 class GaugePortStatePoller(GaugePoller):
     """Abstraction for port state poller."""
 
     def send_req(self):
         """Send a stats request to a datapath."""
```

### Comparing `c65faucet-1.0.49/faucet/gauge_prom.py` & `c65faucet-1.0.50/faucet/gauge_prom.py`

 * *Files 12% similar despite different names*

```diff
@@ -16,158 +16,161 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 import collections
 from functools import partial
 from prometheus_client import Gauge
 
-from faucet.gauge_pollers import GaugePortStatsPoller, GaugePortStatePoller, GaugeFlowTablePoller
+from faucet.gauge_pollers import (
+    GaugePortStatsPoller,
+    GaugePortStatePoller,
+    GaugeFlowTablePoller,
+)
 from faucet.prom_client import PromClient
 
 
-PROM_PREFIX_DELIM = '_'
-PROM_PORT_PREFIX = 'of_port'
+PROM_PREFIX_DELIM = "_"
+PROM_PORT_PREFIX = "of_port"
 PROM_PORT_STATE_VARS = (
-    'reason',
-    'state',
-    'curr_speed',
-    'max_speed',
+    "reason",
+    "state",
+    "curr_speed",
+    "max_speed",
 )
 PROM_PORT_VARS = (
-    'tx_packets',
-    'rx_packets',
-    'tx_bytes',
-    'rx_bytes',
-    'tx_dropped',
-    'rx_dropped',
-    'tx_errors',
-    'rx_errors')
-PROM_FLOW_VARS = (
-    'flow_byte_count',
-    'flow_packet_count'
+    "tx_packets",
+    "rx_packets",
+    "tx_bytes",
+    "rx_bytes",
+    "tx_dropped",
+    "rx_dropped",
+    "tx_errors",
+    "rx_errors",
 )
-PROM_METER_PREFIX = 'of_meter'
+PROM_FLOW_VARS = ("flow_byte_count", "flow_packet_count")
+PROM_METER_PREFIX = "of_meter"
 PROM_METER_VARS = (
-    'flow_count',
-    'byte_in_count',
-    'packet_in_count',
-    'byte_band_count',
-    'packet_band_count'
+    "flow_count",
+    "byte_in_count",
+    "packet_in_count",
+    "byte_band_count",
+    "packet_band_count",
 )
 
 
 class GaugePrometheusClient(PromClient):
     """Wrapper for Prometheus client that is shared between all pollers."""
 
     def __init__(self, reg=None):
         super().__init__(reg=reg)
         self.table_tags = collections.defaultdict(set)
         self.metrics = {}
         self.dp_status = Gauge(
-            'dp_status',
-            'status of datapaths',
-            self.REQUIRED_LABELS,
-            registry=self._reg)
+            "dp_status", "status of datapaths", self.REQUIRED_LABELS, registry=self._reg
+        )
         self.reregister_nonflow_vars()
 
     def _reregister_var(self, var_key, var_func):
         try:
             self._reg.unregister(self.metrics[var_key])
         except KeyError:
             pass
         self.metrics[var_key] = var_func()
 
     def reregister_nonflow_vars(self):
         """Reset all metrics to empty."""
         for prom_var in PROM_PORT_VARS + PROM_PORT_STATE_VARS:
-            exported_prom_var = PROM_PREFIX_DELIM.join(
-                (PROM_PORT_PREFIX, prom_var))
+            exported_prom_var = PROM_PREFIX_DELIM.join((PROM_PORT_PREFIX, prom_var))
             self._reregister_var(
                 exported_prom_var,
                 partial(
                     Gauge,
                     exported_prom_var,
-                    '',
-                    self.REQUIRED_LABELS + ['port', 'port_description'],
-                    registry=self._reg))
+                    "",
+                    self.REQUIRED_LABELS + ["port", "port_description"],
+                    registry=self._reg,
+                ),
+            )
         for prom_var in PROM_METER_VARS:
-            exported_prom_var = PROM_PREFIX_DELIM.join(
-                (PROM_METER_PREFIX, prom_var))
+            exported_prom_var = PROM_PREFIX_DELIM.join((PROM_METER_PREFIX, prom_var))
             self._reregister_var(
                 exported_prom_var,
                 partial(
                     Gauge,
                     exported_prom_var,
-                    '',
-                    self.REQUIRED_LABELS + ['meter_id'],
-                    registry=self._reg))
+                    "",
+                    self.REQUIRED_LABELS + ["meter_id"],
+                    registry=self._reg,
+                ),
+            )
 
     def reregister_flow_vars(self, table_name, table_tags):
         """Register the flow variables needed for this client"""
         for prom_var in PROM_FLOW_VARS:
             table_prom_var = PROM_PREFIX_DELIM.join((prom_var, table_name))
             self._reregister_var(
                 table_prom_var,
                 partial(
-                    Gauge,
-                    table_prom_var,
-                    '',
-                    list(table_tags),
-                    registry=self._reg))
+                    Gauge, table_prom_var, "", list(table_tags), registry=self._reg
+                ),
+            )
 
 
 class GaugePortStatsPrometheusPoller(GaugePortStatsPoller):
     """Exports port stats to Prometheus."""
 
     def __init__(self, conf, logger, prom_client):
-        super().__init__(
-            conf, logger, prom_client)
+        super().__init__(conf, logger, prom_client)
         self.prom_client.start(
-            self.conf.prometheus_port, self.conf.prometheus_addr, self.conf.prometheus_test_thread)
+            self.conf.prometheus_port,
+            self.conf.prometheus_addr,
+            self.conf.prometheus_test_thread,
+        )
 
     def _format_stat_pairs(self, delim, stat):
         stat_pairs = (
             ((delim.join((PROM_PORT_PREFIX, prom_var)),), getattr(stat, prom_var))
-            for prom_var in PROM_PORT_VARS)
+            for prom_var in PROM_PORT_VARS
+        )
         return self._format_stats(delim, stat_pairs)
 
     def _update(self, rcv_time, msg):
         for stat in msg.body:
             port_labels = self.dp.port_labels(stat.port_no)
-            for stat_name, stat_val in self._format_stat_pairs(
-                    PROM_PREFIX_DELIM, stat):
+            for stat_name, stat_val in self._format_stat_pairs(PROM_PREFIX_DELIM, stat):
                 self.prom_client.metrics[stat_name].labels(**port_labels).set(stat_val)
 
 
 class GaugeMeterStatsPrometheusPoller(GaugePortStatsPoller):
     """Exports meter stats to Prometheus."""
 
     def __init__(self, conf, logger, prom_client):
-        super().__init__(
-            conf, logger, prom_client)
+        super().__init__(conf, logger, prom_client)
         self.prom_client.start(
-            self.conf.prometheus_port, self.conf.prometheus_addr, self.conf.prometheus_test_thread)
+            self.conf.prometheus_port,
+            self.conf.prometheus_addr,
+            self.conf.prometheus_test_thread,
+        )
 
     def _format_stat_pairs(self, delim, stat):
         band_stats = stat.band_stats[0]
         stat_pairs = (
-            (('flow', 'count'), stat.flow_count),
-            (('byte', 'in', 'count'), stat.byte_in_count),
-            (('packet', 'in', 'count'), stat.packet_in_count),
-            (('byte', 'band', 'count'), band_stats.byte_band_count),
-            (('packet', 'band', 'count'), band_stats.packet_band_count),
+            (("flow", "count"), stat.flow_count),
+            (("byte", "in", "count"), stat.byte_in_count),
+            (("packet", "in", "count"), stat.packet_in_count),
+            (("byte", "band", "count"), band_stats.byte_band_count),
+            (("packet", "band", "count"), band_stats.packet_band_count),
         )
         return self._format_stats(delim, stat_pairs)
 
     def _update(self, rcv_time, msg):
         for stat in msg.body:
             meter_labels = self.dp.base_prom_labels()
-            meter_labels.update({'meter_id': stat.meter_id})
-            for stat_name, stat_val in self._format_stat_pairs(
-                    PROM_PREFIX_DELIM, stat):
+            meter_labels.update({"meter_id": stat.meter_id})
+            for stat_name, stat_val in self._format_stat_pairs(PROM_PREFIX_DELIM, stat):
                 stat_name = PROM_PREFIX_DELIM.join((PROM_METER_PREFIX, stat_name))
                 self.prom_client.metrics[stat_name].labels(**meter_labels).set(stat_val)
 
 
 class GaugePortStatePrometheusPoller(GaugePortStatePoller):
     """Export port state changes to Prometheus."""
 
@@ -175,16 +178,20 @@
         port_no = msg.desc.port_no
         port = self.dp.ports.get(port_no, None)
         if port is None:
             return
         port_labels = self.dp.port_labels(port_no)
         for prom_var in PROM_PORT_STATE_VARS:
             exported_prom_var = PROM_PREFIX_DELIM.join((PROM_PORT_PREFIX, prom_var))
-            msg_value = msg.reason if prom_var == 'reason' else getattr(msg.desc, prom_var)
-            self.prom_client.metrics[exported_prom_var].labels(**port_labels).set(msg_value)
+            msg_value = (
+                msg.reason if prom_var == "reason" else getattr(msg.desc, prom_var)
+            )
+            self.prom_client.metrics[exported_prom_var].labels(**port_labels).set(
+                msg_value
+            )
 
     def send_req(self):
         """Send a stats request to a datapath."""
         raise NotImplementedError  # pragma: no cover
 
     def no_response(self):
         """Called when a polling cycle passes without receiving a response."""
@@ -192,36 +199,37 @@
 
 
 class GaugeFlowTablePrometheusPoller(GaugeFlowTablePoller):
     """Export flow table entries to Prometheus."""
 
     def _update(self, rcv_time, msg):
         jsondict = msg.to_jsondict()
-        for stats_reply in jsondict['OFPFlowStatsReply']['body']:
-            stats = stats_reply['OFPFlowStats']
+        for stats_reply in jsondict["OFPFlowStatsReply"]["body"]:
+            stats = stats_reply["OFPFlowStats"]
             # TODO: labels based on matches will be dynamic
             # Work around this by unregistering/registering the entire variable.
             for var, tags, count in self._parse_flow_stats(stats):
-                table_id = int(tags['table_id'])
+                table_id = int(tags["table_id"])
                 table_name = self.dp.table_by_id(table_id).name
                 table_tags = self.prom_client.table_tags[table_name]
                 tags_keys = set(tags.keys())
                 if tags_keys != table_tags:
                     unreg_tags = tags_keys - table_tags
                     if unreg_tags:
                         table_tags.update(unreg_tags)
-                        self.prom_client.reregister_flow_vars(
-                            table_name, table_tags)
+                        self.prom_client.reregister_flow_vars(table_name, table_tags)
                         self.logger.info(  # pylint: disable=logging-not-lazy
-                            'Adding tags %s to %s for table %s' % (
-                                unreg_tags, table_tags, table_name))
+                            "Adding tags %s to %s for table %s"
+                            % (unreg_tags, table_tags, table_name)
+                        )
                     # Add blank tags for any tags not present.
                     missing_tags = table_tags - tags_keys
                     for tag in missing_tags:
-                        tags[tag] = ''
+                        tags[tag] = ""
                 table_prom_var = PROM_PREFIX_DELIM.join((var, table_name))
                 try:
                     self.prom_client.metrics[table_prom_var].labels(**tags).set(count)
                 except ValueError:
                     self.logger.error(  # pylint: disable=logging-not-lazy
-                        'labels %s versus %s incorrect on %s' % (
-                            tags, table_tags, table_prom_var))
+                        "labels %s versus %s incorrect on %s"
+                        % (tags, table_tags, table_prom_var)
+                    )
```

### Comparing `c65faucet-1.0.49/faucet/meter.py` & `c65faucet-1.0.50/faucet/meter.py`

 * *Files 11% similar despite different names*

```diff
@@ -24,31 +24,31 @@
     """Implement FAUCET configuration for an OpenFlow meter."""
 
     entry = None
     entry_msg = None
     meter_id = None
 
     defaults = {
-        'entry': None,
-        'meter_id': None,
+        "entry": None,
+        "meter_id": None,
     }
 
     defaults_types = {
-        'entry': dict,
-        'meter_id': int,
+        "entry": dict,
+        "meter_id": int,
     }
 
     def __init__(self, _id, dp_id, conf):
         super().__init__(_id, dp_id, conf)
-        assert conf['entry']
-        assert conf['entry']['flags']
-        assert conf['entry']['bands']
-        conf['entry']['meter_id'] = self.meter_id
+        assert conf["entry"]
+        assert conf["entry"]["flags"]
+        assert conf["entry"]["bands"]
+        conf["entry"]["meter_id"] = self.meter_id
         self.entry_msg = meteradd(self.entry)
 
     def check_config(self):
         super().check_config()
-        test_config_condition(
-            self.meter_id < 0, 'meter_id is than 0')
+        test_config_condition(self.meter_id < 0, "meter_id is than 0")
         test_config_condition(
             self.meter_id > 4294901760,
-            'DP meter_id cannot exceed 4294901760 per OF13 specification')
+            "DP meter_id cannot exceed 4294901760 per OF13 specification",
+        )
```

### Comparing `c65faucet-1.0.49/faucet/port.py` & `c65faucet-1.0.50/faucet/port.py`

 * *Files 0% similar despite different names*

```diff
@@ -302,15 +302,15 @@
             self.dyn_stack_current_state = prev_port.dyn_stack_current_state
             self.dyn_last_lldp_beacon_time = prev_port.dyn_last_lldp_beacon_time
             self.dyn_phys_up = prev_port.dyn_phys_up
             self.dyn_stack_probe_info = prev_port.dyn_stack_probe_info
             self.dyn_update_time = prev_port.dyn_update_time
 
     def stack_descr(self):
-        """ "Return stacking annotation if this is a stacking port."""
+        """Return stacking annotation if this is a stacking port."""
         if self.stack:
             return "remote DP %s %s" % (self.stack["dp"].name, self.stack["port"])
         return ""
 
     def set_defaults(self):
         super().set_defaults()
         self._set_default("number", self._id)
```

### Comparing `c65faucet-1.0.49/faucet/prom_client.py` & `c65faucet-1.0.50/faucet/prom_client.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,64 +27,69 @@
 
 
 # Ryu's WSGI implementation doesn't always set QUERY_STRING
 def make_wsgi_app(registry):
     """Create a WSGI app which serves the metrics from a registry."""
 
     def prometheus_app(environ, start_response):
-        query_str = environ.get('QUERY_STRING', '')
+        query_str = environ.get("QUERY_STRING", "")
         params = parse_qs(query_str)
         reg = registry
-        if 'name[]' in params:
-            reg = reg.restricted_registry(params['name[]'])
+        if "name[]" in params:
+            reg = reg.restricted_registry(params["name[]"])
         output = generate_latest(reg)
-        status = str('200 OK')
-        headers = [(str('Content-type'), CONTENT_TYPE_LATEST)]
+        status = str("200 OK")
+        headers = [(str("Content-type"), CONTENT_TYPE_LATEST)]
         start_response(status, headers)
         return [output]
+
     return prometheus_app
 
 
 class PromClient:  # pylint: disable=too-few-public-methods
     """Prometheus client."""
 
-    REQUIRED_LABELS = ['dp_id', 'dp_name']
+    REQUIRED_LABELS = ["dp_id", "dp_name"]
     _reg = REGISTRY
 
     def __init__(self, reg=None):
         if reg is not None:
             self._reg = reg
-        self.version = VersionInfo('c65faucet').semantic_version().release_string()
+        self.version = VersionInfo("c65faucet").semantic_version().release_string()
+        # pylint: disable=no-member
         self.faucet_version = PromGauge(
-            'faucet_pbr_version',
-            'Faucet PBR version',
-            ['version'],
-            registry=self._reg)
-        self.faucet_version.labels(version=self.version).set(1)  # pylint: disable=no-member
+            "faucet_pbr_version", "Faucet PBR version", ["version"], registry=self._reg
+        )
+        self.faucet_version.labels(version=self.version).set(1)
         self.server = None
         self.thread = None
 
     def start(self, prom_port, prom_addr, use_test_thread=False):
         """Start webserver."""
         if not self.server:
             app = make_wsgi_app(self._reg)
             if use_test_thread:
                 # pylint: disable=import-outside-toplevel
-                from wsgiref.simple_server import (
-                    make_server, WSGIRequestHandler)
+                from wsgiref.simple_server import make_server, WSGIRequestHandler
                 import threading
 
                 class NoLoggingWSGIRequestHandler(WSGIRequestHandler):
                     """Don't log requests."""
 
-                    def log_message(self, format, *args):  # pylint: disable=redefined-builtin
+                    def log_message(
+                        self, format, *args
+                    ):  # pylint: disable=redefined-builtin
                         pass
 
                 self.server = make_server(
-                    prom_addr, int(prom_port), app, handler_class=NoLoggingWSGIRequestHandler)
+                    prom_addr,
+                    int(prom_port),
+                    app,
+                    handler_class=NoLoggingWSGIRequestHandler,
+                )
                 self.thread = threading.Thread(target=self.server.serve_forever)
                 self.thread.daemon = True
                 self.thread.start()
             else:
                 self.server = hub.WSGIServer((prom_addr, int(prom_port)), app)
                 self.thread = hub.spawn(self.server.serve_forever)
-            self.thread.name = 'prometheus'
+            self.thread.name = "prometheus"
```

### Comparing `c65faucet-1.0.49/faucet/router.py` & `c65faucet-1.0.50/faucet/router.py`

 * *Files 13% similar despite different names*

```diff
@@ -27,34 +27,34 @@
         return str([(k, self[k]) for k in sorted(self.keys())])
 
 
 class Router(Conf):
     """Implement FAUCET configuration for a router."""
 
     defaults = {
-        'bgp': {},
-        'vlans': None,
+        "bgp": {},
+        "vlans": None,
     }
 
     defaults_types = {
-        'bgp': dict,
-        'vlans': list,
+        "bgp": dict,
+        "vlans": list,
     }
 
-    ipaddress_fields = ('neighbor_addresses', 'server_addresses')
+    ipaddress_fields = ("neighbor_addresses", "server_addresses")
 
     bgp_defaults_types = {
-        'as': int,
-        'connect_mode': str,
-        'neighbor_addresses': list,
-        'neighbor_as': int,
-        'port': int,
-        'routerid': str,
-        'server_addresses': list,
-        'vlan': (str, int),
+        "as": int,
+        "connect_mode": str,
+        "neighbor_addresses": list,
+        "neighbor_as": int,
+        "port": int,
+        "routerid": str,
+        "server_addresses": list,
+        "vlan": (str, int),
     }
 
     def __init__(self, _id, dp_id, conf):
         self.bgp = {}
         self.vlans = []
         self.vip_map_by_ipv = {}
         super().__init__(_id, dp_id, conf)
@@ -62,109 +62,116 @@
     def _sub_conf_val(self, sub_conf, key):
         try:
             return self.__dict__[sub_conf][key]
         except KeyError:
             return None
 
     def _bgp_val(self, key):
-        return self._sub_conf_val('bgp', key)
+        return self._sub_conf_val("bgp", key)
 
     def __str__(self):
         return str(self._id)
 
     def check_config(self):
         super().check_config()
         if self.bgp:
             self._check_conf_types(self.bgp, self.bgp_defaults_types)
             self.bgp = self._set_unknown_conf(self.bgp, self.bgp_defaults_types)
             if not self.bgp_connect_mode():
-                self.bgp['connect_mode'] = 'passive'
+                self.bgp["connect_mode"] = "passive"
             for field in self.ipaddress_fields:
                 if field in self.bgp:
-                    self.bgp[field] = frozenset([
-                        self._check_ip_str(ip_str) for ip_str in self.bgp[field]])
+                    self.bgp[field] = frozenset(
+                        [self._check_ip_str(ip_str) for ip_str in self.bgp[field]]
+                    )
             for accessor_val, required_field in (
-                    (self.bgp_ipvs(), 'server_addresses'),
-                    (self.bgp_as(), 'as'),
-                    (self.bgp_port(), 'port'),
-                    (self.bgp_connect_mode(), 'connect_mode'),
-                    (self.bgp_routerid(), 'routerid'),
-                    (self.bgp_neighbor_addresses(), 'neighbor_addresses'),
-                    (self.bgp_neighbor_as(), 'neighbor_as')):
-                test_config_condition(not accessor_val, 'BGP %s must be specified' % required_field)
+                (self.bgp_ipvs(), "server_addresses"),
+                (self.bgp_as(), "as"),
+                (self.bgp_port(), "port"),
+                (self.bgp_connect_mode(), "connect_mode"),
+                (self.bgp_routerid(), "routerid"),
+                (self.bgp_neighbor_addresses(), "neighbor_addresses"),
+                (self.bgp_neighbor_as(), "neighbor_as"),
+            ):
+                test_config_condition(
+                    not accessor_val, "BGP %s must be specified" % required_field
+                )
             test_config_condition(
-                self.bgp_connect_mode() != 'passive', 'BGP connect_mode must be passive')
+                self.bgp_connect_mode() != "passive", "BGP connect_mode must be passive"
+            )
             for ipv in self.bgp_ipvs():
                 test_config_condition(
                     len(self.bgp_server_addresses_by_ipv(ipv)) != 1,
-                    'Only one BGP server address per IP version supported')
+                    "Only one BGP server address per IP version supported",
+                )
             if not self.bgp_vlan():
                 test_config_condition(
                     len(self.vlans) != 1,
-                    'If routing more than one VLAN, must specify BGP VLAN')
+                    "If routing more than one VLAN, must specify BGP VLAN",
+                )
                 self.set_bgp_vlan(self.vlans[0])
         else:
             test_config_condition(
-                not self.vlans, 'A router must have least one VLAN specified at top level')
+                not self.vlans,
+                "A router must have least one VLAN specified at top level",
+            )
 
     def vip_map(self, ipa):
         """Return VIP for IP address, if any."""
         if ipa.version in self.vip_map_by_ipv:
             result = self.vip_map_by_ipv[ipa.version].get(ipa)
             if result:
                 return result
         return (None, None)
 
     def finalize(self):
         for vlan in self.vlans:
             for faucet_vip in vlan.faucet_vips:
                 ipv = faucet_vip.version
                 if ipv not in self.vip_map_by_ipv:
-                    self.vip_map_by_ipv[ipv] = _PyTricia(
-                        faucet_vip.ip.max_prefixlen)
-                self.vip_map_by_ipv[ipv][faucet_vip.network] = (
-                    vlan, faucet_vip)
+                    self.vip_map_by_ipv[ipv] = _PyTricia(faucet_vip.ip.max_prefixlen)
+                self.vip_map_by_ipv[ipv][faucet_vip.network] = (vlan, faucet_vip)
         super().finalize()
 
     def bgp_as(self):
         """Return BGP AS."""
-        return self._bgp_val('as')
+        return self._bgp_val("as")
 
     def bgp_connect_mode(self):
         """Return BGP connect mode."""
-        return self._bgp_val('connect_mode')
+        return self._bgp_val("connect_mode")
 
     def bgp_neighbor_addresses(self):
         """Return BGP neighbor addresses."""
-        return self._bgp_val('neighbor_addresses')
+        return self._bgp_val("neighbor_addresses")
 
     def bgp_neighbor_as(self):
         """Return BGP neighbor AS number."""
-        return self._bgp_val('neighbor_as')
+        return self._bgp_val("neighbor_as")
 
     def bgp_port(self):
         """Return BGP port."""
-        return self._bgp_val('port')
+        return self._bgp_val("port")
 
     def bgp_routerid(self):
         """Return BGP router ID."""
-        return self._bgp_val('routerid')
+        return self._bgp_val("routerid")
 
     def bgp_server_addresses(self):
         """Return BGP server addresses."""
-        return self._bgp_val('server_addresses')
+        return self._bgp_val("server_addresses")
 
     def bgp_vlan(self):
         """Return BGP VLAN."""
-        return self._bgp_val('vlan')
+        return self._bgp_val("vlan")
 
     def set_bgp_vlan(self, vlan):
         """Set BGP VLAN."""
         if self.bgp:
-            self.bgp['vlan'] = vlan
+            self.bgp["vlan"] = vlan
 
     def bgp_ipvs(self):
         """Return list of IP versions for BGP configured on this VLAN."""
         return self._ipvs(self.bgp_server_addresses())
 
     def bgp_neighbor_addresses_by_ipv(self, ipv):
         """Return BGP neighbor addresses with specified IP version on this VLAN."""
```

### Comparing `c65faucet-1.0.49/faucet/stack.py` & `c65faucet-1.0.50/faucet/stack.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/tfm_pipeline.py` & `c65faucet-1.0.50/faucet/tfm_pipeline.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve.py` & `c65faucet-1.0.50/faucet/valve.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_acl.py` & `c65faucet-1.0.50/faucet/valve_acl.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_coprocessor.py` & `c65faucet-1.0.50/faucet/valve_coprocessor.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_lldp.py` & `c65faucet-1.0.50/faucet/valve_lldp.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_manager_base.py` & `c65faucet-1.0.50/faucet/valve_manager_base.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_of.py` & `c65faucet-1.0.50/faucet/valve_of.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_of_old.py` & `c65faucet-1.0.50/faucet/valve_of_old.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_outonly.py` & `c65faucet-1.0.50/faucet/valve_outonly.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_packet.py` & `c65faucet-1.0.50/faucet/valve_packet.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_pipeline.py` & `c65faucet-1.0.50/faucet/valve_pipeline.py`

 * *Files 7% similar despite different names*

```diff
@@ -32,22 +32,22 @@
     packets through the pipeline.
 
     Responsible for installing flows in the vlan, egress and classification
     tables"""
 
     def __init__(self, dp):
         self.dp = dp
-        self.vlan_table = dp.tables['vlan']
+        self.vlan_table = dp.tables["vlan"]
         self.classification_table = dp.classification_table()
         self.output_table = dp.output_table()
         self.egress_table = None
         self.egress_acl_table = None
         if dp.egress_pipeline:
-            self.egress_table = dp.tables['egress']
-            self.egress_acl_table = dp.tables.get('egress_acl')
+            self.egress_table = dp.tables["egress"]
+            self.egress_acl_table = dp.tables.get("egress_acl")
         self.filter_priority = self._FILTER_PRIORITY
         self.select_priority = self._HIGH_PRIORITY
 
     @staticmethod
     @functools.lru_cache(maxsize=1024)
     def _accept_to_table(table, actions):
         inst = [table.goto_this()]
@@ -115,106 +115,126 @@
                 table before being output
         returns:
             list of Instructions
         """
         instructions = []
         if self.egress_table:
             metadata, metadata_mask = faucet_md.get_egress_metadata(
-                port.number, vlan.vid)
+                port.number, vlan.vid
+            )
             if self.egress_acl_table:
-                instructions.extend(valve_of.metadata_goto_table(
-                    metadata, metadata_mask, self.egress_acl_table))
+                instructions.extend(
+                    valve_of.metadata_goto_table(
+                        metadata, metadata_mask, self.egress_acl_table
+                    )
+                )
             else:
-                instructions.extend(valve_of.metadata_goto_table(
-                    metadata, metadata_mask, self.egress_table))
+                instructions.extend(
+                    valve_of.metadata_goto_table(
+                        metadata, metadata_mask, self.egress_table
+                    )
+                )
         else:
-            instructions.append(valve_of.apply_actions(vlan.output_port(
-                port, hairpin=hairpin, output_table=self.output_table,
-                external_forwarding_requested=external_forwarding_requested)))
+            instructions.append(
+                valve_of.apply_actions(
+                    vlan.output_port(
+                        port,
+                        hairpin=hairpin,
+                        output_table=self.output_table,
+                        external_forwarding_requested=external_forwarding_requested,
+                    )
+                )
+            )
         return tuple(instructions)
 
     def initialise_tables(self):
         """Install rules to initialise the classification_table"""
         ofmsgs = []
         # drop broadcast sources
         if self.dp.drop_broadcast_source_address:
-            ofmsgs.extend(self.filter_packets(
-                {'eth_src': valve_of.mac.BROADCAST_STR}
-            ))
+            ofmsgs.extend(self.filter_packets({"eth_src": valve_of.mac.BROADCAST_STR}))
 
-        ofmsgs.extend(self.filter_packets(
-            {'eth_type': valve_of.ECTP_ETH_TYPE}, priority_offset=10))
+        ofmsgs.extend(
+            self.filter_packets(
+                {"eth_type": valve_of.ECTP_ETH_TYPE}, priority_offset=10
+            )
+        )
 
         return ofmsgs
 
     def _add_egress_table_rule(self, port, vlan, pop_vlan=True):
-        metadata, metadata_mask = faucet_md.get_egress_metadata(
-            port.number, vlan.vid)
+        metadata, metadata_mask = faucet_md.get_egress_metadata(port.number, vlan.vid)
         actions = copy.copy(port.mirror_actions())
         if pop_vlan:
             actions.append(valve_of.pop_vlan())
         actions.append(valve_of.output_port(port.number))
         inst = (valve_of.apply_actions(tuple(actions)),)
         return self.egress_table.flowmod(
             self.egress_table.match(
-                vlan=vlan,
-                metadata=metadata,
-                metadata_mask=metadata_mask
+                vlan=vlan, metadata=metadata, metadata_mask=metadata_mask
             ),
             priority=self.dp.high_priority,
-            inst=inst
+            inst=inst,
         )
 
     def add_port(self, port):
         ofmsgs = []
         if self.egress_table is None:
             return ofmsgs
         for vlan in port.tagged_vlans:
-            ofmsgs.append(self._add_egress_table_rule(
-                port, vlan, pop_vlan=False))
+            ofmsgs.append(self._add_egress_table_rule(port, vlan, pop_vlan=False))
         if port.native_vlan is not None:
-            ofmsgs.append(self._add_egress_table_rule(
-                port, port.native_vlan))
+            ofmsgs.append(self._add_egress_table_rule(port, port.native_vlan))
         return ofmsgs
 
     def del_port(self, port):
         ofmsgs = []
         if self.egress_table:
             mask = faucet_md.PORT_METADATA_MASK
-            ofmsgs.append(self.egress_table.flowdel(self.egress_table.match(
-                metadata=port.number & mask,
-                metadata_mask=mask
-            )))
+            ofmsgs.append(
+                self.egress_table.flowdel(
+                    self.egress_table.match(
+                        metadata=port.number & mask, metadata_mask=mask
+                    )
+                )
+            )
         return ofmsgs
 
     def filter_packets(self, match_dict, priority_offset=0):
         """get a list of flow modification messages to filter packets from
         the pipeline.
         args:
             match_dict: a dictionary specifying the match fields
             priority_offset: used to prevent overlapping entries
         """
-        return [self.classification_table.flowdrop(
-            self.classification_table.match(**match_dict),
-            priority=self.filter_priority + priority_offset)]
+        return [
+            self.classification_table.flowdrop(
+                self.classification_table.match(**match_dict),
+                priority=self.filter_priority + priority_offset,
+            )
+        ]
 
-    def select_packets(self, target_table, match_dict, actions=None,
-                       priority_offset=0):
+    def select_packets(self, target_table, match_dict, actions=None, priority_offset=0):
         """retrieve rules to redirect packets matching match_dict to table"""
         inst = [target_table.goto_this()]
         if actions is not None:
             inst.append(valve_of.apply_actions(actions))
-        return [self.classification_table.flowmod(
-            self.classification_table.match(**match_dict),
-            priority=self.select_priority + priority_offset,
-            inst=tuple(inst))]
+        return [
+            self.classification_table.flowmod(
+                self.classification_table.match(**match_dict),
+                priority=self.select_priority + priority_offset,
+                inst=tuple(inst),
+            )
+        ]
 
     def remove_filter(self, match_dict, strict=True, priority_offset=0):
-        """retrieve flow mods to remove a filter from the classification table
-        """
+        """retrieve flow mods to remove a filter from the classification table"""
         priority = None
         if strict:
             priority = self.filter_priority + priority_offset
-        return [self.classification_table.flowdel(
-            self.classification_table.match(**match_dict),
-            priority=priority,
-            strict=strict)]
+        return [
+            self.classification_table.flowdel(
+                self.classification_table.match(**match_dict),
+                priority=priority,
+                strict=strict,
+            )
+        ]
```

### Comparing `c65faucet-1.0.49/faucet/valve_route.py` & `c65faucet-1.0.50/faucet/valve_route.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_ryuapp.py` & `c65faucet-1.0.50/faucet/valve_ryuapp.py`

 * *Files 12% similar despite different names*

```diff
@@ -41,54 +41,58 @@
 
 
 class OSKenAppBase(app_manager.OSKenApp):
     """OSKenApp base class for FAUCET/Gauge."""
 
     OFP_VERSIONS = valve_of.OFP_VERSIONS
     _CONTEXTS = {
-        'dpset': dpset.DPSet,
+        "dpset": dpset.DPSet,
     }
-    logname = ''
-    exc_logname = ''
+    logname = ""
+    exc_logname = ""
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        self.dpset = kwargs['dpset']
-        self._reg = kwargs.get('reg', None)
-        self.config_file = self.get_setting('CONFIG', True)
-        self.stat_reload = self.get_setting('CONFIG_STAT_RELOAD')
-        loglevel = self.get_setting('LOG_LEVEL')
-        logfile = self.get_setting('LOG')
-        exc_logfile = self.get_setting('EXCEPTION_LOG')
-        self.logger = get_logger(
-            self.logname, logfile, loglevel, 0)
-        self.exc_logger = get_logger(
-            self.exc_logname, exc_logfile, logging.DEBUG, 1)
+        self.dpset = kwargs["dpset"]
+        self._reg = kwargs.get("reg", None)
+        self.config_file = self.get_setting("CONFIG", True)
+        self.stat_reload = self.get_setting("CONFIG_STAT_RELOAD")
+        loglevel = self.get_setting("LOG_LEVEL")
+        logfile = self.get_setting("LOG")
+        exc_logfile = self.get_setting("EXCEPTION_LOG")
+        self.logger = get_logger(self.logname, logfile, loglevel, 0)
+        self.exc_logger = get_logger(self.exc_logname, exc_logfile, logging.DEBUG, 1)
         self.threads = []
         self.thread_managers = []
         self.prom_client = None
 
     def _get_threads(self):
         """Return started threads."""
         threads = self.threads.copy()
         threads.extend(
-            [thread_manager.thread for thread_manager in self.thread_managers
-             if thread_manager and thread_manager.thread is not None])
+            [
+                thread_manager.thread
+                for thread_manager in self.thread_managers
+                if thread_manager and thread_manager.thread is not None
+            ]
+        )
         return threads
 
     def _check_thread_exception(self):
         """Check for a dead thread and cause/log an exception."""
         dead_threads = [thread for thread in self._get_threads() if thread.dead]
         if dead_threads:
             for thread in dead_threads:
-                thread_name = getattr(thread, 'name', 'unknown')
+                thread_name = getattr(thread, "name", "unknown")
                 # Inconveniently, eventlet and friends helpfully put the last
                 # exception on stderr but not anywhere else where we can log it.
                 self.logger.error(
-                    'unexpected %s thread termination - check Ryu/process stderr log', thread_name)
+                    "unexpected %s thread termination - check Ryu/process stderr log",
+                    thread_name,
+                )
             # If that succeeds (was a temporary error that killed the thread),
             # then raise an exception to make sure we know a thread died.
             raise ValveDeadThreadException
 
     def _thread_jitter(self, period, jitter=2):
         """Reschedule another thread with a random jitter and check for dead threads."""
         hub.sleep(period + (random.random() * jitter))
@@ -104,15 +108,15 @@
         """
         while True:
             self.send_event(self.__class__.__name__, ryu_event)
             self._thread_jitter(period, jitter)
 
     def get_setting(self, setting, path_eval=False):
         """Return config setting prefaced with logname."""
-        return get_setting('_'.join((self.logname.upper(), setting)), path_eval)
+        return get_setting("_".join((self.logname.upper(), setting)), path_eval)
 
     def signal_handler(self, sigid, _):
         """Handle signals.
 
         Args:
             sigid (int): signal received.
         """
@@ -135,49 +139,50 @@
                     self.send_event(self.__class__.__name__, EventReconfigure())
             self._thread_jitter(3)
 
     def start(self):
         """Start controller."""
         super().start()
         if self.prom_client:
-            self.logger.info('version %s', self.prom_client.version)
+            self.logger.info("version %s", self.prom_client.version)
         if self.stat_reload:
-            self.logger.info('will automatically reload new config on changes')
+            self.logger.info("will automatically reload new config on changes")
         self.reload_config(None)
-        self.threads.extend([
-            hub.spawn(thread) for thread in (self._config_file_stat,)])
+        self.threads.extend([hub.spawn(thread) for thread in (self._config_file_stat,)])
         signal.signal(signal.SIGHUP, self.signal_handler)
         signal.signal(signal.SIGINT, self.signal_handler)
 
     def reload_config(self, _ryu_event):
         """Handle reloading configuration."""
-        self.logger.info('Reloading configuration')
+        self.logger.info("Reloading configuration")
 
     def _get_datapath_obj(self, datapath_objs, ryu_event):
         """Get datapath object to response to an event.
 
         Args:
             datapath_objs (dict): datapath objects indexed by DP ID.
             ryu_event (ryu.controller.event.Event): event.
         Returns:
             valve, ryu_dp, msg: Nones, or datapath object, Ryu datapath, and Ryu msg (if any).
         """
         datapath_obj = None
         msg = None
-        if hasattr(ryu_event, 'msg'):
+        if hasattr(ryu_event, "msg"):
             msg = ryu_event.msg
             ryu_dp = msg.datapath
         else:
             ryu_dp = ryu_event.dp
         dp_id = ryu_dp.id
         if dp_id in datapath_objs:
             datapath_obj = datapath_objs[dp_id]
         else:
             ryu_dp.close()
-            self.logger.error('%s: unknown datapath %s', str(ryu_event), dpid_log(dp_id))
+            self.logger.error(
+                "%s: unknown datapath %s", str(ryu_event), dpid_log(dp_id)
+            )
         return (datapath_obj, ryu_dp, msg)
 
     @staticmethod
     def _datapath_connect(_ryu_event):
         raise NotImplementedError  # pragma: no cover
 
     @staticmethod
```

### Comparing `c65faucet-1.0.49/faucet/valve_stack.py` & `c65faucet-1.0.50/faucet/valve_stack.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_switch.py` & `c65faucet-1.0.50/faucet/valve_switch.py`

 * *Files 14% similar despite different names*

```diff
@@ -14,71 +14,77 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from faucet.valve_switch_standalone import (
-    ValveSwitchManager, ValveSwitchFlowRemovedManager)
+    ValveSwitchManager,
+    ValveSwitchFlowRemovedManager,
+)
 from faucet.valve_switch_stack import (
-    ValveSwitchStackManagerNoReflection, ValveSwitchStackManagerReflection)
+    ValveSwitchStackManagerNoReflection,
+    ValveSwitchStackManagerReflection,
+)
 
 
 def valve_switch_factory(logger, dp, pipeline, stack_manager):
     """Return switch flood/learning manager based on datapath configuration.
 
-        Args:
-            logger: logger instance.
-            dp: DP instance.
-            pipeline: ValvePipeline instance.
-        Returns:
-            switch manager instance.
+    Args:
+        logger: logger instance.
+        dp: DP instance.
+        pipeline: ValvePipeline instance.
+    Returns:
+        switch manager instance.
     """
     restricted_bcast_arpnd = bool(dp.restricted_bcast_arpnd_ports())
-    eth_dst_hairpin_table = dp.tables.get('eth_dst_hairpin', None)
-    vlan_acl_table = dp.tables.get('vlan_acl', None)
+    eth_dst_hairpin_table = dp.tables.get("eth_dst_hairpin", None)
+    vlan_acl_table = dp.tables.get("vlan_acl", None)
 
     switch_args = {
-        'logger': logger,
-        'ports': dp.ports,
-        'vlans': dp.vlans,
-        'vlan_table': dp.tables['vlan'],
-        'vlan_acl_table': vlan_acl_table,
-        'eth_src_table': dp.tables['eth_src'],
-        'eth_dst_table': dp.tables['eth_dst'],
-        'eth_dst_hairpin_table': eth_dst_hairpin_table,
-        'flood_table': dp.tables['flood'],
-        'classification_table': dp.classification_table,
-        'pipeline': pipeline,
-        'use_group_table': dp.group_table,
-        'groups': dp.groups,
-        'combinatorial_port_flood': dp.combinatorial_port_flood,
-        'canonical_port_order': dp.canonical_port_order,
-        'restricted_bcast_arpnd': restricted_bcast_arpnd,
-        'has_externals': dp.has_externals,
-        'learn_ban_timeout': dp.learn_ban_timeout,
-        'learn_timeout': dp.timeout,
-        'learn_jitter': dp.learn_jitter,
-        'cache_update_guard_time': dp.cache_update_guard_time,
-        'idle_dst': dp.idle_dst,
-        'dp_high_priority': dp.high_priority,
-        'dp_highest_priority': dp.highest_priority,
-        'faucet_dp_mac': dp.faucet_dp_mac,
-        'drop_spoofed_faucet_mac': dp.drop_spoofed_faucet_mac,
+        "logger": logger,
+        "ports": dp.ports,
+        "vlans": dp.vlans,
+        "vlan_table": dp.tables["vlan"],
+        "vlan_acl_table": vlan_acl_table,
+        "eth_src_table": dp.tables["eth_src"],
+        "eth_dst_table": dp.tables["eth_dst"],
+        "eth_dst_hairpin_table": eth_dst_hairpin_table,
+        "flood_table": dp.tables["flood"],
+        "classification_table": dp.classification_table,
+        "pipeline": pipeline,
+        "use_group_table": dp.group_table,
+        "groups": dp.groups,
+        "combinatorial_port_flood": dp.combinatorial_port_flood,
+        "canonical_port_order": dp.canonical_port_order,
+        "restricted_bcast_arpnd": restricted_bcast_arpnd,
+        "has_externals": dp.has_externals,
+        "learn_ban_timeout": dp.learn_ban_timeout,
+        "learn_timeout": dp.timeout,
+        "learn_jitter": dp.learn_jitter,
+        "cache_update_guard_time": dp.cache_update_guard_time,
+        "idle_dst": dp.idle_dst,
+        "dp_high_priority": dp.high_priority,
+        "dp_highest_priority": dp.highest_priority,
+        "faucet_dp_mac": dp.faucet_dp_mac,
+        "drop_spoofed_faucet_mac": dp.drop_spoofed_faucet_mac,
     }
 
     if dp.stack:
         switch_class = ValveSwitchStackManagerNoReflection
         if dp.stack.root_flood_reflection:
             switch_class = ValveSwitchStackManagerReflection
-            logger.info('Using stacking root flood reflection')
+            logger.info("Using stacking root flood reflection")
         else:
-            logger.info('Not using stacking root flood reflection')
-        switch_args.update({
-            'stack_manager': stack_manager,
-        })
+            logger.info("Not using stacking root flood reflection")
+        switch_args.update(
+            {
+                "stack_manager": stack_manager,
+            }
+        )
         return switch_class(**switch_args)
 
     switch_class = ValveSwitchManager
     if dp.use_idle_timeout:
         switch_class = ValveSwitchFlowRemovedManager
     return switch_class(**switch_args)  # pytype: disable=wrong-keyword-args
```

### Comparing `c65faucet-1.0.49/faucet/valve_switch_stack.py` & `c65faucet-1.0.50/faucet/valve_switch_stack.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_switch_standalone.py` & `c65faucet-1.0.50/faucet/valve_switch_standalone.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_table.py` & `c65faucet-1.0.50/faucet/valve_table.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valve_util.py` & `c65faucet-1.0.50/faucet/valve_util.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/valves_manager.py` & `c65faucet-1.0.50/faucet/valves_manager.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/faucet/vlan.py` & `c65faucet-1.0.50/faucet/vlan.py`

 * *Files 0% similar despite different names*

```diff
@@ -300,29 +300,25 @@
         self.dyn_unresolved_host_ip_gws = collections.defaultdict(list)
 
     def reset_ports(self, ports):
         """Reset tagged and untagged port lists."""
         sorted_ports = sorted(ports, key=lambda i: i.number)
         # pylint: disable=consider-using-generator
         self.tagged = tuple(
-            [
-                port for port in sorted_ports if self in port.tagged_vlans
-            ]
+            [port for port in sorted_ports if self in port.tagged_vlans]
         )
         self.untagged = tuple(
             [
                 port
                 for port in sorted_ports
                 if (self == port.native_vlan and port.dyn_dot1x_native_vlan is None)
             ]
         )
         self.dot1x_untagged = tuple(
-            [
-                port for port in sorted_ports if self == port.dyn_dot1x_native_vlan
-            ]
+            [port for port in sorted_ports if self == port.dyn_dot1x_native_vlan]
         )
 
     def add_cache_host(self, eth_src, port, cache_time):
         """Add/update a host to the cache on a port at at time."""
         existing_entry = self.cached_host(eth_src)
         if existing_entry is None:
             self.dyn_host_cache_stats_stale[port.number] = True
@@ -502,64 +498,42 @@
     def get_ports(self):
         """Return all ports on this VLAN."""
         return self.tagged + self.untagged + self.dot1x_untagged
 
     def restricted_bcast_arpnd_ports(self):
         """Return all ports with restricted broadcast enabled."""
         # pylint: disable=consider-using-generator
-        return tuple(
-            [
-                port for port in self.get_ports() if port.restricted_bcast_arpnd
-            ]
-        )
+        return tuple([port for port in self.get_ports() if port.restricted_bcast_arpnd])
 
     def hairpin_ports(self):
         """Return all ports with hairpin enabled."""
         # pylint: disable=consider-using-generator
-        return tuple(
-            [
-                port for port in self.get_ports() if port.hairpin
-            ]
-        )
+        return tuple([port for port in self.get_ports() if port.hairpin])
 
     def mirrored_ports(self):
         """Return ports that are mirrored on this VLAN."""
         # pylint: disable=consider-using-generator
-        return tuple(
-            [
-                port for port in self.get_ports() if port.mirror
-            ]
-        )
+        return tuple([port for port in self.get_ports() if port.mirror])
 
     def loop_protect_external_ports(self):
         """Return ports wth external loop protection set."""
         # pylint: disable=consider-using-generator
-        return tuple(
-            [
-                port for port in self.get_ports() if port.loop_protect_external
-            ]
-        )
+        return tuple([port for port in self.get_ports() if port.loop_protect_external])
 
     def loop_protect_external_ports_up(self):
         """Return up ports with external loop protection set."""
         # pylint: disable=consider-using-generator
         return tuple(
-            [
-                port for port in self.loop_protect_external_ports() if port.dyn_phys_up
-            ]
+            [port for port in self.loop_protect_external_ports() if port.dyn_phys_up]
         )
 
     def lacp_ports(self):
         """Return ports that have LACP on this VLAN."""
         # pylint: disable=consider-using-generator
-        return tuple(
-            [
-                port for port in self.get_ports() if port.lacp
-            ]
-        )
+        return tuple([port for port in self.get_ports() if port.lacp])
 
     def lacp_up_selected_ports(self):
         """Return LACP ports that have been SELECTED and are UP"""
         # pylint: disable=consider-using-generator
         return tuple(
             [
                 port
```

### Comparing `c65faucet-1.0.49/faucet/watcher.py` & `c65faucet-1.0.50/faucet/watcher.py`

 * *Files 9% similar despite different names*

```diff
@@ -24,115 +24,126 @@
 import time
 
 from os_ken.ofproto import ofproto_v1_3 as ofp
 
 from faucet.conf import InvalidConfigError
 from faucet.valve_util import dpid_log
 from faucet.gauge_influx import (
-    GaugePortStateInfluxDBLogger, GaugePortStatsInfluxDBLogger, GaugeFlowTableInfluxDBLogger)
+    GaugePortStateInfluxDBLogger,
+    GaugePortStatsInfluxDBLogger,
+    GaugeFlowTableInfluxDBLogger,
+)
 from faucet.gauge_pollers import (
-    GaugePortStatePoller, GaugePortStatsPoller, GaugeFlowTablePoller, GaugeMeterStatsPoller)
+    GaugePortStatePoller,
+    GaugePortStatsPoller,
+    GaugeFlowTablePoller,
+    GaugeMeterStatsPoller,
+)
 from faucet.gauge_prom import (
-    GaugePortStatsPrometheusPoller, GaugePortStatePrometheusPoller, GaugeFlowTablePrometheusPoller,
-    GaugeMeterStatsPrometheusPoller)
+    GaugePortStatsPrometheusPoller,
+    GaugePortStatePrometheusPoller,
+    GaugeFlowTablePrometheusPoller,
+    GaugeMeterStatsPrometheusPoller,
+)
 
 
 def watcher_factory(conf):
     """Return a Gauge object based on type.
 
     Args:
         conf (GaugeConf): object with the configuration for this valve.
     """
 
     watcher_types = {
-        'port_state': {
-            'text': GaugePortStateLogger,
-            'influx': GaugePortStateInfluxDBLogger,
-            'prometheus': GaugePortStatePrometheusPoller,
+        "port_state": {
+            "text": GaugePortStateLogger,
+            "influx": GaugePortStateInfluxDBLogger,
+            "prometheus": GaugePortStatePrometheusPoller,
         },
-        'port_stats': {
-            'text': GaugePortStatsLogger,
-            'influx': GaugePortStatsInfluxDBLogger,
-            'prometheus': GaugePortStatsPrometheusPoller,
+        "port_stats": {
+            "text": GaugePortStatsLogger,
+            "influx": GaugePortStatsInfluxDBLogger,
+            "prometheus": GaugePortStatsPrometheusPoller,
         },
-        'flow_table': {
-            'text': GaugeFlowTableLogger,
-            'influx': GaugeFlowTableInfluxDBLogger,
-            'prometheus': GaugeFlowTablePrometheusPoller,
+        "flow_table": {
+            "text": GaugeFlowTableLogger,
+            "influx": GaugeFlowTableInfluxDBLogger,
+            "prometheus": GaugeFlowTablePrometheusPoller,
         },
-        'meter_stats': {
-            'text': GaugeMeterStatsLogger,
-            'prometheus': GaugeMeterStatsPrometheusPoller,
+        "meter_stats": {
+            "text": GaugeMeterStatsLogger,
+            "prometheus": GaugeMeterStatsPrometheusPoller,
         },
     }
 
     w_type = conf.type
     db_type = conf.db_type
     try:
         return watcher_types[w_type][db_type]
     except KeyError as key_error:
-        raise InvalidConfigError('invalid water config') from key_error
+        raise InvalidConfigError("invalid water config") from key_error
 
 
 class GaugePortStateLogger(GaugePortStatePoller):
     """Abstraction for port state logger."""
 
     def _update(self, rcv_time, msg):
         rcv_time_str = self._rcv_time(rcv_time)
         reason = msg.reason
         port_no = msg.desc.port_no
-        log_msg = 'port %s unknown state %s' % (port_no, reason)
+        log_msg = "port %s unknown state %s" % (port_no, reason)
         if reason == ofp.OFPPR_ADD:
-            log_msg = 'port %s added' % port_no
+            log_msg = "port %s added" % port_no
         elif reason == ofp.OFPPR_DELETE:
-            log_msg = 'port %s deleted' % port_no
+            log_msg = "port %s deleted" % port_no
         elif reason == ofp.OFPPR_MODIFY:
-            link_down = (msg.desc.state & ofp.OFPPS_LINK_DOWN)
+            link_down = msg.desc.state & ofp.OFPPS_LINK_DOWN
             if link_down:
-                log_msg = 'port %s down' % port_no
+                log_msg = "port %s down" % port_no
             else:
-                log_msg = 'port %s up' % port_no
-        log_msg = '%s %s' % (dpid_log(self.dp.dp_id), log_msg)
+                log_msg = "port %s up" % port_no
+        log_msg = "%s %s" % (dpid_log(self.dp.dp_id), log_msg)
         self.logger.info(log_msg)
         if self.conf.file:
-            with open(self.conf.file, 'a', encoding='utf-8') as logfile:
-                logfile.write('\t'.join((rcv_time_str, log_msg)) + '\n')
+            with open(self.conf.file, "a", encoding="utf-8") as logfile:
+                logfile.write("\t".join((rcv_time_str, log_msg)) + "\n")
 
     def send_req(self):
         """Send a stats request to a datapath."""
         raise NotImplementedError  # pragma: no cover
 
     def no_response(self):
         """Called when a polling cycle passes without receiving a response."""
         raise NotImplementedError  # pragma: no cover
 
 
 class GaugePortStatsLogger(GaugePortStatsPoller):
     """Abstraction for port statistics logger."""
 
     def _dp_stat_name(self, stat, stat_name):
-        port_name = self.dp.port_labels(stat.port_no)['port']
-        return '-'.join((self.dp.name, port_name, stat_name))
+        port_name = self.dp.port_labels(stat.port_no)["port"]
+        return "-".join((self.dp.name, port_name, stat_name))
 
 
 class GaugeMeterStatsLogger(GaugeMeterStatsPoller):
     """Abstraction for meter statistics logger."""
 
     def _format_stat_pairs(self, delim, stat):
         band_stats = stat.band_stats[0]
         stat_pairs = (
-            (('flow', 'count'), stat.flow_count),
-            (('byte', 'in', 'count'), stat.byte_in_count),
-            (('packet', 'in', 'count'), stat.packet_in_count),
-            (('byte', 'band', 'count'), band_stats.byte_band_count),
-            (('packet', 'band', 'count'), band_stats.packet_band_count))
+            (("flow", "count"), stat.flow_count),
+            (("byte", "in", "count"), stat.byte_in_count),
+            (("packet", "in", "count"), stat.packet_in_count),
+            (("byte", "band", "count"), band_stats.byte_band_count),
+            (("packet", "band", "count"), band_stats.packet_band_count),
+        )
         return self._format_stats(delim, stat_pairs)
 
     def _dp_stat_name(self, stat, stat_name):
-        return '-'.join((self.dp.name, str(stat.meter_id), stat_name))
+        return "-".join((self.dp.name, str(stat.meter_id), stat_name))
 
 
 class GaugeFlowTableLogger(GaugeFlowTablePoller):
     """Periodically dumps the current datapath flow table as a yaml object.
 
     Includes a timestamp and a reference ($DATAPATHNAME-flowtables). The
     flow table is dumped as an OFFlowStatsReply message (in yaml format) that
@@ -140,32 +151,35 @@
 
     optionally the output can be compressed by setting compressed: true in the
     config for this watcher
     """
 
     def _rcv_time(self, rcv_time):
         # Use ISO8601 times for filenames
-        return time.strftime('%Y-%m-%dT%H:%M:%S', time.gmtime(rcv_time))
+        return time.strftime("%Y-%m-%dT%H:%M:%S", time.gmtime(rcv_time))
 
     def _update(self, rcv_time, msg):
         rcv_time_str = self._rcv_time(rcv_time)
         path = self.conf.path
         # Double Hyphen to avoid confusion with ISO8601 times
         filename = os.path.join(
-            path,
-            "{}--flowtable--{}.json".format(self.dp.name, rcv_time_str)
+            path, "{}--flowtable--{}.json".format(self.dp.name, rcv_time_str)
         )
         if os.path.isfile(filename):
             # If this filename already exists, add an increment to the filename
             # (for dealing with parts of a multipart message arriving at the same time)
             inc = 1
             while os.path.isfile(filename):
-                filename = os.path.join(path, "{}--flowtable--{}--{}.json".format(
-                    self.dp.name, rcv_time_str, inc))
+                filename = os.path.join(
+                    path,
+                    "{}--flowtable--{}--{}.json".format(
+                        self.dp.name, rcv_time_str, inc
+                    ),
+                )
                 inc += 1
 
         if self.conf.compress:
-            with gzip.open(filename, 'wt') as outfile:
+            with gzip.open(filename, "wt") as outfile:
                 outfile.write(json.dumps(msg.to_jsondict()))
         else:
-            with open(filename, 'w', encoding='utf-8') as outfile:
+            with open(filename, "w", encoding="utf-8") as outfile:
                 json.dump(msg.to_jsondict(), outfile, indent=2)
```

### Comparing `c65faucet-1.0.49/faucet/watcher_conf.py` & `c65faucet-1.0.50/faucet/watcher_conf.py`

 * *Files 8% similar despite different names*

```diff
@@ -76,75 +76,75 @@
  * prometheus_port (int): The port used to export prometheus data. Defaults to \
        9303.
  * prometheus_addr (ip addr str): The address used to export prometheus data. \
        Defaults to '127.0.0.1'.
 """
 
     db_defaults = {
-        'type': None,
-        'file': None,
-        'path': None,
-        'compress': False,
+        "type": None,
+        "file": None,
+        "path": None,
+        "compress": False,
         # compress flow table file
-        'influx_db': 'faucet',
+        "influx_db": "faucet",
         # influx database name
-        'influx_host': 'localhost',
+        "influx_host": "localhost",
         # influx database location
-        'influx_port': 8086,
-        'influx_user': '',
+        "influx_port": 8086,
+        "influx_user": "",
         # influx username
-        'influx_pwd': '',
+        "influx_pwd": "",
         # influx password
-        'influx_timeout': 10,
+        "influx_timeout": 10,
         # timeout on influx requests
-        'influx_retries': 3,
+        "influx_retries": 3,
         # attempts to retry influx request
         # prometheus config
-        'prometheus_port': 9303,
-        'prometheus_addr': '0.0.0.0',
-        'prometheus_test_thread': False,
+        "prometheus_port": 9303,
+        "prometheus_addr": "0.0.0.0",
+        "prometheus_test_thread": False,
     }
 
     db_defaults_types = {
-        'type': str,
-        'file': str,
-        'path': str,
-        'compress': bool,
-        'influx_db': str,
-        'influx_host': str,
-        'influx_port': int,
-        'influx_user': str,
-        'influx_pwd': str,
-        'influx_timeout': int,
-        'influx_retries': int,
-        'prometheus_port': int,
-        'prometheus_addr': str,
-        'prometheus_test_thread': bool,
+        "type": str,
+        "file": str,
+        "path": str,
+        "compress": bool,
+        "influx_db": str,
+        "influx_host": str,
+        "influx_port": int,
+        "influx_user": str,
+        "influx_pwd": str,
+        "influx_timeout": int,
+        "influx_retries": int,
+        "prometheus_port": int,
+        "prometheus_addr": str,
+        "prometheus_test_thread": bool,
     }
 
     defaults = {
-        'name': None,
-        'type': None,
-        'dps': None,
-        'all_dps': False,
-        'interval': 30,
-        'db': None,
-        'dbs': None,
-        'db_type': 'text',
+        "name": None,
+        "type": None,
+        "dps": None,
+        "all_dps": False,
+        "interval": 30,
+        "db": None,
+        "dbs": None,
+        "db_type": "text",
     }
 
     defaults_types = {
-        'name': str,
-        'type': str,
-        'dps': list,
-        'all_dps': bool,
-        'interval': int,
-        'db': str,
-        'dbs': list,
-        'db_type': str,
+        "name": str,
+        "type": str,
+        "dps": list,
+        "all_dps": bool,
+        "interval": int,
+        "db": str,
+        "dbs": list,
+        "db_type": str,
     }
 
     def __init__(self, _id, dp_id, conf, prom_client):
         self.db = None  # pylint: disable=invalid-name
         self.dbs = None
         self.dp = None  # pylint: disable=invalid-name
         self.all_dps = None
@@ -172,33 +172,39 @@
         self.name = str(self._id)
         self.prom_client = prom_client
 
     def add_db(self, db_conf):
         """Add database config to this watcher."""
         self._check_conf_types(db_conf, self.db_defaults_types)
         db_conf = deepcopy(db_conf)
-        db_type = db_conf.pop('type')
-        db_conf['db_type'] = db_type
+        db_type = db_conf.pop("type")
+        db_conf["db_type"] = db_type
         self.update(db_conf)
         test_config_condition(
-            self.file is not None and not
-            (os.path.dirname(self.file) and os.access(os.path.dirname(self.file), os.W_OK)),
-            '%s is not writable' % self.file)
+            self.file is not None
+            and not (
+                os.path.dirname(self.file)
+                and os.access(os.path.dirname(self.file), os.W_OK)
+            ),
+            "%s is not writable" % self.file,
+        )
         test_config_condition(
             self.path is not None and not os.access(self.path, os.W_OK),
-            '%s is not writable' % self.file)
+            "%s is not writable" % self.file,
+        )
 
     def add_dp(self, dp):
         """Add a datapath to this watcher."""
         self.dp = dp
 
     def check_config(self):
         super().check_config()
         test_config_condition(
             self.all_dps and self.dps is not None,
-            'all_dps and dps cannot be set together')
-        test_config_condition(
-            not self.type, 'type must be set')
-        valid_types = {'flow_table', 'port_stats', 'port_state', 'meter_stats'}
+            "all_dps and dps cannot be set together",
+        )
+        test_config_condition(not self.type, "type must be set")
+        valid_types = {"flow_table", "port_stats", "port_state", "meter_stats"}
         test_config_condition(
             self.type not in valid_types,
-            'type %s not one of %s' % (self.type, valid_types))
+            "type %s not one of %s" % (self.type, valid_types),
+        )
```

### Comparing `c65faucet-1.0.49/hw_switch_config.yaml` & `c65faucet-1.0.50/hw_switch_config.yaml`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/ofctl_rest/ofctl_rest.py` & `c65faucet-1.0.50/ofctl_rest/ofctl_rest.py`

 * *Files 25% similar despite different names*

```diff
@@ -9,14 +9,18 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 # implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+# pylint: disable=unused-argument,disable=missing-function-docstring,disable=use-dict-literal
+# pylint: disable=no-member,disable=invalid-name,disable=too-many-arguments,disable=too-few-public-methods
+# pytype: disable=attribute-error
+
 import os
 import logging
 import json
 import ast
 
 from os_ken.base import app_manager
 from os_ken.controller import ofp_event
@@ -27,29 +31,27 @@
 from os_ken.ofproto import ofproto_v1_3
 from os_ken.lib import ofctl_v1_3
 from os_ken.lib import hub
 from wsgi import ControllerBase
 from wsgi import Response
 from wsgi import WSGIApplication, WSGIServer
 
-LOG = logging.getLogger('os_ken.app.ofctl_rest')
+LOG = logging.getLogger("os_ken.app.ofctl_rest")
 
-DEFAULT_WSGI_HOST = '0.0.0.0'
+DEFAULT_WSGI_HOST = "0.0.0.0"
 DEFAULT_WSGI_PORT = 8080
 
-OFCTL_HOST = os.getenv('OFCTL_HOST', '0.0.0.0')
-OFCTL_PORT = int(os.getenv('OFCTL_PORT', '8080'))
+OFCTL_HOST = os.getenv("OFCTL_HOST", "0.0.0.0")
+OFCTL_PORT = int(os.getenv("OFCTL_PORT", "8080"))
 
 # supported ofctl versions in this restful app
 supported_ofctl = {
     ofproto_v1_3.OFP_VERSION: ofctl_v1_3,
 }
 
-# pylint: disable=missing-function-docstring,disable=invalid-name,disable=missing-class-docstring,disable=too-few-public-methods,disable=unused-argument,disable=no-member
-
 # REST API
 #
 
 # Retrieve the switch stats
 #
 # get the list of all switches
 # GET /stats/switches
@@ -179,133 +181,130 @@
 #
 #
 # send a experimeter message
 # POST /stats/experimenter/<dpid>
 
 
 class CommandNotFoundError(OSKenException):
-    message = 'No such command : %(cmd)s'
+    message = "No such command : %(cmd)s"
 
 
 class PortNotFoundError(OSKenException):
-    message = 'No such port info: %(port_no)s'
+    message = "No such port info: %(port_no)s"
 
 
 def stats_method(method):
     def wrapper(self, req, dpid, *args, **kwargs):
         # Get datapath instance from DPSet
         try:
             dp = self.dpset.get(int(str(dpid), 0))
         except ValueError:
-            LOG.exception('Invalid dpid: %s', dpid)
+            LOG.exception("Invalid dpid: %s", dpid)
             return Response(status=400)
         if dp is None:
-            LOG.error('No such Datapath: %s', dpid)
+            LOG.error("No such Datapath: %s", dpid)
             return Response(status=404)
 
         # Get lib/ofctl_* module
         try:
             ofctl = supported_ofctl.get(dp.ofproto.OFP_VERSION)
         except KeyError:
-            LOG.exception('Unsupported OF version: %s',
-                          dp.ofproto.OFP_VERSION)
+            LOG.exception("Unsupported OF version: %s", dp.ofproto.OFP_VERSION)
             return Response(status=501)
 
         # Invoke StatsController method
         try:
             ret = method(self, req, dp, ofctl, *args, **kwargs)
-            return Response(content_type='application/json',
-                            body=json.dumps(ret))
+            return Response(content_type="application/json", body=json.dumps(ret))
         except ValueError:
-            LOG.exception('Invalid syntax: %s', req.body)
+            LOG.exception("Invalid syntax: %s", req.body)
             return Response(status=400)
         except AttributeError:
-            LOG.exception('Unsupported OF request in this version: %s',
-                          dp.ofproto.OFP_VERSION)
+            LOG.exception(
+                "Unsupported OF request in this version: %s", dp.ofproto.OFP_VERSION
+            )
             return Response(status=501)
 
     return wrapper
 
 
 def command_method(method):
     def wrapper(self, req, *args, **kwargs):
         # Parse request json body
         try:
             if req.body:
                 # We use ast.literal_eval() to parse request json body
                 # instead of json.loads().
                 # Because we need to parse binary format body
                 # in send_experimenter().
-                body = ast.literal_eval(req.body.decode('utf-8'))
+                body = ast.literal_eval(req.body.decode("utf-8"))
             else:
                 body = {}
         except SyntaxError:
-            LOG.exception('Invalid syntax: %s', req.body)
+            LOG.exception("Invalid syntax: %s", req.body)
             return Response(status=400)
 
         # Get datapath_id from request parameters
-        dpid = body.get('dpid', None)
+        dpid = body.get("dpid", None)
         if not dpid:
             try:
-                dpid = kwargs.pop('dpid')
+                dpid = kwargs.pop("dpid")
             except KeyError:
-                LOG.exception('Cannot get dpid from request parameters')
+                LOG.exception("Cannot get dpid from request parameters")
                 return Response(status=400)
 
         # Get datapath instance from DPSet
         try:
             dp = self.dpset.get(int(str(dpid), 0))
         except ValueError:
-            LOG.exception('Invalid dpid: %s', dpid)
+            LOG.exception("Invalid dpid: %s", dpid)
             return Response(status=400)
         if dp is None:
-            LOG.error('No such Datapath: %s', dpid)
+            LOG.error("No such Datapath: %s", dpid)
             return Response(status=404)
 
         # Get lib/ofctl_* module
         try:
             ofctl = supported_ofctl.get(dp.ofproto.OFP_VERSION)
         except KeyError:
-            LOG.exception('Unsupported OF version: version=%s',
-                          dp.ofproto.OFP_VERSION)
+            LOG.exception("Unsupported OF version: version=%s", dp.ofproto.OFP_VERSION)
             return Response(status=501)
 
         # Invoke StatsController method
         try:
             method(self, req, dp, ofctl, body, *args, **kwargs)
             return Response(status=200)
         except ValueError:
-            LOG.exception('Invalid syntax: %s', req.body)
+            LOG.exception("Invalid syntax: %s", req.body)
             return Response(status=400)
         except AttributeError:
-            LOG.exception('Unsupported OF request in this version: %s',
-                          dp.ofproto.OFP_VERSION)
+            LOG.exception(
+                "Unsupported OF request in this version: %s", dp.ofproto.OFP_VERSION
+            )
             return Response(status=501)
         except CommandNotFoundError as e:
             LOG.exception(e.message)
             return Response(status=404)
         except PortNotFoundError as e:
             LOG.exception(e.message)
             return Response(status=404)
 
     return wrapper
 
 
 class StatsController(ControllerBase):
-    # pytype: disable=attribute-error
-
     def __init__(self, req, link, data, **config):
         super().__init__(req, link, data, **config)
-        self.dpset = data['dpset']
-        self.waiters = data['waiters']
+        self.dpset = data["dpset"]
+        self.waiters = data["waiters"]
 
     def get_dpids(self, req, **_kwargs):
         dps = list(self.dpset.dps.keys())
         body = json.dumps(dps)
-        return Response(content_type='application/json', body=body)
+        return Response(content_type="application/json", body=body)
 
     @stats_method
     def get_desc_stats(self, req, dp, ofctl, **kwargs):
         return ofctl.get_desc_stats(dp, self.waiters)
 
     @stats_method
     def get_flow_desc(self, req, dp, ofctl, **kwargs):
@@ -334,16 +333,15 @@
     def get_port_stats(self, req, dp, ofctl, port=None, **kwargs):
         if port == "ALL":
             port = None
 
         return ofctl.get_port_stats(dp, self.waiters, port)
 
     @stats_method
-    def get_queue_stats(self, req, dp, ofctl,
-                        port=None, queue_id=None, **kwargs):
+    def get_queue_stats(self, req, dp, ofctl, port=None, queue_id=None, **kwargs):
         if port == "ALL":
             port = None
 
         if queue_id == "ALL":
             queue_id = None
 
         return ofctl.get_queue_stats(dp, self.waiters, port, queue_id)
@@ -352,16 +350,15 @@
     def get_queue_config(self, req, dp, ofctl, port=None, **kwargs):
         if port == "ALL":
             port = None
 
         return ofctl.get_queue_config(dp, self.waiters, port)
 
     @stats_method
-    def get_queue_desc(self, req, dp, ofctl,
-                       port=None, queue=None, **_kwargs):
+    def get_queue_desc(self, req, dp, ofctl, port=None, queue=None, **_kwargs):
         if port == "ALL":
             port = None
 
         if queue == "ALL":
             queue = None
 
         return ofctl.get_queue_desc(dp, self.waiters, port, queue)
@@ -410,326 +407,476 @@
     def get_port_desc(self, req, dp, ofctl, port_no=None, **kwargs):
         return ofctl.get_port_desc(dp, self.waiters)
 
     @stats_method
     def get_role(self, req, dp, ofctl, **kwargs):
         return ofctl.get_role(dp, self.waiters)
 
-    @staticmethod
     @command_method
-    def mod_flow_entry(req, dp, ofctl, flow, cmd, **kwargs):
+    def mod_flow_entry(self, req, dp, ofctl, flow, cmd, **kwargs):
         cmd_convert = {
-            'add': dp.ofproto.OFPFC_ADD,
-            'modify': dp.ofproto.OFPFC_MODIFY,
-            'modify_strict': dp.ofproto.OFPFC_MODIFY_STRICT,
-            'delete': dp.ofproto.OFPFC_DELETE,
-            'delete_strict': dp.ofproto.OFPFC_DELETE_STRICT,
+            "add": dp.ofproto.OFPFC_ADD,
+            "modify": dp.ofproto.OFPFC_MODIFY,
+            "modify_strict": dp.ofproto.OFPFC_MODIFY_STRICT,
+            "delete": dp.ofproto.OFPFC_DELETE,
+            "delete_strict": dp.ofproto.OFPFC_DELETE_STRICT,
         }
         mod_cmd = cmd_convert.get(cmd, None)
         if mod_cmd is None:
             raise CommandNotFoundError(cmd=cmd)
 
         ofctl.mod_flow_entry(dp, flow, mod_cmd)
 
-    @staticmethod
     @command_method
-    def delete_flow_entry(req, dp, ofctl, flow, **kwargs):
-        flow = {'table_id': dp.ofproto.OFPTT_ALL}
+    def delete_flow_entry(self, req, dp, ofctl, flow, **kwargs):
+        flow = {"table_id": dp.ofproto.OFPTT_ALL}
         ofctl.mod_flow_entry(dp, flow, dp.ofproto.OFPFC_DELETE)
 
-    @staticmethod
     @command_method
-    def mod_meter_entry(req, dp, ofctl, meter, cmd, **kwargs):
+    def mod_meter_entry(self, req, dp, ofctl, meter, cmd, **kwargs):
         cmd_convert = {
-            'add': dp.ofproto.OFPMC_ADD,
-            'modify': dp.ofproto.OFPMC_MODIFY,
-            'delete': dp.ofproto.OFPMC_DELETE,
+            "add": dp.ofproto.OFPMC_ADD,
+            "modify": dp.ofproto.OFPMC_MODIFY,
+            "delete": dp.ofproto.OFPMC_DELETE,
         }
         mod_cmd = cmd_convert.get(cmd, None)
         if mod_cmd is None:
             raise CommandNotFoundError(cmd=cmd)
 
         ofctl.mod_meter_entry(dp, meter, mod_cmd)
 
-    @staticmethod
     @command_method
-    def mod_group_entry(req, dp, ofctl, group, cmd, **kwargs):
+    def mod_group_entry(self, req, dp, ofctl, group, cmd, **kwargs):
         cmd_convert = {
-            'add': dp.ofproto.OFPGC_ADD,
-            'modify': dp.ofproto.OFPGC_MODIFY,
-            'delete': dp.ofproto.OFPGC_DELETE,
+            "add": dp.ofproto.OFPGC_ADD,
+            "modify": dp.ofproto.OFPGC_MODIFY,
+            "delete": dp.ofproto.OFPGC_DELETE,
         }
         mod_cmd = cmd_convert.get(cmd, None)
         if mod_cmd is None:
             raise CommandNotFoundError(cmd=cmd)
 
         ofctl.mod_group_entry(dp, group, mod_cmd)
 
     @command_method
     def mod_port_behavior(self, req, dp, ofctl, port_config, cmd, **kwargs):
-        port_no = port_config.get('port_no', None)
+        port_no = port_config.get("port_no", None)
         port_no = int(str(port_no), 0)
 
         port_info = self.dpset.port_state[int(dp.id)].get(port_no)
         if port_info:
-            port_config.setdefault('hw_addr', port_info.hw_addr)
-            port_config.setdefault('advertise', port_info.advertised)
+            port_config.setdefault("hw_addr", port_info.hw_addr)
+            port_config.setdefault("advertise", port_info.advertised)
         else:
             raise PortNotFoundError(port_no=port_no)
 
-        if cmd != 'modify':
+        if cmd != "modify":
             raise CommandNotFoundError(cmd=cmd)
 
         ofctl.mod_port_behavior(dp, port_config)
 
-    @staticmethod
     @command_method
-    def send_experimenter(req, dp, ofctl, exp, **kwargs):
+    def send_experimenter(self, req, dp, ofctl, exp, **kwargs):
         ofctl.send_experimenter(dp, exp)
 
-    @staticmethod
     @command_method
-    def set_role(req, dp, ofctl, role, **kwargs):
+    def set_role(self, req, dp, ofctl, role, **kwargs):
         ofctl.set_role(dp, role)
 
 
 class RestStatsApi(app_manager.OSKenApp):
     OFP_VERSIONS = [ofproto_v1_3.OFP_VERSION]
-    _CONTEXTS = {
-        'dpset': dpset.DPSet,
-        'wsgi': WSGIApplication
-    }
+    _CONTEXTS = {"dpset": dpset.DPSet, "wsgi": WSGIApplication}
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
-        self.dpset = kwargs['dpset']
-        wsgi = kwargs['wsgi']
+        self.dpset = kwargs["dpset"]
+        wsgi = kwargs["wsgi"]
         self.waiters = {}
         self.data = {}
-        self.data['dpset'] = self.dpset
-        self.data['waiters'] = self.waiters
+        self.data["dpset"] = self.dpset
+        self.data["waiters"] = self.waiters
         mapper = wsgi.mapper
 
-        wsgi.registory['StatsController'] = self.data
-        path = '/stats'
-        uri = path + '/switches'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_dpids',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/desc/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_desc_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/flowdesc/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_flow_stats',
-                       conditions={"method": ['GET', 'POST']})
-
-        uri = path + '/flow/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_flow_stats',
-                       conditions={"method": ['GET', 'POST']})
-
-        uri = path + '/aggregateflow/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController,
-                       action='get_aggregate_flow_stats',
-                       conditions={"method": ['GET', 'POST']})
-
-        uri = path + '/table/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_table_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/tablefeatures/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_table_features',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/port/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_port_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/port/{dpid}/{port}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_port_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/queue/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_queue_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/queue/{dpid}/{port}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_queue_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/queue/{dpid}/{port}/{queue_id}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_queue_stats',
-                       conditions={"method": ['GET']})
-        uri = path + '/queueconfig/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_queue_config',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/queueconfig/{dpid}/{port}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_queue_config',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/queuedesc/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_queue_desc',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/queuedesc/{dpid}/{port}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_queue_desc',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/queuedesc/{dpid}/{port}/{queue}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_queue_desc',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/meterfeatures/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_meter_features',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/meterconfig/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_meter_config',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/meterconfig/{dpid}/{meter_id}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_meter_config',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/meterdesc/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_meter_desc',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/meterdesc/{dpid}/{meter_id}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_meter_desc',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/meter/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_meter_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/meter/{dpid}/{meter_id}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_meter_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/groupfeatures/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_group_features',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/groupdesc/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_group_desc',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/groupdesc/{dpid}/{group_id}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_group_desc',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/group/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_group_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/group/{dpid}/{group_id}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_group_stats',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/portdesc/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_port_desc',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/portdesc/{dpid}/{port_no}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_port_desc',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/role/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='get_role',
-                       conditions={"method": ['GET']})
-
-        uri = path + '/flowentry/{cmd}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='mod_flow_entry',
-                       conditions={"method": ['POST']})
-
-        uri = path + '/flowentry/clear/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='delete_flow_entry',
-                       conditions={"method": ['DELETE']})
-
-        uri = path + '/meterentry/{cmd}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='mod_meter_entry',
-                       conditions={"method": ['POST']})
-
-        uri = path + '/groupentry/{cmd}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='mod_group_entry',
-                       conditions={"method": ['POST']})
-
-        uri = path + '/portdesc/{cmd}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='mod_port_behavior',
-                       conditions={"method": ['POST']})
-
-        uri = path + '/experimenter/{dpid}'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='send_experimenter',
-                       conditions={"method": ['POST']})
-
-        uri = path + '/role'
-        mapper.connect('stats', uri,
-                       controller=StatsController, action='set_role',
-                       conditions={"method": ['POST']})
+        wsgi.registory["StatsController"] = self.data
+        path = "/stats"
+        uri = path + "/switches"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_dpids",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/desc/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_desc_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/flowdesc/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_flow_stats",
+            conditions=dict(method=["GET", "POST"]),
+        )
+
+        uri = path + "/flow/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_flow_stats",
+            conditions=dict(method=["GET", "POST"]),
+        )
+
+        uri = path + "/aggregateflow/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_aggregate_flow_stats",
+            conditions=dict(method=["GET", "POST"]),
+        )
+
+        uri = path + "/table/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_table_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/tablefeatures/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_table_features",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/port/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_port_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/port/{dpid}/{port}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_port_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/queue/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_queue_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/queue/{dpid}/{port}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_queue_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/queue/{dpid}/{port}/{queue_id}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_queue_stats",
+            conditions=dict(method=["GET"]),
+        )
+        uri = path + "/queueconfig/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_queue_config",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/queueconfig/{dpid}/{port}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_queue_config",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/queuedesc/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_queue_desc",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/queuedesc/{dpid}/{port}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_queue_desc",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/queuedesc/{dpid}/{port}/{queue}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_queue_desc",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/meterfeatures/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_meter_features",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/meterconfig/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_meter_config",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/meterconfig/{dpid}/{meter_id}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_meter_config",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/meterdesc/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_meter_desc",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/meterdesc/{dpid}/{meter_id}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_meter_desc",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/meter/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_meter_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/meter/{dpid}/{meter_id}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_meter_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/groupfeatures/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_group_features",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/groupdesc/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_group_desc",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/groupdesc/{dpid}/{group_id}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_group_desc",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/group/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_group_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/group/{dpid}/{group_id}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_group_stats",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/portdesc/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_port_desc",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/portdesc/{dpid}/{port_no}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_port_desc",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/role/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="get_role",
+            conditions=dict(method=["GET"]),
+        )
+
+        uri = path + "/flowentry/{cmd}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="mod_flow_entry",
+            conditions=dict(method=["POST"]),
+        )
+
+        uri = path + "/flowentry/clear/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="delete_flow_entry",
+            conditions=dict(method=["DELETE"]),
+        )
+
+        uri = path + "/meterentry/{cmd}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="mod_meter_entry",
+            conditions=dict(method=["POST"]),
+        )
+
+        uri = path + "/groupentry/{cmd}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="mod_group_entry",
+            conditions=dict(method=["POST"]),
+        )
+
+        uri = path + "/portdesc/{cmd}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="mod_port_behavior",
+            conditions=dict(method=["POST"]),
+        )
+
+        uri = path + "/experimenter/{dpid}"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="send_experimenter",
+            conditions=dict(method=["POST"]),
+        )
+
+        uri = path + "/role"
+        mapper.connect(
+            "stats",
+            uri,
+            controller=StatsController,
+            action="set_role",
+            conditions=dict(method=["POST"]),
+        )
 
         self.server = WSGIServer(wsgi, OFCTL_HOST, OFCTL_PORT)
         self.server_thread = hub.spawn(self.server.serve_forever)
 
-    @set_ev_cls([ofp_event.EventOFPStatsReply,
-                 ofp_event.EventOFPDescStatsReply,
-                 ofp_event.EventOFPFlowStatsReply,
-                 ofp_event.EventOFPAggregateStatsReply,
-                 ofp_event.EventOFPTableStatsReply,
-                 ofp_event.EventOFPTableFeaturesStatsReply,
-                 ofp_event.EventOFPPortStatsReply,
-                 ofp_event.EventOFPQueueStatsReply,
-                 ofp_event.EventOFPQueueDescStatsReply,
-                 ofp_event.EventOFPMeterStatsReply,
-                 ofp_event.EventOFPMeterFeaturesStatsReply,
-                 ofp_event.EventOFPMeterConfigStatsReply,
-                 ofp_event.EventOFPGroupStatsReply,
-                 ofp_event.EventOFPGroupFeaturesStatsReply,
-                 ofp_event.EventOFPGroupDescStatsReply,
-                 ofp_event.EventOFPPortDescStatsReply
-                 ], MAIN_DISPATCHER)
+    @set_ev_cls(
+        [
+            ofp_event.EventOFPStatsReply,
+            ofp_event.EventOFPDescStatsReply,
+            ofp_event.EventOFPFlowStatsReply,
+            ofp_event.EventOFPAggregateStatsReply,
+            ofp_event.EventOFPTableStatsReply,
+            ofp_event.EventOFPTableFeaturesStatsReply,
+            ofp_event.EventOFPPortStatsReply,
+            ofp_event.EventOFPQueueStatsReply,
+            ofp_event.EventOFPQueueDescStatsReply,
+            ofp_event.EventOFPMeterStatsReply,
+            ofp_event.EventOFPMeterFeaturesStatsReply,
+            ofp_event.EventOFPMeterConfigStatsReply,
+            ofp_event.EventOFPGroupStatsReply,
+            ofp_event.EventOFPGroupFeaturesStatsReply,
+            ofp_event.EventOFPGroupDescStatsReply,
+            ofp_event.EventOFPPortDescStatsReply,
+        ],
+        MAIN_DISPATCHER,
+    )
     def stats_reply_handler(self, ev):
         msg = ev.msg
         dp = msg.datapath
 
         if dp.id not in self.waiters:
             return
         if msg.xid not in self.waiters[dp.id]:
@@ -740,18 +887,22 @@
         flags = dp.ofproto.OFPMPF_REPLY_MORE
 
         if msg.flags & flags:
             return
         del self.waiters[dp.id][msg.xid]
         lock.set()
 
-    @set_ev_cls([ofp_event.EventOFPSwitchFeatures,
-                 ofp_event.EventOFPQueueGetConfigReply,
-                 ofp_event.EventOFPRoleReply,
-                 ], MAIN_DISPATCHER)
+    @set_ev_cls(
+        [
+            ofp_event.EventOFPSwitchFeatures,
+            ofp_event.EventOFPQueueGetConfigReply,
+            ofp_event.EventOFPRoleReply,
+        ],
+        MAIN_DISPATCHER,
+    )
     def features_reply_handler(self, ev):
         msg = ev.msg
         dp = msg.datapath
 
         if dp.id not in self.waiters:
             return
         if msg.xid not in self.waiters[dp.id]:
```

### Comparing `c65faucet-1.0.49/ofctl_rest/wsgi.py` & `c65faucet-1.0.50/ofctl_rest/wsgi.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,14 +10,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
 # implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+# pylint: disable=missing-class-docstring,disable=missing-function-docstring,disable=invalid-name
+# pylint: disable=import-outside-toplevel,disable=too-few-public-methods
+
 import inspect
 from types import MethodType
 
 from routes import Mapper
 from routes.util import URLGenerator
 import six
 from tinyrpc.server import RPCServer
@@ -25,119 +28,133 @@
 from tinyrpc.protocols.jsonrpc import JSONRPCProtocol
 from tinyrpc.transports import ServerTransport, ClientTransport
 from tinyrpc.client import RPCClient
 import webob.dec
 import webob.exc
 from webob.request import Request as webob_Request
 from webob.response import Response as webob_Response
-import eventlet.wsgi
 
 from os_ken.lib import hub
 
-# pylint: disable=missing-function-docstring,disable=invalid-name,disable=missing-class-docstring,disable=too-few-public-methods
+HEX_PATTERN = r"0x[0-9a-z]+"
+DIGIT_PATTERN = r"[1-9][0-9]*"
 
 
 def route(name, path, methods=None, requirements=None):
     def _route(controller_method):
         controller_method.routing_info = {
-            'name': name,
-            'path': path,
-            'methods': methods,
-            'requirements': requirements,
+            "name": name,
+            "path": path,
+            "methods": methods,
+            "requirements": requirements,
         }
         return controller_method
+
     return _route
 
 
 class Request(webob_Request):
     """
     Wrapper class for webob.request.Request.
 
     The behavior of this class is the same as webob.request.Request
     except for setting "charset" to "UTF-8" automatically.
     """
+
     DEFAULT_CHARSET = "UTF-8"
 
-    def __init__(self, environ, charset=DEFAULT_CHARSET, *args, **kwargs):
-        super().__init__(
-            environ, charset=charset, *args, **kwargs)
+    def __init__(self, environ, *args, charset=DEFAULT_CHARSET, **kwargs):
+        super().__init__(environ, *args, **kwargs, charset=charset)
 
 
 class Response(webob_Response):
     """
     Wrapper class for webob.response.Response.
 
     The behavior of this class is the same as webob.response.Response
     except for setting "charset" to "UTF-8" automatically.
     """
+
     DEFAULT_CHARSET = "UTF-8"
 
-    def __init__(self, charset=DEFAULT_CHARSET, *args, **kwargs):
-        super().__init__(charset=charset, *args, **kwargs)
+    def __init__(self, *args, charset=DEFAULT_CHARSET, **kwargs):
+        super().__init__(*args, **kwargs, charset=charset)
 
 
 class WebSocketRegistrationWrapper:
-
     def __init__(self, func, controller):
         self._controller = controller
         self._controller_method = MethodType(func, controller)
 
     def __call__(self, ws):
         wsgi_application = self._controller.parent
         ws_manager = wsgi_application.websocketmanager
         ws_manager.add_connection(ws)
         try:
-            self._controller_method(ws)  # pylint: disable=not-callable
+            self._controller_method(ws)
         finally:
             ws_manager.delete_connection(ws)
 
 
 class _AlreadyHandledResponse(Response):
     # XXX: Eventlet API should not be used directly.
     # https://github.com/benoitc/gunicorn/pull/2581
-    _ALREADY_HANDLED = getattr(eventlet.wsgi, "ALREADY_HANDLED", None)
+    from packaging import version
+    import eventlet
+
+    if version.parse(eventlet.__version__) >= version.parse("0.30.3"):
+        import eventlet.wsgi
+
+        _ALREADY_HANDLED = getattr(eventlet.wsgi, "ALREADY_HANDLED", None)
+    else:
+        from eventlet.wsgi import ALREADY_HANDLED  # pylint: disable=no-name-in-module
+
+        _ALREADY_HANDLED = ALREADY_HANDLED
 
     def __call__(self, environ, start_response):
         return self._ALREADY_HANDLED
 
+
 def websocket(name, path):
     def _websocket(controller_func):
         def __websocket(self, req, **_):
             wrapper = WebSocketRegistrationWrapper(controller_func, self)
             ws_wsgi = hub.WebSocketWSGI(wrapper)
             ws_wsgi(req.environ, req.start_response)
             # XXX: In order to prevent the writing to a already closed socket.
             #      This issue is caused by combined use:
             #       - webob.dec.wsgify()
             #       - eventlet.wsgi.HttpProtocol.handle_one_response()
             return _AlreadyHandledResponse()
+
         __websocket.routing_info = {
-            'name': name,
-            'path': path,
-            'methods': None,
-            'requirements': None,
+            "name": name,
+            "path": path,
+            "methods": None,
+            "requirements": None,
         }
         return __websocket
+
     return _websocket
 
 
 class ControllerBase:
-    special_vars = ['action', 'controller']
+    special_vars = ["action", "controller"]
 
     def __init__(self, req, link, data, **config):
         self.req = req
         self.link = link
         self.data = data
         self.parent = None
         for name, value in config.items():
             setattr(self, name, value)
 
     def __call__(self, req):
-        action = self.req.urlvars.get('action', 'index')
-        if hasattr(self, '__before__'):
+        action = self.req.urlvars.get("action", "index")
+        if hasattr(self, "__before__"):
             self.__before__()
 
         kwargs = self.req.urlvars.copy()
         for attr in self.special_vars:
             if attr in kwargs:
                 del kwargs[attr]
 
@@ -180,29 +197,27 @@
             return
 
     def _spawn(self, func, *args, **kwargs):
         hub.spawn(func, *args, **kwargs)
 
 
 class WebSocketClientTransport(ClientTransport):
-
     def __init__(self, ws, queue):
         self.ws = ws
         self.queue = queue
 
     def send_message(self, message, expect_reply=True):
         self.ws.send(six.text_type(message))
 
         if expect_reply:
             return self.queue.get()
         return None
 
 
 class WebSocketRPCClient(RPCClient):
-
     def __init__(self, ws):
         self.ws = ws
         self.queue = hub.Queue()
         super().__init__(
             JSONRPCProtocol(),
             WebSocketClientTransport(ws, self.queue),
         )
@@ -213,20 +228,19 @@
             if msg is None:
                 break
             self.queue.put(msg)
 
 
 class wsgify_hack(webob.dec.wsgify):
     def __call__(self, environ, start_response):
-        self.kwargs['start_response'] = start_response
+        self.kwargs["start_response"] = start_response
         return super().__call__(environ, start_response)
 
 
 class WebSocketManager:
-
     def __init__(self):
         self._connections = []
 
     def add_connection(self, ws):
         self._connections.append(ws)
 
     def delete_connection(self, ws):
@@ -262,44 +276,47 @@
             return webob.exc.HTTPNotFound()
 
         req.start_response = start_response
         req.urlvars = match
         link = URLGenerator(self.mapper, req.environ)
 
         data = None
-        name = match['controller'].__name__
+        name = match["controller"].__name__
         if name in self.registory:
             data = self.registory[name]
 
-        controller = match['controller'](req, link, data, **self.config)
+        controller = match["controller"](req, link, data, **self.config)
         controller.parent = self
         return controller(req)
 
     def register(self, controller, data=None):
         def _target_filter(attr):
             if not inspect.ismethod(attr) and not inspect.isfunction(attr):
                 return False
-            if not hasattr(attr, 'routing_info'):
+            if not hasattr(attr, "routing_info"):
                 return False
             return True
+
         methods = inspect.getmembers(controller, _target_filter)
         for method_name, method in methods:
-            routing_info = getattr(method, 'routing_info')
-            name = routing_info['name']
-            path = routing_info['path']
+            routing_info = getattr(method, "routing_info")
+            name = routing_info["name"]
+            path = routing_info["path"]
             conditions = {}
-            if routing_info.get('methods'):
-                conditions['method'] = routing_info['methods']
-            requirements = routing_info.get('requirements') or {}
-            self.mapper.connect(name,
-                                path,
-                                controller=controller,
-                                requirements=requirements,
-                                action=method_name,
-                                conditions=conditions)
+            if routing_info.get("methods"):
+                conditions["method"] = routing_info["methods"]
+            requirements = routing_info.get("requirements") or {}
+            self.mapper.connect(
+                name,
+                path,
+                controller=controller,
+                requirements=requirements,
+                action=method_name,
+                conditions=conditions,
+            )
         if data:
             self.registory[controller.__name__] = data
 
     @property
     def websocketmanager(self):
         return self._wsmanager
```

### Comparing `c65faucet-1.0.49/setup.cfg` & `c65faucet-1.0.50/setup.cfg`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/setup.py` & `c65faucet-1.0.50/setup.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,41 +11,48 @@
 import shutil
 import sys
 
 from pkg_resources import resource_filename
 from setuptools import setup
 
 if sys.version_info < (3,):
-    print("""You are trying to install faucet on python {py}
+    print(
+        """You are trying to install faucet on python {py}
 
-Faucet is not compatible with python 2, please upgrade to python 3.8 or newer."""
-          .format(py='.'.join([str(v) for v in sys.version_info[:3]])), file=sys.stderr)
+Faucet is not compatible with python 2, please upgrade to python 3.8 or newer.""".format(
+            py=".".join([str(v) for v in sys.version_info[:3]])
+        ),
+        file=sys.stderr,
+    )
     sys.exit(1)
 elif sys.version_info < (3, 8):
-    print("""You are trying to install faucet on python {py}
+    print(
+        """You are trying to install faucet on python {py}
 
 Faucet 1.9.0 and above are no longer compatible with older versions of python 3.
 
-Please upgrade to python 3.7 or newer."""
-          .format(py='.'.join([str(v) for v in sys.version_info[:3]])))
+Please upgrade to python 3.7 or newer.""".format(
+            py=".".join([str(v) for v in sys.version_info[:3]])
+        )
+    )
     sys.exit(1)
 
 
 def install_configs():
-    """ Install configuration files to /etc """
+    """Install configuration files to /etc"""
 
-    dst_ryu_conf_dir = '/etc/faucet/'
-    dst_ryu_conf = os.path.join(dst_ryu_conf_dir, 'ryu.conf')
-    dst_faucet_conf_dir = '/etc/faucet/'
+    dst_ryu_conf_dir = "/etc/faucet/"
+    dst_ryu_conf = os.path.join(dst_ryu_conf_dir, "ryu.conf")
+    dst_faucet_conf_dir = "/etc/faucet/"
     src_ryu_conf = resource_filename(__name__, "etc/faucet/ryu.conf")
     src_faucet_conf_dir = resource_filename(__name__, "etc/faucet/")
-    faucet_log_dir = '/var/log/faucet/'
+    faucet_log_dir = "/var/log/faucet/"
 
-    old_ryu_conf = '/etc/ryu/ryu.conf'
-    old_faucet_conf_dir = '/etc/ryu/faucet/'
+    old_ryu_conf = "/etc/ryu/ryu.conf"
+    old_faucet_conf_dir = "/etc/ryu/faucet/"
 
     def setup_ryu_conf():
         if not os.path.exists(dst_ryu_conf_dir):
             print("Creating %s" % dst_ryu_conf_dir)
             os.makedirs(dst_ryu_conf_dir)
         if not os.path.isfile(dst_ryu_conf):
             if os.path.exists(old_ryu_conf) and os.path.isfile(old_ryu_conf):
@@ -79,26 +86,32 @@
 
     try:
         setup_ryu_conf()
         setup_faucet_conf()
         setup_faucet_log()
     except OSError as exception:
         if exception.errno == errno.EACCES:
-            print("Permission denied creating %s, skipping copying configs"
-                  % exception.filename)
+            print(
+                "Permission denied creating %s, skipping copying configs"
+                % exception.filename
+            )
         elif exception.errno == errno.ENOENT:
-            print("File not found creating %s, skipping copying configs"
-                  % exception.filename)
+            print(
+                "File not found creating %s, skipping copying configs"
+                % exception.filename
+            )
         else:
             raise
 
 
 setup(
-    name='faucet',
-    setup_requires=['pbr>=1.9', 'setuptools>=17.1'],
-    python_requires='>=3.8',
-    pbr=True
+    name="faucet",
+    setup_requires=["pbr>=1.9", "setuptools>=17.1"],
+    python_requires=">=3.8",
+    pbr=True,
 )
 
-if 'install' in sys.argv or 'bdist_wheel' in sys.argv:
-    if os.getenv("DEBINSTALL") is None or (os.getenv("DEBINSTALL") is not None and int(os.environ['DEBINSTALL']) < 1):
+if "install" in sys.argv or "bdist_wheel" in sys.argv:
+    if os.getenv("DEBINSTALL") is None or (
+        os.getenv("DEBINSTALL") is not None and int(os.environ["DEBINSTALL"]) < 1
+    ):
         install_configs()
```

### Comparing `c65faucet-1.0.49/tests/codecheck/src_files.sh` & `c65faucet-1.0.50/tests/codecheck/src_files.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/config/fuzz_config.py` & `c65faucet-1.0.50/tests/generative/fuzzer/config/fuzz_config.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,22 +11,22 @@
 import afl
 
 from faucet import config_parser as cp
 from faucet.conf import InvalidConfigError
 
 
 ROUNDS = 50000
-LOGNAME = 'FAUCET_FUZZER_LOG'
+LOGNAME = "FAUCET_FUZZER_LOG"
 tmpdir = tempfile.mkdtemp()
-conf_file_name = os.path.join(tmpdir, 'faucet.yaml')
+conf_file_name = os.path.join(tmpdir, "faucet.yaml")
 
 
 def create_config_file(config):
     """Create config file with given contents."""
-    with open(conf_file_name, 'w', encoding='utf-8') as conf_file:
+    with open(conf_file_name, "w", encoding="utf-8") as conf_file:
         conf_file.write(config)
     return conf_file_name
 
 
 def main():
     """Runs the py-AFL fuzzer with the faucet config parser"""
     logging.disable(logging.CRITICAL)
```

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/config/generate_dict.py` & `c65faucet-1.0.50/tests/generative/fuzzer/config/generate_dict.py`

 * *Files 12% similar despite different names*

```diff
@@ -22,53 +22,57 @@
     """Generate config fuzzer dict"""
 
     serial = 0
 
     @staticmethod
     def create_config_dict(file_name):
         """Generate YAML dictionary via obtaining possible variables from Faucet CONF objects"""
-        with open(file_name, 'r+', encoding='utf-8') as config_file:
+        with open(file_name, "r+", encoding="utf-8") as config_file:
             # Read set of bogus values already currently in the config.dict file
             bogus_values = []
             for value in config_file.readlines():
                 # Remove quotes and \n from bogus value to get the true bogus value
-                bogus_values.append(fr'{value[1:2]}')
+                bogus_values.append(r"%s" % value[1:2])
             # Make sure to add head values into the dictionary
             for value in V2_TOP_CONFS:
                 for bogus in bogus_values:
-                    to_write = fr'{value}{bogus}'
-                    rev_to_write = fr'{bogus}{value}'
-                    if (to_write in bogus_values
-                            or rev_to_write in bogus_values
-                            or value in bogus_values):
+                    to_write = r"%s%s" % (value, bogus)
+                    rev_to_write = r"%s%s" % (bogus, value)
+                    if (
+                        to_write in bogus_values
+                        or rev_to_write in bogus_values
+                        or value in bogus_values
+                    ):
                         continue
                     config_file.write(f'\n"{to_write}"')
                     config_file.write(f'\n"{rev_to_write}"')
             # Find CONF objects config file options
             for conf_obj in [ACL, Meter, Port, Router, DP, VLAN]:
                 for value in conf_obj.defaults:
                     for bogus in bogus_values:
-                        to_write = fr'{value}{bogus}'
-                        rev_to_write = fr'{bogus}{value}'
-                        if (to_write in bogus_values
-                                or rev_to_write in bogus_values
-                                or value in bogus_values):
+                        to_write = r"%s%s" % (value, bogus)
+                        rev_to_write = r"%s%s" % (bogus, value)
+                        if (
+                            to_write in bogus_values
+                            or rev_to_write in bogus_values
+                            or value in bogus_values
+                        ):
                             continue
                         config_file.write(f'\n"{to_write}"')
                         config_file.write(f'\n"{rev_to_write}"')
 
     def create_examples(self, file_base, file_name):
         """Generate some initial starting configs by generating them via the config_generator"""
         ex_curr = 0
 
         num_hosts = 1
         num_vlans = 2
 
         def get_serialno(*_args, **_kwargs):
-            """"Return mock serial number"""
+            """Return mock serial number"""
             self.serial += 1
             return self.serial
 
         def create_config(network_graph, stack=True):
             """Return topo object and a simple stack config generated from network_graph"""
             host_links = {}
             host_vlans = {}
@@ -76,44 +80,52 @@
             host_n = 0
             for dp_i in network_graph.nodes():
                 for _ in range(num_hosts):
                     for v_i in range(num_vlans):
                         host_links[host_n] = [dp_i]
                         host_vlans[host_n] = v_i
                         host_n += 1
-                dp_options[dp_i] = {'hardware': 'GenericTFM'}
+                dp_options[dp_i] = {"hardware": "GenericTFM"}
                 if dp_i == 0 and stack:
-                    dp_options[dp_i]['stack'] = {'priority': 1}
+                    dp_options[dp_i]["stack"] = {"priority": 1}
             switch_links = list(network_graph.edges()) * 2
             if stack:
                 link_vlans = {link: None for link in switch_links}
             else:
                 link_vlans = {link: list(range(num_vlans)) for link in switch_links}
             topo = FaucetFakeOFTopoGenerator(
-                'ovstype', 'portsock', 'testname',
-                len(network_graph.nodes()), False,
-                host_links, host_vlans, switch_links, link_vlans,
-                start_port=1, port_order=[0, 1, 2, 3],
-                get_serialno=get_serialno)
+                "ovstype",
+                "portsock",
+                "testname",
+                len(network_graph.nodes()),
+                False,
+                host_links,
+                host_vlans,
+                switch_links,
+                link_vlans,
+                start_port=1,
+                port_order=[0, 1, 2, 3],
+                get_serialno=get_serialno,
+            )
             config = topo.get_config(num_vlans, dp_options=dp_options)
             return config
 
         configs = []
         topologies = graph_atlas_g()
         for graph in topologies:
             if not graph or not networkx.is_connected(graph):
                 continue
             if len(graph.nodes()) > 4:
                 break
             for stack in (True, False):
                 configs.append(create_config((graph), stack=stack))
         for config in configs:
-            ex_fn = os.path.join(file_base, f'{file_name}_{ex_curr}')
-            with open(ex_fn, 'w+', encoding='utf-8') as ex_file:
+            ex_fn = os.path.join(file_base, "%s_%s" % (file_name, ex_curr))
+            with open(ex_fn, "w+", encoding="utf-8") as ex_file:
                 ex_file.write(config)
             ex_curr += 1
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     generator = ConfigDictGenerator()
-    generator.create_config_dict('config.dict')
-    generator.create_examples('examples/', 'ex')
+    generator.create_config_dict("config.dict")
+    generator.create_examples("examples/", "ex")
```

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/packet/display_packet_crash.py` & `c65faucet-1.0.50/tests/generative/fuzzer/packet/display_packet_crash.py`

 * *Files 15% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from faucet import faucet
 import fake_packet
 
 
 def main():
     """Shows the crash in the FAUCET log produced by given input."""
 
-    with open(sys.argv[1], encoding='utf-8') as pkt:
+    with open(sys.argv[1], encoding="utf-8") as pkt:
         packet_data = str(pkt.read())
 
     # start faucet
     application = faucet.Faucet(dpset=dpset.DPSet())
     application.start()
 
     # make sure dps are running
@@ -26,27 +26,31 @@
             valve.dp.dyn_finalized = False
             valve.dp.running = True
             valve.dp.dyn_finalized = state
 
     # create data from read file
     byte_data = None
     try:
-        byte_data = bytearray.fromhex(packet_data)  # pytype: disable=missing-parameter,wrong-arg-types
+        byte_data = bytearray.fromhex(
+            packet_data
+        )  # pytype: disable=missing-parameter,wrong-arg-types
     except (ValueError, TypeError):
         pass
 
     if byte_data is not None:
         # create fake packet
         _dp = fake_packet.Datapath(1)
-        msg = fake_packet.Message(datapath=_dp, cookie=15243729, port=1, data=byte_data, in_port=1)
+        msg = fake_packet.Message(
+            datapath=_dp, cookie=15243729, port=1, data=byte_data, in_port=1
+        )
         pkt = fake_packet.RyuEvent(msg)
 
         # send packet to faucet and display error produced
         application.packet_in_handler(pkt)
 
 
 if __name__ == "__main__":
     # make sure user specifies the afl crash folder
     if len(sys.argv) == 2:
         main()
     else:
-        sys.stderr.write('USAGE: python3 display_packet_crash.py <AFL_CRASH_FILE>\n')
+        sys.stderr.write("USAGE: python3 display_packet_crash.py <AFL_CRASH_FILE>\n")
```

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/http.ex2` & `c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/http.ex2`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/icmp.ex1` & `c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/icmp.ex1`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/icmp.ex2` & `c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/icmp.ex2`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/ipv4.ex1` & `c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/ipv4.ex1`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/packet/examples/msger.ex1` & `c65faucet-1.0.50/tests/generative/fuzzer/packet/examples/msger.ex1`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/packet/fake_packet.py` & `c65faucet-1.0.50/tests/generative/fuzzer/packet/fake_packet.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,18 +8,18 @@
         self.msg = msg
 
 
 class Message:  # pylint: disable=too-few-public-methods
     """Fake message class"""
 
     def __init__(self, *args, **kwargs):
-        self.datapath = kwargs['datapath']
-        self.cookie = kwargs['cookie']
-        self.port = kwargs['port']
-        self.data = kwargs['data']
+        self.datapath = kwargs["datapath"]
+        self.cookie = kwargs["cookie"]
+        self.port = kwargs["port"]
+        self.data = kwargs["data"]
         self.total_len = len(self.data)
         self.match = kwargs
         self.args = args
 
 
 class Datapath:  # pylint: disable=too-few-public-methods
     """Fake datapath class"""
```

### Comparing `c65faucet-1.0.49/tests/generative/fuzzer/packet/fuzz_packet.py` & `c65faucet-1.0.50/tests/generative/fuzzer/packet/fuzz_packet.py`

 * *Files 3% similar despite different names*

```diff
@@ -28,21 +28,25 @@
             valve.dp.dyn_finalized = state
 
     while afl.loop(ROUNDS):  # pylint: disable=c-extension-no-member
         # receive input from afl
         rcv = sys.stdin.read()
         data = None
         try:
-            data = bytearray.fromhex(rcv)  # pytype: disable=missing-parameter,wrong-arg-types
+            data = bytearray.fromhex(
+                rcv
+            )  # pytype: disable=missing-parameter,wrong-arg-types
         except (ValueError, TypeError):
             continue
 
         # create fake packet
         _dp = fake_packet.Datapath(1)
-        msg = fake_packet.Message(datapath=_dp, cookie=15243729, port=1, data=data, in_port=1)
+        msg = fake_packet.Message(
+            datapath=_dp, cookie=15243729, port=1, data=data, in_port=1
+        )
         pkt = fake_packet.RyuEvent(msg)
 
         # send fake packet to faucet
         application.packet_in_handler(pkt)
 
 
 if __name__ == "__main__":
```

### Comparing `c65faucet-1.0.49/tests/generative/integration/fault_tolerance_main.py` & `c65faucet-1.0.50/tests/generative/integration/fault_tolerance_main.py`

 * *Files 24% similar despite different names*

```diff
@@ -17,45 +17,49 @@
 from clib.clib_mininet_test_main import test_main
 
 import fault_tolerance_tests
 
 
 def test_generator(func_graph, stack_roots):
     """Return the function that will start the fault-tolerance testing for a graph"""
+
     def test(self):
         """Test fault-tolerance of the topology"""
         self.set_up(func_graph, stack_roots)
         self.network_function()
+
     return test
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     GRAPHS = {}
     GRAPH_ATLAS = graph_atlas_g()
     for graph in GRAPH_ATLAS:
-        if (not graph
-                or graph.number_of_nodes() > fault_tolerance_tests.MAX_NODES
-                or graph.number_of_nodes() < fault_tolerance_tests.MIN_NODES):
+        if (
+            not graph
+            or graph.number_of_nodes() > fault_tolerance_tests.MAX_NODES
+            or graph.number_of_nodes() < fault_tolerance_tests.MIN_NODES
+        ):
             continue
         if networkx.is_connected(graph):
             GRAPHS.setdefault(graph.number_of_nodes(), [])
             GRAPHS[graph.number_of_nodes()].append(graph)
-    TEST_LIMIT = int(os.getenv('FAUCET_GENERATIVE_LIMIT', '0'))
+    TEST_LIMIT = int(os.getenv("FAUCET_GENERATIVE_LIMIT", "0"))
 
     def create_tests():
         """Create multi DP test variations."""
         test_count = 0
         for test_class in fault_tolerance_tests.TEST_CLASS_LIST:
             for test_graph in GRAPHS[test_class.NUM_DPS]:
-                test_name = 'test_%s' % test_graph.name
+                test_name = "test_%s" % test_graph.name
                 test_func = test_generator(test_graph, test_class.STACK_ROOTS)
                 setattr(test_class, test_name, test_func)
                 test_count += 1
                 if TEST_LIMIT and test_count == TEST_LIMIT:
-                    print('Limiting number of tests to', test_count)
+                    print("Limiting number of tests to", test_count)
                     return
 
     # TODO: because we are creating tests dynamically re-using the same base class,
     # we cannot run tests in parallel as tests with the same base class will
     # overwrite each other's metadata (including switch names).
     create_tests()
     test_main([fault_tolerance_tests.__name__], serial_override=True)
```

### Comparing `c65faucet-1.0.49/tests/generative/integration/fault_tolerance_tests.py` & `c65faucet-1.0.50/tests/generative/integration/fault_tolerance_tests.py`

 * *Files 2% similar despite different names*

```diff
@@ -98,139 +98,159 @@
             for dp_i in network_graph.nodes():
                 for v_i in range(self.NUM_VLANS):
                     host_links[host_n] = [dp_i]
                     host_vlans[host_n] = v_i
                     host_n += 1
         dp_options = {}
         for i in network_graph.nodes():
-            dp_options.setdefault(i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(i) if self.debug_log_path else None,
-                'hardware': 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": "Open vSwitch",
+                },
+            )
             if i in stack_roots:
-                dp_options[i]['stack'] = {'priority': stack_roots[i]}
+                dp_options[i]["stack"] = {"priority": stack_roots[i]}
         vlan_options = {}
         routers = {}
         if self.NUM_VLANS >= 2:
             # Setup options for routing
             routers = {0: list(range(self.NUM_VLANS))}
             for i in range(self.NUM_VLANS):
                 vlan_options[i] = {
-                    'faucet_mac': self.faucet_mac(i),
-                    'faucet_vips': [self.faucet_vip(i)],
-                    'targeted_gw_resolution': False
+                    "faucet_mac": self.faucet_mac(i),
+                    "faucet_vips": [self.faucet_vip(i)],
+                    "targeted_gw_resolution": False,
                 }
             for i in network_graph.nodes():
-                dp_options[i]['arp_neighbor_timeout'] = 2
-                dp_options[i]['max_resolve_backoff_time'] = 2
-                dp_options[i]['proactive_learn_v4'] = True
+                dp_options[i]["arp_neighbor_timeout"] = 2
+                dp_options[i]["max_resolve_backoff_time"] = 2
+                dp_options[i]["proactive_learn_v4"] = True
         self.host_links = host_links
         self.switch_links = switch_links
         self.routers = routers
         self.stack_roots = stack_roots
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
             vlan_options=vlan_options,
-            routers=routers
+            routers=routers,
         )
         self.start_net()
 
     def host_connectivity(self, host, dst):
         """Ping to a destination, return True if the ping was successful"""
         try:
             self._ip_ping(host, dst, 5, timeout=50, count=5, require_host_learned=False)
         except AssertionError:
             return False
         return True
 
     def calculate_connectivity(self):
         """Ping between each set of host pairs to calculate host connectivity"""
         connected_hosts = self.topo_watcher.get_connected_hosts(
-            symmetric=self.ASSUME_SYMMETRIC_PING, transitive=self.ASSUME_TRANSITIVE_PING,
-            intervlan_only=self.INTERVLAN_ONLY)
+            symmetric=self.ASSUME_SYMMETRIC_PING,
+            transitive=self.ASSUME_TRANSITIVE_PING,
+            intervlan_only=self.INTERVLAN_ONLY,
+        )
         actual_graph = networkx.MultiDiGraph()
         for src, dst in connected_hosts.edges():
-            src_id = self.topo.nodeInfo(src)['host_n']
-            dst_id = self.topo.nodeInfo(dst)['host_n']
+            src_id = self.topo.nodeInfo(src)["host_n"]
+            dst_id = self.topo.nodeInfo(dst)["host_n"]
             result = self.host_connectivity(
-                self.host_information[src_id]['host'], self.host_information[dst_id]['ip'].ip)
+                self.host_information[src_id]["host"],
+                self.host_information[dst_id]["ip"].ip,
+            )
             if result:
                 actual_graph.add_edge(src, dst)
             if self.INSTANT_FAIL:
-                self.assertTrue(result, 'Connection failed: %s -/-> %s' % (src, dst))
+                self.assertTrue(result, "Connection failed: %s -/-> %s" % (src, dst))
         self.assertEqual(
-            list(connected_hosts.edges()), list(actual_graph.edges()),
-            'Resulting host connectivity graph does not match expected (%s != %s)' % (
-                list(connected_hosts.edges()), list(actual_graph.edges())))
+            list(connected_hosts.edges()),
+            list(actual_graph.edges()),
+            "Resulting host connectivity graph does not match expected (%s != %s)"
+            % (list(connected_hosts.edges()), list(actual_graph.edges())),
+        )
 
     def create_controller_fault(self, *args):
         """
         Set controller down (disconnects all switches from the controller)
         Args:
             index: The index to the controller to take down
         """
         index = args[0]
         controller = self.net.controllers[index]
         controller.stop()
         self.net.controllers.remove(controller)
-        self.topo_watcher.add_fault('Controller %s DOWN' % controller.name)
+        self.topo_watcher.add_fault("Controller %s DOWN" % controller.name)
 
     def get_faucet_controllers(self):
         """Return list of Faucet controllers"""
         return [c for c in self.net.controllers if isinstance(c, FAUCET)]
 
     def create_random_controller_fault(self, *_args):
         """Randomly create a fault for a controller"""
         controllers = self.get_faucet_controllers()
         if len(controllers) == 1:
             return
         i = random.randrange(len(controllers))
         c_name = controllers[i].name
-        controller = next((cont for cont in self.net.controllers if cont.name == c_name), None)
+        controller = next(
+            (cont for cont in self.net.controllers if cont.name == c_name), None
+        )
         if controller is None:
             return
         self.create_controller_fault(self.net.controllers.index(controller))
 
     def create_switch_fault(self, *args):
         """
         Set switch down (Deletes the OVS switch bridge)
         Args:
             index: Index of the switch dpid to take out
         """
         index = args[0]
         dpid = self.dpids[index]
         switch_name = self.topo.switches_by_id[index]
-        switch = next((switch for switch in self.net.switches if switch.name == switch_name), None)
+        switch = next(
+            (switch for switch in self.net.switches if switch.name == switch_name), None
+        )
         if switch is None:
             return
         self.dump_switch_flows(switch)
-        name = '%s:%s DOWN' % (self.topo.switches_by_id[index], self.dpids[index])
+        name = "%s:%s DOWN" % (self.topo.switches_by_id[index], self.dpids[index])
         self.topo_watcher.add_switch_fault(index, name)
         switch.stop()
-        error(switch.cmd(self.VSCTL, 'del-controller', switch.name, '|| true'))
+        error(switch.cmd(self.VSCTL, "del-controller", switch.name, "|| true"))
         self.assertTrue(
-            self.wait_for_prometheus_var('dp_status', 0, default=0, dpid=dpid, retries=10),
-            'DP %s not detected as DOWN' % dpid)
+            self.wait_for_prometheus_var(
+                "dp_status", 0, default=0, dpid=dpid, retries=10
+            ),
+            "DP %s not detected as DOWN" % dpid,
+        )
         self.net.switches.remove(switch)
 
     def random_switch_fault(self, *_args):
         """Randomly take out an available switch"""
         sw_list = self.topo_watcher.get_eligable_switch_events()
         index_list = []
         for sw_name in sw_list:
-            index_list.append(self.topo.nodeInfo(sw_name)['switch_n'])
+            index_list.append(self.topo.nodeInfo(sw_name)["switch_n"])
         if len(self.stack_roots.keys()) <= 1:
             # Prevent the only root from being destroyed
-            sorted_roots = dict(sorted(self.stack_roots.items(), key=lambda item: item[1]))
+            sorted_roots = dict(
+                sorted(self.stack_roots.items(), key=lambda item: item[1])
+            )
             for root_index in sorted_roots.keys():
                 if root_index in index_list:
                     index_list.remove(root_index)
         if not index_list:
             return
         index = self.rng.randrange(len(index_list))
         sw_index = index_list[index]
@@ -253,28 +273,34 @@
             status = self.stack_port_status(src_dpid, s1_name, port)
             if link[0] == s2_name and status == 3:
                 peer_port = link[1]
                 self.set_port_down(port, src_dpid)
                 self.set_port_down(peer_port, dst_dpid)
                 self.wait_for_stack_port_status(src_dpid, s1_name, port, 4)
                 self.wait_for_stack_port_status(dst_dpid, s2_name, peer_port, 4)
-                name = 'Link %s[%s]:%s-%s[%s]:%s DOWN' % (
-                    s1_name, src_dpid, port, s2_name, dst_dpid, peer_port)
+                name = "Link %s[%s]:%s-%s[%s]:%s DOWN" % (
+                    s1_name,
+                    src_dpid,
+                    port,
+                    s2_name,
+                    dst_dpid,
+                    peer_port,
+                )
                 self.topo_watcher.add_link_fault(src_i, dst_i, name)
                 return
 
     def random_link_fault(self, *_args):
         """Randomly create a fault for a DP link"""
         link_list = self.topo_watcher.get_eligable_link_events()
         if not link_list:
             return
         index = self.rng.randrange(len(link_list))
         dp_link = link_list[index]
-        src_i = self.topo.nodeInfo(dp_link[0])['switch_n']
-        dst_i = self.topo.nodeInfo(dp_link[1])['switch_n']
+        src_i = self.topo.nodeInfo(dp_link[0])["switch_n"]
+        dst_i = self.topo.nodeInfo(dp_link[1])["switch_n"]
         self.dp_link_fault(src_i, dst_i)
 
     def create_proportional_random_fault_event(self):
         """Create a fault-event randomly based on the number of link and switch events available"""
         funcs = []
         for _ in self.topo_watcher.get_eligable_link_events():
             funcs.append(self.random_link_fault)
@@ -297,15 +323,16 @@
         self.verify_stack_up()
 
         self.fault_events = fault_events
         self.num_faults = num_faults
         self.rng = random.Random(self.seed)
 
         self.topo_watcher = OptimizedTopologyWatcher(
-            self.topo, self.host_information, self.configuration_options['routers'])
+            self.topo, self.host_information, self.configuration_options["routers"]
+        )
 
         # Calculate stats (before any tear downs)
         self.calculate_connectivity()
         # Start tearing down the network
         if self.fault_events:
             # Do Specified list of faults (in order) until failure or fault list completed
             fault_index = 0
@@ -314,15 +341,17 @@
                     event_func, params = self.fault_events[fault_index]
                     fault_index += 1
                     event_func(*params)
                     self.calculate_connectivity()
         else:
             # Continue creating fault until none are available or expected connectivity does not
             #      match real connectivity
-            while (self.topo_watcher.continue_faults() or bool(len(self.get_faucet_controllers()) - 1)):
+            while self.topo_watcher.continue_faults() or bool(
+                len(self.get_faucet_controllers()) - 1
+            ):
                 for _ in range(self.num_faults):
                     self.create_proportional_random_fault_event()
                 self.calculate_connectivity()
 
     def tearDown(self, ignore_oferrors=False):
         """Make sure to dump the watcher information too"""
         if self.topo_watcher:
@@ -358,24 +387,27 @@
     NUM_HOSTS = 4
     NUM_VLANS = 1
     N_DP_LINKS = 1
     STACK_ROOTS = {0: 1}
 
     def test_ftp2_all_random_switch_failures(self):
         """Test fat-tree-pod-2 randomly tearing down only switches"""
-        fault_events = [(self.random_switch_fault, (None,)) for _ in range(self.NUM_DPS)]
+        fault_events = [
+            (self.random_switch_fault, (None,)) for _ in range(self.NUM_DPS)
+        ]
         stack_roots = {2 * i: 1 for i in range(self.NUM_DPS // 2)}
         self.set_up(networkx.cycle_graph(self.NUM_DPS), stack_roots)
         self.network_function(fault_events=fault_events)
 
     def test_ftp2_all_random_link_failures(self):
         """Test fat-tree-pod-2 randomly tearing down only switch-switch links"""
         network_graph = networkx.cycle_graph(self.NUM_DPS)
         fault_events = [
-            (self.random_link_fault, (None,)) for _ in range(len(network_graph.edges()))]
+            (self.random_link_fault, (None,)) for _ in range(len(network_graph.edges()))
+        ]
         stack_roots = {2 * i: 1 for i in range(self.NUM_DPS // 2)}
         self.set_up(network_graph, stack_roots)
         self.network_function(fault_events=fault_events)
 
     def test_ftp2_edge_root_link_fault(self):
         """Test breaking a link between a edge switch to the root aggregation switch"""
         fault_events = [(self.dp_link_fault, (0, 3))]
@@ -413,15 +445,15 @@
     NUM_DPS = 6
     NUM_HOSTS = 6
     NUM_VLANS = 1
     N_DP_LINKS = 1
     STACK_ROOTS = {0: 1}
 
 
-@unittest.skip('Too computationally complex')
+@unittest.skip("Too computationally complex")
 class FaucetFaultTolerance7DPTest(FaucetFaultToleranceBaseTest):
     """Run a range of fault-tolerance tests for topologies on 5 DPs"""
 
     NUM_DPS = 7
     NUM_HOSTS = 7
     NUM_VLANS = 1
     N_DP_LINKS = 1
@@ -430,11 +462,11 @@
 
 TEST_CLASS_LIST = [
     FaucetFaultTolerance2DPTest,
     FaucetFaultTolerance3DPTest,
     FaucetFaultTolerance4DPTest,
     FaucetFaultTolerance5DPTest,
     FaucetFaultTolerance6DPTest,
-    FaucetFaultTolerance7DPTest
+    FaucetFaultTolerance7DPTest,
 ]
 MIN_NODES = min([c.NUM_DPS for c in TEST_CLASS_LIST])
 MAX_NODES = max([c.NUM_DPS for c in TEST_CLASS_LIST])
```

### Comparing `c65faucet-1.0.49/tests/generative/unit/test_topology.py` & `c65faucet-1.0.50/tests/generative/unit/test_topology.py`

 * *Files 15% similar despite different names*

```diff
@@ -52,26 +52,26 @@
 
     serial = 0
 
     @staticmethod
     def create_bcast_match(in_port, in_vid=None):
         """Return bcast match"""
         bcast_match = {
-            'in_port': in_port,
-            'eth_dst': mac.BROADCAST_STR,
-            'eth_type': 0x0800,
-            'ip_proto': 1
+            "in_port": in_port,
+            "eth_dst": mac.BROADCAST_STR,
+            "eth_type": 0x0800,
+            "ip_proto": 1,
         }
         if in_vid:
             in_vid = in_vid | ofp.OFPVID_PRESENT
-            bcast_match['vlan_vid'] = in_vid
+            bcast_match["vlan_vid"] = in_vid
         return bcast_match
 
     def get_serialno(self, *_args, **_kwargs):
-        """"Return mock serial number"""
+        """Return mock serial number"""
         self.serial += 1
         return self.serial
 
     def create_topo_config(self, network_graph):
         """Return topo object and a simple stack config generated from network_graph"""
         host_links = {}
         host_vlans = {}
@@ -93,25 +93,33 @@
                     host_vlans[host_n] = vlans
                     host_n += 1
                 else:
                     for v_i in range(min_vlans, max_vlans):
                         host_links[host_n] = [dp_i]
                         host_vlans[host_n] = v_i
                         host_n += 1
-            dp_options[dp_i] = {'hardware': 'GenericTFM'}
+            dp_options[dp_i] = {"hardware": "GenericTFM"}
             if dp_i == 0:
-                dp_options[dp_i]['stack'] = {'priority': 1}
+                dp_options[dp_i]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges()) * self.SWITCH_TO_SWITCH_LINKS
         link_vlans = {link: None for link in switch_links}
         topo = FaucetFakeOFTopoGenerator(
-            'ovstype', 'portsock', 'testname',
-            self.NUM_DPS, False,
-            host_links, host_vlans, switch_links, link_vlans,
-            start_port=self.START_PORT, port_order=self.PORT_ORDER,
-            get_serialno=self.get_serialno)
+            "ovstype",
+            "portsock",
+            "testname",
+            self.NUM_DPS,
+            False,
+            host_links,
+            host_vlans,
+            switch_links,
+            link_vlans,
+            start_port=self.START_PORT,
+            port_order=self.PORT_ORDER,
+            get_serialno=self.get_serialno,
+        )
         config = topo.get_config(self.NUM_VLANS, dp_options=dp_options)
         return topo, config
 
     def verify_flood_traversal(self):
         """Verify broadcasts flooding reach all destination hosts"""
         _, host_port_maps, _ = self.topo.create_port_maps()
         for src_host in host_port_maps:
@@ -122,33 +130,35 @@
                 for switch_n, ports in host_port_maps[src_host].items():
                     src_dpid = self.topo.dpids_by_id[switch_n]
                     src_port = ports[0]
                 for switch_n, ports in host_port_maps[dst_host].items():
                     dst_dpid = self.topo.dpids_by_id[switch_n]
                     dst_port = ports[0]
                 match = self.create_bcast_match(src_port)
-                self.network.is_output(match, int(src_dpid), int(dst_dpid), port=dst_port)
+                self.network.is_output(
+                    match, int(src_dpid), int(dst_dpid), port=dst_port
+                )
 
     def verify_vlan_change(self):
         """Change host VLAN, check restart of rules consistent"""
         _, host_port_maps, _ = self.topo.create_port_maps()
         yaml_config = yaml_load(self.CONFIG)
-        intf_config = yaml_config['dps'][self.topo.switches_by_id[1]]['interfaces']
+        intf_config = yaml_config["dps"][self.topo.switches_by_id[1]]["interfaces"]
 
         for host_i in host_port_maps:
             # Find a host on the second switch
             if 1 in host_port_maps[host_i]:
                 port = host_port_maps[host_i][1][0]
-                if 'native_vlan' in intf_config[port]:
-                    prev_name = intf_config[port]['native_vlan']
+                if "native_vlan" in intf_config[port]:
+                    prev_name = intf_config[port]["native_vlan"]
                     for v_i in range(self.NUM_VLANS):
                         # Make sure that the new VLAN will be different
                         new_name = self.topo.vlan_name(v_i)
                         if new_name != prev_name:
-                            intf_config[port]['native_vlan'] = new_name
+                            intf_config[port]["native_vlan"] = new_name
                             break
                     else:
                         # Keep on searching for a host VLAN to change
                         continue
                     # Created a different VLAN so now stop searching
                     break
 
@@ -183,115 +193,130 @@
                 continue
             self.graphs.setdefault(graph.number_of_nodes(), [])
             self.graphs[graph.number_of_nodes()].append(graph)
 
     @staticmethod
     def setup_generator(func):
         """Returns the class set_up function"""
+
         def set_up(self, graphs):
             self.graphs = graphs
             self.topo, self.CONFIG = self.create_topo_config(graphs[0])
             self.setup_valves(self.CONFIG)
             func(self)
+
         return set_up
 
     @staticmethod
     def test_generator(graphs):
         """Returns the test set_up function"""
+
         def test(self):
             self.set_up(graphs)
+
         return test
 
     @staticmethod
     def sums(length, total_sum):
         """Returns the permutations of `length` numbers that sum to `total_sum`"""
         if length == 1:
             yield (total_sum,)
         else:
             for value in range(total_sum + 1):
                 for permutation in ClassGenerator.sums(length - 1, total_sum - value):
                     yield (value,) + permutation
 
     def generate_atlas_class(self, class_name, verify_name, constants):
         """Return a class type generated as each test generated from the graph atlas"""
-        test_class = type(class_name, (ValveGenerativeBase, ), {**constants})
+        test_class = type(class_name, (ValveGenerativeBase,), {**constants})
         verify_func = getattr(test_class, verify_name)
         set_up = self.setup_generator(verify_func)
-        setattr(test_class, 'set_up', set_up)
+        setattr(test_class, "set_up", set_up)
         for graphs in self.graphs.values():
             for graph in graphs:
                 test_func = self.test_generator([graph])
-                test_name = 'test_%s' % graph.name
+                test_name = "test_%s" % graph.name
                 setattr(test_class, test_name, test_func)
         return test_class
 
     def generate_atlas_size_class(self, class_name, verify_name, constants):
         """Return a class type as each test generated from a set of tests in the graph atlas"""
-        test_class = type(class_name, (ValveGenerativeBase, ), {**constants})
+        test_class = type(class_name, (ValveGenerativeBase,), {**constants})
         verify_func = getattr(test_class, verify_name)
         set_up = self.setup_generator(verify_func)
-        setattr(test_class, 'set_up', set_up)
+        setattr(test_class, "set_up", set_up)
         for num_dps, graph_list in self.graphs.items():
             test_func = self.test_generator(graph_list)
-            test_name = 'test_reconfigure_topologies_%s_dps' % num_dps
+            test_name = "test_reconfigure_topologies_%s_dps" % num_dps
             setattr(test_class, test_name, test_func)
         return test_class
 
     def generate_spine_and_leaf_class(self, class_name, verify_name, constants):
         """Return a class type as each test generated from a set of tests in the graph atlas"""
-        test_class = type(class_name, (ValveGenerativeBase, ), {**constants})
+        test_class = type(class_name, (ValveGenerativeBase,), {**constants})
         verify_func = getattr(test_class, verify_name)
         set_up = self.setup_generator(verify_func)
-        setattr(test_class, 'set_up', set_up)
+        setattr(test_class, "set_up", set_up)
         curr_nodes = 8
         curr_tests = 0
         # Iteratively generate spine & leaf networks until `MAX_TESTS` stopping point
         # By testing all non-isomorphic topologies up to (and including) 7 nodes,
         #   SPINE_NODES + LEAF_NODES <= 7 are already tested
         # Loop until we have reached a desired number of tests
         while curr_tests <= self.MAX_TESTS:
             # Get permutations of numbers that sum to the current number of nodes
             # The current number of nodes will be split between the two partites of the topology
             for nodes in ClassGenerator.sums(2, curr_nodes):
                 if 0 in nodes or nodes[0] > nodes[1]:
                     # Ignore empty partites or inverse solutions
                     continue
-                test_name = 'test_%s_%s_spine_and_%s_leaf_topology' % (
-                    curr_tests, nodes[0], nodes[1])
+                test_name = "test_%s_%s_spine_and_%s_leaf_topology" % (
+                    curr_tests,
+                    nodes[0],
+                    nodes[1],
+                )
                 graph = networkx.complete_multipartite_graph(*nodes)
                 test_func = self.test_generator([graph])
                 setattr(test_class, test_name, test_func)
                 curr_tests += 1
                 if curr_tests > self.MAX_TESTS:
                     break
             # Increase current number of nodes
             curr_nodes += 1
         return test_class
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     class_generator = ClassGenerator()
     # Generate generative tests of all non-isomorphic, complete toplogies with 7 nodes or less
     ValveTopologyVLANTest = class_generator.generate_atlas_class(
-        'ValveTopologyVLANTest', 'verify_vlan_change', {'NUM_HOSTS': 2})
+        "ValveTopologyVLANTest", "verify_vlan_change", {"NUM_HOSTS": 2}
+    )
     ValveTopologyTableTest = class_generator.generate_atlas_class(
-        'ValveTopologyTableTest', 'verify_flood_traversal', {})
+        "ValveTopologyTableTest", "verify_flood_traversal", {}
+    )
     ValveTopologyRestartTest = class_generator.generate_atlas_size_class(
-        'ValveTopologyRestartTest', 'validate_topology_change', {})
+        "ValveTopologyRestartTest", "validate_topology_change", {}
+    )
     # Generate spine and leaf topologies
     ValveTopologySpineAndLeafTest = class_generator.generate_spine_and_leaf_class(
-        'ValveTopologySpineAndLeafTest', 'verify_flood_traversal', {})
+        "ValveTopologySpineAndLeafTest", "verify_flood_traversal", {}
+    )
     # Create new tests that are copies of the previous tests but test with redundant links
     ValveTopologyVLANMultilinkTest = type(
-        'ValveTopologyVLANMultilinkTest', (ValveTopologyVLANTest,), {})
+        "ValveTopologyVLANMultilinkTest", (ValveTopologyVLANTest,), {}
+    )
     ValveTopologyVLANMultilinkTest.SWITCH_TO_SWITCH_LINKS = 2
     ValveTopologyTableMultilinkTest = type(
-        'ValveTopologyTableMultilinkTest', (ValveTopologyTableTest,), {})
+        "ValveTopologyTableMultilinkTest", (ValveTopologyTableTest,), {}
+    )
     ValveTopologyTableMultilinkTest.SWITCH_TO_SWITCH_LINKS = 2
     ValveTopologyRestartMultilinkTest = type(
-        'ValveTopologyRestartMultilinkTest', (ValveTopologyRestartTest,), {})
+        "ValveTopologyRestartMultilinkTest", (ValveTopologyRestartTest,), {}
+    )
     ValveTopologyRestartMultilinkTest.SWITCH_TO_SWITCH_LINKS = 2
     ValveTopologySpineAndLeafMultilinkTest = type(
-        'ValveTopologySpineAndLeafMultilinkTest', (ValveTopologySpineAndLeafTest,), {})
+        "ValveTopologySpineAndLeafMultilinkTest", (ValveTopologySpineAndLeafTest,), {}
+    )
     ValveTopologySpineAndLeafMultilinkTest.SWITCH_TO_SWITCH_LINKS = 2
     # Run unit tests
     unittest.main()
```

### Comparing `c65faucet-1.0.49/tests/integration/mininet_main.py` & `c65faucet-1.0.50/tests/integration/mininet_main.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,9 +11,9 @@
 """
 
 from clib.clib_mininet_test_main import test_main
 
 import mininet_tests
 import mininet_multidp_tests
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     test_main([mininet_tests.__name__, mininet_multidp_tests.__name__])
```

### Comparing `c65faucet-1.0.49/tests/integration/mininet_multidp_tests.py` & `c65faucet-1.0.50/tests/integration/mininet_multidp_tests.py`

 * *Files 20% similar despite different names*

```diff
@@ -68,17 +68,26 @@
     def output_only():
         return set()
 
     @staticmethod
     def setUp():
         pass
 
-    def set_up(self, stack=False, n_dps=1, n_tagged=0, n_untagged=0,
-               switch_to_switch_links=1, stack_ring=False,
-               lacp_trunk=False, use_external=False, routers=None):
+    def set_up(
+        self,
+        stack=False,
+        n_dps=1,
+        n_tagged=0,
+        n_untagged=0,
+        switch_to_switch_links=1,
+        stack_ring=False,
+        lacp_trunk=False,
+        use_external=False,
+        routers=None,
+    ):
         """
         Args:
             stack (bool): Whether to use stack or trunk links
             n_dps (int): The number of DPs in the topology
             n_tagged (int): The number of tagged hosts per DP
             n_untagged (int): The number of untagged hosts per DP
             switch_to_switch_links (int): The number of switch-switch links to generate
@@ -105,18 +114,15 @@
                 link_vlans[link] = vlans
         # Create link configuration options for DP interfaces
         link_options = {}
         for dp_i in dp_links.nodes():
             for link in dp_links.edges(dp_i):
                 if lacp_trunk:
                     link_options.setdefault(link, {})
-                    link_options[link] = {
-                        'lacp': 1,
-                        'lacp_active': True
-                    }
+                    link_options[link] = {"lacp": 1, "lacp_active": True}
         if self.link_options():
             for dp_i in dp_links.nodes():
                 for link in dp_links.edges(dp_i):
                     link_options.setdefault(link, {})
                     for opt_key, opt_value in self.link_options().items():
                         link_options[link][opt_key] = opt_value
         # Create host link topology and vlan information
@@ -144,72 +150,79 @@
             for host, links in host_links.items():
                 make_external = False
                 for link in links:
                     if not values[link]:
                         make_external = True
                         values[link] = True
                 host_options.setdefault(host, {})
-                host_options[host]['loop_protect_external'] = make_external
+                host_options[host]["loop_protect_external"] = make_external
         for host in host_links:
             for h_key, h_value in self.host_options().items():
                 host_options[host][h_key] = h_value
         # Create DP configuration options
         dp_options = {}
         for dp_i in range(n_dps):
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if stack and dp_i == 0:
-                dp_options[dp_i]['stack'] = {'priority': 1}
+                dp_options[dp_i]["stack"] = {"priority": 1}
             if lacp_trunk:
-                dp_options[dp_i]['lacp_timeout'] = 10
+                dp_options[dp_i]["lacp_timeout"] = 10
             for dp_key, dp_value in self.dp_options().items():
                 dp_options[dp_i][dp_key] = dp_value
         # Create VLAN configuration options
         vlan_options = {}
         if routers:
             for vlans in routers:
                 for vlan in vlans:
                     if vlan not in vlan_options:
                         vlan_options[vlan] = {
-                            'faucet_mac': self.faucet_mac(vlan),
-                            'faucet_vips': [self.faucet_vip(vlan)],
-                            'targeted_gw_resolution': False
+                            "faucet_mac": self.faucet_mac(vlan),
+                            "faucet_vips": [self.faucet_vip(vlan)],
+                            "targeted_gw_resolution": False,
                         }
         for vlan in range(n_vlans):
             vlan_options.setdefault(vlan, {})
             for vlan_key, vlan_value in self.vlan_options().items():
                 vlan_options[vlan][vlan_key] = vlan_value
         if self.link_acls():
             for link, acls in self.link_acls().items():
                 if isinstance(link, tuple):
                     # link ACL
                     link_options.setdefault(link, {})
-                    link_options[link]['acls_in'] = acls
+                    link_options[link]["acls_in"] = acls
                 elif isinstance(link, int):
                     # host ACL
                     host_options.setdefault(link, {})
-                    host_options[link]['acls_in'] = acls
+                    host_options[link]["acls_in"] = acls
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             mininet_host_options=self.mininet_host_extra_options(),
             n_vlans=n_vlans,
             dp_options=dp_options,
             host_options=host_options,
             link_options=link_options,
             vlan_options=vlan_options,
             routers=routers,
             router_options=self.router_options(),
             include=self.include(),
-            include_optional=self.include_optional()
+            include_optional=self.include_optional(),
         )
         self.start_net()
 
 
 class FaucetStringOfDPUntaggedTest(FaucetMultiDPTestBase):
     """Test untagged hosts"""
 
@@ -240,94 +253,109 @@
     """Test topology of stacked datapaths with tagged hosts."""
 
     NUM_DPS = 3
 
     @staticmethod
     def dp_options():
         """DP options"""
-        return {
-            'stack': {
-                'priority': 1
-            }
-        }
+        return {"stack": {"priority": 1}}
 
     def _test_tagged(self):
         """All tagged hosts in stack topology can reach each other."""
         self.set_up(
-            stack=True, n_dps=self.NUM_DPS, n_tagged=self.NUM_HOSTS, switch_to_switch_links=2)
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_tagged=self.NUM_HOSTS,
+            switch_to_switch_links=2,
+        )
         self.verify_stack_up()
         for coldstart in (False, True):
             self.verify_one_stack_down(0, coldstart)
 
     def test_dp_root_hop_port(self):
         """Test if dp_root_hop_port is set"""
         self.set_up(
-            stack=True, n_dps=self.NUM_DPS, n_tagged=self.NUM_HOSTS, switch_to_switch_links=1)
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_tagged=self.NUM_HOSTS,
+            switch_to_switch_links=1,
+        )
         self.verify_stack_up()
         for index in range(self.NUM_DPS):
             dp_id = self.topo.dpids_by_id[index]
             dp_name = self.topo.switches_by_id[index]
             root_port = 0
             for link, ports in self.link_port_maps.items():
                 if link == (index, index - 1):
                     root_port = ports[0]
-            labels = {'dp_id': '0x%x' % int(dp_id), 'dp_name': dp_name}
-            self.assertEqual(self.scrape_prometheus_var(
-                var='dp_root_hop_port', labels=labels, default=0,
-                dpid=dp_id), root_port)
+            labels = {"dp_id": "0x%x" % int(dp_id), "dp_name": dp_name}
+            self.assertEqual(
+                self.scrape_prometheus_var(
+                    var="dp_root_hop_port", labels=labels, default=0, dpid=dp_id
+                ),
+                root_port,
+            )
         # Stop switch 1
         self.net.switches[0].stop()
         dp_id = self.topo.dpids_by_id[2]
         dp_name = self.topo.switches_by_id[2]
-        labels = {'dp_id': '0x%x' % int(dp_id), 'dp_name': dp_name}
+        labels = {"dp_id": "0x%x" % int(dp_id), "dp_name": dp_name}
         if not self.wait_for_prometheus_var(
-                'is_dp_stack_root', 1,
-                labels=labels, dpid=dp_id, timeout=30):
-            self.fail('wanted is_dp_stack_root for %s to be %u' % (labels, 1))
+            "is_dp_stack_root", 1, labels=labels, dpid=dp_id, timeout=30
+        ):
+            self.fail("wanted is_dp_stack_root for %s to be %u" % (labels, 1))
         dp_id = self.topo.dpids_by_id[1]
         dp_name = self.topo.switches_by_id[1]
-        labels = {'dp_id': '0x%x' % int(dp_id), 'dp_name': dp_name}
+        labels = {"dp_id": "0x%x" % int(dp_id), "dp_name": dp_name}
         sw2_root_port = min(self.link_port_maps[(1, 2)])
-        self.assertEqual(self.scrape_prometheus_var(
-            var='dp_root_hop_port', labels=labels, default=0,
-            dpid=dp_id), sw2_root_port)
+        self.assertEqual(
+            self.scrape_prometheus_var(
+                var="dp_root_hop_port", labels=labels, default=0, dpid=dp_id
+            ),
+            sw2_root_port,
+        )
         self.net.switches[0].start(self.net.controllers)
 
 
 class FaucetSingleStackStringOfDPTagged1Test(FaucetMultiDPTestBase):
     """Test topology of stacked datapaths with tagged hosts."""
 
     NUM_DPS = 3
 
     def test_tagged(self):
         """Test all tagged hosts in stack topology can reach each other with one stack down"""
         self.set_up(
-            stack=True, n_dps=self.NUM_DPS, n_tagged=self.NUM_HOSTS, switch_to_switch_links=2)
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_tagged=self.NUM_HOSTS,
+            switch_to_switch_links=2,
+        )
         self.verify_stack_up()
         for coldstart in (False, True):
             self.verify_one_stack_down(1, coldstart)
 
 
 class FaucetStringOfDPLACPUntaggedTest(FaucetMultiDPTestBase):
     """Test topology of LACP-connected datapaths with untagged hosts."""
 
     NUM_DPS = 2
     NUM_HOSTS = 2
     SOFTWARE_ONLY = True
-    match_bcast = {'dl_vlan': 100, 'dl_dst': 'ff:ff:ff:ff:ff:ff'}
-    action_str = 'OUTPUT:%u'
+    match_bcast = {"dl_vlan": 100, "dl_dst": "ff:ff:ff:ff:ff:ff"}
+    action_str = "OUTPUT:%u"
 
     def setUp(self):
         """Setup network & create config file"""
         super().set_up(
             stack=False,
             n_dps=self.NUM_DPS,
             n_untagged=self.NUM_HOSTS,
             switch_to_switch_links=2,
-            lacp_trunk=True)
+            lacp_trunk=True,
+        )
 
     def lacp_ports(self):
         """Return LACP ports"""
         # We sort non_host_links by port because FAUCET sorts its ports
         # and only floods out of the first active LACP port in that list
         sname = self.topo.switches_by_id[0]
         dname = self.topo.switches_by_id[1]
@@ -337,25 +365,29 @@
                 if first_link is None:
                     first_link = (sport, link[1])
                 else:
                     second_link = (sport, link[1])
         first_link, second_link = sorted([first_link, second_link])
         first_lacp_port, remote_first_lacp_port = first_link
         second_lacp_port, remote_second_lacp_port = second_link
-        return (first_lacp_port, second_lacp_port,
-                remote_first_lacp_port, remote_second_lacp_port)
+        return (
+            first_lacp_port,
+            second_lacp_port,
+            remote_first_lacp_port,
+            remote_second_lacp_port,
+        )
 
     def wait_for_lacp_state(self, port_no, wanted_state, dpid, dp_name, timeout=30):
         """Wait for LACP port state"""
         labels = self.port_labels(port_no)
-        labels.update({'dp_id': '0x%x' % int(dpid), 'dp_name': dp_name})
+        labels.update({"dp_id": "0x%x" % int(dpid), "dp_name": dp_name})
         if not self.wait_for_prometheus_var(
-                'port_lacp_state', wanted_state,
-                labels=labels, dpid=False, timeout=timeout):
-            self.fail('wanted LACP state for %s to be %u' % (labels, wanted_state))
+            "port_lacp_state", wanted_state, labels=labels, dpid=False, timeout=timeout
+        ):
+            self.fail("wanted LACP state for %s to be %u" % (labels, wanted_state))
 
     def wait_for_lacp_port_none(self, port_no, dpid, dp_name):
         """Wait for LACP state NONE"""
         self.wait_for_lacp_state(port_no, 0, dpid, dp_name)
 
     def wait_for_lacp_port_init(self, port_no, dpid, dp_name):
         """Wait for LACP state INIT"""
@@ -367,146 +399,200 @@
 
     def wait_for_lacp_port_nosync(self, port_no, dpid, dp_name):
         """Wait for LACP state NOSYNC"""
         self.wait_for_lacp_state(port_no, 5, dpid, dp_name)
 
     def wait_for_all_lacp_up(self):
         """Wait for all LACP ports to be up"""
-        (first_lacp_port, second_lacp_port, remote_first_lacp_port, _) = self.lacp_ports()
-        self.wait_for_lacp_port_up(first_lacp_port, self.dpids[0], self.topo.switches_by_id[0])
-        self.wait_for_lacp_port_up(second_lacp_port, self.dpids[0], self.topo.switches_by_id[0])
+        (
+            first_lacp_port,
+            second_lacp_port,
+            remote_first_lacp_port,
+            _,
+        ) = self.lacp_ports()
+        self.wait_for_lacp_port_up(
+            first_lacp_port, self.dpids[0], self.topo.switches_by_id[0]
+        )
+        self.wait_for_lacp_port_up(
+            second_lacp_port, self.dpids[0], self.topo.switches_by_id[0]
+        )
         self.wait_until_matching_flow(
-            self.match_bcast, self._FLOOD_TABLE, actions=[self.action_str % first_lacp_port])
+            self.match_bcast,
+            self._FLOOD_TABLE,
+            actions=[self.action_str % first_lacp_port],
+        )
         self.wait_until_matching_flow(
-            self.match_bcast, self._FLOOD_TABLE, actions=[self.action_str % remote_first_lacp_port],
-            dpid=self.dpids[1])
+            self.match_bcast,
+            self._FLOOD_TABLE,
+            actions=[self.action_str % remote_first_lacp_port],
+            dpid=self.dpids[1],
+        )
 
     def test_lacp_port_down(self):
         """LACP works with any member down."""
-        (first_lacp_port, second_lacp_port,
-         remote_first_lacp_port, remote_second_lacp_port) = self.lacp_ports()
+        (
+            first_lacp_port,
+            second_lacp_port,
+            remote_first_lacp_port,
+            remote_second_lacp_port,
+        ) = self.lacp_ports()
         local_ports = {first_lacp_port, second_lacp_port}
         remote_ports = {remote_first_lacp_port, remote_second_lacp_port}
 
         self.wait_for_all_lacp_up()
         self.retry_net_ping()
 
         for local_lacp_port, remote_lacp_port in (
-                (first_lacp_port, remote_first_lacp_port),
-                (second_lacp_port, remote_second_lacp_port)):
+            (first_lacp_port, remote_first_lacp_port),
+            (second_lacp_port, remote_second_lacp_port),
+        ):
             other_local_lacp_port = list(local_ports - {local_lacp_port})[0]
             other_remote_lacp_port = list(remote_ports - {remote_lacp_port})[0]
             self.set_port_down(local_lacp_port, wait=False)
             self.wait_for_lacp_port_none(
-                local_lacp_port, self.dpids[0], self.topo.switches_by_id[0])
+                local_lacp_port, self.dpids[0], self.topo.switches_by_id[0]
+            )
             self.wait_for_lacp_port_none(
-                remote_lacp_port, self.dpids[1], self.topo.switches_by_id[1])
+                remote_lacp_port, self.dpids[1], self.topo.switches_by_id[1]
+            )
             self.wait_until_matching_flow(
-                self.match_bcast, self._FLOOD_TABLE, actions=[
-                    self.action_str % other_local_lacp_port])
+                self.match_bcast,
+                self._FLOOD_TABLE,
+                actions=[self.action_str % other_local_lacp_port],
+            )
             self.wait_until_matching_flow(
-                self.match_bcast, self._FLOOD_TABLE, actions=[
-                    self.action_str % other_remote_lacp_port],
-                dpid=self.dpids[1])
+                self.match_bcast,
+                self._FLOOD_TABLE,
+                actions=[self.action_str % other_remote_lacp_port],
+                dpid=self.dpids[1],
+            )
             self.retry_net_ping()
             self.set_port_up(local_lacp_port)
             self.wait_for_all_lacp_up()
 
     def test_untagged(self):
         """All untagged hosts in stack topology can reach each other, LAG_CHANGE event emitted."""
         self._enable_event_log()
         for _ in range(3):
             self.wait_for_all_lacp_up()
             self.verify_stack_hosts()
             self.flap_all_switch_ports()
         # Check for presence of LAG_CHANGE event in event socket log and check for it's structure
         lag_event_found = None
-        with open(self.event_log, 'r', encoding='utf-8') as event_log_file:
+        with open(self.event_log, "r", encoding="utf-8") as event_log_file:
             for event_log_line in event_log_file.readlines():
                 event = json.loads(event_log_line.strip())
-                if 'LAG_CHANGE' in event:
-                    lag_event_found = event.get('LAG_CHANGE')
+                if "LAG_CHANGE" in event:
+                    lag_event_found = event.get("LAG_CHANGE")
                     break
         self.assertTrue(lag_event_found)
         if lag_event_found:
-            self.assertIn('state', lag_event_found)
-            self.assertIn('role', lag_event_found)
+            self.assertIn("state", lag_event_found)
+            self.assertIn("role", lag_event_found)
 
     def test_dyn_fail(self):
         """Test lacp fail on reload with dynamic lacp status."""
 
         conf = self._get_faucet_conf()
         (src_port, dst_port, fail_port, _) = self.lacp_ports()
 
         self.wait_for_lacp_port_up(src_port, self.dpids[0], self.topo.switches_by_id[0])
         self.wait_for_lacp_port_up(dst_port, self.dpids[0], self.topo.switches_by_id[0])
 
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[1]]['interfaces']
-        interfaces_conf[fail_port]['lacp'] = 0
-        interfaces_conf[fail_port]['lacp_active'] = False
-        self.reload_conf(conf, self.faucet_config_path, restart=True,
-                         cold_start=False, change_expected=True, dpid=self.dpids[1])
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[1]]["interfaces"]
+        interfaces_conf[fail_port]["lacp"] = 0
+        interfaces_conf[fail_port]["lacp_active"] = False
+        self.reload_conf(
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.dpids[1],
+        )
 
-        self.wait_for_lacp_port_init(src_port, self.dpids[0], self.topo.switches_by_id[0])
+        self.wait_for_lacp_port_init(
+            src_port, self.dpids[0], self.topo.switches_by_id[0]
+        )
         self.wait_for_lacp_port_up(dst_port, self.dpids[0], self.topo.switches_by_id[0])
 
     def test_passthrough(self):
         """Test lacp passthrough on port fail."""
 
         conf = self._get_faucet_conf()
         (src_port, dst_port, fail_port, end_port) = self.lacp_ports()
 
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[0]]['interfaces']
-        interfaces_conf[dst_port]['lacp_passthrough'] = [src_port]
-        interfaces_conf[dst_port]['loop_protect_external'] = True
-        interfaces_conf[dst_port]['lacp'] = 2
-        interfaces_conf[src_port]['loop_protect_external'] = True
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[1]]['interfaces']
-        interfaces_conf[fail_port]['loop_protect_external'] = True
-        interfaces_conf[end_port]['loop_protect_external'] = True
-        interfaces_conf[end_port]['lacp'] = 2
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[0]]["interfaces"]
+        interfaces_conf[dst_port]["lacp_passthrough"] = [src_port]
+        interfaces_conf[dst_port]["loop_protect_external"] = True
+        interfaces_conf[dst_port]["lacp"] = 2
+        interfaces_conf[src_port]["loop_protect_external"] = True
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[1]]["interfaces"]
+        interfaces_conf[fail_port]["loop_protect_external"] = True
+        interfaces_conf[end_port]["loop_protect_external"] = True
+        interfaces_conf[end_port]["lacp"] = 2
 
-        self.reload_conf(conf, self.faucet_config_path, restart=True,
-                         cold_start=None, change_expected=True)
+        self.reload_conf(
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=None,
+            change_expected=True,
+        )
 
         self.wait_for_all_lacp_up()
         self.verify_stack_hosts()
 
-        interfaces_conf[fail_port]['lacp'] = 0
-        interfaces_conf[fail_port]['lacp_active'] = False
-        self.reload_conf(conf, self.faucet_config_path, restart=True,
-                         cold_start=False, change_expected=True, dpid=self.dpids[1])
+        interfaces_conf[fail_port]["lacp"] = 0
+        interfaces_conf[fail_port]["lacp_active"] = False
+        self.reload_conf(
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.dpids[1],
+        )
 
-        self.wait_for_lacp_port_init(src_port, self.dpids[0], self.topo.switches_by_id[0])
+        self.wait_for_lacp_port_init(
+            src_port, self.dpids[0], self.topo.switches_by_id[0]
+        )
         self.wait_for_lacp_port_up(dst_port, self.dpids[0], self.topo.switches_by_id[0])
-        self.wait_for_lacp_port_init(end_port, self.dpids[1], self.topo.switches_by_id[1])
+        self.wait_for_lacp_port_init(
+            end_port, self.dpids[1], self.topo.switches_by_id[1]
+        )
 
 
 class FaucetStackStringOfDPUntaggedTest(FaucetMultiDPTestBase):
     """Test topology of stacked datapaths with untagged hosts."""
 
     NUM_DPS = 2
     NUM_HOSTS = 2
     SOFTWARE_ONLY = True
 
     def verify_events_log(self, event_log):
         """Verify event log has correct L2 learn events"""
-        with open(event_log, 'r', encoding='utf-8') as event_log_file:
-            events = [json.loads(event_log_line.strip()) for event_log_line in event_log_file]
-            l2_learns = [event['L2_LEARN'] for event in events if 'L2_LEARN' in event]
+        with open(event_log, "r", encoding="utf-8") as event_log_file:
+            events = [
+                json.loads(event_log_line.strip()) for event_log_line in event_log_file
+            ]
+            l2_learns = [event["L2_LEARN"] for event in events if "L2_LEARN" in event]
             for event in l2_learns:
-                if event.get('stack_descr', None):
+                if event.get("stack_descr", None):
                     return
-            self.fail('stack_descr not in events: %s' % l2_learns)
+            self.fail("stack_descr not in events: %s" % l2_learns)
 
     def test_untagged(self):
         """All untagged hosts in stack topology can reach each other."""
         self.set_up(
-            stack=True, n_dps=self.NUM_DPS, n_untagged=self.NUM_HOSTS,
-            switch_to_switch_links=2)
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_untagged=self.NUM_HOSTS,
+            switch_to_switch_links=2,
+        )
         self._enable_event_log()
         self.verify_stack_hosts()
         self.verify_events_log(self.event_log)
 
 
 class FaucetSingleStackStringOfDPExtLoopProtUntaggedTest(FaucetMultiDPTestBase):
     """Test topology of stacked datapaths with untagged hosts."""
@@ -517,49 +603,57 @@
     def setUp(self):
         """Setup network & configuration file"""
         super().set_up(
             stack=True,
             n_dps=self.NUM_DPS,
             n_untagged=self.NUM_HOSTS,
             switch_to_switch_links=2,
-            use_external=True)
+            use_external=True,
+        )
 
     def test_untagged(self):
         """Host can reach each other, unless both marked loop_protect_external"""
         for host in self.hosts_name_ordered():
             self.require_host_learned(host)
 
         # Part 1: Make sure things are connected properly.
         self.verify_protected_connectivity()  # Before reload
 
         # Part 2: Test the code on pipeline reconfiguration path.
         conf = self._get_faucet_conf()
         loop_interface = None
         dp_name = self.topo.switches_by_id[1]
-        for interface, interface_conf in conf['dps'][dp_name]['interfaces'].items():
-            if 'stack' in interface_conf:
+        for interface, interface_conf in conf["dps"][dp_name]["interfaces"].items():
+            if "stack" in interface_conf:
                 continue
-            if not interface_conf.get('loop_protect_external', False):
+            if not interface_conf.get("loop_protect_external", False):
                 loop_interface = interface
                 break
 
         self._mark_external(loop_interface, True)
         self._mark_external(loop_interface, False)
 
         # Part 3: Make sure things are the same after reload.
         self.verify_protected_connectivity()  # After reload
 
     def _mark_external(self, loop_intf, protect_external):
         """Change the loop interfaces loop_protect_external option"""
         conf = self._get_faucet_conf()
         dp_name = self.topo.switches_by_id[1]
-        conf['dps'][dp_name]['interfaces'][loop_intf]['loop_protect_external'] = protect_external
+        conf["dps"][dp_name]["interfaces"][loop_intf][
+            "loop_protect_external"
+        ] = protect_external
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=False, change_expected=True, dpid=self.topo.dpids_by_id[1])
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.topo.dpids_by_id[1],
+        )
 
     def test_missing_ext(self):
         """Test stacked dp with all external ports down on a switch"""
         self.validate_with_externals_down_fails(self.topo.switches_by_id[0])
         self.validate_with_externals_down_fails(self.topo.switches_by_id[1])
 
 
@@ -567,44 +661,54 @@
     """Test topology of stacked datapaths with untagged hosts."""
 
     NUM_DPS = 3
     NUM_HOSTS = 3
 
     def test_untagged(self):
         """Test the external loop protect with stacked DPs and untagged hosts"""
-        self.set_up(stack=True, n_dps=self.NUM_DPS, n_untagged=self.NUM_HOSTS,
-                    switch_to_switch_links=2, use_external=True)
+        self.set_up(
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_untagged=self.NUM_HOSTS,
+            switch_to_switch_links=2,
+            use_external=True,
+        )
         self.verify_stack_up()
         int_hosts, ext_hosts, dp_hosts = self.map_int_ext_hosts()
         _, root_ext_hosts = dp_hosts[self.topo.switches_by_id[0]]
 
         for int_host in int_hosts:
             # All internal hosts can reach other internal hosts.
             for other_int_host in int_hosts - {int_host}:
                 self.verify_broadcast(
-                    hosts=(int_host, other_int_host), broadcast_expected=True)
+                    hosts=(int_host, other_int_host), broadcast_expected=True
+                )
                 self.verify_unicast(
-                    hosts=(int_host, other_int_host), unicast_expected=True)
+                    hosts=(int_host, other_int_host), unicast_expected=True
+                )
 
             # All internal hosts should reach exactly one external host.
             self.verify_one_broadcast(int_host, ext_hosts)
 
         for ext_host in ext_hosts:
             # All external hosts cannot flood to each other
             for other_ext_host in ext_hosts - {ext_host}:
                 self.verify_broadcast(
-                    hosts=(ext_host, other_ext_host), broadcast_expected=False)
+                    hosts=(ext_host, other_ext_host), broadcast_expected=False
+                )
 
         remote_ext_hosts = ext_hosts - set(root_ext_hosts)
         # int host should never be broadcast to an ext host that is not on the root.
         for local_int_hosts, _ in dp_hosts.values():
             for local_int_host in local_int_hosts:
                 for remote_ext_host in remote_ext_hosts:
                     self.verify_broadcast(
-                        hosts=(local_int_host, remote_ext_host), broadcast_expected=False)
+                        hosts=(local_int_host, remote_ext_host),
+                        broadcast_expected=False,
+                    )
 
 
 class FaucetGroupStackStringOfDPUntaggedTest(FaucetStackStringOfDPUntaggedTest):
     """Test topology of stacked datapaths with untagged hosts."""
 
     GROUP_TABLE = True
 
@@ -613,16 +717,21 @@
     """Test Faucet with a 3-cycle topology"""
 
     NUM_DPS = 3
     SOFTWARE_ONLY = True
 
     def test_untagged(self):
         """Stack loop prevention works and hosts can ping each other."""
-        self.set_up(stack=True, n_dps=self.NUM_DPS, n_untagged=self.NUM_HOSTS,
-                    switch_to_switch_links=1, stack_ring=True)
+        self.set_up(
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_untagged=self.NUM_HOSTS,
+            switch_to_switch_links=1,
+            stack_ring=True,
+        )
         self.verify_stack_up()
         self.verify_stack_has_no_loop()
         self.retry_net_ping()
         self.verify_traveling_dhcp_mac()
         # Move through each DP breaking either side of the ring
         for link, ports in self.link_port_maps.items():
             dp_i, _ = link
@@ -646,27 +755,37 @@
     NUM_DPS = 3
     NUM_HOSTS = 1
     SOFTWARE_ONLY = True
     port_order = [3, 2, 1, 0]
 
     def test_sequential_connection(self):
         """Ping in sequence respective to hosts names"""
-        self.set_up(stack=True, n_dps=self.NUM_DPS, n_untagged=self.NUM_HOSTS,
-                    switch_to_switch_links=1, stack_ring=True)
+        self.set_up(
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_untagged=self.NUM_HOSTS,
+            switch_to_switch_links=1,
+            stack_ring=True,
+        )
         self.verify_stack_up()
         hosts = self.hosts_name_ordered()
         for src in hosts:
             for dst in hosts:
                 if src != dst:
                     self.one_ipv4_ping(src, dst.IP())
 
     def test_reverse_sequential_connection(self):
         """Ping in reverse sequence respective to hosts names"""
-        self.set_up(stack=True, n_dps=self.NUM_DPS, n_untagged=self.NUM_HOSTS,
-                    switch_to_switch_links=1, stack_ring=True)
+        self.set_up(
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_untagged=self.NUM_HOSTS,
+            switch_to_switch_links=1,
+            stack_ring=True,
+        )
         self.verify_stack_up()
         hosts = self.hosts_name_ordered()
         hosts.reverse()
         for src in hosts:
             for dst in hosts:
                 if src != dst:
                     self.one_ipv4_ping(src, dst.IP())
@@ -687,104 +806,117 @@
     NUM_HOSTS = 3
 
     def acls(self):
         """Configuration ACLs"""
         # 3 hosts on each DP (3 DPS)
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'nw_dst': '10.1.0.2',
-                    'actions': {
-                        'output': {
-                            'port': self.host_port_maps[1][0][0]  # Host 1
-                        }
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'dl_dst': 'ff:ff:ff:ff:ff:ff',
-                    'actions': {
-                        'output': {
-                            'ports': [
-                                self.host_port_maps[1][0][0],  # Host 1
-                                self.link_port_maps[(0, 1)][0]]  # link (0, 1)
-                        }
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'actions': {
-                        'output': {
-                            'port': self.link_port_maps[(0, 1)][0]  # link (0, 1)
-                        }
-                    },
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "nw_dst": "10.1.0.2",
+                        "actions": {
+                            "output": {"port": self.host_port_maps[1][0][0]}  # Host 1
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "dl_dst": "ff:ff:ff:ff:ff:ff",
+                        "actions": {
+                            "output": {
+                                "ports": [
+                                    self.host_port_maps[1][0][0],  # Host 1
+                                    self.link_port_maps[(0, 1)][0],
+                                ]  # link (0, 1)
+                            }
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "actions": {
+                            "output": {
+                                "port": self.link_port_maps[(0, 1)][0]  # link (0, 1)
+                            }
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
             ],
             2: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'actions': {
-                        'output': {
-                            'port': self.link_port_maps[(1, 2)][0]  # link (1, 2)
-                        }
-                    },
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "actions": {
+                            "output": {
+                                "port": self.link_port_maps[(1, 2)][0]  # link (1, 2)
+                            }
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
             ],
             3: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'nw_dst': '10.1.0.7',
-                    'actions': {
-                        'output': {
-                            'port': self.host_port_maps[6][2][0]  # host 6
-                        }
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'dl_dst': 'ff:ff:ff:ff:ff:ff',
-                    'actions': {
-                        'output': {
-                            'ports': [self.host_port_maps[6][2][0]]  # host 6
-                        }
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'actions': {
-                        'allow': 0,
-                    },
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "nw_dst": "10.1.0.7",
+                        "actions": {
+                            "output": {"port": self.host_port_maps[6][2][0]}  # host 6
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "dl_dst": "ff:ff:ff:ff:ff:ff",
+                        "actions": {
+                            "output": {
+                                "ports": [self.host_port_maps[6][2][0]]  # host 6
+                            }
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
             ],
         }
 
     # DP-to-acl_in port mapping.
     def link_acls(self):
         """Host/link map to acls_in"""
-        return {
-            0: [1],  # Host 0 dp 0 'acls_in': [1]
-            (1, 0): [2],
-            (2, 1): [3]
-        }
+        return {0: [1], (1, 0): [2], (2, 1): [3]}  # Host 0 dp 0 'acls_in': [1]
 
     def setUp(self):
         """Setup network & create configuration file"""
         super().set_up(
             stack=True,
             n_dps=self.NUM_DPS,
             n_untagged=self.NUM_HOSTS,
@@ -825,103 +957,118 @@
     NUM_DPS = 3
     NUM_HOSTS = 3
 
     def acls(self):
         """Configuration ACLs"""
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'nw_dst': '10.1.0.2',
-                    'actions': {
-                        'output': [
-                            {'port': self.host_port_maps[1][0][0]}  # Host 0
-                        ]
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'dl_dst': 'ff:ff:ff:ff:ff:ff',
-                    'actions': {
-                        'output': [
-                            {'ports': [
-                                self.host_port_maps[1][0][0],  # Host 0
-                                self.link_port_maps[(0, 1)][0]]}  # Link (0, 1)
-                        ]
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'actions': {
-                        'output': [
-                            {'port': self.link_port_maps[(0, 1)][0]}  # Link (0, 1)
-                        ]
-                    },
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "nw_dst": "10.1.0.2",
+                        "actions": {
+                            "output": [{"port": self.host_port_maps[1][0][0]}]  # Host 0
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "dl_dst": "ff:ff:ff:ff:ff:ff",
+                        "actions": {
+                            "output": [
+                                {
+                                    "ports": [
+                                        self.host_port_maps[1][0][0],  # Host 0
+                                        self.link_port_maps[(0, 1)][0],
+                                    ]
+                                }  # Link (0, 1)
+                            ]
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "actions": {
+                            "output": [
+                                {"port": self.link_port_maps[(0, 1)][0]}  # Link (0, 1)
+                            ]
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
             ],
             2: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'actions': {
-                        'output': [
-                            {'port': self.link_port_maps[(1, 2)][0]}  # Link (0, 2)
-                        ]
-                    },
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "actions": {
+                            "output": [
+                                {"port": self.link_port_maps[(1, 2)][0]}  # Link (0, 2)
+                            ]
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
             ],
             3: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'nw_dst': '10.1.0.7',
-                    'actions': {
-                        'output': {
-                            'port': self.host_port_maps[6][2][0]  # Host 6
-                        }
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'dl_dst': 'ff:ff:ff:ff:ff:ff',
-                    'actions': {
-                        'output': [
-                            {'ports': [self.host_port_maps[6][2][0]]}  # Host 6
-                        ]
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'actions': {
-                        'allow': 0,
-                    },
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "nw_dst": "10.1.0.7",
+                        "actions": {
+                            "output": {"port": self.host_port_maps[6][2][0]}  # Host 6
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "dl_dst": "ff:ff:ff:ff:ff:ff",
+                        "actions": {
+                            "output": [
+                                {"ports": [self.host_port_maps[6][2][0]]}  # Host 6
+                            ]
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
             ],
         }
 
     def link_acls(self):
         """Host/link map to acls in"""
-        return {
-            0: [1],  # Host 0 dp 0 'acls_in': [1]
-            (1, 0): [2],
-            (2, 1): [3]
-        }
+        return {0: [1], (1, 0): [2], (2, 1): [3]}  # Host 0 dp 0 'acls_in': [1]
 
     def setUp(self):
         """Setup network & create configuration file"""
         super().set_up(
             stack=True,
             n_dps=self.NUM_DPS,
             n_untagged=self.NUM_HOSTS,
@@ -965,110 +1112,122 @@
 
     # ACL rules which will get overridden.
     @staticmethod
     def acls():
         """Return config ACLs"""
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 6,
-                    'tcp_dst': 5001,
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 6,
-                    'tcp_dst': 5002,
-                    'actions': {
-                        'allow': 0,
-                    },
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 6,
+                        "tcp_dst": 5001,
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 6,
+                        "tcp_dst": 5002,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
             ],
         }
 
     # ACL rules which get put into an include-optional
     # file, then reloaded into FAUCET.
     @staticmethod
     def acls_override():
         """Return override ACLs option"""
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 6,
-                    'tcp_dst': 5001,
-                    'actions': {
-                        'allow': 0,
-                    },
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 6,
-                    'tcp_dst': 5002,
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    },
-                }},
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 6,
+                        "tcp_dst": 5001,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 6,
+                        "tcp_dst": 5002,
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        },
+                    }
+                },
             ],
         }
 
     # DP-to-acl_in port mapping.
     def link_acls(self):
         """Host/link port map to acls in"""
-        return {
-            0: [1]  # Host 0 'acls_in': [1]
-        }
+        return {0: [1]}  # Host 0 'acls_in': [1]
 
     def include_optional(self):
         if self.acls_config is None:
-            self.acls_config = os.path.join(self.tmpdir, 'acls.yaml')
+            self.acls_config = os.path.join(self.tmpdir, "acls.yaml")
         if self.missing_config is None:
-            self.missing_config = os.path.join(self.tmpdir, 'missing_config.yaml')
+            self.missing_config = os.path.join(self.tmpdir, "missing_config.yaml")
         return [self.acls_config, self.missing_config]
 
     def setUp(self):
         """Setup network & create configuration file"""
         self.acls_config = None
         self.missing_config = None
-        super().set_up(
-            n_dps=self.NUM_DPS,
-            n_untagged=self.NUM_HOSTS)
+        super().set_up(n_dps=self.NUM_DPS, n_untagged=self.NUM_HOSTS)
 
     def test_port5001_blocked(self):
         """Test that TCP port 5001 is blocked."""
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_notblocked(5001, first_host, second_host)
-        with open(self.acls_config, 'w', encoding='utf-8') as config_file:
-            self.configuration_options['acl_options'] = self.acls_override()
-            config_file.write(self.topo.get_config(n_vlans=1, **self.configuration_options))
+        with open(self.acls_config, "w", encoding="utf-8") as config_file:
+            self.configuration_options["acl_options"] = self.acls_override()
+            config_file.write(
+                self.topo.get_config(n_vlans=1, **self.configuration_options)
+            )
         self.verify_faucet_reconf(cold_start=False, change_expected=True)
         self.verify_tp_dst_blocked(5001, first_host, second_host)
 
     def test_port5002_notblocked(self):
         """Test that TCP port 5002 is not blocked."""
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_blocked(5002, first_host, second_host)
-        with open(self.acls_config, 'w', encoding='utf-8') as config_file:
-            self.configuration_options['acl_options'] = self.acls_override()
-            config_file.write(self.topo.get_config(n_vlans=1, **self.configuration_options))
+        with open(self.acls_config, "w", encoding="utf-8") as config_file:
+            self.configuration_options["acl_options"] = self.acls_override()
+            config_file.write(
+                self.topo.get_config(n_vlans=1, **self.configuration_options)
+            )
         self.verify_faucet_reconf(cold_start=False, change_expected=True)
         self.verify_tp_dst_notblocked(5002, first_host, second_host)
 
 
 class FaucetTunnelSameDpTest(FaucetMultiDPTestBase):
     """Test the tunnel ACL option with output to the same DP"""
 
@@ -1078,41 +1237,46 @@
     SWITCH_TO_SWITCH_LINKS = 2
 
     def acls(self):
         """Return ACL config"""
         # Tunnel from host 0 (switch 0) to host 1 (switch 0)
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'actions': {
-                        'allow': 0,
-                        'output': {
-                            'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[0],  # Switch 0
-                                'port': self.host_port_maps[1][0][0]}  # Switch 0 host 1
-                        }
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "actions": {
+                            "allow": 0,
+                            "output": {
+                                "tunnel": {
+                                    "type": "vlan",
+                                    "tunnel_id": 200,
+                                    "dp": self.topo.switches_by_id[0],  # Switch 0
+                                    "port": self.host_port_maps[1][0][0],
+                                }  # Switch 0 host 1
+                            },
+                        },
                     }
-                }}
+                }
             ]
         }
 
     def link_acls(self):
         """DP to acl port mapping"""
-        return {
-            0: [1]  # Host 0 'acls_in': [1]
-        }
+        return {0: [1]}  # Host 0 'acls_in': [1]
 
     def test_tunnel_established(self):
         """Test a tunnel path can be created."""
-        self.set_up(stack=True, n_dps=self.NUM_DPS, n_untagged=self.NUM_HOSTS,
-                    switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS)
+        self.set_up(
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_untagged=self.NUM_HOSTS,
+            switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS,
+        )
         self.verify_stack_up()
         src_host = self.net.get(self.topo.hosts_by_id[0])
         dst_host = self.net.get(self.topo.hosts_by_id[1])
         other_host = self.net.get(self.topo.hosts_by_id[2])
         self.verify_tunnel_established(src_host, dst_host, other_host)
 
 
@@ -1125,77 +1289,86 @@
     SWITCH_TO_SWITCH_LINKS = 2
 
     def acls(self):
         """Return config ACL options"""
         # Tunnel from host 0 (switch 0) to host 2 (switch 1)
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'actions': {
-                        'allow': 0,
-                        'output': {
-                            'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[1],
-                                'port': self.host_port_maps[2][1][0]}
-                        }
-                    }
-                }},
-                {'rule': {
-                    'dl_type': IPV6_ETH,
-                    'ip_proto': 56,
-                    'actions': {
-                        'allow': 0,
-                        'output': {
-                            'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[1],
-                                'port': self.host_port_maps[2][1][0]}
-                        }
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "actions": {
+                            "allow": 0,
+                            "output": {
+                                "tunnel": {
+                                    "type": "vlan",
+                                    "tunnel_id": 200,
+                                    "dp": self.topo.switches_by_id[1],
+                                    "port": self.host_port_maps[2][1][0],
+                                }
+                            },
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV6_ETH,
+                        "ip_proto": 56,
+                        "actions": {
+                            "allow": 0,
+                            "output": {
+                                "tunnel": {
+                                    "type": "vlan",
+                                    "tunnel_id": 200,
+                                    "dp": self.topo.switches_by_id[1],
+                                    "port": self.host_port_maps[2][1][0],
+                                }
+                            },
+                        },
                     }
-                }},
+                },
             ]
         }
 
     def link_acls(self):
         """DP-to-acl port mapping"""
         return {
             0: [1],  # Host 0 'acls_in': [1]
             3: [1],  # Host 3 'acls_in': [1]
         }
 
     def output_only(self):
-        return {2}   # Host 2 (first port, second switch).
+        return {2}  # Host 2 (first port, second switch).
 
     def setUp(self):
         """Start the network"""
         super().set_up(
             stack=True,
             n_dps=self.NUM_DPS,
             n_untagged=self.NUM_HOSTS,
-            switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS)
+            switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS,
+        )
 
     def verify_tunnels(self):
         """Test tunnel connectivity from local and remote switches."""
         other_host = self.net.get(self.topo.hosts_by_id[1])
         dst_host = self.net.get(self.topo.hosts_by_id[2])
         for src_host_id in (0, 3):
             src_host = self.net.get(self.topo.hosts_by_id[src_host_id])
             self.verify_tunnel_established(src_host, dst_host, other_host, packets=10)
 
     def test_tunnel_path_rerouted(self):
         """Test remote tunnel path is rerouted when a link is down."""
         self.verify_stack_up()
         self.verify_tunnels()
         first_stack_port = min(self.link_port_maps[(0, 1)])
-        self.one_stack_port_down(self.dpids[0], self.topo.switches_by_id[0], first_stack_port)
+        self.one_stack_port_down(
+            self.dpids[0], self.topo.switches_by_id[0], first_stack_port
+        )
         self.verify_tunnels()
 
 
 class FaucetTunnelLoopTest(FaucetSingleTunnelTest):
     """Test tunnel on a loop topology"""
 
     NUM_DPS = 3
@@ -1204,15 +1377,16 @@
     def setUp(self):
         """Start a loop topology network"""
         super().set_up(
             stack=True,
             n_dps=self.NUM_DPS,
             n_untagged=self.NUM_HOSTS,
             switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS,
-            stack_ring=True)
+            stack_ring=True,
+        )
 
 
 class FaucetTunnelAllowTest(FaucetTopoTestBase):
     """Test Tunnels with ACLs containing allow=True"""
 
     NUM_DPS = 2
     NUM_HOSTS = 4
@@ -1220,54 +1394,66 @@
     SOFTWARE_ONLY = True
 
     def acls(self):
         # Tunnel from host 0 (switch 0) to host 2 (switch 1)
         """Return config ACL options"""
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'actions': {
-                        'allow': 1,
-                        'output': {
-                            'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 300,
-                                'dp': self.topo.switches_by_id[1],
-                                'port': self.host_port_maps[2][1][0]}
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "actions": {
+                            "allow": 1,
+                            "output": {
+                                "tunnel": {
+                                    "type": "vlan",
+                                    "tunnel_id": 300,
+                                    "dp": self.topo.switches_by_id[1],
+                                    "port": self.host_port_maps[2][1][0],
+                                }
+                            },
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
                         }
                     }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    }
-                }},
+                },
             ]
         }
 
     def setUp(self):
         """Start the network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [0], 2: [1], 3: [1]}
         host_vlans = {0: 0, 1: 0, 2: 1, 3: 0}
-        host_options = {0: {'acls_in': [1]}}
+        host_options = {0: {"acls_in": [1]}}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
@@ -1300,41 +1486,48 @@
     SWITCH_TO_SWITCH_LINKS = 2
 
     def acls(self):
         """Return ACL config"""
         # Tunnel from host 0 (switch 0) to host 1 (switch 0)
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[0],
-                                'port': self.host_port_maps[1][0][0]}}
-                        ]
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {
+                                    "tunnel": {
+                                        "type": "vlan",
+                                        "tunnel_id": 200,
+                                        "dp": self.topo.switches_by_id[0],
+                                        "port": self.host_port_maps[1][0][0],
+                                    }
+                                }
+                            ],
+                        },
                     }
-                }}
+                }
             ]
         }
 
     def link_acls(self):
         """DP to acl port mapping"""
-        return {
-            0: [1]  # Host 0 'acls_in': [1]
-        }
+        return {0: [1]}  # Host 0 'acls_in': [1]
 
     def test_tunnel_established(self):
         """Test a tunnel path can be created."""
-        self.set_up(stack=True, n_dps=self.NUM_DPS, n_untagged=self.NUM_HOSTS,
-                    switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS)
+        self.set_up(
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_untagged=self.NUM_HOSTS,
+            switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS,
+        )
         self.verify_stack_up()
         src_host = self.net.get(self.topo.hosts_by_id[0])
         dst_host = self.net.get(self.topo.hosts_by_id[1])
         other_host = self.net.get(self.topo.hosts_by_id[2])
         self.verify_tunnel_established(src_host, dst_host, other_host)
 
 
@@ -1346,44 +1539,48 @@
     SOFTWARE_ONLY = True
     SWITCH_TO_SWITCH_LINKS = 2
 
     def acls(self):
         """Return config ACL options"""
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[1],
-                                'port': self.host_port_maps[2][1][0]}}
-                        ]
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {
+                                    "tunnel": {
+                                        "type": "vlan",
+                                        "tunnel_id": 200,
+                                        "dp": self.topo.switches_by_id[1],
+                                        "port": self.host_port_maps[2][1][0],
+                                    }
+                                }
+                            ],
+                        },
                     }
-                }}
+                }
             ]
         }
 
     def link_acls(self):
         """DP link to list of acls to apply"""
-        return {
-            0: [1]  # Host 0 'acls_in': [1]
-        }
+        return {0: [1]}  # Host 0 'acls_in': [1]
 
     def setUp(self):
         """Start the network"""
         super().set_up(
             stack=True,
             n_dps=self.NUM_DPS,
             n_tagged=self.NUM_HOSTS,
-            switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS)
+            switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS,
+        )
 
     def test_tunnel_established(self):
         """Test a tunnel path can be created."""
         self.verify_stack_up()
         src_host = self.net.get(self.topo.hosts_by_id[0])
         dst_host = self.net.get(self.topo.hosts_by_id[2])
         other_host = self.net.get(self.topo.hosts_by_id[1])
@@ -1393,15 +1590,17 @@
         """Test a tunnel path is rerouted when a link is down."""
         self.verify_stack_up()
         src_host = self.net.get(self.topo.hosts_by_id[0])
         dst_host = self.net.get(self.topo.hosts_by_id[2])
         other_host = self.net.get(self.topo.hosts_by_id[1])
         self.verify_tunnel_established(src_host, dst_host, other_host, packets=10)
         first_stack_port = min(self.link_port_maps[(0, 1)])
-        self.one_stack_port_down(self.dpids[0], self.topo.switches_by_id[0], first_stack_port)
+        self.one_stack_port_down(
+            self.dpids[0], self.topo.switches_by_id[0], first_stack_port
+        )
         self.verify_tunnel_established(src_host, dst_host, other_host, packets=10)
 
 
 class FaucetTunnelLoopOrderedTest(FaucetSingleTunnelOrderedTest):
     """Test tunnel on a loop topology"""
 
     NUM_DPS = 3
@@ -1410,71 +1609,84 @@
     def setUp(self):
         """Start a loop topology network"""
         super().set_up(
             stack=True,
             n_dps=self.NUM_DPS,
             n_untagged=self.NUM_HOSTS,
             switch_to_switch_links=self.SWITCH_TO_SWITCH_LINKS,
-            stack_ring=True)
+            stack_ring=True,
+        )
 
 
 class FaucetTunnelAllowOrderedTest(FaucetTopoTestBase):
     """Test Tunnels with ACLs containing allow=True"""
 
     NUM_DPS = 2
     NUM_HOSTS = 4
     NUM_VLANS = 2
     SOFTWARE_ONLY = True
 
     def acls(self):
         """Return config ACL options"""
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'actions': {
-                        'allow': 1,
-                        'output': [
-                            {'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 300,
-                                'dp': self.topo.switches_by_id[1],
-                                'port': self.host_port_maps[2][1][0]}}
-                        ]
-                    }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "actions": {
+                            "allow": 1,
+                            "output": [
+                                {
+                                    "tunnel": {
+                                        "type": "vlan",
+                                        "tunnel_id": 300,
+                                        "dp": self.topo.switches_by_id[1],
+                                        "port": self.host_port_maps[2][1][0],
+                                    }
+                                }
+                            ],
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        }
                     }
-                }},
+                },
             ]
-
-
         }
 
     def setUp(self):
         """Start the network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [0], 2: [1], 3: [1]}
         host_vlans = {0: 0, 1: 0, 2: 1, 3: 0}
-        host_options = {0: {'acls_in': [1]}}
+        host_options = {0: {"acls_in": [1]}}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
@@ -1515,23 +1727,30 @@
             n_dps: Number of DPs
             host_links: How to connect each host to the DPs
             host_vlans: The VLAN each host is on
         """
         network_graph = networkx.path_graph(n_dps)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             for key, value in self.dp_options().items():
                 dp_options[dp_i][key] = value
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         routed_vlans = 2
         if host_links is None or host_vlans is None:
             host_links = {}
             host_vlans = {}
             host_n = 0
@@ -1539,38 +1758,38 @@
                 for vlan in range(routed_vlans):
                     host_links[host_n] = [dp_i]
                     host_vlans[host_n] = vlan
                     host_n += 1
         vlan_options = {}
         for v_i in range(routed_vlans):
             vlan_options[v_i] = {
-                'faucet_mac': self.faucet_mac(v_i),
-                'faucet_vips': [self.faucet_vip(v_i)],
-                'targeted_gw_resolution': False
+                "faucet_mac": self.faucet_mac(v_i),
+                "faucet_vips": [self.faucet_vip(v_i)],
+                "targeted_gw_resolution": False,
             }
         routers = {0: list(range(routed_vlans))}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
             vlan_options=vlan_options,
-            routers=routers
+            routers=routers,
         )
         self.start_net()
 
     @staticmethod
     def dp_options():
         """Return DP config options"""
         return {
-            'arp_neighbor_timeout': 2,
-            'max_resolve_backoff_time': 2,
-            'proactive_learn_v4': True
+            "arp_neighbor_timeout": 2,
+            "max_resolve_backoff_time": 2,
+            "proactive_learn_v4": True,
         }
 
     def test_intervlan_routing_2stack(self):
         """Verify intervlan routing works with 2 DPs in a stack"""
         self.NUM_DPS = 2
         self.set_up(self.NUM_DPS)
         self.verify_stack_up()
@@ -1618,35 +1837,35 @@
     NETPREFIX = 64
     ETH_TYPE = IPV6_ETH
 
     @staticmethod
     def dp_options():
         """Return DP config options"""
         return {
-            'nd_neighbor_timeout': 2,
-            'max_resolve_backoff_time': 1,
-            'proactive_learn_v6': True
+            "nd_neighbor_timeout": 2,
+            "max_resolve_backoff_time": 1,
+            "proactive_learn_v6": True,
         }
 
     def host_ping(self, src_host, dst_ip, _intf=None):
         """Override to ping ipv6 addresses"""
         self.one_ipv6_ping(src_host, dst_ip, require_host_learned=False)
 
     def set_host_ip(self, host, host_ip):
         """Override to setup host ipv6 ip address"""
         self.add_host_ipv6_address(host, host_ip)
 
     @staticmethod
     def faucet_vip(i):
         """Get the IPV6 faucet vip"""
-        return 'fc0%u::1:254/112' % (i + 1)
+        return "fc0%u::1:254/112" % (i + 1)
 
     def host_ip_address(self, host_index, vlan_index):
         """Get the IPV6 host ip"""
-        return 'fc0%u::1:%u/%u' % (vlan_index + 1, host_index + 1, self.NETPREFIX)
+        return "fc0%u::1:%u/%u" % (vlan_index + 1, host_index + 1, self.NETPREFIX)
 
 
 class FaucetSingleUntaggedVlanStackFloodTest(FaucetTopoTestBase):
     """Test InterVLAN routing can flood packets to stack ports"""
 
     IPV = 4
     NETPREFIX = 24
@@ -1661,67 +1880,74 @@
 
     def set_up(self):
         """Start the network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             for key, value in self.dp_options().items():
                 dp_options[dp_i][key] = value
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [1]}
         host_vlans = {0: 0, 1: 1}
         vlan_options = {}
         for v_i in range(self.NUM_VLANS):
             vlan_options[v_i] = {
-                'faucet_mac': self.faucet_mac(v_i),
-                'faucet_vips': [self.faucet_vip(v_i)],
-                'targeted_gw_resolution': False
+                "faucet_mac": self.faucet_mac(v_i),
+                "faucet_vips": [self.faucet_vip(v_i)],
+                "targeted_gw_resolution": False,
             }
         routers = {0: list(range(self.NUM_VLANS))}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
             vlan_options=vlan_options,
-            routers=routers
+            routers=routers,
         )
         self.start_net()
 
     @staticmethod
     def dp_options():
         """Return DP config options"""
         return {
-            'arp_neighbor_timeout': 2,
-            'max_resolve_backoff_time': 2,
-            'proactive_learn_v4': True
+            "arp_neighbor_timeout": 2,
+            "max_resolve_backoff_time": 2,
+            "proactive_learn_v4": True,
         }
 
     def test_intervlan_stack_flooding(self):
         """
         Test intervlan can flood to stack ports
         h1 (dst_host) should not have talked on the network so Faucet does not know about
             it. h2 (src_host) -> h1 ping will normally fail (without flooding to the stack)
             because the ARP packet for resolving h1 does not make it across the stack.
         """
         self.set_up()
         self.verify_stack_up()
-        src_host = self.host_information[1]['host']
-        dst_ip = self.host_information[0]['ip']
+        src_host = self.host_information[1]["host"]
+        dst_ip = self.host_information[0]["ip"]
         self.host_ping(src_host, dst_ip.ip)
 
 
 class FaucetUntaggedStackTransitTest(FaucetTopoTestBase):
     """Test that L2 connectivity exists over a transit switch with no VLANs"""
 
     NUM_DPS = 3
@@ -1731,21 +1957,28 @@
 
     def setUp(self):
         """Set up network with transit switch with no hosts"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [2]}
         host_vlans = {0: 0, 1: 0}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
@@ -1772,21 +2005,28 @@
 
     def setUp(self):
         """Set up network with transit switch on different VLAN"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [1], 2: [2]}
         host_vlans = {0: 0, 1: 1, 2: 0}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
@@ -1813,18 +2053,18 @@
 
     LACP_HOST = 2
 
     @staticmethod
     def dp_options():
         """Return DP config options"""
         return {
-            'arp_neighbor_timeout': 2,
-            'max_resolve_backoff_time': 2,
-            'proactive_learn_v4': True,
-            'lacp_timeout': 10
+            "arp_neighbor_timeout": 2,
+            "max_resolve_backoff_time": 2,
+            "proactive_learn_v4": True,
+            "lacp_timeout": 10,
         }
 
     def setUp(self):
         """Disabling allows for each test case to start the test"""
 
     def set_up(self, lacp_host_links, host_vlans=None):
         """
@@ -1833,47 +2073,54 @@
             host_vlans: Default generate with one host on each VLAN, on each DP
                 plus one LAG host the same VLAN as hosts
         """
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             for key, value in self.dp_options().items():
                 dp_options[dp_i][key] = value
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [0], self.LACP_HOST: lacp_host_links, 3: [1], 4: [1]}
         if host_vlans is None:
             host_vlans = {0: 0, 1: 1, 2: 1, 3: 0, 4: 1}
         vlan_options = {}
         for v_i in range(self.NUM_VLANS):
             vlan_options[v_i] = {
-                'faucet_mac': self.faucet_mac(v_i),
-                'faucet_vips': [self.faucet_vip(v_i)],
-                'targeted_gw_resolution': False
+                "faucet_mac": self.faucet_mac(v_i),
+                "faucet_vips": [self.faucet_vip(v_i)],
+                "targeted_gw_resolution": False,
             }
         routers = {0: list(range(self.NUM_VLANS))}
-        host_options = {self.LACP_HOST: {'lacp': 1}}
+        host_options = {self.LACP_HOST: {"lacp": 1}}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
             vlan_options=vlan_options,
             routers=routers,
-            host_options=host_options
+            host_options=host_options,
         )
         self.start_net()
 
     def test_lacp_lag(self):
         """Test LACP LAG, where LAG bundle is connected to the same DP"""
         lacp_host_links = [0, 0]
         self.set_up(lacp_host_links)
@@ -1898,19 +2145,25 @@
 
         # Take down single LAG port
         self.set_port_down(self.host_port_maps[self.LACP_HOST][0][0], self.dpids[0])
         self.verify_num_lag_up_ports(1, self.dpids[0])
 
         # Force warm start on switch by changing native VLAN of host1
         conf = self._get_faucet_conf()
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[0]]['interfaces']
-        interfaces_conf[self.host_port_maps[1][0][0]]['native_vlan'] = self.topo.vlan_name(0)
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[0]]["interfaces"]
+        interfaces_conf[self.host_port_maps[1][0][0]][
+            "native_vlan"
+        ] = self.topo.vlan_name(0)
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=False, change_expected=False)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=False,
+        )
         self.host_information.pop(1)
 
         # Set a single LAG port back UP
         self.set_port_up(self.host_port_maps[self.LACP_HOST][0][0], self.dpids[0])
         self.verify_num_lag_up_ports(2, self.dpids[0])
 
         # Verify connectivity
@@ -1962,51 +2215,58 @@
 
     LACP_HOST = 3
 
     @staticmethod
     def dp_options():
         """Return config DP options"""
         return {
-            'arp_neighbor_timeout': 2,
-            'max_resolve_backoff_time': 2,
-            'proactive_learn_v4': True,
-            'lacp_timeout': 10
+            "arp_neighbor_timeout": 2,
+            "max_resolve_backoff_time": 2,
+            "proactive_learn_v4": True,
+            "lacp_timeout": 10,
         }
 
     def setUp(self):
         """Ignore to allow for setting up network in each test"""
 
     def set_up(self):
         """Set up network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             for key, value in self.dp_options().items():
                 dp_options[dp_i][key] = value
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [1], 2: [2], 3: [0, 0, 2, 2]}
         host_vlans = {host_id: 0 for host_id in range(self.NUM_HOSTS)}
-        host_options = {self.LACP_HOST: {'lacp': 1}}
+        host_options = {self.LACP_HOST: {"lacp": 1}}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
-            host_options=host_options
+            host_options=host_options,
         )
         self.start_net()
 
     def test_lag_connectivity(self):
         """Test whether the LAG host can connect to any other host"""
         self.set_up()
         self.verify_stack_up()
@@ -2017,63 +2277,76 @@
         """
         All of the LAG links should work, test by using the xmit_hash_policy
             with different IP addresses to change the link used by the packet
         """
         self.set_up()
         self.verify_stack_up()
         self.require_linux_bond_up(self.LACP_HOST)
-        lacp_host = self.host_information[self.LACP_HOST]['host']
+        lacp_host = self.host_information[self.LACP_HOST]["host"]
         lacp_switches = {
             self.net.get(self.topo.switches_by_id[i])
-            for i in self.host_port_maps[self.LACP_HOST]}
-        lacp_intfs = sorted({
-            pair[0].name for switch in lacp_switches for pair in lacp_host.connectionsTo(switch)})
+            for i in self.host_port_maps[self.LACP_HOST]
+        }
+        lacp_intfs = sorted(
+            {
+                pair[0].name
+                for switch in lacp_switches
+                for pair in lacp_host.connectionsTo(switch)
+            }
+        )
         dst_host_id = 1
-        dst_host = self.host_information[dst_host_id]['host']
+        dst_host = self.host_information[dst_host_id]["host"]
         tcpdump_filter = (
-            'ip and ether src 0e:00:00:00:00:99 '
-            'and src net %s and dst net %s' % (lacp_host.IP(), dst_host.IP()))
+            "ip and ether src 0e:00:00:00:00:99 "
+            "and src net %s and dst net %s" % (lacp_host.IP(), dst_host.IP())
+        )
         # Loop until all links have been used to prove that they can be used
         link_used = [False for _ in range(len(lacp_intfs))]
         max_iter = len(lacp_intfs) * 2
         iterations = 0
         while link_used.count(False) > 2 and iterations <= max_iter:
             no_packets = True
             for i, intf in enumerate(lacp_intfs):
                 funcs = []
-                funcs.append(lambda: lacp_host.cmd('ping -c5 %s' % dst_host.IP()))
+                funcs.append(lambda: lacp_host.cmd("ping -c5 %s" % dst_host.IP()))
                 tcpdump_txt = self.tcpdump_helper(
-                    lacp_host, tcpdump_filter, intf_name=intf, funcs=funcs)
+                    lacp_host, tcpdump_filter, intf_name=intf, funcs=funcs
+                )
                 no_packets = self.tcpdump_rx_packets(tcpdump_txt, packets=0)
                 if not no_packets:
                     # Packets detected on link so can stop testing and
                     #   goto a new IP value for the remaining links
                     link_used[i] = True
-                    error('%s via %s\n' % (dst_host.IP(), intf))
+                    error("%s via %s\n" % (dst_host.IP(), intf))
                     break
             # If no packets have been detected on any port then something
             #   has gone terribly wrong
             self.assertFalse(
-                no_packets, 'Ping packets to host IP %s could not be found' % dst_host.IP())
+                no_packets,
+                "Ping packets to host IP %s could not be found" % dst_host.IP(),
+            )
             # Increment the host IP address to change the LACP hash value,
             #   potentially changing the link used
             self.increment_host_ip(dst_host_id)
             tcpdump_filter = (
-                'ip and ether src 0e:00:00:00:00:99 '
-                'and src net %s and dst net %s' % (lacp_host.IP(), dst_host.IP()))
+                "ip and ether src 0e:00:00:00:00:99 "
+                "and src net %s and dst net %s" % (lacp_host.IP(), dst_host.IP())
+            )
             iterations += 1
-        not_used = [list(lacp_intfs)[i] for i, value in enumerate(link_used) if not value]
+        not_used = [
+            list(lacp_intfs)[i] for i, value in enumerate(link_used) if not value
+        ]
         expected_links = [True, True, False, False]
-        self.assertEqual(link_used, expected_links, 'Links %s not used' % not_used)
+        self.assertEqual(link_used, expected_links, "Links %s not used" % not_used)
 
     def increment_host_ip(self, host_id):
         """Increases the host ip address"""
-        host = self.host_information[host_id]['host']
-        self.host_information[host_id]['ip'] += 3
-        self.set_host_ip(host, self.host_information[host_id]['ip'])
+        host = self.host_information[host_id]["host"]
+        self.host_information[host_id]["ip"] += 3
+        self.set_host_ip(host, self.host_information[host_id]["ip"])
 
     def test_lacp_port_change(self):
         """
         Test that communication to a host on a LAG is possible
             after the original selected link goes DOWN
         """
         self.set_up()
@@ -2096,71 +2369,86 @@
             one link (the sending link), if the broadcast packet is detected
             on the other links, then the packet was returned to it (via the
             Faucet network)
         """
         self.set_up()
         self.verify_stack_up()
         self.require_linux_bond_up(self.LACP_HOST)
-        lacp_host = self.host_information[self.LACP_HOST]['host']
+        lacp_host = self.host_information[self.LACP_HOST]["host"]
         lacp_switches = {
             self.net.get(self.topo.switches_by_id[i])
-            for i in self.host_port_maps[self.LACP_HOST]}
+            for i in self.host_port_maps[self.LACP_HOST]
+        }
         lacp_intfs = {
-            pair[0].name for switch in lacp_switches for pair in lacp_host.connectionsTo(switch)}
-        dst_host = self.host_information[1]['host']
+            pair[0].name
+            for switch in lacp_switches
+            for pair in lacp_host.connectionsTo(switch)
+        }
+        dst_host = self.host_information[1]["host"]
         # Detect initial broadcast ARP
-        tcpdump_filter = ('arp and ether src 0e:00:00:00:00:99 '
-                          'and ether dst ff:ff:ff:ff:ff:ff')
+        tcpdump_filter = (
+            "arp and ether src 0e:00:00:00:00:99 " "and ether dst ff:ff:ff:ff:ff:ff"
+        )
         # Count the number of links that contained the broadcast ARP packet
         except_count = 0
         for intf in lacp_intfs:
             funcs = []
             # Delete all ARP records of the lacp host
-            for host in [info['host'] for info in self.host_information.values()]:
-                funcs.append(lambda host=host: host.cmd('arp -d %s' % lacp_host.IP()))
-                funcs.append(lambda host=host: host.cmd('arp -d %s' % dst_host.IP()))
-                funcs.append(lambda host=host: lacp_host.cmd('arp -d %s' % host.IP()))
+            for host in [info["host"] for info in self.host_information.values()]:
+                funcs.append(lambda host=host: host.cmd("arp -d %s" % lacp_host.IP()))
+                funcs.append(lambda host=host: host.cmd("arp -d %s" % dst_host.IP()))
+                funcs.append(lambda host=host: lacp_host.cmd("arp -d %s" % host.IP()))
             # Ping to cause broadcast ARP request
-            funcs.append(lambda: lacp_host.cmd('ping -c5 %s' % dst_host.IP()))
+            funcs.append(lambda: lacp_host.cmd("ping -c5 %s" % dst_host.IP()))
             # Start tcpdump looking for broadcast ARP packets
             tcpdump_txt = self.tcpdump_helper(
-                lacp_host, tcpdump_filter, intf_name=intf, funcs=funcs)
+                lacp_host, tcpdump_filter, intf_name=intf, funcs=funcs
+            )
             try:
                 self.verify_no_packets(tcpdump_txt)
             except AssertionError:
-                error('Broadcast detected on %s\n' % intf)
+                error("Broadcast detected on %s\n" % intf)
                 except_count += 1
         # Only the source LACP link should detect the packet
         self.assertEqual(
-            except_count, 1,
-            'Number of links detecting the broadcast ARP %s (!= 1)' % except_count)
+            except_count,
+            1,
+            "Number of links detecting the broadcast ARP %s (!= 1)" % except_count,
+        )
 
 
 class FaucetStackTopoChangeTest(FaucetMultiDPTestBase):
     """Test STACK_TOPO_CHANGE event structure"""
 
     NUM_DPS = 3
 
     def test_graph_object(self):
         """Parse event log and validate graph object in event."""
         self.set_up(
-            stack=True, n_dps=self.NUM_DPS, n_tagged=self.NUM_HOSTS, switch_to_switch_links=2)
+            stack=True,
+            n_dps=self.NUM_DPS,
+            n_tagged=self.NUM_HOSTS,
+            switch_to_switch_links=2,
+        )
         self._enable_event_log()
         self.verify_stack_up()
         stack_event_found = False
-        with open(self.event_log, 'r', encoding='utf-8') as event_log_file:
+        with open(self.event_log, "r", encoding="utf-8") as event_log_file:
             for event_log_line in event_log_file.readlines():
                 event = json.loads(event_log_line.strip())
-                if 'STACK_TOPO_CHANGE' in event:
+                if "STACK_TOPO_CHANGE" in event:
                     stack_event_found = True
-                    graph = event.get('STACK_TOPO_CHANGE').get('graph')
+                    graph = event.get("STACK_TOPO_CHANGE").get("graph")
                     self.assertTrue(graph)
-                    node_count = len(graph.get('nodes'))
-                    self.assertEqual(node_count, 3,
-                                     'Number of nodes in graph object is %s (!=3)' % node_count)
+                    node_count = len(graph.get("nodes"))
+                    self.assertEqual(
+                        node_count,
+                        3,
+                        "Number of nodes in graph object is %s (!=3)" % node_count,
+                    )
         self.assertTrue(stack_event_found)
 
 
 class FaucetStackWarmStartTest(FaucetTopoTestBase):
     """Test various stack warm starting conditions to ensure stack port stays UP"""
 
     NUM_DPS = 3
@@ -2177,21 +2465,28 @@
             host_links (dict): Host index map to list of DPs it is connected to
             host_vlans (dict): Host index map to list of vlans it belongs to
         """
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges()) * switch_to_switch_links
         link_vlans = {edge: None for edge in switch_links}
         if host_links is None:
             host_links = {0: [0], 1: [1], 2: [2]}
         if host_vlans is None:
             host_vlans = {h_i: 0 for h_i in host_links.keys()}
         self.build_net(
@@ -2207,172 +2502,234 @@
     def test_native_vlan(self):
         """Test warm starting changing host native VLAN"""
         host_vlans = {0: 0, 1: 0, 2: 1}
         self.set_up(host_vlans=host_vlans)
         self.verify_stack_up()
         self.verify_intervlan_routing()
         conf = self._get_faucet_conf()
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[2]]['interfaces']
-        interfaces_conf[self.host_port_maps[2][2][0]]['native_vlan'] = self.topo.vlan_name(0)
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[2]]["interfaces"]
+        interfaces_conf[self.host_port_maps[2][2][0]][
+            "native_vlan"
+        ] = self.topo.vlan_name(0)
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=False, change_expected=True, dpid=self.topo.dpids_by_id[2])
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.topo.dpids_by_id[2],
+        )
         self.verify_stack_up(timeout=1)
         self.verify_intervlan_routing()
 
     def test_vlan_change(self):
         """Test warm starting changing a VLAN option"""
         host_vlans = {0: 0, 1: 0, 2: 1}
         self.set_up(host_vlans=host_vlans)
         self.verify_stack_up()
         self.verify_intervlan_routing()
         conf = self._get_faucet_conf()
-        conf['vlans'][self.topo.vlan_name(0)]['edge_learn_stack_root'] = False
+        conf["vlans"][self.topo.vlan_name(0)]["edge_learn_stack_root"] = False
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=False, change_expected=True)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+        )
         self.verify_stack_up(timeout=1)
         self.verify_intervlan_routing()
 
     def test_transit_vlan_change(self):
         """Test warm starting changing host native VLAN with a transit stack switch"""
         host_links = {0: [0], 1: [0], 2: [2], 3: [2]}
         host_vlans = {0: 0, 1: 0, 2: 0, 3: 1}
         self.set_up(host_links=host_links, host_vlans=host_vlans)
         self.verify_stack_up()
         self.verify_intervlan_routing()
         conf = self._get_faucet_conf()
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[0]]['interfaces']
-        interfaces_conf[self.host_port_maps[0][0][0]]['native_vlan'] = self.topo.vlan_name(1)
-        self.host_information[0]['vlan'] = 1
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[0]]["interfaces"]
+        interfaces_conf[self.host_port_maps[0][0][0]][
+            "native_vlan"
+        ] = self.topo.vlan_name(1)
+        self.host_information[0]["vlan"] = 1
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=False, change_expected=True, dpid=self.topo.dpids_by_id[0])
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.topo.dpids_by_id[0],
+        )
         self.verify_stack_up(timeout=1)
         ip_intf = ipaddress.ip_interface(self.host_ip_address(0, 1))
-        self.host_information[0]['ip'] = ip_intf
-        self.set_host_ip(self.host_information[0]['host'], ip_intf)
+        self.host_information[0]["ip"] = ip_intf
+        self.set_host_ip(self.host_information[0]["host"], ip_intf)
         self.verify_intervlan_routing()
 
     def test_del_seconday_stack_port(self):
         """Test deleting stack port"""
         self.set_up(switch_to_switch_links=2)
         self.verify_stack_up()
         self.verify_intervlan_routing()
         conf = self._get_faucet_conf()
-        del conf['dps'][self.topo.switches_by_id[1]]['interfaces'][self.link_port_maps[(1, 2)][0]]
-        del conf['dps'][self.topo.switches_by_id[2]]['interfaces'][self.link_port_maps[(2, 1)][0]]
+        del conf["dps"][self.topo.switches_by_id[1]]["interfaces"][
+            self.link_port_maps[(1, 2)][0]
+        ]
+        del conf["dps"][self.topo.switches_by_id[2]]["interfaces"][
+            self.link_port_maps[(2, 1)][0]
+        ]
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=False, change_expected=True, dpid=self.topo.dpids_by_id[0])
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.topo.dpids_by_id[0],
+        )
         # Due to flood table size changes, some DPs will be cold starting
         self.verify_stack_up(timeout=1, prop=0.5)
         self.verify_intervlan_routing()
 
     def test_del_primary_stack_port(self):
         """Test deleting lowest/primary stack port"""
         self.set_up(switch_to_switch_links=2)
         self.verify_stack_up()
         self.verify_intervlan_routing()
         conf = self._get_faucet_conf()
-        del conf['dps'][self.topo.switches_by_id[1]]['interfaces'][self.link_port_maps[(1, 2)][1]]
-        del conf['dps'][self.topo.switches_by_id[2]]['interfaces'][self.link_port_maps[(2, 1)][1]]
+        del conf["dps"][self.topo.switches_by_id[1]]["interfaces"][
+            self.link_port_maps[(1, 2)][1]
+        ]
+        del conf["dps"][self.topo.switches_by_id[2]]["interfaces"][
+            self.link_port_maps[(2, 1)][1]
+        ]
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=False, change_expected=True, dpid=self.topo.dpids_by_id[0])
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.topo.dpids_by_id[0],
+        )
         # Due to flood table size changes, some DPs will be cold starting
         self.verify_stack_up(timeout=1, prop=0.5)
         self.verify_intervlan_routing()
 
     def test_del_host(self):
         """Test removing a port/host from Faucet"""
         host_links = {0: [0], 1: [0], 2: [1], 3: [2]}
         self.set_up(host_links=host_links)
         self.verify_stack_up()
         self.verify_intervlan_routing()
         conf = self._get_faucet_conf()
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[0]]['interfaces']
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[0]]["interfaces"]
         del interfaces_conf[self.host_port_maps[0][0][0]]
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=None, change_expected=True, dpid=self.topo.dpids_by_id[0])
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=None,
+            change_expected=True,
+            dpid=self.topo.dpids_by_id[0],
+        )
         self.verify_stack_up()
         del self.host_information[0]
         self.verify_intervlan_routing()
 
     def test_root_add_stack_link(self):
         """Add a redundant stack link between two switches (one a root)"""
         host_links = {0: [0], 1: [0], 2: [1], 3: [1], 4: [2], 5: [2]}
         self.set_up(host_links=host_links)
         self.verify_stack_up()
         self.verify_intervlan_routing()
         conf = self._get_faucet_conf()
         # Create an additional link between S1-S2
         port_num = self.topo._create_next_port(self.topo.switches_by_id[0])
         rev_port_num = self.topo._create_next_port(self.topo.switches_by_id[1])
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[0]]['interfaces']
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[0]]["interfaces"]
         interfaces_conf[port_num] = {
-            'name': 'b%u' % port_num,
-            'stack': {'dp': self.topo.switches_by_id[1], 'port': rev_port_num}}
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[1]]['interfaces']
+            "name": "b%u" % port_num,
+            "stack": {"dp": self.topo.switches_by_id[1], "port": rev_port_num},
+        }
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[1]]["interfaces"]
         interfaces_conf[rev_port_num] = {
-            'name': 'b%u' % rev_port_num,
-            'stack': {'dp': self.topo.switches_by_id[0], 'port': port_num}}
+            "name": "b%u" % rev_port_num,
+            "stack": {"dp": self.topo.switches_by_id[0], "port": port_num},
+        }
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=False, change_expected=True, dpid=self.topo.dpids_by_id[2])
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.topo.dpids_by_id[2],
+        )
         self.verify_stack_up()
         self.verify_intervlan_routing()
 
     def test_add_stack_link(self):
         """Add a redundant stack link between two non-root switches"""
         host_links = {0: [0], 1: [0], 2: [1], 3: [1], 4: [2], 5: [2]}
         self.set_up(host_links=host_links)
         self.verify_stack_up()
         self.verify_intervlan_routing()
         conf = self._get_faucet_conf()
         # Create an additional link between S1-S2
         port_num = self.topo._create_next_port(self.topo.switches_by_id[1])
         rev_port_num = self.topo._create_next_port(self.topo.switches_by_id[2])
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[1]]['interfaces']
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[1]]["interfaces"]
         interfaces_conf[port_num] = {
-            'name': 'b%u' % port_num,
-            'stack': {'dp': self.topo.switches_by_id[2], 'port': rev_port_num}}
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[2]]['interfaces']
+            "name": "b%u" % port_num,
+            "stack": {"dp": self.topo.switches_by_id[2], "port": rev_port_num},
+        }
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[2]]["interfaces"]
         interfaces_conf[rev_port_num] = {
-            'name': 'b%u' % rev_port_num,
-            'stack': {'dp': self.topo.switches_by_id[1], 'port': port_num}}
+            "name": "b%u" % rev_port_num,
+            "stack": {"dp": self.topo.switches_by_id[1], "port": port_num},
+        }
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=False, change_expected=True, dpid=self.topo.dpids_by_id[0])
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.topo.dpids_by_id[0],
+        )
         self.verify_stack_up()
         self.verify_intervlan_routing()
 
     def test_add_stack_link_transit(self):
         """Add a redundant stack link between two non-root switches (with a transit switch)"""
         host_links = {0: [0], 1: [0], 4: [2], 5: [2]}
         self.set_up(host_links=host_links)
         self.verify_stack_up()
         self.verify_intervlan_routing()
         conf = self._get_faucet_conf()
         # Create an additional link between S1-S2
         port_num = self.topo._create_next_port(self.topo.switches_by_id[1])
         rev_port_num = self.topo._create_next_port(self.topo.switches_by_id[2])
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[1]]['interfaces']
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[1]]["interfaces"]
         interfaces_conf[port_num] = {
-            'name': 'b%u' % port_num,
-            'stack': {'dp': self.topo.switches_by_id[2], 'port': rev_port_num}}
-        interfaces_conf = conf['dps'][self.topo.switches_by_id[2]]['interfaces']
+            "name": "b%u" % port_num,
+            "stack": {"dp": self.topo.switches_by_id[2], "port": rev_port_num},
+        }
+        interfaces_conf = conf["dps"][self.topo.switches_by_id[2]]["interfaces"]
         interfaces_conf[rev_port_num] = {
-            'name': 'b%u' % rev_port_num,
-            'stack': {'dp': self.topo.switches_by_id[1], 'port': port_num}}
+            "name": "b%u" % rev_port_num,
+            "stack": {"dp": self.topo.switches_by_id[1], "port": port_num},
+        }
         # Expected cold start as topology changed with all ports being stack ports
         self.reload_conf(
-            conf, self.faucet_config_path, restart=True,
-            cold_start=False, change_expected=True, dpid=self.topo.dpids_by_id[0])
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            dpid=self.topo.dpids_by_id[0],
+        )
         self.verify_stack_up()
         self.verify_intervlan_routing()
 
 
 class FaucetDHCPSingleVLANTest(FaucetTopoTestBase):
     """Test Faucet in a single DP network with DHCP allocating IP addresses to
     hosts on a single VLAN"""
@@ -2385,66 +2742,83 @@
     N_UNTAGGED = 3
 
     SOFTWARE_ONLY = True
 
     @staticmethod
     def host_ip_address(_host_index, _vlan_index):
         """Create a string of the host IP address"""
-        return '0.0.0.0'
+        return "0.0.0.0"
 
     def setUp(self):
         """Ignore to allow for setting up network in each test"""
 
     def set_up(self):
         """Set up network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
         switch_links = []
         link_vlans = {}
         host_links = {0: [0], 1: [0], 2: [0]}
         # two host on each native VLAN then DHCP host on tagged VLANs
         host_vlans = {0: 0, 1: 0, 2: [0]}
         # Configure no-IP for non-dhcp hosts as they will obtain IP from DHCP
-        mininet_host_options = {h_i: {'ip': '0.0.0.0'} for h_i in range(self.NUM_HOSTS - 1)}
-        mininet_host_options[2] = {'vlan_intfs': {0: '10.1.0.20/24'}, 'ip': '0.0.0.0'}
+        mininet_host_options = {
+            h_i: {"ip": "0.0.0.0"} for h_i in range(self.NUM_HOSTS - 1)
+        }
+        mininet_host_options[2] = {"vlan_intfs": {0: "10.1.0.20/24"}, "ip": "0.0.0.0"}
         vlan_options = {
-            v_i: {'faucet_vips': [self.faucet_vip(v_i)], 'faucet_mac': self.faucet_mac(v_i)}
-            for v_i in range(self.NUM_VLANS)}
+            v_i: {
+                "faucet_vips": [self.faucet_vip(v_i)],
+                "faucet_mac": self.faucet_mac(v_i),
+            }
+            for v_i in range(self.NUM_VLANS)
+        }
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
             vlan_options=vlan_options,
-            mininet_host_options=mininet_host_options
+            mininet_host_options=mininet_host_options,
         )
         self.start_net()
 
     def test_dhcp_ip_allocation(self):
         """Test that hosts can get allocated addresses from DHCP and can then ping each other"""
         self.set_up()
-        iprange = '10.1.0.10,10.1.0.20'
-        router = '10.1.0.254'
+        iprange = "10.1.0.10,10.1.0.20"
+        router = "10.1.0.254"
         vlan = 100
         host = self.net.get(self.topo.hosts_by_id[2])
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, host.vlan_intfs[0])
         for host_n in range(self.NUM_HOSTS - 1):
             host = self.net.get(self.topo.hosts_by_id[host_n])
             host.run_dhclient(self.tmpdir)
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], '10.1.0.10')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], '10.1.0.11')
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], "10.1.0.10"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], "10.1.0.11"
+        )
         self.check_host_connectivity_by_id(0, 1)
 
 
 class FaucetStackDHCPSingleVLANTest(FaucetTopoTestBase):
     """Test Faucet in a multi DP network with DHCP allocating IP addresses to
     hosts on a single VLAN"""
 
@@ -2456,70 +2830,91 @@
     N_UNTAGGED = 4
 
     SOFTWARE_ONLY = True
 
     @staticmethod
     def host_ip_address(_host_index, _vlan_index):
         """Create a string of the host IP address"""
-        return '0.0.0.0'
+        return "0.0.0.0"
 
     def setUp(self):
         """Ignore to allow for setting up network in each test"""
 
     def set_up(self):
         """Set up network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[dp_i]['stack'] = {'priority': 1}
+                dp_options[dp_i]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [0], 2: [1], 3: [1], 4: [0]}
         # two host on each native VLAN then DHCP host on tagged VLANs
         host_vlans = {0: 0, 1: 0, 2: 0, 3: 0, 4: [0]}
         # Configure no-IP for non-dhcp hosts as they will obtain IP from DHCP
-        mininet_host_options = {h_i: {'ip': '0.0.0.0'} for h_i in range(self.NUM_HOSTS - 1)}
-        mininet_host_options[4] = {'vlan_intfs': {0: '10.1.0.20/24'}, 'ip': '0.0.0.0'}
+        mininet_host_options = {
+            h_i: {"ip": "0.0.0.0"} for h_i in range(self.NUM_HOSTS - 1)
+        }
+        mininet_host_options[4] = {"vlan_intfs": {0: "10.1.0.20/24"}, "ip": "0.0.0.0"}
         vlan_options = {
-            v_i: {'faucet_vips': [self.faucet_vip(v_i)], 'faucet_mac': self.faucet_mac(v_i)}
-            for v_i in range(self.NUM_VLANS)}
+            v_i: {
+                "faucet_vips": [self.faucet_vip(v_i)],
+                "faucet_mac": self.faucet_mac(v_i),
+            }
+            for v_i in range(self.NUM_VLANS)
+        }
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
             vlan_options=vlan_options,
-            mininet_host_options=mininet_host_options
+            mininet_host_options=mininet_host_options,
         )
         self.start_net()
 
     def test_dhcp_ip_allocation(self):
         """Test that hosts can get allocated addresses from DHCP and can then ping each other"""
         self.set_up()
-        iprange = '10.1.0.10,10.1.0.20'
-        router = '10.1.0.254'
+        iprange = "10.1.0.10,10.1.0.20"
+        router = "10.1.0.254"
         vlan = 100
         host = self.net.get(self.topo.hosts_by_id[4])
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, host.vlan_intfs[0])
         for host_n in range(self.NUM_HOSTS - 1):
             host = self.net.get(self.topo.hosts_by_id[host_n])
             host.run_dhclient(self.tmpdir)
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], '10.1.0.10')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], '10.1.0.11')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], '10.1.0.12')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], '10.1.0.13')
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], "10.1.0.10"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], "10.1.0.11"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], "10.1.0.12"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], "10.1.0.13"
+        )
         self.check_host_connectivity_by_id(0, 1)
         self.check_host_connectivity_by_id(1, 2)
         self.check_host_connectivity_by_id(2, 3)
 
 
 class FaucetDHCPSingleTaggedInterfaceTest(FaucetTopoTestBase):
     """
@@ -2535,75 +2930,96 @@
     N_UNTAGGED = 4
 
     SOFTWARE_ONLY = True
 
     @staticmethod
     def host_ip_address(_host_index, _vlan_index):
         """Create a string of the host IP address"""
-        return '0.0.0.0'
+        return "0.0.0.0"
 
     def setUp(self):
         """Ignore to allow for setting up network in each test"""
 
     def set_up(self):
         """Set up network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
         switch_links = []
         link_vlans = {}
         host_links = {0: [0], 1: [0], 2: [0], 3: [0], 4: [0]}
         # two host on each native VLAN then DHCP host on tagged VLANs
         host_vlans = {0: 0, 1: 0, 2: 1, 3: 1, 4: [0, 1]}
         # Configure no-IP for non-dhcp hosts as they will obtain IP from DHCP
-        mininet_host_options = {h_i: {'ip': '0.0.0.0'} for h_i in range(self.NUM_HOSTS - 1)}
+        mininet_host_options = {
+            h_i: {"ip": "0.0.0.0"} for h_i in range(self.NUM_HOSTS - 1)
+        }
         mininet_host_options[4] = {
-            'vlan_intfs': {0: '10.1.0.20/24', 1: '10.2.0.20/24'},
-            'ip': '0.0.0.0'
+            "vlan_intfs": {0: "10.1.0.20/24", 1: "10.2.0.20/24"},
+            "ip": "0.0.0.0",
         }
         vlan_options = {
-            v_i: {'faucet_vips': [self.faucet_vip(v_i)], 'faucet_mac': self.faucet_mac(v_i)}
-            for v_i in range(self.NUM_VLANS)}
+            v_i: {
+                "faucet_vips": [self.faucet_vip(v_i)],
+                "faucet_mac": self.faucet_mac(v_i),
+            }
+            for v_i in range(self.NUM_VLANS)
+        }
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
             vlan_options=vlan_options,
-            mininet_host_options=mininet_host_options
+            mininet_host_options=mininet_host_options,
         )
         self.start_net()
 
     def test_dhcp_ip_allocation(self):
         """Test that hosts can get allocated addresses from DHCP and can then ping each other"""
         self.set_up()
         host = self.net.get(self.topo.hosts_by_id[4])
-        iprange = '10.1.0.10,10.1.0.20'
-        router = '10.1.0.254'
+        iprange = "10.1.0.10,10.1.0.20"
+        router = "10.1.0.254"
         vlan = 100
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, host.vlan_intfs[0])
-        iprange = '10.2.0.10,10.2.0.20'
-        router = '10.2.0.254'
+        iprange = "10.2.0.10,10.2.0.20"
+        router = "10.2.0.254"
         vlan = 200
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, host.vlan_intfs[1])
         for host_n in range(self.NUM_HOSTS - 1):
             host = self.net.get(self.topo.hosts_by_id[host_n])
             host.run_dhclient(self.tmpdir)
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], '10.1.0.10')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], '10.1.0.11')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], '10.2.0.10')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], '10.2.0.11')
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], "10.1.0.10"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], "10.1.0.11"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], "10.2.0.10"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], "10.2.0.11"
+        )
         self.check_host_connectivity_by_id(0, 1)
         self.check_host_connectivity_by_id(2, 3)
 
 
 class FaucetStackDHCPSingleTaggedInterfaceTest(FaucetTopoTestBase):
     """
     Test Faucet in a multi DP network with DHCP allocating IP addresses to
@@ -2618,77 +3034,98 @@
     N_UNTAGGED = 4
 
     SOFTWARE_ONLY = True
 
     @staticmethod
     def host_ip_address(_host_index, _vlan_index):
         """Create a string of the host IP address"""
-        return '0.0.0.0'
+        return "0.0.0.0"
 
     def setUp(self):
         """Ignore to allow for setting up network in each test"""
 
     def set_up(self):
         """Set up network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[dp_i]['stack'] = {'priority': 1}
+                dp_options[dp_i]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [0], 2: [1], 3: [1], 4: [0]}
         # two host on each native VLAN then DHCP host on tagged VLANs
         host_vlans = {0: 0, 1: 1, 2: 0, 3: 1, 4: [0, 1]}
         # Configure no-IP for non-dhcp hosts as they will obtain IP from DHCP
-        mininet_host_options = {h_i: {'ip': '0.0.0.0'} for h_i in range(self.NUM_HOSTS - 1)}
+        mininet_host_options = {
+            h_i: {"ip": "0.0.0.0"} for h_i in range(self.NUM_HOSTS - 1)
+        }
         mininet_host_options[4] = {
-            'vlan_intfs': {0: '10.1.0.20/24', 1: '10.2.0.20/24'},
-            'ip': '0.0.0.0'
+            "vlan_intfs": {0: "10.1.0.20/24", 1: "10.2.0.20/24"},
+            "ip": "0.0.0.0",
         }
         vlan_options = {
-            v_i: {'faucet_vips': [self.faucet_vip(v_i)], 'faucet_mac': self.faucet_mac(v_i)}
-            for v_i in range(self.NUM_VLANS)}
+            v_i: {
+                "faucet_vips": [self.faucet_vip(v_i)],
+                "faucet_mac": self.faucet_mac(v_i),
+            }
+            for v_i in range(self.NUM_VLANS)
+        }
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
             vlan_options=vlan_options,
-            mininet_host_options=mininet_host_options
+            mininet_host_options=mininet_host_options,
         )
         self.start_net()
 
     def test_dhcp_ip_allocation(self):
         """Test that hosts can get allocated addresses from DHCP and can then ping each other"""
         self.set_up()
         host = self.net.get(self.topo.hosts_by_id[4])
-        iprange = '10.1.0.10,10.1.0.20'
-        router = '10.1.0.254'
+        iprange = "10.1.0.10,10.1.0.20"
+        router = "10.1.0.254"
         vlan = 100
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, host.vlan_intfs[0])
-        iprange = '10.2.0.10,10.2.0.20'
-        router = '10.2.0.254'
+        iprange = "10.2.0.10,10.2.0.20"
+        router = "10.2.0.254"
         vlan = 200
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, host.vlan_intfs[1])
         for host_n in range(self.NUM_HOSTS - 1):
             host = self.net.get(self.topo.hosts_by_id[host_n])
             host.run_dhclient(self.tmpdir)
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], '10.1.0.10')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], '10.1.0.11')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], '10.2.0.10')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], '10.2.0.11')
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], "10.1.0.10"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], "10.1.0.11"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], "10.2.0.10"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], "10.2.0.11"
+        )
         self.check_host_connectivity_by_id(0, 2)
         self.check_host_connectivity_by_id(1, 3)
 
 
 class FaucetBipartiteGraphPortDownTest(FaucetTopoTestBase):
     """Test a specific topology correctly floods after 2 ports are taken down"""
 
@@ -2705,62 +3142,97 @@
 
     def setUp(self):
         """Ignore to allow for setting up network in each test"""
 
     def set_up(self):
         """Set up network"""
         super().setUp()
-        network_graph = networkx.algorithms.bipartite.generators.complete_bipartite_graph(2, 3)
+        network_graph = (
+            networkx.algorithms.bipartite.generators.complete_bipartite_graph(2, 3)
+        )
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i in [0, 1]:
-                dp_options[dp_i]['stack'] = {'priority': 1}
-                dp_options[dp_i]['lacp_timeout'] = 5
+                dp_options[dp_i]["stack"] = {"priority": 1}
+                dp_options[dp_i]["lacp_timeout"] = 5
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         # Host 0 is a tap
         # Tier 1 switches contain egress LACP hosts
         # Host devices on tier 2 switches
         host_links = {
-            0: [0], 1: [0],
+            0: [0],
+            1: [0],
             2: [1],
-            3: [2], 4: [2], 5: [2], 6: [2],
-            7: [3], 8: [3], 9: [3], 10: [3],
-            11: [4], 12: [4], 13: [4], 14: [4]}
+            3: [2],
+            4: [2],
+            5: [2],
+            6: [2],
+            7: [3],
+            8: [3],
+            9: [3],
+            10: [3],
+            11: [4],
+            12: [4],
+            13: [4],
+            14: [4],
+        }
         host_vlans = {
-            0: [0, 1], 1: [0],
+            0: [0, 1],
+            1: [0],
             2: [0],
-            3: 0, 4: 0, 5: 0, 6: 0,
-            7: 0, 8: 0, 9: 0, 10: 0,
-            11: 0, 12: 0, 13: 0, 14: 0}
-        host_options = {1: {'lacp': 3}, 2: {'lacp': 3}}
+            3: 0,
+            4: 0,
+            5: 0,
+            6: 0,
+            7: 0,
+            8: 0,
+            9: 0,
+            10: 0,
+            11: 0,
+            12: 0,
+            13: 0,
+            14: 0,
+        }
+        host_options = {1: {"lacp": 3}, 2: {"lacp": 3}}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
-            host_options=host_options
+            host_options=host_options,
         )
         self.start_net()
 
     def test_flooding_on_stack_port_downs(self):
         """Take down 2 stack ports and test flood rules"""
         self.set_up()
         self.verify_stack_up()
         sw1_stack_port = min(self.link_port_maps[(0, 2)])
         sw2_stack_port = min(self.link_port_maps[(1, 3)])
-        self.one_stack_port_down(self.dpids[0], self.topo.switches_by_id[0], sw1_stack_port)
-        self.one_stack_port_down(self.dpids[1], self.topo.switches_by_id[1], sw2_stack_port)
+        self.one_stack_port_down(
+            self.dpids[0], self.topo.switches_by_id[0], sw1_stack_port
+        )
+        self.one_stack_port_down(
+            self.dpids[1], self.topo.switches_by_id[1], sw2_stack_port
+        )
         self.check_host_connectivity_by_id(3, 7)
         self.check_host_connectivity_by_id(3, 11)
 
 
 class FaucetStackDHCPTaggedSingleDHCPInterfaceTest(FaucetTopoTestBase):
     """
     Test Faucet in a multi DP network with DHCP allocating IP addresses to
@@ -2775,77 +3247,98 @@
     N_UNTAGGED = 0
 
     SOFTWARE_ONLY = True
 
     @staticmethod
     def host_ip_address(_host_index, _vlan_index):
         """Create a string of the host IP address"""
-        return '0.0.0.0'
+        return "0.0.0.0"
 
     def setUp(self):
         """Ignore to allow for setting up network in each test"""
 
     def set_up(self):
         """Set up network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[dp_i]['stack'] = {'priority': 1}
+                dp_options[dp_i]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [0], 2: [1], 3: [1], 4: [0]}
         # two host on each native VLAN then DHCP host on tagged VLANs
         host_vlans = {0: [0], 1: [1], 2: [0], 3: [1], 4: [0, 1]}
         # Configure no-IP for non-dhcp hosts as they will obtain IP from DHCP
-        mininet_host_options = {h_i: {'ip': '0.0.0.0'} for h_i in range(self.NUM_HOSTS - 1)}
+        mininet_host_options = {
+            h_i: {"ip": "0.0.0.0"} for h_i in range(self.NUM_HOSTS - 1)
+        }
         mininet_host_options[4] = {
-            'vlan_intfs': {0: '10.1.0.20/24', 1: '10.2.0.20/24'},
-            'ip': '0.0.0.0'
+            "vlan_intfs": {0: "10.1.0.20/24", 1: "10.2.0.20/24"},
+            "ip": "0.0.0.0",
         }
         vlan_options = {
-            v_i: {'faucet_vips': [self.faucet_vip(v_i)], 'faucet_mac': self.faucet_mac(v_i)}
-            for v_i in range(self.NUM_VLANS)}
+            v_i: {
+                "faucet_vips": [self.faucet_vip(v_i)],
+                "faucet_mac": self.faucet_mac(v_i),
+            }
+            for v_i in range(self.NUM_VLANS)
+        }
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
             vlan_options=vlan_options,
-            mininet_host_options=mininet_host_options
+            mininet_host_options=mininet_host_options,
         )
         self.start_net()
 
     def test_dhcp_ip_allocation(self):
         """Test that hosts can get allocated addresses from DHCP and can then ping each other"""
         self.set_up()
         host = self.net.get(self.topo.hosts_by_id[4])
-        iprange = '10.1.0.10,10.1.0.20'
-        router = '10.1.0.254'
+        iprange = "10.1.0.10,10.1.0.20"
+        router = "10.1.0.254"
         vlan = 100
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, host.vlan_intfs[0])
-        iprange = '10.2.0.10,10.2.0.20'
-        router = '10.2.0.254'
+        iprange = "10.2.0.10,10.2.0.20"
+        router = "10.2.0.254"
         vlan = 200
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, host.vlan_intfs[1])
         for host_n in range(self.NUM_HOSTS - 1):
             host = self.net.get(self.topo.hosts_by_id[host_n])
             host.run_dhclient(self.tmpdir)
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], '10.1.0.10')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], '10.1.0.11')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], '10.2.0.10')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], '10.2.0.11')
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], "10.1.0.10"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], "10.1.0.11"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], "10.2.0.10"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], "10.2.0.11"
+        )
         self.check_host_connectivity_by_id(0, 2)
         self.check_host_connectivity_by_id(1, 3)
 
 
 class FaucetTunneltoCoprocessorTest(FaucetTopoTestBase):
     """Test network topology with a tunnel exiting on a corprocessor port"""
 
@@ -2858,54 +3351,69 @@
     SOFTWARE_ONLY = True
 
     def acls(self):
         """Return ACL config"""
         # Tunnel from host 3 (switch 1) to host 0 (switch 0)
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'actions': {
-                        'allow': 0,
-                        'output': {
-                            'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[0],  # Switch 0
-                                'port': self.host_port_maps[0][0][0]}  # Switch 0 host 0
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "actions": {
+                            "allow": 0,
+                            "output": {
+                                "tunnel": {
+                                    "type": "vlan",
+                                    "tunnel_id": 200,
+                                    "dp": self.topo.switches_by_id[0],  # Switch 0
+                                    "port": self.host_port_maps[0][0][0],
+                                }  # Switch 0 host 0
+                            },
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
                         }
                     }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    }
-                }},
+                },
             ]
         }
 
     def setUp(self):
         """Start the network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges())
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [0], 2: [1], 3: [1]}
         host_vlans = {0: None, 1: 0, 2: 0, 3: 0}
-        host_options = {0: {'coprocessor': {'strategy': 'vlan_vid'}}, 3: {'acls_in': [1]}}
+        host_options = {
+            0: {"coprocessor": {"strategy": "vlan_vid"}},
+            3: {"acls_in": [1]},
+        }
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
@@ -2915,15 +3423,17 @@
 
     def test_tunnel_into_coprocessor_port(self):
         """Test tunnel gets encapsulated into a coprocessor port"""
         self.verify_stack_up()
         src_host = self.net.get(self.topo.hosts_by_id[3])
         other_host = self.net.get(self.topo.hosts_by_id[2])
         dst_host = self.net.get(self.topo.hosts_by_id[0])
-        self.verify_tunnel_established(src_host, dst_host, other_host, dpid=self.dpids[1])
+        self.verify_tunnel_established(
+            src_host, dst_host, other_host, dpid=self.dpids[1]
+        )
 
 
 class FaucetDPACLTunnelTest(FaucetTopoTestBase):
     """Test tunnel ACL configured as a DP ACL"""
 
     NUM_DPS = 2
     NUM_HOSTS = 4
@@ -2938,92 +3448,112 @@
         """Return ACL config"""
         # Tunnel from switch 0 to host 2 (switch 1)
         # DP ACLs normally apply to all ports (wildcarded), so there are rules
         #   to match each host port to ensure that packets from the stack ports do not get
         #   sent into the tunnel.
         return {
             1: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'in_port': self.host_port_maps[0][0][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': {
-                            'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[1],  # Switch 1
-                                'port': self.host_port_maps[2][1][0]}  # Switch 1 host 2
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "in_port": self.host_port_maps[0][0][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": {
+                                "tunnel": {
+                                    "type": "vlan",
+                                    "tunnel_id": 200,
+                                    "dp": self.topo.switches_by_id[1],  # Switch 1
+                                    "port": self.host_port_maps[2][1][0],
+                                }  # Switch 1 host 2
+                            },
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "in_port": self.host_port_maps[1][0][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": {
+                                "tunnel": {
+                                    "type": "vlan",
+                                    "tunnel_id": 200,
+                                    "dp": self.topo.switches_by_id[1],
+                                    "port": self.host_port_maps[2][1][0],
+                                }
+                            },
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
                         }
                     }
-                }},
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'in_port': self.host_port_maps[1][0][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': {
-                            'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[1],
-                                'port': self.host_port_maps[2][1][0]}
-                        }
-                    }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    }
-                }},
+                },
             ],
             2: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'actions': {
-                        'allow': 0,
-                        'output': {
-                            'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[1],
-                                'port': self.host_port_maps[2][1][0]}
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "actions": {
+                            "allow": 0,
+                            "output": {
+                                "tunnel": {
+                                    "type": "vlan",
+                                    "tunnel_id": 200,
+                                    "dp": self.topo.switches_by_id[1],
+                                    "port": self.host_port_maps[2][1][0],
+                                }
+                            },
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
                         }
                     }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    }
-                }},
-            ]
+                },
+            ],
         }
 
     def setUp(self):
         """Start the network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
-                dp_options[0]['dp_acls'] = [1]
+                dp_options[0]["stack"] = {"priority": 1}
+                dp_options[0]["dp_acls"] = [1]
         switch_links = list(network_graph.edges()) * self.SWITCH_TO_SWITCH_LINKS
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [0], 2: [1], 3: [1]}
         host_vlans = {0: 0, 1: 0, 2: 0, 3: 0}
-        host_options = {3: {'acls_in': [2]}}
+        host_options = {3: {"acls_in": [2]}}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
@@ -3043,15 +3573,17 @@
         """Test a DP ACL tunnel path is rerouted when a link is down."""
         self.verify_stack_up()
         src_host = self.net.get(self.topo.hosts_by_id[0])
         dst_host = self.net.get(self.topo.hosts_by_id[2])
         other_host = self.net.get(self.topo.hosts_by_id[3])
         self.verify_tunnel_established(src_host, dst_host, other_host, packets=10)
         first_stack_port = min(self.link_port_maps[(0, 1)])
-        self.one_stack_port_down(self.dpids[0], self.topo.switches_by_id[0], first_stack_port)
+        self.one_stack_port_down(
+            self.dpids[0], self.topo.switches_by_id[0], first_stack_port
+        )
         self.verify_tunnel_established(src_host, dst_host, other_host, packets=10)
 
 
 class FaucetACLTunnelDPDestinationTest(FaucetTopoTestBase):
     """Test tunnel ACL configured as a DP ACL"""
 
     NUM_DPS = 2
@@ -3069,55 +3601,66 @@
         """Return ACL config"""
         # Tunnel from switch 0 to host 2 (switch 1)
         # DP ACLs normally apply to all ports (wildcarded), so there are rules
         #   to match each host port to ensure that packets from the stack ports do not get
         #   sent into the tunnel.
         return {
             2: [
-                {'rule': {
-                    'dl_type': IPV4_ETH,
-                    'ip_proto': 1,
-                    'actions': {
-                        'allow': 0,
-                        'output': {
-                            'tunnel': {
-                                'type': 'vlan',
-                                'tunnel_id': 200,
-                                'dp': self.topo.switches_by_id[1],
-                                'exit_instructions': [{'vlan_vid': 100}]
-                            }
+                {
+                    "rule": {
+                        "dl_type": IPV4_ETH,
+                        "ip_proto": 1,
+                        "actions": {
+                            "allow": 0,
+                            "output": {
+                                "tunnel": {
+                                    "type": "vlan",
+                                    "tunnel_id": 200,
+                                    "dp": self.topo.switches_by_id[1],
+                                    "exit_instructions": [{"vlan_vid": 100}],
+                                }
+                            },
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
                         }
                     }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
-                    }
-                }},
+                },
             ]
         }
 
     def setUp(self):
         """Start the network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges()) * self.SWITCH_TO_SWITCH_LINKS
         link_vlans = {edge: None for edge in switch_links}
         host_links = {0: [0], 1: [0], 2: [1], 3: [1]}
         host_vlans = {0: 0, 1: 0, 2: 0, 3: 0}
-        host_options = {0: {'acls_in': [2]}}
+        host_options = {0: {"acls_in": [2]}}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
@@ -3136,15 +3679,17 @@
 
     def test_tunnel_path_rerouted(self):
         """Test a DP ACL tunnel path is rerouted when a link is down."""
         self.verify_stack_up()
         self.check_host_connectivity_by_id(0, 2)
         self.check_host_connectivity_by_id(0, 3)
         first_stack_port = min(self.link_port_maps[(0, 1)])
-        self.one_stack_port_down(self.dpids[0], self.topo.switches_by_id[0], first_stack_port)
+        self.one_stack_port_down(
+            self.dpids[0], self.topo.switches_by_id[0], first_stack_port
+        )
         self.check_host_connectivity_by_id(0, 2)
         self.check_host_connectivity_by_id(0, 3)
 
 
 class FaucetRemoteDHCPCoprocessorTunnelTest(FaucetTopoTestBase):
     """Test tunnel ACL configured with a reverse path"""
 
@@ -3161,138 +3706,171 @@
 
     SOFTWARE_ONLY = True
 
     def acls(self):
         """Return ACL config"""
         return {
             1: [
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'in_port': self.host_port_maps[0][0][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'vlan_vid': 100},
-                            {'tunnel': {
-                                'dp': self.topo.switches_by_id[0],
-                                'port': self.link_port_maps[(0, 2)][0],
-                                'maintain_encapsulation': True,
-                                'bi_directional': True,
-                                'tunnel_id': 200}}
-                        ]
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 67,
-                    'udp_dst': 68,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "in_port": self.host_port_maps[0][0][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {"vlan_vid": 100},
+                                {
+                                    "tunnel": {
+                                        "dp": self.topo.switches_by_id[0],
+                                        "port": self.link_port_maps[(0, 2)][0],
+                                        "maintain_encapsulation": True,
+                                        "bi_directional": True,
+                                        "tunnel_id": 200,
+                                    }
+                                },
+                            ],
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 67,
+                        "udp_dst": 68,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        }
                     }
-                }},
+                },
             ],
             2: [
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'in_port': self.host_port_maps[1][1][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'vlan_vid': 100},
-                            {'tunnel': {
-                                'dp': self.topo.switches_by_id[0],
-                                'port': self.link_port_maps[(0, 2)][0],
-                                'maintain_encapsulation': True,
-                                'bi_directional': True,
-                                'tunnel_id': 300}}
-                        ]
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 67,
-                    'udp_dst': 68,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "in_port": self.host_port_maps[1][1][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {"vlan_vid": 100},
+                                {
+                                    "tunnel": {
+                                        "dp": self.topo.switches_by_id[0],
+                                        "port": self.link_port_maps[(0, 2)][0],
+                                        "maintain_encapsulation": True,
+                                        "bi_directional": True,
+                                        "tunnel_id": 300,
+                                    }
+                                },
+                            ],
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 67,
+                        "udp_dst": 68,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        }
                     }
-                }},
+                },
             ],
         }
 
     @staticmethod
     def host_ip_address(_host_index, _vlan_index):
         """Create a string of the host IP address"""
-        return '0.0.0.0'
+        return "0.0.0.0"
 
     def setUp(self):
         """Start the network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS - 1)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
-            dp_options[dp_i]['dp_acls'] = [dp_i + 1]
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
+            dp_options[dp_i]["dp_acls"] = [dp_i + 1]
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges()) * self.SWITCH_TO_SWITCH_LINKS
         # Add in coprocessor switch link
         switch_links.append((0, 2))
         link_vlans = {edge: None for edge in switch_links}
-        link_options = {(0, 2): {'coprocessor': {'strategy': 'vlan_vid'}}}
+        link_options = {(0, 2): {"coprocessor": {"strategy": "vlan_vid"}}}
         host_links = {0: [0], 1: [1], 2: [2]}
         host_vlans = {0: 0, 1: 0, 2: [0, 1, 2]}
         # Configure no-IP for non-dhcp hosts as they will obtain IP from DHCP
-        mininet_host_options = {h_i: {'ip': '0.0.0.0'} for h_i in range(self.NUM_HOSTS - 1)}
+        mininet_host_options = {
+            h_i: {"ip": "0.0.0.0"} for h_i in range(self.NUM_HOSTS - 1)
+        }
         mininet_host_options[2] = {
-            'vlan_intfs': {(1, 0): '10.1.0.20/24', (2, 0): '10.1.0.30/24'},
-            'ip': '0.0.0.0'
+            "vlan_intfs": {(1, 0): "10.1.0.20/24", (2, 0): "10.1.0.30/24"},
+            "ip": "0.0.0.0",
         }
-        vlan_options = {0: {'faucet_vips': [self.faucet_vip(0)], 'faucet_mac': self.faucet_mac(0)}}
-        vlan_options[1] = {'reserved_internal_vlan': True}
-        vlan_options[2] = {'reserved_internal_vlan': True}
+        vlan_options = {
+            0: {"faucet_vips": [self.faucet_vip(0)], "faucet_mac": self.faucet_mac(0)}
+        }
+        vlan_options[1] = {"reserved_internal_vlan": True}
+        vlan_options[2] = {"reserved_internal_vlan": True}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
@@ -3305,54 +3883,86 @@
     def configure_coprocessor_network(self):
         """Configure the dummy switch and host behind the coprocessor port (DHCP NFV)"""
         switch = self.net.get(self.topo.switches_by_id[2])
         # Change DHCP server reply packet to a `reverse` (indicated by the VLAN_PCP) tunnel packet,
         #   and output to the Faucet network.
         # Packets returned from the server do not have a PCP value set
         switch.cmd(
-            ('ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=67,tp_dst=68,dl_vlan=200,'
-             'actions=set_field:4-\\>vlan_pcp,output:%s') % (
-                 switch.name, self.host_port_maps[2][2][0], self.link_port_maps[(2, 0)][0]))
+            (
+                "ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=67,tp_dst=68,dl_vlan=200,"
+                "actions=set_field:4-\\>vlan_pcp,output:%s"
+            )
+            % (
+                switch.name,
+                self.host_port_maps[2][2][0],
+                self.link_port_maps[(2, 0)][0],
+            )
+        )
         switch.cmd(
-            ('ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=67,tp_dst=68,dl_vlan=300,'
-             'actions=set_field:4-\\>vlan_pcp,output:%s') % (
-                 switch.name, self.host_port_maps[2][2][0], self.link_port_maps[(2, 0)][0]))
+            (
+                "ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=67,tp_dst=68,dl_vlan=300,"
+                "actions=set_field:4-\\>vlan_pcp,output:%s"
+            )
+            % (
+                switch.name,
+                self.host_port_maps[2][2][0],
+                self.link_port_maps[(2, 0)][0],
+            )
+        )
         # Forward tunneled DHCP packets to the DNSMASQ server
         switch.cmd(
-            ('ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=68,tp_dst=67,dl_vlan=200,'
-             'vlan_pcp=3,actions=output:%s') % (
-                 switch.name, self.link_port_maps[(2, 0)][0], self.host_port_maps[2][2][0]))
+            (
+                "ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=68,tp_dst=67,dl_vlan=200,"
+                "vlan_pcp=3,actions=output:%s"
+            )
+            % (
+                switch.name,
+                self.link_port_maps[(2, 0)][0],
+                self.host_port_maps[2][2][0],
+            )
+        )
         switch.cmd(
-            ('ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=68,tp_dst=67,dl_vlan=300,'
-             'vlan_pcp=3,actions=output:%s') % (
-                 switch.name, self.link_port_maps[(2, 0)][0], self.host_port_maps[2][2][0]))
+            (
+                "ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=68,tp_dst=67,dl_vlan=300,"
+                "vlan_pcp=3,actions=output:%s"
+            )
+            % (
+                switch.name,
+                self.link_port_maps[(2, 0)][0],
+                self.host_port_maps[2][2][0],
+            )
+        )
         # Drop all other (non-DHCP tunnelled) traffic
-        switch.cmd('ovs-ofctl add-flow %s priority=0,actions=drop' % (switch.name))
+        switch.cmd("ovs-ofctl add-flow %s priority=0,actions=drop" % (switch.name))
         # Setup DHCP server
         host = self.net.get(self.topo.hosts_by_id[2])
-        iprange = '10.1.0.10,10.1.0.20'
-        router = '10.1.0.254'
+        iprange = "10.1.0.10,10.1.0.20"
+        router = "10.1.0.254"
         vlan = host.vlans[0]
         intf0 = host.vlan_intfs[(1, 0)][-1]
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, intf0)
-        iprange = '10.1.0.21,10.1.0.30'
-        router = '10.1.0.254'
+        iprange = "10.1.0.21,10.1.0.30"
+        router = "10.1.0.254"
         vlan = host.vlans[0]
         intf1 = host.vlan_intfs[(2, 0)][-1]
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, intf1)
 
     def test_dhcp_ip_allocation(self):
         """Test that hosts can get allocated addresses from DHCP and can then ping each other"""
         self.verify_stack_up()
         self.configure_coprocessor_network()
         for host_n in range(self.NUM_HOSTS - 1):
             host = self.net.get(self.topo.hosts_by_id[host_n])
             host.run_dhclient(self.tmpdir)
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], '10.1.0.10')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], '10.1.0.21')
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], "10.1.0.10"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], "10.1.0.21"
+        )
         self.check_host_connectivity_by_id(0, 1)
 
 
 class FaucetRemoteDHCPCoprocessor2VLANTunnelTest(FaucetTopoTestBase):
     """Test tunnel ACL configured with a reverse path"""
 
     # 3 Faucet DPs, 1 NFV coprocessor DP
@@ -3369,247 +3979,307 @@
 
     SOFTWARE_ONLY = True
 
     def acls(self):
         """Return ACL config"""
         return {
             1: [
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'in_port': self.host_port_maps[0][0][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'vlan_vid': 100},
-                            {'tunnel': {
-                                'dp': self.topo.switches_by_id[0],
-                                'port': self.link_port_maps[(0, 3)][0],
-                                'maintain_encapsulation': True,
-                                'bi_directional': True,
-                                'tunnel_id': 300}}
-                        ]
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'in_port': self.host_port_maps[1][0][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'vlan_vid': 200},
-                            {'tunnel': {
-                                'dp': self.topo.switches_by_id[0],
-                                'port': self.link_port_maps[(0, 3)][0],
-                                'maintain_encapsulation': True,
-                                'bi_directional': True,
-                                'tunnel_id': 300}}
-                        ]
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 67,
-                    'udp_dst': 68,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "in_port": self.host_port_maps[0][0][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {"vlan_vid": 100},
+                                {
+                                    "tunnel": {
+                                        "dp": self.topo.switches_by_id[0],
+                                        "port": self.link_port_maps[(0, 3)][0],
+                                        "maintain_encapsulation": True,
+                                        "bi_directional": True,
+                                        "tunnel_id": 300,
+                                    }
+                                },
+                            ],
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "in_port": self.host_port_maps[1][0][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {"vlan_vid": 200},
+                                {
+                                    "tunnel": {
+                                        "dp": self.topo.switches_by_id[0],
+                                        "port": self.link_port_maps[(0, 3)][0],
+                                        "maintain_encapsulation": True,
+                                        "bi_directional": True,
+                                        "tunnel_id": 300,
+                                    }
+                                },
+                            ],
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 67,
+                        "udp_dst": 68,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        }
                     }
-                }},
+                },
             ],
             2: [
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'in_port': self.host_port_maps[2][1][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'vlan_vid': 100},
-                            {'tunnel': {
-                                'dp': self.topo.switches_by_id[0],
-                                'port': self.link_port_maps[(0, 3)][0],
-                                'maintain_encapsulation': True,
-                                'bi_directional': True,
-                                'tunnel_id': 400}}
-                        ]
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'in_port': self.host_port_maps[3][1][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'vlan_vid': 200},
-                            {'tunnel': {
-                                'dp': self.topo.switches_by_id[0],
-                                'port': self.link_port_maps[(0, 3)][0],
-                                'maintain_encapsulation': True,
-                                'bi_directional': True,
-                                'tunnel_id': 400}}
-                        ]
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 67,
-                    'udp_dst': 68,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "in_port": self.host_port_maps[2][1][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {"vlan_vid": 100},
+                                {
+                                    "tunnel": {
+                                        "dp": self.topo.switches_by_id[0],
+                                        "port": self.link_port_maps[(0, 3)][0],
+                                        "maintain_encapsulation": True,
+                                        "bi_directional": True,
+                                        "tunnel_id": 400,
+                                    }
+                                },
+                            ],
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "in_port": self.host_port_maps[3][1][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {"vlan_vid": 200},
+                                {
+                                    "tunnel": {
+                                        "dp": self.topo.switches_by_id[0],
+                                        "port": self.link_port_maps[(0, 3)][0],
+                                        "maintain_encapsulation": True,
+                                        "bi_directional": True,
+                                        "tunnel_id": 400,
+                                    }
+                                },
+                            ],
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 67,
+                        "udp_dst": 68,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        }
                     }
-                }},
+                },
             ],
             3: [
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'in_port': self.host_port_maps[4][2][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'vlan_vid': 100},
-                            {'tunnel': {
-                                'dp': self.topo.switches_by_id[0],
-                                'port': self.link_port_maps[(0, 3)][0],
-                                'maintain_encapsulation': True,
-                                'bi_directional': True,
-                                'tunnel_id': 500}}
-                        ]
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'in_port': self.host_port_maps[5][2][0],
-                    'actions': {
-                        'allow': 0,
-                        'output': [
-                            {'vlan_vid': 200},
-                            {'tunnel': {
-                                'dp': self.topo.switches_by_id[0],
-                                'port': self.link_port_maps[(0, 3)][0],
-                                'maintain_encapsulation': True,
-                                'bi_directional': True,
-                                'tunnel_id': 500}}
-                        ]
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 68,
-                    'udp_dst': 67,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'nw_proto': 17,
-                    'udp_src': 67,
-                    'udp_dst': 68,
-                    'dl_type': 0x0800,
-                    'actions': {
-                        'allow': 0,
-                    }
-                }},
-                {'rule': {
-                    'actions': {
-                        'allow': 1,
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "in_port": self.host_port_maps[4][2][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {"vlan_vid": 100},
+                                {
+                                    "tunnel": {
+                                        "dp": self.topo.switches_by_id[0],
+                                        "port": self.link_port_maps[(0, 3)][0],
+                                        "maintain_encapsulation": True,
+                                        "bi_directional": True,
+                                        "tunnel_id": 500,
+                                    }
+                                },
+                            ],
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "in_port": self.host_port_maps[5][2][0],
+                        "actions": {
+                            "allow": 0,
+                            "output": [
+                                {"vlan_vid": 200},
+                                {
+                                    "tunnel": {
+                                        "dp": self.topo.switches_by_id[0],
+                                        "port": self.link_port_maps[(0, 3)][0],
+                                        "maintain_encapsulation": True,
+                                        "bi_directional": True,
+                                        "tunnel_id": 500,
+                                    }
+                                },
+                            ],
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 68,
+                        "udp_dst": 67,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "nw_proto": 17,
+                        "udp_src": 67,
+                        "udp_dst": 68,
+                        "dl_type": 0x0800,
+                        "actions": {
+                            "allow": 0,
+                        },
+                    }
+                },
+                {
+                    "rule": {
+                        "actions": {
+                            "allow": 1,
+                        }
                     }
-                }},
+                },
             ],
         }
 
     @staticmethod
     def host_ip_address(_host_index, _vlan_index):
         """Create a string of the host IP address"""
-        return '0.0.0.0'
+        return "0.0.0.0"
 
     def setUp(self):
         """Start the network"""
         super().setUp()
         network_graph = networkx.path_graph(self.NUM_DPS - 1)
         dp_options = {}
         for dp_i in network_graph.nodes():
-            dp_options.setdefault(dp_i, {
-                'group_table': self.GROUP_TABLE,
-                'ofchannel_log': self.debug_log_path + str(dp_i) if self.debug_log_path else None,
-                'hardware': self.hardware if dp_i == 0 and self.hw_dpid else 'Open vSwitch'
-            })
-            dp_options[dp_i]['dp_acls'] = [dp_i + 1]
+            dp_options.setdefault(
+                dp_i,
+                {
+                    "group_table": self.GROUP_TABLE,
+                    "ofchannel_log": self.debug_log_path + str(dp_i)
+                    if self.debug_log_path
+                    else None,
+                    "hardware": self.hardware
+                    if dp_i == 0 and self.hw_dpid
+                    else "Open vSwitch",
+                },
+            )
+            dp_options[dp_i]["dp_acls"] = [dp_i + 1]
             if dp_i == 0:
-                dp_options[0]['stack'] = {'priority': 1}
+                dp_options[0]["stack"] = {"priority": 1}
         switch_links = list(network_graph.edges()) * self.SWITCH_TO_SWITCH_LINKS
         # Add in coprocessor switch link
         switch_links.append((0, 3))
         link_vlans = {edge: None for edge in switch_links}
-        link_options = {(0, 3): {'coprocessor': {'strategy': 'vlan_vid'}}}
+        link_options = {(0, 3): {"coprocessor": {"strategy": "vlan_vid"}}}
         host_links = {0: [0], 1: [0], 2: [1], 3: [1], 4: [2], 5: [2], 6: [3]}
         host_vlans = {0: 0, 1: 1, 2: 0, 3: 1, 4: 0, 5: 1, 6: [0, 1, 2, 3, 4]}
         # Configure no-IP for non-dhcp hosts as they will obtain IP from DHCP
-        mininet_host_options = {h_i: {'ip': '0.0.0.0'} for h_i in range(self.NUM_HOSTS - 1)}
+        mininet_host_options = {
+            h_i: {"ip": "0.0.0.0"} for h_i in range(self.NUM_HOSTS - 1)
+        }
         mininet_host_options[6] = {
-            'vlan_intfs': {
-                (2, 0): '10.1.0.20/24',
-                (3, 0): '10.1.0.30/24',
-                (4, 0): '10.1.0.40/24',
-                (2, 1): '10.2.0.20/24',
-                (3, 1): '10.2.0.30/24',
-                (4, 1): '10.2.0.40/24'},
-            'ip': '0.0.0.0'}
+            "vlan_intfs": {
+                (2, 0): "10.1.0.20/24",
+                (3, 0): "10.1.0.30/24",
+                (4, 0): "10.1.0.40/24",
+                (2, 1): "10.2.0.20/24",
+                (3, 1): "10.2.0.30/24",
+                (4, 1): "10.2.0.40/24",
+            },
+            "ip": "0.0.0.0",
+        }
         vlan_options = {
-            0: {'faucet_vips': [self.faucet_vip(0)], 'faucet_mac': self.faucet_mac(0)},
-            1: {'faucet_vips': [self.faucet_vip(1)], 'faucet_mac': self.faucet_mac(1)}}
-        vlan_options[2] = {'reserved_internal_vlan': True}
-        vlan_options[3] = {'reserved_internal_vlan': True}
-        vlan_options[4] = {'reserved_internal_vlan': True}
+            0: {"faucet_vips": [self.faucet_vip(0)], "faucet_mac": self.faucet_mac(0)},
+            1: {"faucet_vips": [self.faucet_vip(1)], "faucet_mac": self.faucet_mac(1)},
+        }
+        vlan_options[2] = {"reserved_internal_vlan": True}
+        vlan_options[3] = {"reserved_internal_vlan": True}
+        vlan_options[4] = {"reserved_internal_vlan": True}
         self.build_net(
             host_links=host_links,
             host_vlans=host_vlans,
             switch_links=switch_links,
             link_vlans=link_vlans,
             n_vlans=self.NUM_VLANS,
             dp_options=dp_options,
@@ -3619,59 +4289,90 @@
         )
         self.start_net()
 
     def create_dnsmasq_link(self, tunnel_id, host_id):
         """Create the DNSMASQ interface link"""
         host = self.net.get(self.topo.hosts_by_id[6])
         tunnel_ip = tunnel_id - 1
-        iprange = '10.%u.0.%u,10.%u.0.%u' % (
-            host_id + 1, (tunnel_ip * 10) + 1, host_id + 1, (tunnel_ip + 1) * 10
+        iprange = "10.%u.0.%u,10.%u.0.%u" % (
+            host_id + 1,
+            (tunnel_ip * 10) + 1,
+            host_id + 1,
+            (tunnel_ip + 1) * 10,
         )
-        router = '10.%u.0.254' % (host_id + 1)
+        router = "10.%u.0.254" % (host_id + 1)
         vlan = host.vlans[host_id]
         intf = host.vlan_intfs[(tunnel_id, host_id)][-1]
         host.create_dnsmasq(self.tmpdir, iprange, router, vlan, intf)
 
     def configure_coprocessor_network(self):
         """Configure the dummy switch and host behind the coprocessor port (DHCP NFV)"""
         switch = self.net.get(self.topo.switches_by_id[3])
         # Change DHCP server reply packet to a `reverse` (indicated by the VLAN_PCP) tunnel packet,
         #   and output to the Faucet network.
         # Packets returned from the server do not have a PCP value set
         for i in [300, 400, 500]:
             switch.cmd(
-                ('ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=67,tp_dst=68,dl_vlan=%s,'
-                 'actions=set_field:4-\\>vlan_pcp,output:%s') % (
-                     switch.name, self.host_port_maps[6][3][0], i, self.link_port_maps[(3, 0)][0]))
+                (
+                    "ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=67,tp_dst=68,dl_vlan=%s,"
+                    "actions=set_field:4-\\>vlan_pcp,output:%s"
+                )
+                % (
+                    switch.name,
+                    self.host_port_maps[6][3][0],
+                    i,
+                    self.link_port_maps[(3, 0)][0],
+                )
+            )
         # Forward tunneled DHCP packets to the DNSMASQ server
         for i in [300, 400, 500]:
             switch.cmd(
-                ('ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=68,tp_dst=67,dl_vlan=%s,'
-                 'vlan_pcp=3,actions=output:%s') % (
-                     switch.name, self.link_port_maps[(3, 0)][0], i, self.host_port_maps[6][3][0]))
+                (
+                    "ovs-ofctl add-flow %s priority=1,in_port=%s,udp,tp_src=68,tp_dst=67,dl_vlan=%s,"
+                    "vlan_pcp=3,actions=output:%s"
+                )
+                % (
+                    switch.name,
+                    self.link_port_maps[(3, 0)][0],
+                    i,
+                    self.host_port_maps[6][3][0],
+                )
+            )
         # Drop all other (non-DHCP tunnelled) traffic
-        switch.cmd('ovs-ofctl add-flow %s priority=0,actions=drop' % (switch.name))
+        switch.cmd("ovs-ofctl add-flow %s priority=0,actions=drop" % (switch.name))
         # Setup DHCP server
         for i in [0, 1]:
             # i: host VLANS
             for j in [2, 3, 4]:
                 # j: DP Tunnel VLANS
                 self.create_dnsmasq_link(j, i)
 
     def test_dhcp_ip_allocation(self):
         """Test that hosts can get allocated addresses from DHCP and can then ping each other"""
         self.configure_coprocessor_network()
         self.verify_stack_up()
         for host_n in range(self.NUM_HOSTS - 1):
             host = self.net.get(self.topo.hosts_by_id[host_n])
             host.run_dhclient(self.tmpdir)
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], '10.1.0.11')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], '10.2.0.11')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], '10.1.0.21')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], '10.2.0.21')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[4]).return_ip()[:-3], '10.1.0.31')
-        self.assertEqual(self.net.get(self.topo.hosts_by_id[5]).return_ip()[:-3], '10.2.0.31')
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[0]).return_ip()[:-3], "10.1.0.11"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[1]).return_ip()[:-3], "10.2.0.11"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[2]).return_ip()[:-3], "10.1.0.21"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[3]).return_ip()[:-3], "10.2.0.21"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[4]).return_ip()[:-3], "10.1.0.31"
+        )
+        self.assertEqual(
+            self.net.get(self.topo.hosts_by_id[5]).return_ip()[:-3], "10.2.0.31"
+        )
         self.check_host_connectivity_by_id(0, 1)
         self.check_host_connectivity_by_id(0, 2)
         self.check_host_connectivity_by_id(0, 3)
         self.check_host_connectivity_by_id(0, 4)
         self.check_host_connectivity_by_id(0, 5)
```

### Comparing `c65faucet-1.0.49/tests/integration/mininet_tests.py` & `c65faucet-1.0.50/tests/integration/mininet_tests.py`

 * *Files 16% similar despite different names*

```diff
@@ -65,91 +65,98 @@
             %(port_4)d:
                 tagged_vlans: [100]
                 count_untag_vlan_miss: true
 """
 
 
 class QuietHTTPServer(HTTPServer):
-
     allow_reuse_address = True
     timeout = None
 
     @staticmethod
     def handle_error(_request, _client_address):
         return
 
 
 class PostHandler(SimpleHTTPRequestHandler):
-
     def log_message(self, format, *args):  # pylint: disable=redefined-builtin
         return
 
     def _log_post(self):
-        content_len = int(self.headers.get('content-length', 0))
+        content_len = int(self.headers.get("content-length", 0))
         content = self.rfile.read(content_len).decode().strip()
-        if content and hasattr(self.server, 'influx_log'):
-            with open(self.server.influx_log, 'a', encoding='utf-8') as influx_log:
-                influx_log.write(content + '\n')
+        if content and hasattr(self.server, "influx_log"):
+            with open(self.server.influx_log, "a", encoding="utf-8") as influx_log:
+                influx_log.write(content + "\n")
 
 
 class InfluxPostHandler(PostHandler):
-
     def do_POST(self):  # pylint: disable=invalid-name
         self._log_post()
         return self.send_response(204)
 
 
 class SlowInfluxPostHandler(PostHandler):
-
     def do_POST(self):  # pylint: disable=invalid-name
         self._log_post()
         time.sleep(self.server.timeout * 3)
         return self.send_response(500)
 
 
 class FaucetTest(mininet_test_base.FaucetTestBase):
-
     pass
 
 
 class FaucetUntaggedTest(FaucetTest):
     """Basic untagged VLAN test."""
 
     HOST_NAMESPACE = {}
     N_UNTAGGED = 4
     N_TAGGED = 0
     LINKS_PER_HOST = 1
-    EVENT_SOCK_HEARTBEAT = '5'
+    EVENT_SOCK_HEARTBEAT = "5"
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 """
 
     # pylint: disable=invalid-name
     CONFIG = CONFIG_BOILER_UNTAGGED
 
     def setUp(self):
         super().setUp()
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
-            links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid,
-            host_namespace=self.HOST_NAMESPACE)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=self.N_TAGGED,
+            n_untagged=self.N_UNTAGGED,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+            host_namespace=self.HOST_NAMESPACE,
+        )
         self.start_net()
 
     def verify_events_log(self, event_log, timeout=10):
-        required_events = {'CONFIG_CHANGE', 'PORT_CHANGE', 'L2_LEARN', 'PORTS_STATUS', 'EVENT_SOCK_HEARTBEAT'}
+        required_events = {
+            "CONFIG_CHANGE",
+            "PORT_CHANGE",
+            "L2_LEARN",
+            "PORTS_STATUS",
+            "EVENT_SOCK_HEARTBEAT",
+        }
         for _ in range(timeout):
-            prom_event_id = self.scrape_prometheus_var('faucet_event_id', dpid=False)
+            prom_event_id = self.scrape_prometheus_var("faucet_event_id", dpid=False)
             event_id = None
-            with open(event_log, 'r', encoding='utf-8') as event_log_file:
+            with open(event_log, "r", encoding="utf-8") as event_log_file:
                 for event_log_line in event_log_file.readlines():
                     event = json.loads(event_log_line.strip())
-                    event_id = event['event_id']
+                    event_id = event["event_id"]
                     required_events -= set(event.keys())
             if prom_event_id == event_id:
                 return
             time.sleep(1)
         self.assertEqual(prom_event_id, event_id)
         self.assertFalse(required_events)
 
@@ -162,28 +169,27 @@
         self.gauge_smoke_test()
         self.prometheus_smoke_test()
         self.assertGreater(os.path.getsize(self.event_log), 0)
         self.verify_events_log(self.event_log)
 
 
 class Faucet8021XBase(FaucetTest):
-
     NUM_FAUCET_CONTROLLERS = 1
     NUM_GAUGE_CONTROLLERS = 1
 
     HOST_NAMESPACE = {3: False}
     N_UNTAGGED = 4
     N_TAGGED = 0
     LINKS_PER_HOST = 1
 
     RADIUS_PORT = None
 
     DOT1X_EXPECTED_EVENTS = []
     SESSION_TIMEOUT = 3600
-    LOG_LEVEL = 'DEBUG'
+    LOG_LEVEL = "DEBUG"
 
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 """
 
@@ -254,387 +260,535 @@
     ping_host = None
     nfv_host = None
     nfv_intf = None
     nfv_portno = None
 
     @staticmethod
     def _priv_mac(host_id):
-        two_byte_port_num = '%04x' % host_id
-        two_byte_port_num_formatted = ':'.join((two_byte_port_num[:2], two_byte_port_num[2:]))
-        return '00:00:00:00:%s' % two_byte_port_num_formatted
+        two_byte_port_num = "%04x" % host_id
+        two_byte_port_num_formatted = ":".join(
+            (two_byte_port_num[:2], two_byte_port_num[2:])
+        )
+        return "00:00:00:00:%s" % two_byte_port_num_formatted
 
     def pre_start_net(self):
-        self.eapol1_host, self.eapol2_host, self.ping_host, self.nfv_host = self.hosts_name_ordered()
+        (
+            self.eapol1_host,
+            self.eapol2_host,
+            self.ping_host,
+            self.nfv_host,
+        ) = self.hosts_name_ordered()
         switch = self.first_switch()
         last_host_switch_link = switch.connectionsTo(self.nfv_host)[0]
         nfv_intf = [
-            intf for intf in last_host_switch_link if intf in switch.intfList()][0]
+            intf for intf in last_host_switch_link if intf in switch.intfList()
+        ][0]
         self.nfv_intf = str(nfv_intf)
         nfv_intf = self.nfv_host.intf()
 
-        self.RADIUS_PORT = mininet_test_util.find_free_udp_port(self.ports_sock, self._test_name())
+        self.RADIUS_PORT = mininet_test_util.find_free_udp_port(
+            self.ports_sock, self._test_name()
+        )
 
-        self.CONFIG = self.CONFIG.replace('NFV_INTF', str(nfv_intf))
-        self.CONFIG = self.CONFIG.replace('RADIUS_PORT', str(self.RADIUS_PORT))
+        self.CONFIG = self.CONFIG.replace("NFV_INTF", str(nfv_intf))
+        self.CONFIG = self.CONFIG.replace("RADIUS_PORT", str(self.RADIUS_PORT))
         super()._init_faucet_config()
 
     def setUp(self):
         super().setUp()
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
-            links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid,
-            host_namespace=self.HOST_NAMESPACE)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=self.N_TAGGED,
+            n_untagged=self.N_UNTAGGED,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+            host_namespace=self.HOST_NAMESPACE,
+        )
         self.start_net()
 
-        self.nfv_portno = self.port_map['port_4']
+        self.nfv_portno = self.port_map["port_4"]
 
         self.host_drop_all_ips(self.nfv_host)
         self.nfv_pids = []
 
-        tcpdump_args = '-e -n -U'
+        tcpdump_args = "-e -n -U"
         self.eapol1_host.cmd(
             mininet_test_util.timeout_cmd(
-                'tcpdump -w %s/%s-start.pcap %s ether proto 0x888e &' % (
-                    self.tmpdir, self.eapol1_host.name, tcpdump_args), 300))
+                "tcpdump -w %s/%s-start.pcap %s ether proto 0x888e &"
+                % (self.tmpdir, self.eapol1_host.name, tcpdump_args),
+                300,
+            )
+        )
 
         self.nfv_host.cmd(
             mininet_test_util.timeout_cmd(
-                'tcpdump -i %s-eth0 -w %s/eap-lo.pcap %s ether proto 0x888e &' % (
-                    self.nfv_host.name, self.tmpdir, tcpdump_args), 300))
+                "tcpdump -i %s-eth0 -w %s/eap-lo.pcap %s ether proto 0x888e &"
+                % (self.nfv_host.name, self.tmpdir, tcpdump_args),
+                300,
+            )
+        )
         self.nfv_pids.append(int(self.nfv_host.lastPid))
         self.nfv_host.cmd(
             mininet_test_util.timeout_cmd(
-                'tcpdump -i lo -w %s/radius.pcap %s udp port %d &' % (
-                    self.tmpdir, tcpdump_args, self.RADIUS_PORT), 300))
+                "tcpdump -i lo -w %s/radius.pcap %s udp port %d &"
+                % (self.tmpdir, tcpdump_args, self.RADIUS_PORT),
+                300,
+            )
+        )
         self.nfv_pids.append(int(self.nfv_host.lastPid))
         self.radius_log_path = self.start_freeradius()
         self.nfv_pids.append(int(self.nfv_host.lastPid))
         self._enable_event_log(300)
 
     def tearDown(self, ignore_oferrors=False):
         for pid in self.nfv_pids:
-            self.nfv_host.cmd('kill %u' % pid)
+            self.nfv_host.cmd("kill %u" % pid)
         super().tearDown(ignore_oferrors=ignore_oferrors)
 
     def post_test_checks(self):
         self.assertGreater(os.path.getsize(self.event_log), 0)
         self.verify_dot1x_events_log()
 
     def verify_dot1x_events_log(self):
-
         def replace_mac(host_no):
             replacement_macs = {
-                'HOST1_MAC': self.eapol1_host.MAC(),
-                'HOST2_MAC': self.eapol2_host.MAC(),
-                'HOST3_MAC': self.ping_host.MAC(),
-                'HOST4_MAC': self.nfv_host.MAC(),
+                "HOST1_MAC": self.eapol1_host.MAC(),
+                "HOST2_MAC": self.eapol2_host.MAC(),
+                "HOST3_MAC": self.ping_host.MAC(),
+                "HOST4_MAC": self.nfv_host.MAC(),
             }
             return replacement_macs.get(host_no, None)
 
         def insert_dynamic_values(dot1x_expected_events):
             for dot1x_event in dot1x_expected_events:
                 top_level_key = list(dot1x_event.keys())[0]
-                dot1x_params = {'dp_id': int(self.dpid)}
+                dot1x_params = {"dp_id": int(self.dpid)}
                 for key, val in dot1x_event[top_level_key].items():
-                    if key == 'port':
+                    if key == "port":
                         dot1x_params[key] = self.port_map[val]
-                    elif key == 'eth_src':
+                    elif key == "eth_src":
                         dot1x_params[key] = replace_mac(val)
                 dot1x_event[top_level_key].update(dot1x_params)
 
         if not self.DOT1X_EXPECTED_EVENTS:
             return
 
         dot1x_expected_events = copy.deepcopy(self.DOT1X_EXPECTED_EVENTS)
         insert_dynamic_values(dot1x_expected_events)
 
-        with open(self.event_log, 'r', encoding='utf-8') as event_file:
+        with open(self.event_log, "r", encoding="utf-8") as event_file:
             events_that_happened = []
             for event_log_line in event_file.readlines():
-                if 'DOT1X' not in event_log_line:
+                if "DOT1X" not in event_log_line:
                     continue
                 event = json.loads(event_log_line.strip())
-                events_that_happened.append(event['DOT1X'])
+                events_that_happened.append(event["DOT1X"])
 
             for expected_event in dot1x_expected_events:
-                self.assertTrue(expected_event in events_that_happened,
-                                msg='expected event: {} not in events_that_happened {}'.format(
-                                    expected_event, events_that_happened))
+                self.assertTrue(
+                    expected_event in events_that_happened,
+                    msg="expected event: {} not in events_that_happened {}".format(
+                        expected_event, events_that_happened
+                    ),
+                )
 
     @staticmethod
     def _eapol_filter(fields):
-        return '(' + ' and '.join(('ether proto 0x888e',) + fields) + ')'
+        return "(" + " and ".join(("ether proto 0x888e",) + fields) + ")"
 
     def _success_eapol_filter(self, expect_success):
-        eap_code = '0x04'
+        eap_code = "0x04"
         if expect_success:
-            eap_code = '0x03'
-        return self._eapol_filter(('ether[14:4] == 0x01000004', 'ether[18] == %s' % eap_code))
+            eap_code = "0x03"
+        return self._eapol_filter(
+            ("ether[14:4] == 0x01000004", "ether[18] == %s" % eap_code)
+        )
 
     def _logoff_eapol_filter(self):
-        return self._eapol_filter(('ether[14:4] == 0x01020000',))
+        return self._eapol_filter(("ether[14:4] == 0x01020000",))
 
-    def try_8021x(self, host, port_num, conf, and_logoff=False, terminate_wpasupplicant=False,
-                  wpasup_timeout=180, tcpdump_timeout=30, expect_success=True):
+    def try_8021x(
+        self,
+        host,
+        port_num,
+        conf,
+        and_logoff=False,
+        terminate_wpasupplicant=False,
+        wpasup_timeout=180,
+        tcpdump_timeout=30,
+        expect_success=True,
+    ):
         if expect_success:
             self.wait_8021x_flows(port_num)
         port_labels = self.port_labels(port_num)
         success_total = self.scrape_prometheus_var(
-            'port_dot1x_success_total', labels=port_labels, default=0)
+            "port_dot1x_success_total", labels=port_labels, default=0
+        )
         failure_total = self.scrape_prometheus_var(
-            'port_dot1x_failure_total', labels=port_labels, default=0)
+            "port_dot1x_failure_total", labels=port_labels, default=0
+        )
         logoff_total = self.scrape_prometheus_var(
-            'port_dot1x_logoff_total', labels=port_labels, default=0)
+            "port_dot1x_logoff_total", labels=port_labels, default=0
+        )
         dp_success_total = self.scrape_prometheus_var(
-            'dp_dot1x_success_total', default=0)
+            "dp_dot1x_success_total", default=0
+        )
         dp_failure_total = self.scrape_prometheus_var(
-            'dp_dot1x_failure_total', default=0)
-        dp_logoff_total = self.scrape_prometheus_var(
-            'dp_dot1x_logoff_total', default=0)
+            "dp_dot1x_failure_total", default=0
+        )
+        dp_logoff_total = self.scrape_prometheus_var("dp_dot1x_logoff_total", default=0)
         tcpdump_filters = [self._success_eapol_filter(expect_success)]
         if and_logoff:
             tcpdump_filters.append(self._logoff_eapol_filter())
         tcpdump_packets = len(tcpdump_filters)
-        tcpdump_filter = ' or '.join(tcpdump_filters)
+        tcpdump_filter = " or ".join(tcpdump_filters)
         tcpdump_txt = self.tcpdump_helper(
-            host, tcpdump_filter, [
+            host,
+            tcpdump_filter,
+            [
                 lambda: self.wpa_supplicant_callback(
-                    host, port_num, conf, and_logoff,
+                    host,
+                    port_num,
+                    conf,
+                    and_logoff,
                     timeout=wpasup_timeout,
-                    terminate_wpasupplicant=terminate_wpasupplicant)],
-            timeout=tcpdump_timeout, vflags='-vvv', packets=tcpdump_packets)
+                    terminate_wpasupplicant=terminate_wpasupplicant,
+                )
+            ],
+            timeout=tcpdump_timeout,
+            vflags="-vvv",
+            packets=tcpdump_packets,
+        )
         if expect_success:
             self.wait_for_eap_success(host, self.get_wpa_ctrl_path(host))
             if not and_logoff:
                 self.wait_8021x_success_flows(host, port_num)
-        success = 'Success' in tcpdump_txt
+        success = "Success" in tcpdump_txt
         if expect_success != success:
             return False
         new_success_total = self.scrape_prometheus_var(
-            'port_dot1x_success_total', labels=port_labels, default=0)
+            "port_dot1x_success_total", labels=port_labels, default=0
+        )
         new_failure_total = self.scrape_prometheus_var(
-            'port_dot1x_failure_total', labels=port_labels, default=0)
+            "port_dot1x_failure_total", labels=port_labels, default=0
+        )
         new_logoff_total = self.scrape_prometheus_var(
-            'port_dot1x_logoff_total', labels=port_labels, default=0)
+            "port_dot1x_logoff_total", labels=port_labels, default=0
+        )
         new_dp_success_total = self.scrape_prometheus_var(
-            'dp_dot1x_success_total', default=0)
+            "dp_dot1x_success_total", default=0
+        )
         new_dp_failure_total = self.scrape_prometheus_var(
-            'dp_dot1x_failure_total', default=0)
+            "dp_dot1x_failure_total", default=0
+        )
         new_dp_logoff_total = self.scrape_prometheus_var(
-            'dp_dot1x_logoff_total', default=0)
+            "dp_dot1x_logoff_total", default=0
+        )
         if expect_success and success:
             self.assertGreater(new_success_total, success_total)
             self.assertGreater(new_dp_success_total, dp_success_total)
             self.assertEqual(failure_total, new_failure_total)
             self.assertEqual(dp_failure_total, new_dp_failure_total)
-            logoff = 'logoff' in tcpdump_txt
+            logoff = "logoff" in tcpdump_txt
             if logoff != and_logoff:
                 return False
             if and_logoff:
                 self.assertGreater(new_logoff_total, logoff_total)
             return True
         self.assertEqual(logoff_total, new_logoff_total)
         self.assertEqual(dp_logoff_total, new_dp_logoff_total)
         self.assertEqual(dp_success_total, new_dp_success_total)
         self.assertGreaterEqual(new_failure_total, failure_total)
         self.assertGreaterEqual(new_dp_failure_total, dp_failure_total)
         return False
 
-    def retry_8021x(self, host, port_num, conf, and_logoff=False, retries=2, expect_success=True):
+    def retry_8021x(
+        self, host, port_num, conf, and_logoff=False, retries=2, expect_success=True
+    ):
         for _ in range(retries):
-            if self.try_8021x(host, port_num, conf, and_logoff, expect_success=expect_success):
+            if self.try_8021x(
+                host, port_num, conf, and_logoff, expect_success=expect_success
+            ):
                 return True
             time.sleep(1)
         return False
 
     def wait_8021x_flows(self, port_no):
         port_actions = [
-            'SET_FIELD: {eth_dst:%s}' % self._priv_mac(port_no), 'OUTPUT:%u' % self.nfv_portno]
+            "SET_FIELD: {eth_dst:%s}" % self._priv_mac(port_no),
+            "OUTPUT:%u" % self.nfv_portno,
+        ]
         from_nfv_actions = [
-            'SET_FIELD: {eth_src:01:80:c2:00:00:03}', 'OUTPUT:%d' % port_no]
+            "SET_FIELD: {eth_src:01:80:c2:00:00:03}",
+            "OUTPUT:%d" % port_no,
+        ]
         from_nfv_match = {
-            'in_port': self.nfv_portno, 'dl_src': self._priv_mac(port_no), 'dl_type': 0x888e}
+            "in_port": self.nfv_portno,
+            "dl_src": self._priv_mac(port_no),
+            "dl_type": 0x888E,
+        }
         self.wait_until_matching_flow(None, table_id=0, actions=port_actions)
-        self.wait_until_matching_flow(from_nfv_match, table_id=0, actions=from_nfv_actions)
+        self.wait_until_matching_flow(
+            from_nfv_match, table_id=0, actions=from_nfv_actions
+        )
 
     def wait_8021x_success_flows(self, host, port_no):
-        from_host_actions = [
-            'GOTO_TABLE:1']
-        from_host_match = {
-            'in_port': port_no, 'dl_src': host.MAC()}
-        self.wait_until_matching_flow(from_host_match, table_id=0, actions=from_host_actions)
+        from_host_actions = ["GOTO_TABLE:1"]
+        from_host_match = {"in_port": port_no, "dl_src": host.MAC()}
+        self.wait_until_matching_flow(
+            from_host_match, table_id=0, actions=from_host_actions
+        )
 
     def verify_host_success(self, eapol_host, port_no, wpasupplicant_conf, and_logoff):
         self.one_ipv4_ping(
-            eapol_host, self.ping_host.IP(), require_host_learned=False, expected_result=False)
+            eapol_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
         self.assertTrue(
             self.try_8021x(
-                eapol_host, port_no, wpasupplicant_conf, and_logoff=and_logoff))
+                eapol_host, port_no, wpasupplicant_conf, and_logoff=and_logoff
+            )
+        )
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(), require_host_learned=False, expected_result=True)
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=True,
+        )
 
-    def wpa_supplicant_callback(self, host, port_num, conf, and_logoff, timeout=10, terminate_wpasupplicant=False):
+    def wpa_supplicant_callback(
+        self,
+        host,
+        port_num,
+        conf,
+        and_logoff,
+        timeout=10,
+        terminate_wpasupplicant=False,
+    ):
         wpa_ctrl_path = self.get_wpa_ctrl_path(host)
         if os.path.exists(wpa_ctrl_path):
             self.terminate_wpasupplicant(host)
-            for pid in host.cmd('lsof -t %s' % wpa_ctrl_path).splitlines():
+            for pid in host.cmd("lsof -t %s" % wpa_ctrl_path).splitlines():
                 try:
                     os.kill(int(pid), 15)
                 except (ValueError, ProcessLookupError):
                     pass
             try:
                 shutil.rmtree(wpa_ctrl_path)
             except FileNotFoundError:
                 pass
-        log_prefix = host.name + '_'
+        log_prefix = host.name + "_"
         self.start_wpasupplicant(
-            host, conf, timeout=timeout,
-            wpa_ctrl_socket_path=wpa_ctrl_path, log_prefix=log_prefix)
+            host,
+            conf,
+            timeout=timeout,
+            wpa_ctrl_socket_path=wpa_ctrl_path,
+            log_prefix=log_prefix,
+        )
         if and_logoff:
             self.wait_for_eap_success(host, wpa_ctrl_path)
             self.wait_until_matching_flow(
-                {'eth_src': host.MAC(), 'in_port': port_num}, table_id=0)
-            self.one_ipv4_ping(
-                host, self.ping_host.IP(), require_host_learned=False)
-            host.cmd('wpa_cli -p %s logoff' % wpa_ctrl_path)
+                {"eth_src": host.MAC(), "in_port": port_num}, table_id=0
+            )
+            self.one_ipv4_ping(host, self.ping_host.IP(), require_host_learned=False)
+            host.cmd("wpa_cli -p %s logoff" % wpa_ctrl_path)
             self.wait_until_no_matching_flow(
-                {'eth_src': host.MAC(), 'in_port': port_num}, table_id=0)
+                {"eth_src": host.MAC(), "in_port": port_num}, table_id=0
+            )
             self.one_ipv4_ping(
-                host, self.ping_host.IP(),
-                require_host_learned=False, expected_result=False)
+                host,
+                self.ping_host.IP(),
+                require_host_learned=False,
+                expected_result=False,
+            )
 
         if terminate_wpasupplicant:
             self.terminate_wpasupplicant(host)
 
     def terminate_wpasupplicant(self, host):
         wpa_ctrl_path = self.get_wpa_ctrl_path(host)
-        host.cmd('wpa_cli -p %s terminate' % wpa_ctrl_path)
+        host.cmd("wpa_cli -p %s terminate" % wpa_ctrl_path)
 
     def get_wpa_ctrl_path(self, host):
         wpa_ctrl_path = os.path.join(
-            self.tmpdir, '%s/%s-wpasupplicant' % (self.tmpdir, host.name))
+            self.tmpdir, "%s/%s-wpasupplicant" % (self.tmpdir, host.name)
+        )
         return wpa_ctrl_path
 
     @staticmethod
     def get_wpa_status(host, wpa_ctrl_path):
-        status = host.cmdPrint('wpa_cli -p %s status' % wpa_ctrl_path)
+        status = host.cmdPrint("wpa_cli -p %s status" % wpa_ctrl_path)
         for line in status.splitlines():
-            if line.startswith('EAP state'):
-                return line.split('=')[1].strip()
+            if line.startswith("EAP state"):
+                return line.split("=")[1].strip()
         return None
 
     def wait_for_eap_success(self, host, wpa_ctrl_path, timeout=5):
         for _ in range(timeout):
             eap_state = self.get_wpa_status(host, wpa_ctrl_path)
-            if eap_state == 'SUCCESS':
+            if eap_state == "SUCCESS":
                 return
             time.sleep(1)
-        self.fail('did not get EAP success: %s' % eap_state)
+        self.fail("did not get EAP success: %s" % eap_state)
 
     def wait_for_radius(self, radius_log_path):
         self.wait_until_matching_lines_from_file(
-            r'.*Ready to process requests', radius_log_path)
+            r".*Ready to process requests", radius_log_path
+        )
 
     def start_freeradius(self):
-        radius_log_path = '%s/radius.log' % self.tmpdir
+        radius_log_path = "%s/radius.log" % self.tmpdir
 
-        listen_match = r'(listen {[^}]*(limit {[^}]*})[^}]*})|(listen {[^}]*})'
+        listen_match = r"(listen {[^}]*(limit {[^}]*})[^}]*})|(listen {[^}]*})"
         listen_config = """listen {
         type = auth
         ipaddr = *
         port = %s
 }
 listen {
         type = acct
         ipaddr = *
         port = %d
-}""" % (self.RADIUS_PORT, self.RADIUS_PORT + 1)
+}""" % (
+            self.RADIUS_PORT,
+            self.RADIUS_PORT + 1,
+        )
 
-        if os.path.isfile('/etc/freeradius/users'):
+        if os.path.isfile("/etc/freeradius/users"):
             # Assume we are dealing with freeradius 2 configuration
-            shutil.copytree('/etc/freeradius/', '%s/freeradius' % self.tmpdir)
-            users_path = '%s/freeradius/users' % self.tmpdir
+            shutil.copytree("/etc/freeradius/", "%s/freeradius" % self.tmpdir)
+            users_path = "%s/freeradius/users" % self.tmpdir
 
-            with open('%s/freeradius/radiusd.conf' % self.tmpdir, 'r+', encoding='utf-8') as default_site:
+            with open(
+                "%s/freeradius/radiusd.conf" % self.tmpdir, "r+", encoding="utf-8"
+            ) as default_site:
                 default_config = default_site.read()
-                default_config = re.sub(listen_match, '', default_config)
+                default_config = re.sub(listen_match, "", default_config)
                 default_site.seek(0)
                 default_site.write(default_config)
                 default_site.write(listen_config)
                 default_site.truncate()
         else:
             # Assume we are dealing with freeradius >=3 configuration
-            freerad_version = os.popen(
-                r'freeradius -v | egrep -o -m 1 "Version ([0-9]\.[0.9])"').read().rstrip()
-            freerad_major_version = freerad_version.split(' ')[1]
-            shutil.copytree('/etc/freeradius/%s/' % freerad_major_version,
-                            '%s/freeradius' % self.tmpdir)
-            users_path = '%s/freeradius/mods-config/files/authorize' % self.tmpdir
-
-            with open('%s/freeradius/sites-enabled/default' % self.tmpdir, 'r+', encoding='utf-8') as default_site:
+            freerad_version = (
+                os.popen(r'freeradius -v | egrep -o -m 1 "Version ([0-9]\.[0.9])"')
+                .read()
+                .rstrip()
+            )
+            freerad_major_version = freerad_version.split(" ")[1]
+            shutil.copytree(
+                "/etc/freeradius/%s/" % freerad_major_version,
+                "%s/freeradius" % self.tmpdir,
+            )
+            users_path = "%s/freeradius/mods-config/files/authorize" % self.tmpdir
+
+            with open(
+                "%s/freeradius/sites-enabled/default" % self.tmpdir,
+                "r+",
+                encoding="utf-8",
+            ) as default_site:
                 default_config = default_site.read()
+                default_config = re.sub(listen_match, "", default_config)
                 default_config = re.sub(
-                    listen_match, '', default_config)
-                default_config = re.sub(
-                    r'server default {', 'server default {\n' + listen_config, default_config)
+                    r"server default {",
+                    "server default {\n" + listen_config,
+                    default_config,
+                )
                 default_site.seek(0)
                 default_site.write(default_config)
                 default_site.truncate()
 
-        with open(users_path, 'w', encoding='utf-8') as users_file:
+        with open(users_path, "w", encoding="utf-8") as users_file:
             users_file.write(self.freeradius_user_conf.format(self.SESSION_TIMEOUT))
 
-        with open('%s/freeradius/clients.conf' % self.tmpdir, 'w', encoding='utf-8') as clients:
-            clients.write("""client localhost {
+        with open(
+            "%s/freeradius/clients.conf" % self.tmpdir, "w", encoding="utf-8"
+        ) as clients:
+            clients.write(
+                """client localhost {
     ipaddr = 127.0.0.1
     secret = SECRET
-}""")
+}"""
+            )
 
-        with open('%s/freeradius/sites-enabled/inner-tunnel' % self.tmpdir, 'r+', encoding='utf-8') as innertunnel_site:
+        with open(
+            "%s/freeradius/sites-enabled/inner-tunnel" % self.tmpdir,
+            "r+",
+            encoding="utf-8",
+        ) as innertunnel_site:
             tunnel_config = innertunnel_site.read()
             listen_config = """listen {
        ipaddr = 127.0.0.1
        port = %d
        type = auth
-}""" % (self.RADIUS_PORT + 2)
+}""" % (
+                self.RADIUS_PORT + 2
+            )
             tunnel_config = re.sub(listen_match, listen_config, tunnel_config)
             innertunnel_site.seek(0)
             innertunnel_site.write(tunnel_config)
             innertunnel_site.truncate()
 
-        os.system('chmod o+rx %s' % self.root_tmpdir)
-        os.system('chown -R root:freerad %s/freeradius/' % self.tmpdir)
+        os.system("chmod o+rx %s" % self.root_tmpdir)
+        os.system("chown -R root:freerad %s/freeradius/" % self.tmpdir)
 
         self.nfv_host.cmd(
             mininet_test_util.timeout_cmd(
-                'freeradius -X -l %s -d %s/freeradius &' % (radius_log_path, self.tmpdir),
-                300))
+                "freeradius -X -l %s -d %s/freeradius &"
+                % (radius_log_path, self.tmpdir),
+                300,
+            )
+        )
 
         self.wait_for_radius(radius_log_path)
         return radius_log_path
 
 
 class Faucet8021XSuccessTest(Faucet8021XBase):
-
     DOT1X_EXPECTED_EVENTS = [
-        {'ENABLED': {}},
-        {'PORT_UP': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_2', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_4', 'port_type': 'nfv'}},
-        {'AUTHENTICATION': {'port': 'port_1', 'eth_src': 'HOST1_MAC', 'status': 'success'}},
-        {'AUTHENTICATION': {'port': 'port_2', 'eth_src': 'HOST2_MAC', 'status': 'success'}},
-        {'AUTHENTICATION': {'port': 'port_2', 'eth_src': 'HOST2_MAC', 'status': 'logoff'}}]
+        {"ENABLED": {}},
+        {"PORT_UP": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_2", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_4", "port_type": "nfv"}},
+        {
+            "AUTHENTICATION": {
+                "port": "port_1",
+                "eth_src": "HOST1_MAC",
+                "status": "success",
+            }
+        },
+        {
+            "AUTHENTICATION": {
+                "port": "port_2",
+                "eth_src": "HOST2_MAC",
+                "status": "success",
+            }
+        },
+        {
+            "AUTHENTICATION": {
+                "port": "port_2",
+                "eth_src": "HOST2_MAC",
+                "status": "logoff",
+            }
+        },
+    ]
     SESSION_TIMEOUT = 3600
 
     def test_untagged(self):
         self.verify_host_success(
-            self.eapol1_host, self.port_map['port_1'], self.wpasupplicant_conf_1, False)
+            self.eapol1_host, self.port_map["port_1"], self.wpasupplicant_conf_1, False
+        )
         self.verify_host_success(
-            self.eapol2_host, self.port_map['port_2'], self.wpasupplicant_conf_1, True)
+            self.eapol2_host, self.port_map["port_2"], self.wpasupplicant_conf_1, True
+        )
         self.post_test_checks()
 
 
 class Faucet8021XFailureTest(Faucet8021XBase):
     """Failure due to incorrect identity/password"""
 
     wpasupplicant_conf_1 = """
@@ -644,50 +798,67 @@
         eap=MD5
         identity="user"
         password="wrongpassword"
     }
     """
 
     DOT1X_EXPECTED_EVENTS = [
-        {'ENABLED': {}},
-        {'PORT_UP': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_2', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_4', 'port_type': 'nfv'}},
-        {'AUTHENTICATION': {'port': 'port_1', 'eth_src': 'HOST1_MAC', 'status': 'failure'}}]
+        {"ENABLED": {}},
+        {"PORT_UP": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_2", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_4", "port_type": "nfv"}},
+        {
+            "AUTHENTICATION": {
+                "port": "port_1",
+                "eth_src": "HOST1_MAC",
+                "status": "failure",
+            }
+        },
+    ]
 
     def test_untagged(self):
         self.assertFalse(
             self.try_8021x(
-                self.eapol1_host, self.port_map['port_1'],
-                self.wpasupplicant_conf_1, and_logoff=False,
-                expect_success=False))
+                self.eapol1_host,
+                self.port_map["port_1"],
+                self.wpasupplicant_conf_1,
+                and_logoff=False,
+                expect_success=False,
+            )
+        )
         self.post_test_checks()
 
 
 class Faucet8021XPortStatusTest(Faucet8021XBase):
-
     DOT1X_EXPECTED_EVENTS = [
-        {'ENABLED': {}},
-        {'PORT_UP': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_2', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_4', 'port_type': 'nfv'}},
-        {'PORT_DOWN': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_4', 'port_type': 'nfv'}},
-        {'PORT_DOWN': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_4', 'port_type': 'nfv'}},
-        {'PORT_UP': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'AUTHENTICATION': {'port': 'port_1', 'eth_src': 'HOST1_MAC', 'status': 'success'}},
-        {'PORT_DOWN': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_1', 'port_type': 'supplicant'}}]
-
-    def test_untagged(self):
-        port_no1 = self.port_map['port_1']
-        port_no2 = self.port_map['port_2']
-        port_no4 = self.port_map['port_4']
+        {"ENABLED": {}},
+        {"PORT_UP": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_2", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_4", "port_type": "nfv"}},
+        {"PORT_DOWN": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_4", "port_type": "nfv"}},
+        {"PORT_DOWN": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_4", "port_type": "nfv"}},
+        {"PORT_UP": {"port": "port_1", "port_type": "supplicant"}},
+        {
+            "AUTHENTICATION": {
+                "port": "port_1",
+                "eth_src": "HOST1_MAC",
+                "status": "success",
+            }
+        },
+        {"PORT_DOWN": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_1", "port_type": "supplicant"}},
+    ]
+
+    def test_untagged(self):
+        port_no1 = self.port_map["port_1"]
+        port_no2 = self.port_map["port_2"]
+        port_no4 = self.port_map["port_4"]
 
         self.wait_8021x_flows(port_no1)
         self.set_port_down(port_no1)
         # self.wait_until_no_matching_flow(None, table_id=0, actions=actions)
         self.set_port_up(port_no1)
         self.wait_8021x_flows(port_no1)
 
@@ -702,136 +873,184 @@
         self.wait_8021x_flows(port_no2)
         # no portno1
 
         self.set_port_up(port_no1)
         self.wait_8021x_flows(port_no1)
 
         # When the port goes down, and up the host should not be authenticated anymore.
-        self.assertTrue(self.retry_8021x(
-            self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False))
-        self.one_ipv4_ping(self.eapol1_host, self.ping_host.IP(), require_host_learned=False)
+        self.assertTrue(
+            self.retry_8021x(
+                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False
+            )
+        )
+        self.one_ipv4_ping(
+            self.eapol1_host, self.ping_host.IP(), require_host_learned=False
+        )
 
         # terminate so don't automatically reauthenticate when port goes back up.
         self.terminate_wpasupplicant(self.eapol1_host)
 
         self.flap_port(port_no1)
         self.wait_8021x_flows(port_no1)
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=False)
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
         self.post_test_checks()
 
 
 class Faucet8021XPortFlapTest(Faucet8021XBase):
-
     def test_untagged(self):
-        port_no1 = self.port_map['port_1']
+        port_no1 = self.port_map["port_1"]
 
         for _ in range(2):
-
             self.set_port_up(port_no1)
-            self.assertTrue(self.retry_8021x(
-                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=True))
+            self.assertTrue(
+                self.retry_8021x(
+                    self.eapol1_host,
+                    port_no1,
+                    self.wpasupplicant_conf_1,
+                    and_logoff=True,
+                )
+            )
 
             self.set_port_down(port_no1)
-            self.assertFalse(self.try_8021x(
-                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False, expect_success=False))
+            self.assertFalse(
+                self.try_8021x(
+                    self.eapol1_host,
+                    port_no1,
+                    self.wpasupplicant_conf_1,
+                    and_logoff=False,
+                    expect_success=False,
+                )
+            )
             self.one_ipv4_ping(
-                self.eapol1_host, self.ping_host.IP(),
-                require_host_learned=False, expected_result=False)
+                self.eapol1_host,
+                self.ping_host.IP(),
+                require_host_learned=False,
+                expected_result=False,
+            )
             wpa_status = self.get_wpa_status(
-                self.eapol1_host, self.get_wpa_ctrl_path(self.eapol1_host))
-            self.assertNotEqual('SUCCESS', wpa_status)
+                self.eapol1_host, self.get_wpa_ctrl_path(self.eapol1_host)
+            )
+            self.assertNotEqual("SUCCESS", wpa_status)
             # Kill supplicant so cant reply to the port up identity request.
             self.terminate_wpasupplicant(self.eapol1_host)
 
         self.post_test_checks()
 
 
 class Faucet8021XIdentityOnPortUpTest(Faucet8021XBase):
-
     def test_untagged(self):
-        port_no1 = self.port_map['port_1']
+        port_no1 = self.port_map["port_1"]
 
         # start wpa sup, logon, then send id request.
         self.set_port_up(port_no1)
-        self.assertTrue(self.try_8021x(
-            self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False))
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False
+            )
+        )
         self.set_port_down(port_no1)
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=False)
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
 
         def port_up(port):
             self.set_port_up(port)
             self.wait_8021x_flows(port)
 
-        username = 'user'
-        username_bytes = ''.join(('%2x' % ord(c) for c in username))
-        tcpdump_filter = ' or '.join((
-            self._success_eapol_filter(True),
-            self._eapol_filter(('ether[23:4] == 0x%s' % username_bytes,))))
+        username = "user"
+        username_bytes = "".join(("%2x" % ord(c) for c in username))
+        tcpdump_filter = " or ".join(
+            (
+                self._success_eapol_filter(True),
+                self._eapol_filter(("ether[23:4] == 0x%s" % username_bytes,)),
+            )
+        )
         tcpdump_txt = self.tcpdump_helper(
-            self.eapol1_host, tcpdump_filter, [
-                lambda: port_up(port_no1)],
-            timeout=30, vflags='-vvv', packets=2)
+            self.eapol1_host,
+            tcpdump_filter,
+            [lambda: port_up(port_no1)],
+            timeout=30,
+            vflags="-vvv",
+            packets=2,
+        )
         for req_str in (
-                'Identity: %s' % username,  # supplicant replies with username
-                'Success',  # supplicant success
+            "Identity: %s" % username,  # supplicant replies with username
+            "Success",  # supplicant success
         ):
-            self.assertTrue(req_str in tcpdump_txt, msg='%s not in %s' % (req_str, tcpdump_txt))
+            self.assertTrue(
+                req_str in tcpdump_txt, msg="%s not in %s" % (req_str, tcpdump_txt)
+            )
 
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=True, retries=10)
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=True,
+            retries=10,
+        )
 
         self.post_test_checks()
 
 
 class Faucet8021XPeriodicReauthTest(Faucet8021XBase):
-
     SESSION_TIMEOUT = 15
 
     def test_untagged(self):
-        port_no1 = self.port_map['port_1']
+        port_no1 = self.port_map["port_1"]
         port_labels1 = self.port_labels(port_no1)
 
         self.set_port_up(port_no1)
-        self.assertTrue(self.try_8021x(
-            self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False))
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False
+            )
+        )
 
         last_total = self.scrape_prometheus_var(
-            'port_dot1x_success_total', labels=port_labels1, default=0)
+            "port_dot1x_success_total", labels=port_labels1, default=0
+        )
         for _ in range(4):
             for _ in range(self.SESSION_TIMEOUT * 2):
                 total = self.scrape_prometheus_var(
-                    'port_dot1x_success_total', labels=port_labels1, default=0)
+                    "port_dot1x_success_total", labels=port_labels1, default=0
+                )
                 if total > last_total:
                     break
                 time.sleep(1)
-            self.assertGreater(total, last_total, msg='failed to successfully re-auth')
+            self.assertGreater(total, last_total, msg="failed to successfully re-auth")
             last_total = total
         self.post_test_checks()
 
 
 class Faucet8021XConfigReloadTest(Faucet8021XBase):
-
     def test_untagged(self):
-        port_no1 = self.port_map['port_1']
-        port_no2 = self.port_map['port_2']
+        port_no1 = self.port_map["port_1"]
+        port_no2 = self.port_map["port_2"]
 
         self.wait_8021x_flows(port_no1)
         self.wait_8021x_flows(port_no2)
 
         conf = self._get_faucet_conf()
-        conf['dps'][self.DP_NAME]['interfaces'][port_no1]['dot1x'] = False
+        conf["dps"][self.DP_NAME]["interfaces"][port_no1]["dot1x"] = False
 
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=False, change_expected=True)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+        )
 
         self.wait_8021x_flows(port_no2)
         self.post_test_checks()
 
 
 class Faucet8021XCustomACLLoginTest(Faucet8021XBase):
     """Ensure that 8021X Port ACLs Work before and after Login"""
@@ -897,41 +1116,62 @@
                 description: "b4"
                 output_only: True
                 # "NFV host - interface used by controller."
     """
 
     def test_untagged(self):
         self.verify_host_success(
-            self.eapol1_host, self.port_map['port_1'], self.wpasupplicant_conf_1, False)
+            self.eapol1_host, self.port_map["port_1"], self.wpasupplicant_conf_1, False
+        )
         self.post_test_checks()
 
 
 class Faucet8021XCustomACLLogoutTest(Faucet8021XCustomACLLoginTest):
     """Ensure that 8021X Port ACLs Work before and after Logout"""
 
     def test_untagged(self):
-        self.one_ipv4_ping(self.eapol1_host, self.ping_host.IP(),
-                           require_host_learned=False, expected_result=False)
-        self.assertTrue(self.try_8021x(
-            self.eapol1_host, self.port_map['port_1'], self.wpasupplicant_conf_1, and_logoff=True))
-        self.one_ipv4_ping(self.eapol1_host, self.ping_host.IP(),
-                           require_host_learned=False, expected_result=False)
+        self.one_ipv4_ping(
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol1_host,
+                self.port_map["port_1"],
+                self.wpasupplicant_conf_1,
+                and_logoff=True,
+            )
+        )
+        self.one_ipv4_ping(
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
         self.post_test_checks()
 
 
 class Faucet8021XMABTest(Faucet8021XSuccessTest):
     """Ensure that 802.1x Port Supports Mac Auth Bypass."""
 
-    DOT1X_EXPECTED_EVENTS = [{'ENABLED': {}},
-                             {'PORT_UP': {'port': 'port_1', 'port_type': 'supplicant'}},
-                             {'PORT_UP': {'port': 'port_2', 'port_type': 'supplicant'}},
-                             {'PORT_UP': {'port': 'port_4', 'port_type': 'nfv'}},
-                             {'AUTHENTICATION': {'port': 'port_1', 'eth_src': 'HOST1_MAC',
-                                                 'status': 'success'}},
-                             ]
+    DOT1X_EXPECTED_EVENTS = [
+        {"ENABLED": {}},
+        {"PORT_UP": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_2", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_4", "port_type": "nfv"}},
+        {
+            "AUTHENTICATION": {
+                "port": "port_1",
+                "eth_src": "HOST1_MAC",
+                "status": "success",
+            }
+        },
+    ]
 
     CONFIG = """
         dot1x:
             nfv_intf: NFV_INTF
             nfv_sw_port: %(port_4)d
             radius_ip: 127.0.0.1
             radius_port: RADIUS_PORT
@@ -953,43 +1193,65 @@
                 output_only: True
                 # "NFV host - interface used by controller."
     """
 
     def start_freeradius(self):
         # Add the host mac address to the FreeRADIUS config
         self.freeradius_user_conf += '\n{0}  Cleartext-Password := "{0}"'.format(
-            str(self.eapol1_host.MAC()).replace(':', '')
+            str(self.eapol1_host.MAC()).replace(":", "")
         )
         return super().start_freeradius()
 
     def test_untagged(self):
-        port_no1 = self.port_map['port_1']
+        port_no1 = self.port_map["port_1"]
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=False)
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
         self.eapol1_host.run_dhclient(self.tmpdir)
-        self.wait_until_matching_lines_from_faucet_log_files(r'.*AAA_SUCCESS.*')
+        self.wait_until_matching_lines_from_faucet_log_files(r".*AAA_SUCCESS.*")
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=True)
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=True,
+        )
         self.assertEqual(
             1,
-            self.scrape_prometheus_var('port_dot1x_success_total', labels=self.port_labels(port_no1), default=0))
+            self.scrape_prometheus_var(
+                "port_dot1x_success_total", labels=self.port_labels(port_no1), default=0
+            ),
+        )
         self.post_test_checks()
 
 
 class Faucet8021XDynACLLoginTest(Faucet8021XCustomACLLoginTest):
     """Ensure that 8021X Port ACLs Work before and after Logout"""
+
     DOT1X_EXPECTED_EVENTS = [
-        {'ENABLED': {}},
-        {'PORT_UP': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_2', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_4', 'port_type': 'nfv'}},
-        {'AUTHENTICATION': {'port': 'port_1', 'eth_src': 'HOST1_MAC', 'status': 'success'}},
-        {'AUTHENTICATION': {'port': 'port_2', 'eth_src': 'HOST2_MAC', 'status': 'success'}},
+        {"ENABLED": {}},
+        {"PORT_UP": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_2", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_4", "port_type": "nfv"}},
+        {
+            "AUTHENTICATION": {
+                "port": "port_1",
+                "eth_src": "HOST1_MAC",
+                "status": "success",
+            }
+        },
+        {
+            "AUTHENTICATION": {
+                "port": "port_2",
+                "eth_src": "HOST2_MAC",
+                "status": "success",
+            }
+        },
     ]
 
     wpasupplicant_conf_1 = """
 ap_scan=0
 network={
     key_mgmt=IEEE8021X
     eap=MD5
@@ -1069,50 +1331,94 @@
                name: b4
                description: "b4"
                output_only: True
                # "NFV host - interface used by controller."
            """
 
     def test_untagged(self):
-        port_no1 = self.port_map['port_1']
-        port_no2 = self.port_map['port_2']
+        port_no1 = self.port_map["port_1"]
+        port_no2 = self.port_map["port_2"]
 
-        self.one_ipv4_ping(self.eapol1_host, self.ping_host.IP(),
-                           require_host_learned=False, expected_result=False)
-        self.one_ipv4_ping(self.eapol2_host, self.ping_host.IP(),
-                           require_host_learned=False, expected_result=False)
-        self.assertTrue(self.try_8021x(
-            self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False))
-        self.assertTrue(self.try_8021x(
-            self.eapol2_host, port_no2, self.wpasupplicant_conf_2, and_logoff=False))
-        self.one_ipv4_ping(self.eapol1_host, self.ping_host.IP(),
-                           require_host_learned=False, expected_result=True)
-        self.one_ipv4_ping(self.eapol2_host, self.ping_host.IP(),
-                           require_host_learned=False, expected_result=False)
+        self.one_ipv4_ping(
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
+        self.one_ipv4_ping(
+            self.eapol2_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False
+            )
+        )
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol2_host, port_no2, self.wpasupplicant_conf_2, and_logoff=False
+            )
+        )
+        self.one_ipv4_ping(
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=True,
+        )
+        self.one_ipv4_ping(
+            self.eapol2_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
         self.post_test_checks()
 
 
 class Faucet8021XDynACLLogoutTest(Faucet8021XDynACLLoginTest):
-
     DOT1X_EXPECTED_EVENTS = [
-        {'ENABLED': {}},
-        {'PORT_UP': {'port': 'port_1', 'port_type': 'supplicant'}},
-        {'PORT_UP': {'port': 'port_4', 'port_type': 'nfv'}},
-        {'AUTHENTICATION': {'port': 'port_1', 'eth_src': 'HOST1_MAC', 'status': 'success'}},
-        {'AUTHENTICATION': {'port': 'port_1', 'eth_src': 'HOST1_MAC', 'status': 'logoff'}}
+        {"ENABLED": {}},
+        {"PORT_UP": {"port": "port_1", "port_type": "supplicant"}},
+        {"PORT_UP": {"port": "port_4", "port_type": "nfv"}},
+        {
+            "AUTHENTICATION": {
+                "port": "port_1",
+                "eth_src": "HOST1_MAC",
+                "status": "success",
+            }
+        },
+        {
+            "AUTHENTICATION": {
+                "port": "port_1",
+                "eth_src": "HOST1_MAC",
+                "status": "logoff",
+            }
+        },
     ]
 
     def test_untagged(self):
-        port_no1 = self.port_map['port_1']
-        self.one_ipv4_ping(self.eapol1_host, self.ping_host.IP(),
-                           require_host_learned=False, expected_result=False)
-        self.assertTrue(self.try_8021x(
-            self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=True))
-        self.one_ipv4_ping(self.eapol1_host, self.ping_host.IP(),
-                           require_host_learned=False, expected_result=False)
+        port_no1 = self.port_map["port_1"]
+        self.one_ipv4_ping(
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=True
+            )
+        )
+        self.one_ipv4_ping(
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
         self.post_test_checks()
 
 
 class Faucet8021XVLANTest(Faucet8021XSuccessTest):
     """Test that two hosts are put into vlans.
     Same VLAN, Logoff, diff VLANs, port flap."""
 
@@ -1124,16 +1430,18 @@
             vid: %u
             description: "untagged"
             dot1x_assigned: True
         radiusassignedvlan2:
             vid: %u
             description: "untagged"
             dot1x_assigned: True
-    """ % (mininet_test_base.MAX_TEST_VID - 1,
-           mininet_test_base.MAX_TEST_VID)
+    """ % (
+        mininet_test_base.MAX_TEST_VID - 1,
+        mininet_test_base.MAX_TEST_VID,
+    )
 
     CONFIG = """
         dot1x:
             nfv_intf: NFV_INTF
             nfv_sw_port: %(port_4)d
             radius_ip: 127.0.0.1
             radius_port: RADIUS_PORT
@@ -1176,157 +1484,225 @@
         identity="vlanuser2222"
         password="milliphone"
     }
     """
 
     def test_untagged(self):
         vid = 100 ^ mininet_test_base.OFPVID_PRESENT
-        radius_vid1 = (mininet_test_base.MAX_TEST_VID - 1) ^ mininet_test_base.OFPVID_PRESENT
+        radius_vid1 = (
+            mininet_test_base.MAX_TEST_VID - 1
+        ) ^ mininet_test_base.OFPVID_PRESENT
         radius_vid2 = mininet_test_base.MAX_TEST_VID ^ mininet_test_base.OFPVID_PRESENT
-        port_no1 = self.port_map['port_1']
-        port_no2 = self.port_map['port_2']
-        port_no3 = self.port_map['port_3']
-        self.assertTrue(self.try_8021x(
-            self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False))
+        port_no1 = self.port_map["port_1"]
+        port_no2 = self.port_map["port_2"]
+        port_no3 = self.port_map["port_3"]
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False
+            )
+        )
 
         self.wait_until_matching_flow(
-            {'in_port': port_no1},
+            {"in_port": port_no1},
             table_id=self._VLAN_TABLE,
-            actions=['SET_FIELD: {vlan_vid:%u}' % radius_vid1])
+            actions=["SET_FIELD: {vlan_vid:%u}" % radius_vid1],
+        )
         self.wait_until_matching_flow(
-            {'vlan_vid': radius_vid1},
+            {"vlan_vid": radius_vid1},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no3])
+            actions=["POP_VLAN", "OUTPUT:%s" % port_no1, "OUTPUT:%s" % port_no3],
+        )
         self.wait_until_matching_flow(
-            {'vlan_vid': vid},
+            {"vlan_vid": vid},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', 'OUTPUT:%s' % port_no2])
+            actions=["POP_VLAN", "OUTPUT:%s" % port_no2],
+        )
         self.wait_until_no_matching_flow(
-            {'vlan_vid': radius_vid2},
+            {"vlan_vid": radius_vid2},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no2])
+            actions=["POP_VLAN", "OUTPUT:%s" % port_no1, "OUTPUT:%s" % port_no2],
+        )
 
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=True)
-        self.assertTrue(self.try_8021x(
-            self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=True))
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=True,
+        )
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=True
+            )
+        )
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=False)
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
 
         # check ports are back in the right vlans.
         self.wait_until_no_matching_flow(
-            {'in_port': port_no1},
+            {"in_port": port_no1},
             table_id=self._VLAN_TABLE,
-            actions=['SET_FIELD: {vlan_vid:%u}' % radius_vid1])
+            actions=["SET_FIELD: {vlan_vid:%u}" % radius_vid1],
+        )
         self.wait_until_matching_flow(
-            {'in_port': port_no1},
+            {"in_port": port_no1},
             table_id=self._VLAN_TABLE,
-            actions=['SET_FIELD: {vlan_vid:%u}' % vid])
+            actions=["SET_FIELD: {vlan_vid:%u}" % vid],
+        )
 
         # check flood ports are in the right vlans
         self.wait_until_no_matching_flow(
-            {'vlan_vid': radius_vid1},
+            {"vlan_vid": radius_vid1},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no3])
+            actions=["POP_VLAN", "OUTPUT:%s" % port_no1, "OUTPUT:%s" % port_no3],
+        )
         self.wait_until_matching_flow(
-            {'vlan_vid': vid},
+            {"vlan_vid": vid},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no2])
+            actions=["POP_VLAN", "OUTPUT:%s" % port_no1, "OUTPUT:%s" % port_no2],
+        )
 
         # check two 1x hosts play nicely. (same dyn vlan)
-        self.assertTrue(self.try_8021x(
-            self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False))
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol1_host, port_no1, self.wpasupplicant_conf_1, and_logoff=False
+            )
+        )
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=True)
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=True,
+        )
         self.one_ipv4_ping(
-            self.eapol1_host, self.eapol2_host.IP(),
-            require_host_learned=False, expected_result=False)
-        self.assertTrue(self.try_8021x(
-            self.eapol2_host, port_no2, self.wpasupplicant_conf_1, and_logoff=False))
+            self.eapol1_host,
+            self.eapol2_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol2_host, port_no2, self.wpasupplicant_conf_1, and_logoff=False
+            )
+        )
         self.one_ipv4_ping(
-            self.eapol2_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=True)
+            self.eapol2_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=True,
+        )
         self.one_ipv4_ping(
-            self.eapol2_host, self.eapol1_host.IP(),
-            require_host_learned=False, expected_result=True)
+            self.eapol2_host,
+            self.eapol1_host.IP(),
+            require_host_learned=False,
+            expected_result=True,
+        )
 
         # check two 1x hosts dont play (diff dyn vlan).
-        self.assertTrue(self.try_8021x(
-            self.eapol2_host, port_no2, self.wpasupplicant_conf_2, and_logoff=False))
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol2_host, port_no2, self.wpasupplicant_conf_2, and_logoff=False
+            )
+        )
         self.one_ipv4_ping(
-            self.eapol2_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=False)
+            self.eapol2_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
         self.one_ipv4_ping(
-            self.eapol2_host, self.eapol1_host.IP(),
-            require_host_learned=False, expected_result=False)
+            self.eapol2_host,
+            self.eapol1_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
 
         # move host1 to new VLAN
-        self.assertTrue(self.try_8021x(
-            self.eapol1_host, port_no1, self.wpasupplicant_conf_2, and_logoff=False))
+        self.assertTrue(
+            self.try_8021x(
+                self.eapol1_host, port_no1, self.wpasupplicant_conf_2, and_logoff=False
+            )
+        )
         self.one_ipv4_ping(
-            self.eapol1_host, self.ping_host.IP(),
-            require_host_learned=False, expected_result=False)
+            self.eapol1_host,
+            self.ping_host.IP(),
+            require_host_learned=False,
+            expected_result=False,
+        )
         self.one_ipv4_ping(
-            self.eapol1_host, self.eapol2_host.IP(),
-            require_host_learned=False, expected_result=True)
+            self.eapol1_host,
+            self.eapol2_host.IP(),
+            require_host_learned=False,
+            expected_result=True,
+        )
 
         self.wait_until_matching_flow(
-            {'eth_src': self.eapol1_host.MAC(), 'vlan_vid': radius_vid2},
-            table_id=self._ETH_SRC_TABLE)
+            {"eth_src": self.eapol1_host.MAC(), "vlan_vid": radius_vid2},
+            table_id=self._ETH_SRC_TABLE,
+        )
         self.wait_until_matching_flow(
-            {'eth_src': self.eapol1_host.MAC(), 'vlan_vid': radius_vid2},
-            table_id=self._ETH_SRC_TABLE)
+            {"eth_src": self.eapol1_host.MAC(), "vlan_vid": radius_vid2},
+            table_id=self._ETH_SRC_TABLE,
+        )
         self.wait_until_no_matching_flow(
-            {'eth_src': self.eapol1_host.MAC(), 'vlan_vid': vid},
-            table_id=self._ETH_SRC_TABLE)
+            {"eth_src": self.eapol1_host.MAC(), "vlan_vid": vid},
+            table_id=self._ETH_SRC_TABLE,
+        )
         self.wait_until_no_matching_flow(
-            {'eth_src': self.eapol1_host.MAC(), 'vlan_vid': radius_vid1},
-            table_id=self._ETH_SRC_TABLE)
+            {"eth_src": self.eapol1_host.MAC(), "vlan_vid": radius_vid1},
+            table_id=self._ETH_SRC_TABLE,
+        )
         self.wait_until_no_matching_flow(
-            {'eth_dst': self.eapol1_host.MAC(), 'vlan_vid': vid},
-            table_id=self._ETH_DST_TABLE)
+            {"eth_dst": self.eapol1_host.MAC(), "vlan_vid": vid},
+            table_id=self._ETH_DST_TABLE,
+        )
         self.wait_until_no_matching_flow(
-            {'eth_dst': self.eapol1_host.MAC(), 'vlan_vid': radius_vid1},
-            table_id=self._ETH_DST_TABLE)
+            {"eth_dst": self.eapol1_host.MAC(), "vlan_vid": radius_vid1},
+            table_id=self._ETH_DST_TABLE,
+        )
 
         # test port up/down. removes the dynamic vlan & host cache.
         self.flap_port(port_no2)
 
         self.wait_until_matching_flow(
-            {'in_port': port_no2},
+            {"in_port": port_no2},
             table_id=self._VLAN_TABLE,
-            actions=['SET_FIELD: {vlan_vid:%u}' % vid])
+            actions=["SET_FIELD: {vlan_vid:%u}" % vid],
+        )
         self.wait_until_matching_flow(
-            {'vlan_vid': vid},
+            {"vlan_vid": vid},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', 'OUTPUT:%s' % port_no2])
+            actions=["POP_VLAN", "OUTPUT:%s" % port_no2],
+        )
         self.wait_until_no_matching_flow(
-            {'in_port': port_no2},
+            {"in_port": port_no2},
             table_id=self._VLAN_TABLE,
-            actions=['SET_FIELD: {vlan_vid:%u}' % radius_vid2])
+            actions=["SET_FIELD: {vlan_vid:%u}" % radius_vid2],
+        )
         self.wait_until_no_matching_flow(
-            {'vlan_vid': radius_vid2},
+            {"vlan_vid": radius_vid2},
             table_id=self._FLOOD_TABLE,
-            actions=['POP_VLAN', 'OUTPUT:%s' % port_no1, 'OUTPUT:%s' % port_no2])
+            actions=["POP_VLAN", "OUTPUT:%s" % port_no1, "OUTPUT:%s" % port_no2],
+        )
         self.wait_until_no_matching_flow(
-            {'eth_src': self.eapol2_host.MAC()},
-            table_id=self._ETH_SRC_TABLE)
+            {"eth_src": self.eapol2_host.MAC()}, table_id=self._ETH_SRC_TABLE
+        )
         self.wait_until_no_matching_flow(
-            {'eth_dst': self.eapol2_host.MAC(), 'vlan_vid': radius_vid1},
+            {"eth_dst": self.eapol2_host.MAC(), "vlan_vid": radius_vid1},
             table_id=self._ETH_DST_TABLE,
-            actions=['POP_VLAN', 'OUTPUT:%s' % port_no2])
+            actions=["POP_VLAN", "OUTPUT:%s" % port_no2],
+        )
 
         self.post_test_checks()
 
 
 class FaucetUntaggedRandomVidTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     randvlan:
         vid: 100
         description: "untagged"
 """
 
@@ -1344,73 +1720,78 @@
 
     def test_untagged(self):
         last_vid = None
         for _ in range(5):
             vid = random.randint(2, mininet_test_base.MAX_TEST_VID)
             if vid == last_vid:
                 continue
-            self.change_vlan_config(
-                'randvlan', 'vid', vid, cold_start=True, hup=True)
+            self.change_vlan_config("randvlan", "vid", vid, cold_start=True, hup=True)
             self.ping_all_when_learned()
             last_vid = vid
 
 
 class FaucetUntaggedNoCombinatorialFloodTest(FaucetUntaggedTest):
-
-    CONFIG = """
+    CONFIG = (
+        """
         combinatorial_port_flood: False
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
 
 class FaucetUntaggedControllerNfvTest(FaucetUntaggedTest):
-
     def test_untagged(self):
         # Name of switch interface connected to last host,
         #  accessible to controller
         last_host = self.hosts_name_ordered()[-1]
         switch = self.first_switch()
         last_host_switch_link = switch.connectionsTo(last_host)[0]
-        last_host_switch_intf = [intf for intf in last_host_switch_link if intf in switch.intfList()][0]
+        last_host_switch_intf = [
+            intf for intf in last_host_switch_link if intf in switch.intfList()
+        ][0]
 
         super().test_untagged()
 
         # Confirm controller can see switch interface with traffic.
-        ifconfig_output = self.net.controllers[0].cmd('ifconfig %s' % last_host_switch_intf)
+        ifconfig_output = self.net.controllers[0].cmd(
+            "ifconfig %s" % last_host_switch_intf
+        )
         self.assertTrue(
-            re.search('(R|T)X packets[: ][1-9]', ifconfig_output),
-            msg=ifconfig_output)
+            re.search("(R|T)X packets[: ][1-9]", ifconfig_output), msg=ifconfig_output
+        )
 
 
 class FaucetUntaggedBroadcastTest(FaucetUntaggedTest):
-
     def test_untagged(self):
         super().test_untagged()
         self.verify_broadcast()
         self.verify_no_bcast_to_self()
         self.verify_unicast_not_looped()
 
 
 class FaucetUntaggedNSLoopTest(FaucetUntaggedTest):
-
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 acls:
     nsonly:
         - rule:
             dl_type: %u
             ip_proto: 58
             icmpv6_type: 135
             actions:
                 allow: 1
         - rule:
             actions:
                 allow: 0
 vlans:
     100:
         description: "untagged"
-""" % IPV6_ETH
+"""
+        % IPV6_ETH
+    )
 
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acl_in: nsonly
             %(port_2)d:
@@ -1425,33 +1806,33 @@
     """
 
     def test_untagged(self):
         self.verify_no_bcast_to_self()
 
 
 class FaucetUntaggedNoCombinatorialBroadcastTest(FaucetUntaggedBroadcastTest):
-
-    CONFIG = """
+    CONFIG = (
+        """
         combinatorial_port_flood: False
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
 
 class FaucetUntaggedLogRotateTest(FaucetUntaggedTest):
-
     def test_untagged(self):
-        faucet_log = self.env[self.faucet_controllers[0].name]['FAUCET_LOG']
+        faucet_log = self.env[self.faucet_controllers[0].name]["FAUCET_LOG"]
         self.assertTrue(os.path.exists(faucet_log))
-        os.rename(faucet_log, faucet_log + '.old')
-        self.assertTrue(os.path.exists(faucet_log + '.old'))
+        os.rename(faucet_log, faucet_log + ".old")
+        self.assertTrue(os.path.exists(faucet_log + ".old"))
         self.flap_all_switch_ports()
         self.assertTrue(os.path.exists(faucet_log))
 
 
 class FaucetUntaggedLLDPTest(FaucetUntaggedTest):
-
     CONFIG = """
         lldp_beacon:
             send_interval: 5
             max_per_interval: 5
         interfaces:
             %(port_1)d:
                 native_vlan: 100
@@ -1467,51 +1848,59 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
     @staticmethod
     def wireshark_payload_format(payload_str):
-        formatted_payload_str = ''
+        formatted_payload_str = ""
         groupsize = 4
         for payload_offset in range(len(payload_str) // groupsize):
             char_count = payload_offset * 2
             if char_count % 0x10 == 0:
-                formatted_payload_str += '0x%4.4x: ' % char_count
-            payload_fragment = payload_str[payload_offset * groupsize:][:groupsize]
-            formatted_payload_str += ' ' + payload_fragment
+                formatted_payload_str += "0x%4.4x: " % char_count
+            payload_fragment = payload_str[payload_offset * groupsize :][:groupsize]
+            formatted_payload_str += " " + payload_fragment
         return formatted_payload_str
 
     def test_untagged(self):
         first_host = self.hosts_name_ordered()[0]
-        tcpdump_filter = 'ether proto 0x88cc'
+        tcpdump_filter = "ether proto 0x88cc"
         timeout = 5 * 3
         tcpdump_txt = self.tcpdump_helper(
-            first_host, tcpdump_filter, [
-                lambda: first_host.cmd('sleep %u' % timeout)],
-            timeout=timeout, vflags='-vv', packets=1)
-        oui_prefix = ''.join(self.FAUCET_MAC.split(':')[:3])
-        faucet_lldp_dp_id_attr = '%2.2x' % 1
-        expected_lldp_dp_id = ''.join((
-            oui_prefix,
-            faucet_lldp_dp_id_attr,
-            binascii.hexlify(str(self.dpid).encode('UTF-8')).decode()))
+            first_host,
+            tcpdump_filter,
+            [lambda: first_host.cmd("sleep %u" % timeout)],
+            timeout=timeout,
+            vflags="-vv",
+            packets=1,
+        )
+        oui_prefix = "".join(self.FAUCET_MAC.split(":")[:3])
+        faucet_lldp_dp_id_attr = "%2.2x" % 1
+        expected_lldp_dp_id = "".join(
+            (
+                oui_prefix,
+                faucet_lldp_dp_id_attr,
+                binascii.hexlify(str(self.dpid).encode("UTF-8")).decode(),
+            )
+        )
         for lldp_required in (
-                r'%s > 01:80:c2:00:00:0e, ethertype LLDP' % self.FAUCET_MAC,
-                r'Application type \[voice\] \(0x01\), Flags \[Tagged\]Vlan id 50',
-                r'System Name TLV \(5\), length 6: faucet',
-                r'Port Description TLV \(4\), length 10: first_port',
-                self.wireshark_payload_format(expected_lldp_dp_id)):
+            r"%s > 01:80:c2:00:00:0e, ethertype LLDP" % self.FAUCET_MAC,
+            r"Application type \[voice\] \(0x01\), Flags \[Tagged\]Vlan id 50",
+            r"System Name TLV \(5\), length 6: faucet",
+            r"Port Description TLV \(4\), length 10: first_port",
+            self.wireshark_payload_format(expected_lldp_dp_id),
+        ):
             self.assertTrue(
                 re.search(lldp_required, tcpdump_txt),
-                msg='%s: %s' % (lldp_required, tcpdump_txt))
+                msg="%s: %s" % (lldp_required, tcpdump_txt),
+            )
 
 
 class FaucetLLDPIntervalTest(FaucetUntaggedTest):
-
     NUM_FAUCET_CONTROLLERS = 1
 
     CONFIG = """
         lldp_beacon:
             send_interval: 10
             max_per_interval: 5
         interfaces:
@@ -1529,29 +1918,32 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host = self.hosts_name_ordered()[0]
-        tcpdump_filter = 'ether proto 0x88cc'
+        tcpdump_filter = "ether proto 0x88cc"
         interval = 10
         timeout = interval * 3
         tcpdump_txt = self.tcpdump_helper(
-            first_host, tcpdump_filter, [
-                lambda: first_host.cmd('sleep %u' % timeout)],
+            first_host,
+            tcpdump_filter,
+            [lambda: first_host.cmd("sleep %u" % timeout)],
             # output epoch secs
-            timeout=timeout, vflags='-tt', packets=2)
-        timestamps = re.findall(r'(\d+)\.\d+ [0-9a-f:]+ \> [0-9a-f:]+', tcpdump_txt)
+            timeout=timeout,
+            vflags="-tt",
+            packets=2,
+        )
+        timestamps = re.findall(r"(\d+)\.\d+ [0-9a-f:]+ \> [0-9a-f:]+", tcpdump_txt)
         timestamps = [int(timestamp) for timestamp in timestamps]
         self.assertTrue(timestamps[1] - timestamps[0] >= interval, msg=tcpdump_txt)
 
 
 class FaucetUntaggedLLDPDefaultFallbackTest(FaucetUntaggedTest):
-
     CONFIG = """
         lldp_beacon:
             send_interval: 5
             max_per_interval: 5
         interfaces:
             %(port_1)d:
                 native_vlan: 100
@@ -1559,34 +1951,39 @@
                     enable: True
                     org_tlvs:
                         - {oui: 0x12bb, subtype: 2, info: "01406500"}
 """
 
     def test_untagged(self):
         first_host = self.hosts_name_ordered()[0]
-        tcpdump_filter = 'ether proto 0x88cc'
+        tcpdump_filter = "ether proto 0x88cc"
         timeout = 5 * 3
         tcpdump_txt = self.tcpdump_helper(
-            first_host, tcpdump_filter, [
-                lambda: first_host.cmd('sleep %u' % timeout)],
-            timeout=timeout, vflags='-vv', packets=1)
+            first_host,
+            tcpdump_filter,
+            [lambda: first_host.cmd("sleep %u" % timeout)],
+            timeout=timeout,
+            vflags="-vv",
+            packets=1,
+        )
         for lldp_required in (
-                r'%s > 01:80:c2:00:00:0e, ethertype LLDP' % self.FAUCET_MAC,
-                r'Application type \[voice\] \(0x01\), Flags \[Tagged\]Vlan id 50',
-                r'System Name TLV \(5\), length 8: faucet-1',
-                r'Port Description TLV \(4\), length [1-9]: b%u' % self.port_map['port_1']):
+            r"%s > 01:80:c2:00:00:0e, ethertype LLDP" % self.FAUCET_MAC,
+            r"Application type \[voice\] \(0x01\), Flags \[Tagged\]Vlan id 50",
+            r"System Name TLV \(5\), length 8: faucet-1",
+            r"Port Description TLV \(4\), length [1-9]: b%u" % self.port_map["port_1"],
+        ):
             self.assertTrue(
                 re.search(lldp_required, tcpdump_txt),
-                msg='%s: %s' % (lldp_required, tcpdump_txt))
+                msg="%s: %s" % (lldp_required, tcpdump_txt),
+            )
 
 
 class FaucetUntaggedMeterParseTest(FaucetUntaggedTest):
-
     REQUIRES_METERS = True
-    OVS_TYPE = 'user'
+    OVS_TYPE = "user"
     CONFIG_GLOBAL = """
 meters:
     lossymeter:
         meter_id: 1
         entry:
             flags: "KBPS"
             bands:
@@ -1625,28 +2022,36 @@
         interval: 5
         db: 'meter_file'
     meter_stats_prom:
         dps: ['%s']
         type: 'meter_stats'
         db: 'prometheus'
         interval: 5
-""" % (self.DP_NAME, self.DP_NAME, self.DP_NAME, self.DP_NAME)
+""" % (
+            self.DP_NAME,
+            self.DP_NAME,
+            self.DP_NAME,
+            self.DP_NAME,
+        )
 
     GAUGE_CONFIG_DBS = """
     prometheus:
         type: 'prometheus'
         prometheus_addr: '::1'
         prometheus_port: %(gauge_prom_port)d
 """
-    config_ports = {'gauge_prom_port': None}
+    config_ports = {"gauge_prom_port": None}
 
-    def _get_gauge_meter_config(self, faucet_config_file,
-                                monitor_stats_file,
-                                monitor_state_file,
-                                monitor_meter_stats_file):
+    def _get_gauge_meter_config(
+        self,
+        faucet_config_file,
+        monitor_stats_file,
+        monitor_state_file,
+        monitor_meter_stats_file,
+    ):
         """Build Gauge Meter config."""
         return """
 faucet_configs:
     - %s
 watchers:
     %s
 dbs:
@@ -1656,36 +2061,41 @@
     state_file:
         type: 'text'
         file: %s
     meter_file:
         type: 'text'
         file: %s
 %s
-    """ % (faucet_config_file, self.get_gauge_watcher_config(),
-           monitor_stats_file, monitor_state_file, monitor_meter_stats_file,
-           self.GAUGE_CONFIG_DBS)
+    """ % (
+            faucet_config_file,
+            self.get_gauge_watcher_config(),
+            monitor_stats_file,
+            monitor_state_file,
+            monitor_meter_stats_file,
+            self.GAUGE_CONFIG_DBS,
+        )
 
     def _init_gauge_config(self):
         gauge_config = self._get_gauge_meter_config(
             self.faucet_config_path,
             self.monitor_stats_file,
             self.monitor_state_file,
-            self.monitor_meter_stats_file)
+            self.monitor_meter_stats_file,
+        )
         if self.config_ports:
             gauge_config = gauge_config % self.config_ports
         self._write_yaml_conf(self.gauge_config_path, yaml_load(gauge_config))
 
     def test_untagged(self):
         """All hosts on the same untagged VLAN should have connectivity."""
         # TODO: userspace DP port status not reliable.
         self.ping_all_when_learned()
 
 
 class FaucetUntaggedApplyMeterTest(FaucetUntaggedMeterParseTest):
-
     CONFIG = """
         interfaces:
             %(port_1)d:
                 acl_in: lossyacl
                 native_vlan: 100
             %(port_2)d:
                 native_vlan: 100
@@ -1694,92 +2104,103 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         super().test_untagged()
         first_host, second_host = self.hosts_name_ordered()[:2]
-        error('metered ping flood: %s' % first_host.cmd(
-            'ping -c 1000 -f %s' % second_host.IP()))
+        error(
+            "metered ping flood: %s"
+            % first_host.cmd("ping -c 1000 -f %s" % second_host.IP())
+        )
         # Require meter band bytes to match.
         self.wait_until_matching_lines_from_file(
-            r'.+faucet-1-1-byte-band-count.+[1-9].+',
-            self.monitor_meter_stats_file)
-        meter_labels = {
-            'dp_id': self.dpid,
-            'dp_name': self.DP_NAME,
-            'meter_id': 1
-        }
+            r".+faucet-1-1-byte-band-count.+[1-9].+", self.monitor_meter_stats_file
+        )
+        meter_labels = {"dp_id": self.dpid, "dp_name": self.DP_NAME, "meter_id": 1}
         byte_band_count = self.scrape_prometheus_var(
-            'of_meter_byte_band_count', labels=meter_labels, controller=self.gauge_controller.name)
+            "of_meter_byte_band_count",
+            labels=meter_labels,
+            controller=self.gauge_controller.name,
+        )
         self.assertTrue(byte_band_count)
 
 
 class FaucetUntaggedMeterAddTest(FaucetUntaggedMeterParseTest):
-
     NUM_FAUCET_CONTROLLERS = 1
 
     def test_untagged(self):
         super().test_untagged()
         conf = self._get_faucet_conf()
-        conf['meters']['lossymeter2'] = {
-            'meter_id': 2,
-            'entry': {
-                'flags': ['PKTPS'],
-                'bands': [{'rate': '1000', 'type': 'DROP'}]
-            },
+        conf["meters"]["lossymeter2"] = {
+            "meter_id": 2,
+            "entry": {"flags": ["PKTPS"], "bands": [{"rate": "1000", "type": "DROP"}]},
         }
-        conf['acls']['lossyacl2'] = [{
-            'rule': {
-                'actions': {
-                    'allow': 1,
-                    'meter': 'lossymeter2'
-                }
-            }
-        }]
-        port_conf = conf['dps'][self.DP_NAME]['interfaces'][self.port_map['port_2']]
-        port_conf['acls_in'] = ['lossyacl2']
+        conf["acls"]["lossyacl2"] = [
+            {"rule": {"actions": {"allow": 1, "meter": "lossymeter2"}}}
+        ]
+        port_conf = conf["dps"][self.DP_NAME]["interfaces"][self.port_map["port_2"]]
+        port_conf["acls_in"] = ["lossyacl2"]
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=True, change_expected=True, hup=True)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=True,
+            change_expected=True,
+            hup=True,
+        )
         self.wait_until_matching_lines_from_file(
-            r'.+\'meter_id\'\: 2+',
-            self.get_matching_meters_on_dpid(self.dpid))
-        port_conf['acls_in'] = []
+            r".+\'meter_id\'\: 2+", self.get_matching_meters_on_dpid(self.dpid)
+        )
+        port_conf["acls_in"] = []
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=True, change_expected=True)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=True,
+            change_expected=True,
+        )
         self.wait_until_no_matching_lines_from_file(
-            r'.+\'meter_id\'\: 2+',
-            self.get_matching_meters_on_dpid(self.dpid))
+            r".+\'meter_id\'\: 2+", self.get_matching_meters_on_dpid(self.dpid)
+        )
 
 
 class FaucetUntaggedMeterModTest(FaucetUntaggedMeterParseTest):
-
     def test_untagged(self):
         super().test_untagged()
         conf = self._get_faucet_conf()
-        conf['dps'][self.DP_NAME]['interfaces'][self.port_map['port_1']]['acls_in'] = ['lossyacl']
+        conf["dps"][self.DP_NAME]["interfaces"][self.port_map["port_1"]]["acls_in"] = [
+            "lossyacl"
+        ]
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=True, change_expected=True, hup=True)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=True,
+            change_expected=True,
+            hup=True,
+        )
         self.wait_until_matching_lines_from_file(
-            r'.+KBPS+',
-            self.get_matching_meters_on_dpid(self.dpid))
-        conf['meters']['lossymeter']['entry']['flags'] = ['PKTPS']
+            r".+KBPS+", self.get_matching_meters_on_dpid(self.dpid)
+        )
+        conf["meters"]["lossymeter"]["entry"]["flags"] = ["PKTPS"]
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=False, change_expected=True, hup=True)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=True,
+            hup=True,
+        )
         self.wait_until_matching_lines_from_file(
-            r'.+PKTPS+',
-            self.get_matching_meters_on_dpid(self.dpid))
+            r".+PKTPS+", self.get_matching_meters_on_dpid(self.dpid)
+        )
 
 
 class FaucetUntaggedHairpinTest(FaucetUntaggedTest):
-
     NETNS = True
     CONFIG = """
         interfaces:
             %(port_1)d:
                 hairpin: True
                 native_vlan: 100
             %(port_2)d:
@@ -1790,46 +2211,46 @@
                 native_vlan: 100
 """
 
     def test_untagged(self):
         # Create macvlan interfaces, with one in a separate namespace,
         # to force traffic between them to be hairpinned via FAUCET.
         first_host, second_host = self.hosts_name_ordered()[:2]
-        macvlan1_intf = 'macvlan1'
-        macvlan1_ipv4 = '10.0.0.100'
-        macvlan2_intf = 'macvlan2'
-        macvlan2_ipv4 = '10.0.0.101'
-        self.add_macvlan(first_host, macvlan1_intf, ipa=macvlan1_ipv4, mode='vepa')
-        self.add_macvlan(first_host, macvlan2_intf, mode='vepa')
+        macvlan1_intf = "macvlan1"
+        macvlan1_ipv4 = "10.0.0.100"
+        macvlan2_intf = "macvlan2"
+        macvlan2_ipv4 = "10.0.0.101"
+        self.add_macvlan(first_host, macvlan1_intf, ipa=macvlan1_ipv4, mode="vepa")
+        self.add_macvlan(first_host, macvlan2_intf, mode="vepa")
         macvlan2_mac = self.get_host_intf_mac(first_host, macvlan2_intf)
         netns = self.hostns(first_host)
         setup_cmds = []
-        setup_cmds.extend(
-            ['ip link set %s netns %s' % (macvlan2_intf, netns)])
+        setup_cmds.extend(["ip link set %s netns %s" % (macvlan2_intf, netns)])
         for exec_cmd in (
-                ('ip address add %s/24 brd + dev %s' % (
-                    macvlan2_ipv4, macvlan2_intf),
-                 'ip link set %s up' % macvlan2_intf)):
-            setup_cmds.append('ip netns exec %s %s' % (netns, exec_cmd))
+            "ip address add %s/24 brd + dev %s" % (macvlan2_ipv4, macvlan2_intf),
+            "ip link set %s up" % macvlan2_intf,
+        ):
+            setup_cmds.append("ip netns exec %s %s" % (netns, exec_cmd))
         self.quiet_commands(first_host, setup_cmds)
         self.one_ipv4_ping(first_host, macvlan2_ipv4, intf=macvlan1_intf)
         self.one_ipv4_ping(first_host, second_host.IP())
         # Verify OUTPUT:IN_PORT flood rules are exercised.
         self.wait_nonzero_packet_count_flow(
-            {'in_port': self.port_map['port_1'],
-             'dl_dst': 'ff:ff:ff:ff:ff:ff'},
-            table_id=self._FLOOD_TABLE, actions=['OUTPUT:IN_PORT'])
+            {"in_port": self.port_map["port_1"], "dl_dst": "ff:ff:ff:ff:ff:ff"},
+            table_id=self._FLOOD_TABLE,
+            actions=["OUTPUT:IN_PORT"],
+        )
         self.wait_nonzero_packet_count_flow(
-            {'in_port': self.port_map['port_1'],
-             'dl_dst': macvlan2_mac},
-            table_id=self._ETH_DST_HAIRPIN_TABLE, actions=['OUTPUT:IN_PORT'])
+            {"in_port": self.port_map["port_1"], "dl_dst": macvlan2_mac},
+            table_id=self._ETH_DST_HAIRPIN_TABLE,
+            actions=["OUTPUT:IN_PORT"],
+        )
 
 
 class FaucetUntaggedGroupHairpinTest(FaucetUntaggedHairpinTest):
-
     CONFIG = """
         group_table: True
         interfaces:
             %(port_1)d:
                 hairpin: True
                 native_vlan: 100
             %(port_2)d:
@@ -1838,153 +2259,176 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
     """
 
 
 class FaucetUntaggedTcpIPv4IperfTest(FaucetUntaggedTest):
-
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         first_host_ip = ipaddress.ip_address(first_host.IP())
         second_host_ip = ipaddress.ip_address(second_host.IP())
         for _ in range(3):
             self.ping_all_when_learned()
             self.verify_iperf_min(
-                ((first_host, self.port_map['port_1']),
-                 (second_host, self.port_map['port_2'])),
-                MIN_MBPS, first_host_ip, second_host_ip,
-                sync_counters_func=lambda: self.one_ipv4_ping(first_host, second_host_ip))
+                (
+                    (first_host, self.port_map["port_1"]),
+                    (second_host, self.port_map["port_2"]),
+                ),
+                MIN_MBPS,
+                first_host_ip,
+                second_host_ip,
+                sync_counters_func=lambda: self.one_ipv4_ping(
+                    first_host, second_host_ip
+                ),
+            )
             self.flap_all_switch_ports()
 
 
 class FaucetUntaggedTcpIPv6IperfTest(FaucetUntaggedTest):
-
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
-        first_host_ip = ipaddress.ip_interface('fc00::1:1/112')
-        second_host_ip = ipaddress.ip_interface('fc00::1:2/112')
+        first_host_ip = ipaddress.ip_interface("fc00::1:1/112")
+        second_host_ip = ipaddress.ip_interface("fc00::1:2/112")
         self.add_host_ipv6_address(first_host, first_host_ip)
         self.add_host_ipv6_address(second_host, second_host_ip)
         for _ in range(3):
             self.ping_all_when_learned()
             self.verify_iperf_min(
-                ((first_host, self.port_map['port_1']),
-                 (second_host, self.port_map['port_2'])),
-                MIN_MBPS, first_host_ip.ip, second_host_ip.ip,
-                sync_counters_func=lambda: self.one_ipv6_ping(first_host, second_host_ip.ip))
+                (
+                    (first_host, self.port_map["port_1"]),
+                    (second_host, self.port_map["port_2"]),
+                ),
+                MIN_MBPS,
+                first_host_ip.ip,
+                second_host_ip.ip,
+                sync_counters_func=lambda: self.one_ipv6_ping(
+                    first_host, second_host_ip.ip
+                ),
+            )
             self.flap_all_switch_ports()
 
 
 class FaucetSanityTest(FaucetUntaggedTest):
     """Sanity test - make sure test environment is correct before running all tess."""
 
     def test_scapy_fuzz(self):
         # Scapy 2.4.5 has issues with 'fuzz' generation
         #  so black-list that version with a test
         # https://github.com/secdev/scapy/issues/3306
         # TODO: fix expected in next scapy release, > 2.4.5.
         exception = False
         try:
-            scapy.all.send(scapy.all.fuzz(scapy.all.Ether()))  # pylint: disable=no-member
+            scapy.all.send(
+                scapy.all.fuzz(scapy.all.Ether())
+            )  # pylint: disable=no-member
         except Exception as e:  # pylint: disable=broad-except
-            error('%s:' % self._test_name(), e)
+            error("%s:" % self._test_name(), e)
             exception = True
-        self.assertFalse(exception, 'Scapy threw an exception in send(fuzz())')
+        self.assertFalse(exception, "Scapy threw an exception in send(fuzz())")
 
     def test_ryu_config(self):
-        varstr = ', '.join(self.scrape_prometheus(var='ryu_config'))
+        varstr = ", ".join(self.scrape_prometheus(var="ryu_config"))
         self.assertTrue('echo_request_interval"} 10.0' in varstr)
         self.assertTrue('maximum_unreplied_echo_requests"} 5.0' in varstr)
 
     def verify_dp_port_healthy(self, dp_port, retries=5, min_mbps=MIN_MBPS):
         for _ in range(retries):
             port_desc = self.get_port_desc_from_dpid(self.dpid, dp_port)
-            port_name = port_desc['name']
-            port_state = port_desc['state']
-            port_config = port_desc['config']
-            port_speed_mbps = (port_desc['curr_speed'] * 1e3) / 1e6
-            error('DP %u is %s, at %u mbps\n' % (dp_port, port_name, port_speed_mbps))
+            port_name = port_desc["name"]
+            port_state = port_desc["state"]
+            port_config = port_desc["config"]
+            port_speed_mbps = (port_desc["curr_speed"] * 1e3) / 1e6
+            error("DP %u is %s, at %u mbps\n" % (dp_port, port_name, port_speed_mbps))
             if port_speed_mbps < min_mbps:
-                error('port speed %u below minimum %u mbps\n' % (
-                    port_speed_mbps, min_mbps))
+                error(
+                    "port speed %u below minimum %u mbps\n"
+                    % (port_speed_mbps, min_mbps)
+                )
             elif port_config != 0:
-                error('port config %u must be 0 (all clear)' % port_config)
+                error("port config %u must be 0 (all clear)" % port_config)
             elif port_state not in (0, 4):
-                error('state %u must be 0 (all flags clear or live)\n' % (
-                    port_state))
+                error("state %u must be 0 (all flags clear or live)\n" % (port_state))
             else:
                 return
             time.sleep(1)
-        self.fail('DP port %u not healthy (%s)' % (dp_port, port_desc))
+        self.fail("DP port %u not healthy (%s)" % (dp_port, port_desc))
 
     def test_portmap(self):
-        prom_desc = self.scrape_prometheus(var='of_dp_desc_stats')
-        self.assertIsNotNone(prom_desc, msg='Cannot scrape of_dp_desc_stats')
-        error('DP: %s\n' % prom_desc[0])
-        error('port_map: %s\n' % self.port_map)
+        prom_desc = self.scrape_prometheus(var="of_dp_desc_stats")
+        self.assertIsNotNone(prom_desc, msg="Cannot scrape of_dp_desc_stats")
+        error("DP: %s\n" % prom_desc[0])
+        error("port_map: %s\n" % self.port_map)
         for i, host in enumerate(self.hosts_name_ordered(), start=1):
-            in_port = 'port_%u' % i
+            in_port = "port_%u" % i
             dp_port = self.port_map[in_port]
             if dp_port in self.switch_map:
-                error('verifying cabling for %s: host %s -> dp %u\n' % (
-                    in_port, self.switch_map[dp_port], dp_port))
+                error(
+                    "verifying cabling for %s: host %s -> dp %u\n"
+                    % (in_port, self.switch_map[dp_port], dp_port)
+                )
             else:
-                error('verifying host %s -> dp %s\n' % (
-                    in_port, dp_port))
+                error("verifying host %s -> dp %s\n" % (in_port, dp_port))
             self.verify_dp_port_healthy(dp_port)
             self.require_host_learned(host, in_port=dp_port)
         learned = self.prom_macs_learned()
         self.assertEqual(
-            len(self.hosts_name_ordered()), len(learned),
-            msg='test requires exactly %u hosts learned (got %s)' % (
-                len(self.hosts_name_ordered()), learned))
+            len(self.hosts_name_ordered()),
+            len(learned),
+            msg="test requires exactly %u hosts learned (got %s)"
+            % (len(self.hosts_name_ordered()), learned),
+        )
 
     def test_listening(self):
         msg_template = (
-            'Processes listening on test, or all interfaces may interfere with tests. '
-            'Please deconfigure them (e.g. configure interface as "unmanaged"):\n\n%s')
+            "Processes listening on test, or all interfaces may interfere with tests. "
+            'Please deconfigure them (e.g. configure interface as "unmanaged"):\n\n%s'
+        )
         controller = self._get_controller()
-        ss_out = controller.cmd('ss -lnep').splitlines()
-        listening_all_re = re.compile(r'^.+\s+(\*:\d+|:::\d+)\s+(:+\*|\*:\*).+$')
+        ss_out = controller.cmd("ss -lnep").splitlines()
+        listening_all_re = re.compile(r"^.+\s+(\*:\d+|:::\d+)\s+(:+\*|\*:\*).+$")
         listening_all = [line for line in ss_out if listening_all_re.match(line)]
         for test_intf in list(self.switch_map.values()):
-            int_re = re.compile(r'^.+\b%s\b.+$' % test_intf)
+            int_re = re.compile(r"^.+\b%s\b.+$" % test_intf)
             listening_int = [line for line in ss_out if int_re.match(line)]
             self.assertFalse(
-                len(listening_int),
-                msg=(msg_template % '\n'.join(listening_int)))
+                len(listening_int), msg=(msg_template % "\n".join(listening_int))
+            )
         if listening_all:
-            print('Warning: %s' % (msg_template % '\n'.join(listening_all)))
+            print("Warning: %s" % (msg_template % "\n".join(listening_all)))
 
     def test_silence(self):
         # Make all test hosts silent and ensure we hear no other packets.
         for host in self.hosts_name_ordered():
             self.host_drop_all_ips(host)
-            host.cmd('echo 1 > /proc/sys/net/ipv6/conf/%s/disable_ipv6' % host.defaultIntf())
+            host.cmd(
+                "echo 1 > /proc/sys/net/ipv6/conf/%s/disable_ipv6" % host.defaultIntf()
+            )
         for host in self.hosts_name_ordered():
-            tcpdump_filter = ''
+            tcpdump_filter = ""
             tcpdump_txt = self.tcpdump_helper(
-                host, tcpdump_filter, [], timeout=10, vflags='-vv', packets=1)
+                host, tcpdump_filter, [], timeout=10, vflags="-vv", packets=1
+            )
             self.tcpdump_rx_packets(tcpdump_txt, 0)
             self.assertTrue(
                 self.tcpdump_rx_packets(tcpdump_txt, 0),
-                msg='got unexpected packet from test switch: %s' % tcpdump_txt)
+                msg="got unexpected packet from test switch: %s" % tcpdump_txt,
+            )
 
 
 class FaucetUntaggedPrometheusGaugeTest(FaucetUntaggedTest):
     """Testing Gauge Prometheus"""
 
     GAUGE_CONFIG_DBS = """
     prometheus:
         type: 'prometheus'
         prometheus_addr: '::1'
         prometheus_port: %(gauge_prom_port)d
 """
-    config_ports = {'gauge_prom_port': None}
+    config_ports = {"gauge_prom_port": None}
 
     def get_gauge_watcher_config(self):
         return """
     port_stats:
         dps: ['%s']
         type: 'port_stats'
         interval: 5
@@ -1995,75 +2439,95 @@
         interval: 5
         db: 'prometheus'
     flow_table:
         dps: ['%s']
         type: 'flow_table'
         interval: 5
         db: 'prometheus'
-""" % (self.DP_NAME, self.DP_NAME, self.DP_NAME)
+""" % (
+            self.DP_NAME,
+            self.DP_NAME,
+            self.DP_NAME,
+        )
 
     def _start_gauge_check(self):
-        if not self.gauge_controller.listen_port(self.config_ports['gauge_prom_port']):
-            return 'gauge not listening on prometheus port'
+        if not self.gauge_controller.listen_port(self.config_ports["gauge_prom_port"]):
+            return "gauge not listening on prometheus port"
         return None
 
     def test_untagged(self):
         self.wait_dp_status(1, controller=self.gauge_controller.name)
-        self.assertIsNotNone(self.scrape_prometheus_var(
-            'faucet_pbr_version', any_labels=True, controller=self.gauge_controller.name, retries=3))
+        self.assertIsNotNone(
+            self.scrape_prometheus_var(
+                "faucet_pbr_version",
+                any_labels=True,
+                controller=self.gauge_controller.name,
+                retries=3,
+            )
+        )
         conf = self._get_faucet_conf()
-        cookie = conf['dps'][self.DP_NAME]['cookie']
+        cookie = conf["dps"][self.DP_NAME]["cookie"]
 
         if not self.wait_ports_updating(self.port_map.keys(), self.PORT_VARS):
-            self.fail(msg='Gauge Prometheus port counters not increasing')
+            self.fail(msg="Gauge Prometheus port counters not increasing")
 
         for _ in range(self.DB_TIMEOUT * 3):
             updated_counters = True
             for host in self.hosts_name_ordered():
                 host_labels = {
-                    'dp_id': self.dpid,
-                    'dp_name': self.DP_NAME,
-                    'cookie': cookie,
-                    'eth_dst': host.MAC(),
-                    'inst_count': str(1),
-                    'table_id': str(self._ETH_DST_TABLE),
-                    'vlan': str(100),
-                    'vlan_vid': str(4196)
+                    "dp_id": self.dpid,
+                    "dp_name": self.DP_NAME,
+                    "cookie": cookie,
+                    "eth_dst": host.MAC(),
+                    "inst_count": str(1),
+                    "table_id": str(self._ETH_DST_TABLE),
+                    "vlan": str(100),
+                    "vlan_vid": str(4196),
                 }
                 packet_count = self.scrape_prometheus_var(
-                    'flow_packet_count_eth_dst', labels=host_labels, controller=self.gauge_controller.name)
+                    "flow_packet_count_eth_dst",
+                    labels=host_labels,
+                    controller=self.gauge_controller.name,
+                )
                 byte_count = self.scrape_prometheus_var(
-                    'flow_byte_count_eth_dst', labels=host_labels, controller=self.gauge_controller.name)
+                    "flow_byte_count_eth_dst",
+                    labels=host_labels,
+                    controller=self.gauge_controller.name,
+                )
                 if packet_count is None or packet_count == 0:
                     updated_counters = False
                 if byte_count is None or byte_count == 0:
                     updated_counters = False
             if updated_counters:
                 return
             time.sleep(1)
 
-        self.fail(msg='Gauge Prometheus flow counters not increasing')
+        self.fail(msg="Gauge Prometheus flow counters not increasing")
 
 
 class FaucetUntaggedInfluxTest(FaucetUntaggedTest):
     """Basic untagged VLAN test with Influx."""
 
-    GAUGE_CONFIG_DBS = """
+    GAUGE_CONFIG_DBS = (
+        """
     influx:
         type: 'influx'
         influx_db: 'faucet'
         influx_host: '127.0.0.1'
         influx_port: %(gauge_influx_port)d
         influx_user: 'faucet'
         influx_pwd: ''
         influx_retries: 1
-""" + """
+"""
+        + """
         influx_timeout: %u
-""" % FaucetUntaggedTest.DB_TIMEOUT
-    config_ports = {'gauge_influx_port': None}
+"""
+        % FaucetUntaggedTest.DB_TIMEOUT
+    )
+    config_ports = {"gauge_influx_port": None}
     influx_log = None
     server_thread = None
     server = None
 
     def get_gauge_watcher_config(self):
         return """
     port_stats:
@@ -2077,18 +2541,22 @@
         interval: 2
         db: 'influx'
     flow_table:
         dps: ['%s']
         type: 'flow_table'
         interval: 2
         db: 'influx'
-""" % (self.DP_NAME, self.DP_NAME, self.DP_NAME)
+""" % (
+            self.DP_NAME,
+            self.DP_NAME,
+            self.DP_NAME,
+        )
 
     def setup_influx(self):
-        self.influx_log = os.path.join(self.tmpdir, 'influx.log')
+        self.influx_log = os.path.join(self.tmpdir, "influx.log")
         if self.server:
             self.server.influx_log = self.influx_log
             self.server.timeout = self.DB_TIMEOUT
 
     def setUp(self):
         self.handler = InfluxPostHandler
         super().setUp()
@@ -2100,103 +2568,116 @@
             self.server.socket.close()
         super().tearDown(ignore_oferrors=ignore_oferrors)
 
     def _wait_error_shipping(self, timeout=None):
         if timeout is None:
             timeout = self.DB_TIMEOUT * 3 * 2
         self.wait_until_matching_lines_from_gauge_log_files(
-            r'.+error shipping.+', timeout=timeout)
+            r".+error shipping.+", timeout=timeout
+        )
 
     def _verify_influx_log(self, retries=3):
         self.assertTrue(os.path.exists(self.influx_log))
         expected_vars = {
-            'dropped_in', 'dropped_out', 'bytes_out', 'flow_packet_count',
-            'errors_in', 'errors_out', 'bytes_in', 'flow_byte_count',
-            'port_state_reason', 'packets_in', 'packets_out'}
+            "dropped_in",
+            "dropped_out",
+            "bytes_out",
+            "flow_packet_count",
+            "errors_in",
+            "errors_out",
+            "bytes_in",
+            "flow_byte_count",
+            "port_state_reason",
+            "packets_in",
+            "packets_out",
+        }
 
         observed_vars = set()
         for _ in range(retries):
-            with open(self.influx_log, encoding='utf-8') as influx_log:
+            with open(self.influx_log, encoding="utf-8") as influx_log:
                 influx_log_lines = influx_log.readlines()
             for point_line in influx_log_lines:
                 point_fields = point_line.strip().split()
                 self.assertEqual(3, len(point_fields), msg=point_fields)
                 ts_name, value_field, _ = point_fields
-                value = float(value_field.split('=')[1])
-                ts_name_fields = ts_name.split(',')
+                value = float(value_field.split("=")[1])
+                ts_name_fields = ts_name.split(",")
                 self.assertGreater(len(ts_name_fields), 1)
                 observed_vars.add(ts_name_fields[0])
                 label_values = {}
                 for label_value in ts_name_fields[1:]:
-                    label, value = label_value.split('=')
+                    label, value = label_value.split("=")
                     label_values[label] = value
-                if ts_name.startswith('flow'):
-                    self.assertTrue('inst_count' in label_values, msg=point_line)
-                    if 'vlan_vid' in label_values:
-                        self.assertEqual(
-                            int(label_values['vlan']), int(value) ^ 0x1000)
+                if ts_name.startswith("flow"):
+                    self.assertTrue("inst_count" in label_values, msg=point_line)
+                    if "vlan_vid" in label_values:
+                        self.assertEqual(int(label_values["vlan"]), int(value) ^ 0x1000)
             if expected_vars == observed_vars:
                 break
             time.sleep(1)
 
         self.assertEqual(expected_vars, observed_vars)
-        self.verify_no_exception(self.env[self.gauge_controller.name]['GAUGE_EXCEPTION_LOG'])
+        self.verify_no_exception(
+            self.env[self.gauge_controller.name]["GAUGE_EXCEPTION_LOG"]
+        )
 
     def _wait_influx_log(self):
         for _ in range(self.DB_TIMEOUT * 3):
             if os.path.exists(self.influx_log):
                 return
             time.sleep(1)
 
     def _start_gauge_check(self):
         if self.server_thread:
             return None
-        influx_port = self.config_ports['gauge_influx_port']
+        influx_port = self.config_ports["gauge_influx_port"]
         try:
             self.server = QuietHTTPServer(
-                (mininet_test_util.LOCALHOST, influx_port),
-                self.handler)  # pytype: disable=attribute-error
+                (mininet_test_util.LOCALHOST, influx_port), self.handler
+            )  # pytype: disable=attribute-error
             self.server.timeout = self.DB_TIMEOUT
-            self.server_thread = threading.Thread(
-                target=self.server.serve_forever)
+            self.server_thread = threading.Thread(target=self.server.serve_forever)
             self.server_thread.daemon = True
             self.server_thread.start()
             return None
         except socket.error as err:
-            return 'cannot start Influx test server: %s' % err
+            return "cannot start Influx test server: %s" % err
 
     def test_untagged(self):
         self.ping_all_when_learned()
         self.hup_controller(self.gauge_controller.name)
         self.flap_all_switch_ports()
         self._wait_influx_log()
         self._verify_influx_log()
 
 
 class FaucetUntaggedMultiDBWatcherTest(
-        FaucetUntaggedInfluxTest, FaucetUntaggedPrometheusGaugeTest):
-    GAUGE_CONFIG_DBS = """
+    FaucetUntaggedInfluxTest, FaucetUntaggedPrometheusGaugeTest
+):
+    GAUGE_CONFIG_DBS = (
+        """
     prometheus:
         type: 'prometheus'
         prometheus_addr: '::1'
         prometheus_port: %(gauge_prom_port)d
     influx:
         type: 'influx'
         influx_db: 'faucet'
         influx_host: '127.0.0.1'
         influx_port: %(gauge_influx_port)d
         influx_user: 'faucet'
         influx_pwd: ''
         influx_retries: 1
-""" + """
+"""
+        + """
         influx_timeout: %u
-""" % FaucetUntaggedTest.DB_TIMEOUT
-    config_ports = {
-        'gauge_prom_port': None,
-        'gauge_influx_port': None}
+"""
+        % FaucetUntaggedTest.DB_TIMEOUT
+    )
+    config_ports = {"gauge_prom_port": None, "gauge_influx_port": None}
 
     def get_gauge_watcher_config(self):
         return """
     port_stats:
         dps: ['%s']
         type: 'port_stats'
         interval: 5
@@ -2207,15 +2688,19 @@
         interval: 5
         dbs: ['prometheus', 'influx']
     flow_table:
         dps: ['%s']
         type: 'flow_table'
         interval: 5
         dbs: ['prometheus', 'influx']
-""" % (self.DP_NAME, self.DP_NAME, self.DP_NAME)
+""" % (
+            self.DP_NAME,
+            self.DP_NAME,
+            self.DP_NAME,
+        )
 
     @staticmethod
     def test_tagged():
         return
 
     def test_untagged(self):
         self.wait_dp_status(1, controller=self.gauge_controller.name)
@@ -2224,26 +2709,26 @@
         self.hup_controller(controller=self.gauge_controller.name)
         self.flap_all_switch_ports()
         self._wait_influx_log()
         self._verify_influx_log()
 
 
 class FaucetUntaggedInfluxDownTest(FaucetUntaggedInfluxTest):
-
     def _start_gauge_check(self):
         return None
 
     def test_untagged(self):
         self.ping_all_when_learned()
         self._wait_error_shipping()
-        self.verify_no_exception(self.env[self.gauge_controller.name]['GAUGE_EXCEPTION_LOG'])
+        self.verify_no_exception(
+            self.env[self.gauge_controller.name]["GAUGE_EXCEPTION_LOG"]
+        )
 
 
 class FaucetUntaggedInfluxUnreachableTest(FaucetUntaggedInfluxTest):
-
     GAUGE_CONFIG_DBS = """
     influx:
         type: 'influx'
         influx_db: 'faucet'
         influx_host: '127.0.0.2'
         influx_port: %(gauge_influx_port)d
         influx_user: 'faucet'
@@ -2251,38 +2736,39 @@
         influx_timeout: 2
 """
 
     def _start_gauge_check(self):
         return None
 
     def test_untagged(self):
-        self.gauge_controller.cmd(
-            'route add 127.0.0.2 gw 127.0.0.1 lo')
+        self.gauge_controller.cmd("route add 127.0.0.2 gw 127.0.0.1 lo")
         self.ping_all_when_learned()
         self._wait_error_shipping()
-        self.verify_no_exception(self.env[self.gauge_controller.name]['GAUGE_EXCEPTION_LOG'])
+        self.verify_no_exception(
+            self.env[self.gauge_controller.name]["GAUGE_EXCEPTION_LOG"]
+        )
 
 
 class FaucetSingleUntaggedInfluxTooSlowTest(FaucetUntaggedInfluxTest):
-
     def setUp(self):
         self.handler = SlowInfluxPostHandler
         super().setUp()
         self.setup_influx()
 
     def test_untagged(self):
         self.ping_all_when_learned()
         self._wait_influx_log()
         self.assertTrue(os.path.exists(self.influx_log))
         self._wait_error_shipping()
-        self.verify_no_exception(self.env[self.gauge_controller.name]['GAUGE_EXCEPTION_LOG'])
+        self.verify_no_exception(
+            self.env[self.gauge_controller.name]["GAUGE_EXCEPTION_LOG"]
+        )
 
 
 class FaucetNailedForwardingTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 acls:
     1:
         - rule:
@@ -2340,24 +2826,21 @@
             %(port_4)d:
                 native_vlan: 100
                 acl_in: 4
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        first_host.setMAC('0e:00:00:00:01:01')
-        second_host.setMAC('0e:00:00:00:02:02')
-        self.one_ipv4_ping(
-            first_host, second_host.IP(), require_host_learned=False)
-        self.one_ipv4_ping(
-            second_host, first_host.IP(), require_host_learned=False)
+        first_host.setMAC("0e:00:00:00:01:01")
+        second_host.setMAC("0e:00:00:00:02:02")
+        self.one_ipv4_ping(first_host, second_host.IP(), require_host_learned=False)
+        self.one_ipv4_ping(second_host, first_host.IP(), require_host_learned=False)
 
 
 class FaucetNailedForwardingOrderedTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 acls:
     1:
         - rule:
@@ -2415,24 +2898,21 @@
             %(port_4)d:
                 native_vlan: 100
                 acl_in: 4
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        first_host.setMAC('0e:00:00:00:01:01')
-        second_host.setMAC('0e:00:00:00:02:02')
-        self.one_ipv4_ping(
-            first_host, second_host.IP(), require_host_learned=False)
-        self.one_ipv4_ping(
-            second_host, first_host.IP(), require_host_learned=False)
+        first_host.setMAC("0e:00:00:00:01:01")
+        second_host.setMAC("0e:00:00:00:02:02")
+        self.one_ipv4_ping(first_host, second_host.IP(), require_host_learned=False)
+        self.one_ipv4_ping(second_host, first_host.IP(), require_host_learned=False)
 
 
 class FaucetNailedFailoverForwardingTest(FaucetNailedForwardingTest):
-
     NUM_FAUCET_CONTROLLERS = 1
 
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 acls:
@@ -2492,31 +2972,26 @@
         - rule:
             actions:
                 allow: 0
 """
 
     def test_untagged(self):
         first_host, second_host, third_host = self.hosts_name_ordered()[0:3]
-        first_host.setMAC('0e:00:00:00:01:01')
-        second_host.setMAC('0e:00:00:00:02:02')
-        third_host.setMAC('0e:00:00:00:02:02')
+        first_host.setMAC("0e:00:00:00:01:01")
+        second_host.setMAC("0e:00:00:00:02:02")
+        third_host.setMAC("0e:00:00:00:02:02")
         third_host.setIP(second_host.IP())
-        self.one_ipv4_ping(
-            first_host, second_host.IP(), require_host_learned=False)
-        self.one_ipv4_ping(
-            second_host, first_host.IP(), require_host_learned=False)
-        self.set_port_down(self.port_map['port_2'])
-        self.one_ipv4_ping(
-            first_host, third_host.IP(), require_host_learned=False)
-        self.one_ipv4_ping(
-            third_host, first_host.IP(), require_host_learned=False)
+        self.one_ipv4_ping(first_host, second_host.IP(), require_host_learned=False)
+        self.one_ipv4_ping(second_host, first_host.IP(), require_host_learned=False)
+        self.set_port_down(self.port_map["port_2"])
+        self.one_ipv4_ping(first_host, third_host.IP(), require_host_learned=False)
+        self.one_ipv4_ping(third_host, first_host.IP(), require_host_learned=False)
 
 
 class FaucetNailedFailoverForwardingOrderedTest(FaucetNailedForwardingTest):
-
     NUM_FAUCET_CONTROLLERS = 1
 
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 acls:
@@ -2576,42 +3051,37 @@
         - rule:
             actions:
                 allow: 0
 """
 
     def test_untagged(self):
         first_host, second_host, third_host = self.hosts_name_ordered()[0:3]
-        first_host.setMAC('0e:00:00:00:01:01')
-        second_host.setMAC('0e:00:00:00:02:02')
-        third_host.setMAC('0e:00:00:00:02:02')
+        first_host.setMAC("0e:00:00:00:01:01")
+        second_host.setMAC("0e:00:00:00:02:02")
+        third_host.setMAC("0e:00:00:00:02:02")
         third_host.setIP(second_host.IP())
-        self.one_ipv4_ping(
-            first_host, second_host.IP(), require_host_learned=False)
-        self.one_ipv4_ping(
-            second_host, first_host.IP(), require_host_learned=False)
-        self.set_port_down(self.port_map['port_2'])
-        self.one_ipv4_ping(
-            first_host, third_host.IP(), require_host_learned=False)
-        self.one_ipv4_ping(
-            third_host, first_host.IP(), require_host_learned=False)
+        self.one_ipv4_ping(first_host, second_host.IP(), require_host_learned=False)
+        self.one_ipv4_ping(second_host, first_host.IP(), require_host_learned=False)
+        self.set_port_down(self.port_map["port_2"])
+        self.one_ipv4_ping(first_host, third_host.IP(), require_host_learned=False)
+        self.one_ipv4_ping(third_host, first_host.IP(), require_host_learned=False)
 
 
 class FaucetUntaggedLLDPBlockedTest(FaucetUntaggedTest):
-
     def test_untagged(self):
         self.ping_all_when_learned()
         self.verify_lldp_blocked()
         # Verify 802.1x flood block triggered.
         self.wait_nonzero_packet_count_flow(
-            {'dl_dst': '01:80:c2:00:00:00/ff:ff:ff:ff:ff:f0'},
-            table_id=self._FLOOD_TABLE)
+            {"dl_dst": "01:80:c2:00:00:00/ff:ff:ff:ff:ff:f0"},
+            table_id=self._FLOOD_TABLE,
+        )
 
 
 class FaucetUntaggedCDPTest(FaucetUntaggedTest):
-
     def test_untagged(self):
         self.ping_all_when_learned()
         self.verify_cdp_blocked()
 
 
 class FaucetTaggedAndUntaggedSameVlanTest(FaucetTest):
     """Test mixture of tagged and untagged hosts on the same VLAN."""
@@ -2636,30 +3106,35 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def setUp(self):
         super().setUp()
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=1, n_untagged=3, links_per_host=self.LINKS_PER_HOST,
-            hw_dpid=self.hw_dpid)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=1,
+            n_untagged=3,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+        )
         self.start_net()
 
     def test_untagged(self):
         """Test connectivity including after port flapping."""
         self.ping_all_when_learned()
         self.flap_all_switch_ports()
         self.ping_all_when_learned()
         self.verify_broadcast()
         self.verify_no_bcast_to_self()
 
 
 class FaucetTaggedAndUntaggedSameVlanEgressTest(FaucetTaggedAndUntaggedSameVlanTest):
-
     REQUIRES_METADATA = True
     CONFIG = """
         egress_pipeline: True
         interfaces:
             %(port_1)d:
                 tagged_vlans: [100]
             %(port_2)d:
@@ -2668,15 +3143,14 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
 
 class FaucetTaggedAndUntaggedSameVlanGroupTest(FaucetTaggedAndUntaggedSameVlanTest):
-
     CONFIG = """
         group_table: True
         interfaces:
             %(port_1)d:
                 tagged_vlans: [100]
             %(port_2)d:
                 native_vlan: 100
@@ -2684,38 +3158,38 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
 
 class FaucetUntaggedMaxHostsTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         max_hosts: 2
 """
 
     CONFIG = CONFIG_BOILER_UNTAGGED
 
     def test_untagged(self):
         self.ping_all()
         learned_hosts = [
-            host for host in self.hosts_name_ordered() if self.host_learned(host)]
+            host for host in self.hosts_name_ordered() if self.host_learned(host)
+        ]
         self.assertEqual(2, len(learned_hosts))
-        self.assertEqual(2, self.scrape_prometheus_var(
-            'vlan_hosts_learned', {'vlan': '100'}))
+        self.assertEqual(
+            2, self.scrape_prometheus_var("vlan_hosts_learned", {"vlan": "100"})
+        )
         self.assertGreater(
-            self.scrape_prometheus_var(
-                'vlan_learn_bans', {'vlan': '100'}), 0)
+            self.scrape_prometheus_var("vlan_learn_bans", {"vlan": "100"}), 0
+        )
 
 
 class FaucetMaxHostsPortTest(FaucetUntaggedTest):
-
     MAX_HOSTS = 3
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 """
 
@@ -2732,111 +3206,125 @@
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         self.ping_all_when_learned()
         for i in range(10, 10 + (self.MAX_HOSTS * 2)):
-            mac_intf = 'mac%u' % i
-            mac_ipv4 = '10.0.0.%u' % i
+            mac_intf = "mac%u" % i
+            mac_ipv4 = "10.0.0.%u" % i
             self.add_macvlan(second_host, mac_intf, ipa=mac_ipv4)
             ping_cmd = mininet_test_util.timeout_cmd(
-                'fping %s -c1 -t1 -I%s %s > /dev/null 2> /dev/null' % (
-                    self.FPING_ARGS_SHORT, mac_intf, first_host.IP()),
-                2)
+                "fping %s -c1 -t1 -I%s %s > /dev/null 2> /dev/null"
+                % (self.FPING_ARGS_SHORT, mac_intf, first_host.IP()),
+                2,
+            )
             second_host.cmd(ping_cmd)
         flows = self.get_matching_flows_on_dpid(
             self.dpid,
-            {'dl_vlan': '100', 'in_port': int(self.port_map['port_2'])},
-            table_id=self._ETH_SRC_TABLE)
+            {"dl_vlan": "100", "in_port": int(self.port_map["port_2"])},
+            table_id=self._ETH_SRC_TABLE,
+        )
         self.assertEqual(self.MAX_HOSTS, len(flows))
-        port_labels = self.port_labels(self.port_map['port_2'])
+        port_labels = self.port_labels(self.port_map["port_2"])
         self.assertGreater(
-            self.scrape_prometheus_var(
-                'port_learn_bans', port_labels), 0)
+            self.scrape_prometheus_var("port_learn_bans", port_labels), 0
+        )
         learned_macs = [
-            mac for _, mac in self.scrape_prometheus_var(
-                'learned_macs', dict(port_labels, vlan=100),
-                multiple=True) if mac]
+            mac
+            for _, mac in self.scrape_prometheus_var(
+                "learned_macs", dict(port_labels, vlan=100), multiple=True
+            )
+            if mac
+        ]
         self.assertEqual(self.MAX_HOSTS, len(learned_macs))
 
 
 class FaucetSingleHostsTimeoutPrometheusTest(FaucetUntaggedTest):
     """Test that hosts learned and reported in Prometheus, time out."""
 
     TIMEOUT = 15
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         timeout: 25
         arp_neighbor_timeout: 12
         nd_neighbor_timeout: 12
         ignore_learn_ins: 0
         learn_jitter: 0
         cache_update_guard_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def hosts_learned(self, hosts):
         """Check that hosts are learned by FAUCET on the expected ports."""
         macs_learned = []
         for mac, port in hosts.items():
             if self.prom_mac_learned(mac, port=port):
                 self.mac_learned(mac, in_port=port)
                 macs_learned.append(mac)
         return macs_learned
 
     def verify_hosts_learned(self, first_host, second_host, mac_ips, hosts):
         mac_ipv4s = [mac_ipv4 for mac_ipv4, _ in mac_ips]
         fping_cmd = mininet_test_util.timeout_cmd(
-            'fping %s -c%u %s' % (
-                self.FPING_ARGS_SHORT, int(self.TIMEOUT / 3), ' '.join(mac_ipv4s)),
-            self.TIMEOUT / 2)
+            "fping %s -c%u %s"
+            % (self.FPING_ARGS_SHORT, int(self.TIMEOUT / 3), " ".join(mac_ipv4s)),
+            self.TIMEOUT / 2,
+        )
         for _ in range(3):
             fping_out = first_host.cmd(fping_cmd)
-            self.assertTrue(fping_out, msg='fping did not complete: %s' % fping_cmd)
+            self.assertTrue(fping_out, msg="fping did not complete: %s" % fping_cmd)
             macs_learned = self.hosts_learned(hosts)
             if len(macs_learned) == len(hosts):
                 return
             time.sleep(1)
-        first_host_diag = first_host.cmd('ifconfig -a ; arp -an')
-        second_host_diag = second_host.cmd('ifconfig -a ; arp -an')
-        self.fail('%s cannot be learned (%s != %s)\nfirst host %s\nsecond host %s\n' % (
-            mac_ips, macs_learned, fping_out, first_host_diag, second_host_diag))
+        first_host_diag = first_host.cmd("ifconfig -a ; arp -an")
+        second_host_diag = second_host.cmd("ifconfig -a ; arp -an")
+        self.fail(
+            "%s cannot be learned (%s != %s)\nfirst host %s\nsecond host %s\n"
+            % (mac_ips, macs_learned, fping_out, first_host_diag, second_host_diag)
+        )
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         all_learned_mac_ports = {}
 
         # learn batches of hosts, then down them
         for base in (10, 20, 30):
+
             def add_macvlans(base, count):
                 mac_intfs = []
                 mac_ips = []
                 learned_mac_ports = {}
                 for i in range(base, base + count):
-                    mac_intf = 'mac%u' % i
+                    mac_intf = "mac%u" % i
                     mac_intfs.append(mac_intf)
-                    mac_ipv4 = '10.0.0.%u' % i
+                    mac_ipv4 = "10.0.0.%u" % i
                     self.add_macvlan(second_host, mac_intf, ipa=mac_ipv4)
                     macvlan_mac = self.get_mac_of_intf(mac_intf, second_host)
-                    learned_mac_ports[macvlan_mac] = self.port_map['port_2']
+                    learned_mac_ports[macvlan_mac] = self.port_map["port_2"]
                     mac_ips.append((mac_ipv4, macvlan_mac))
                 return (mac_intfs, mac_ips, learned_mac_ports)
 
             def down_macvlans(macvlans):
                 for macvlan in macvlans:
-                    second_host.cmd('ip link set dev %s down' % macvlan)
+                    second_host.cmd("ip link set dev %s down" % macvlan)
 
             def learn_then_down_hosts(base, count):
                 mac_intfs, mac_ips, learned_mac_ports = add_macvlans(base, count)
-                self.verify_hosts_learned(first_host, second_host, mac_ips, learned_mac_ports)
+                self.verify_hosts_learned(
+                    first_host, second_host, mac_ips, learned_mac_ports
+                )
                 down_macvlans(mac_intfs)
                 return learned_mac_ports
 
             learned_mac_ports = learn_then_down_hosts(base, 5)
             all_learned_mac_ports.update(learned_mac_ports)
 
         # make sure at least one host still learned
@@ -2844,131 +3332,151 @@
         self.assertTrue(learned_macs)
         before_expiry_learned_macs = learned_macs
 
         # make sure they all eventually expire
         for _ in range(self.TIMEOUT * 3):
             learned_macs = self.hosts_learned(all_learned_mac_ports)
             self.verify_learn_counters(
-                100, list(range(1, len(self.hosts_name_ordered()) + 1)))
+                100, list(range(1, len(self.hosts_name_ordered()) + 1))
+            )
             if not learned_macs:
                 break
             time.sleep(1)
 
-        self.assertFalse(learned_macs, msg='MACs did not expire: %s' % learned_macs)
+        self.assertFalse(learned_macs, msg="MACs did not expire: %s" % learned_macs)
 
         self.assertTrue(before_expiry_learned_macs)
         for mac in before_expiry_learned_macs:
-            self.wait_until_no_matching_flow({'eth_dst': mac}, table_id=self._ETH_DST_TABLE)
+            self.wait_until_no_matching_flow(
+                {"eth_dst": mac}, table_id=self._ETH_DST_TABLE
+            )
 
 
-class FaucetSingleHostsNoIdleTimeoutPrometheusTest(FaucetSingleHostsTimeoutPrometheusTest):
+class FaucetSingleHostsNoIdleTimeoutPrometheusTest(
+    FaucetSingleHostsTimeoutPrometheusTest
+):
 
     """Test broken reset idle timer on flow refresh workaround."""
 
-    CONFIG = """
+    CONFIG = (
+        """
         timeout: 15
         arp_neighbor_timeout: 4
         nd_neighbor_timeout: 4
         ignore_learn_ins: 0
         learn_jitter: 0
         cache_update_guard_time: 1
         idle_dst: False
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
 
 class FaucetSingleL3LearnMACsOnPortTest(FaucetUntaggedTest):
-
     # TODO: currently set to accommodate least hardware
     def _max_hosts():  # pylint: disable=no-method-argument
         return 512
 
     MAX_HOSTS = _max_hosts()
-    TEST_IPV4_NET = '10.0.0.0'
+    TEST_IPV4_NET = "10.0.0.0"
     TEST_IPV4_PREFIX = 16  # must hold more than MAX_HOSTS + 4
-    LEARN_IPV4 = '10.0.254.254'
+    LEARN_IPV4 = "10.0.254.254"
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         max_hosts: %u
         faucet_vips: ["10.0.254.254/16"]
-""" % (_max_hosts() + 4)
+""" % (
+        _max_hosts() + 4
+    )
 
-    CONFIG = ("""
+    CONFIG = (
+        """
         ignore_learn_ins: 0
         metrics_rate_limit_sec: 3
         table_sizes:
             eth_src: %u
             eth_dst: %u
             ipv4_fib: %u
-""" % (_max_hosts() + 64, _max_hosts() + 64, _max_hosts() + 64) + """
+"""
+        % (_max_hosts() + 64, _max_hosts() + 64, _max_hosts() + 64)
+        + """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 max_hosts: 4096
             %(port_2)d:
                 native_vlan: 100
                 max_hosts: 4096
             %(port_3)d:
                 native_vlan: 100
                 max_hosts: 4096
             %(port_4)d:
                 native_vlan: 100
                 max_hosts: 4096
-""")
+"""
+    )
 
     def test_untagged(self):
         test_net = ipaddress.IPv4Network(
-            '%s/%s' % (self.TEST_IPV4_NET, self.TEST_IPV4_PREFIX))
+            "%s/%s" % (self.TEST_IPV4_NET, self.TEST_IPV4_PREFIX)
+        )
         learn_ip = ipaddress.IPv4Address(self.LEARN_IPV4)
         self.verify_learning(test_net, learn_ip, 64, self.MAX_HOSTS)
 
 
 class FaucetSingleL2LearnMACsOnPortTest(FaucetUntaggedTest):
-
     # TODO: currently set to accommodate least hardware
     def _max_hosts():  # pylint: disable=no-method-argument
         return 1024
 
     MAX_HOSTS = _max_hosts()
-    TEST_IPV4_NET = '10.0.0.0'
+    TEST_IPV4_NET = "10.0.0.0"
     TEST_IPV4_PREFIX = 16  # must hold more than MAX_HOSTS + 4
-    LEARN_IPV4 = '10.0.0.1'
+    LEARN_IPV4 = "10.0.0.1"
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         max_hosts: %u
-""" % (_max_hosts() + 4)
+""" % (
+        _max_hosts() + 4
+    )
 
-    CONFIG = ("""
+    CONFIG = (
+        """
         ignore_learn_ins: 0
         metrics_rate_limit_sec: 3
         table_sizes:
             eth_src: %u
             eth_dst: %u
-""" % (_max_hosts() + 64, _max_hosts() + 64) + """
+"""
+        % (_max_hosts() + 64, _max_hosts() + 64)
+        + """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 max_hosts: 4096
             %(port_2)d:
                 native_vlan: 100
                 max_hosts: 4096
             %(port_3)d:
                 native_vlan: 100
                 max_hosts: 4096
             %(port_4)d:
                 native_vlan: 100
                 max_hosts: 4096
-""")
+"""
+    )
 
     def test_untagged(self):
         test_net = ipaddress.IPv4Network(
-            '%s/%s' % (self.TEST_IPV4_NET, self.TEST_IPV4_PREFIX))
+            "%s/%s" % (self.TEST_IPV4_NET, self.TEST_IPV4_PREFIX)
+        )
         learn_ip = ipaddress.IPv4Address(self.LEARN_IPV4)
         self.verify_learning(test_net, learn_ip, 64, self.MAX_HOSTS)
 
 
 class FaucetUntaggedHUPTest(FaucetUntaggedTest):
     """Test handling HUP signal without config change."""
 
@@ -2980,51 +3488,54 @@
             for controller in self.faucet_controllers:
                 count = self.get_configure_count(controller=controller.name)
                 counts.append(count)
             if counts == expected:
                 break
             time.sleep(1)
         self.assertEqual(
-            counts, expected,
-            'Controller configure counts %s != expected counts %s' % (counts, expected))
+            counts,
+            expected,
+            "Controller configure counts %s != expected counts %s" % (counts, expected),
+        )
 
     def test_untagged(self):
         """Test that FAUCET receives HUP signal and keeps switching."""
         init_config_count = self.get_configure_count()
-        reload_type_vars = (
-            'faucet_config_reload_cold',
-            'faucet_config_reload_warm')
+        reload_type_vars = ("faucet_config_reload_cold", "faucet_config_reload_warm")
         reload_vals = {}
         for var in reload_type_vars:
-            reload_vals[var] = self.scrape_prometheus_var(
-                var, dpid=True, default=None)
+            reload_vals[var] = self.scrape_prometheus_var(var, dpid=True, default=None)
         for i in range(init_config_count, init_config_count + 3):
             self._configure_count_with_retry(i)
-            with open(self.faucet_config_path, 'a', encoding='utf-8') as config_file:
-                config_file.write('\n')
+            with open(self.faucet_config_path, "a", encoding="utf-8") as config_file:
+                config_file.write("\n")
             self.verify_faucet_reconf(change_expected=False)
             self._configure_count_with_retry(i + 1)
             self.assertEqual(
                 self.scrape_prometheus_var(
-                    'of_dp_disconnections_total', dpid=True, default=None),
-                0)
+                    "of_dp_disconnections_total", dpid=True, default=None
+                ),
+                0,
+            )
             self.assertEqual(
                 self.scrape_prometheus_var(
-                    'of_dp_connections_total', dpid=True, default=None),
-                1)
+                    "of_dp_connections_total", dpid=True, default=None
+                ),
+                1,
+            )
             self.wait_until_controller_flow()
             self.ping_all_when_learned()
         for var in reload_type_vars:
             self.assertEqual(
                 reload_vals[var],
-                self.scrape_prometheus_var(var, dpid=True, default=None))
+                self.scrape_prometheus_var(var, dpid=True, default=None),
+            )
 
 
 class FaucetChangeVlanACLTest(FaucetTest):
-
     N_UNTAGGED = 4
     N_TAGGED = 0
     LINKS_PER_HOST = 1
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
@@ -3057,52 +3568,62 @@
 """
 
     # pylint: disable=invalid-name
     CONFIG = CONFIG_BOILER_UNTAGGED
 
     def setUp(self):
         super().setUp()
-        self.acl_config_file = os.path.join(self.tmpdir, 'acl.txt')
-        self.CONFIG = '\n'.join(
-            (self.CONFIG, 'include:\n     - %s' % self.acl_config_file))
-        with open(self.acl_config_file, 'w', encoding='utf-8') as acf:
+        self.acl_config_file = os.path.join(self.tmpdir, "acl.txt")
+        self.CONFIG = "\n".join(
+            (self.CONFIG, "include:\n     - %s" % self.acl_config_file)
+        )
+        with open(self.acl_config_file, "w", encoding="utf-8") as acf:
             acf.write(self.START_ACL_CONFIG)
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
-            links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=self.N_TAGGED,
+            n_untagged=self.N_UNTAGGED,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+        )
         self.start_net()
 
     def test_vlan_acl_update(self):
         self.ping_all_when_learned()
         new_yaml_acl_conf = yaml_load(self.UPDATE_ACL_CONFIG)
         self.reload_conf(
-            new_yaml_acl_conf, self.acl_config_file,  # pytype: disable=attribute-error
-            restart=True, cold_start=True)
-        self.wait_until_matching_flow(
-            {'dl_type': 0x800}, table_id=self._VLAN_ACL_TABLE)
-        self.wait_until_matching_flow(
-            {'dl_type': 0x806}, table_id=self._VLAN_ACL_TABLE)
+            new_yaml_acl_conf,
+            self.acl_config_file,  # pytype: disable=attribute-error
+            restart=True,
+            cold_start=True,
+        )
+        self.wait_until_matching_flow({"dl_type": 0x800}, table_id=self._VLAN_ACL_TABLE)
+        self.wait_until_matching_flow({"dl_type": 0x806}, table_id=self._VLAN_ACL_TABLE)
         self.ping_all_when_learned()
         orig_yaml_acl_conf = yaml_load(self.START_ACL_CONFIG)
         self.reload_conf(
-            orig_yaml_acl_conf, self.acl_config_file,  # pytype: disable=attribute-error
-            restart=True, cold_start=True)
-        self.wait_until_matching_flow(
-            {'dl_type': 0x800}, table_id=self._VLAN_ACL_TABLE)
+            orig_yaml_acl_conf,
+            self.acl_config_file,  # pytype: disable=attribute-error
+            restart=True,
+            cold_start=True,
+        )
+        self.wait_until_matching_flow({"dl_type": 0x800}, table_id=self._VLAN_ACL_TABLE)
         self.wait_until_no_matching_flow(
-            {'dl_type': 0x806}, table_id=self._VLAN_ACL_TABLE)
+            {"dl_type": 0x806}, table_id=self._VLAN_ACL_TABLE
+        )
         self.ping_all_when_learned()
 
 
 class FaucetIPv4TupleTest(FaucetTest):
-
     MAX_RULES = 1024
     ETH_TYPE = IPV4_ETH
-    NET_BASE = ipaddress.IPv4Network('10.0.0.0/16')
+    NET_BASE = ipaddress.IPv4Network("10.0.0.0/16")
     N_UNTAGGED = 4
     N_TAGGED = 0
     LINKS_PER_HOST = 1
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
@@ -3128,65 +3649,75 @@
         ipv4_src: 127.0.0.1
         tcp_dst: 65535
         tcp_src: 65535
 """
 
     def setUp(self):
         super().setUp()
-        self.acl_config_file = os.path.join(self.tmpdir, 'acl.txt')
-        self.CONFIG = '\n'.join(
-            (self.CONFIG, 'include:\n     - %s' % self.acl_config_file))
-        with open(self.acl_config_file, 'w', encoding='utf-8') as acf:
+        self.acl_config_file = os.path.join(self.tmpdir, "acl.txt")
+        self.CONFIG = "\n".join(
+            (self.CONFIG, "include:\n     - %s" % self.acl_config_file)
+        )
+        with open(self.acl_config_file, "w", encoding="utf-8") as acf:
             acf.write(self.START_ACL_CONFIG)
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
-            links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=self.N_TAGGED,
+            n_untagged=self.N_UNTAGGED,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+        )
         self.start_net()
 
     def _push_tuples(self, eth_type, host_ips):
         max_rules = len(host_ips)
         rules = 1
         while rules <= max_rules:
             rules_yaml = []
             for rule in range(rules):
                 host_ip = host_ips[rule]
                 port = (rule + 1) % 2**16
                 ip_match = str(host_ip)
                 rule_yaml = {
-                    'eth_type': eth_type,
-                    'ip_proto': 6,
-                    'tcp_src': port,
-                    'tcp_dst': port,
-                    'ipv%u_src' % host_ip.version: ip_match,
-                    'ipv%u_dst' % host_ip.version: ip_match,
-                    'actions': {'allow': 1},
+                    "eth_type": eth_type,
+                    "ip_proto": 6,
+                    "tcp_src": port,
+                    "tcp_dst": port,
+                    "ipv%u_src" % host_ip.version: ip_match,
+                    "ipv%u_dst" % host_ip.version: ip_match,
+                    "actions": {"allow": 1},
                 }
-                rules_yaml.append({'rule': rule_yaml})
-            yaml_acl_conf = {'acls': {1: {'exact_match': True, 'rules': rules_yaml}}}
-            tuple_txt = '%u IPv%u tuples\n' % (len(rules_yaml), host_ip.version)
-            error('pushing %s' % tuple_txt)
+                rules_yaml.append({"rule": rule_yaml})
+            yaml_acl_conf = {"acls": {1: {"exact_match": True, "rules": rules_yaml}}}
+            tuple_txt = "%u IPv%u tuples\n" % (len(rules_yaml), host_ip.version)
+            error("pushing %s" % tuple_txt)
             self.reload_conf(
-                yaml_acl_conf, self.acl_config_file,  # pytype: disable=attribute-error
-                restart=True, cold_start=False)
-            error('pushed %s' % tuple_txt)
+                yaml_acl_conf,
+                self.acl_config_file,  # pytype: disable=attribute-error
+                restart=True,
+                cold_start=False,
+            )
+            error("pushed %s" % tuple_txt)
             self.wait_until_matching_flow(
-                {'tp_src': port, 'ip_proto': 6, 'dl_type': eth_type}, table_id=0)
+                {"tp_src": port, "ip_proto": 6, "dl_type": eth_type}, table_id=0
+            )
             rules *= 2
 
     def test_tuples(self):
         host_ips = list(itertools.islice(self.NET_BASE.hosts(), self.MAX_RULES))
         self._push_tuples(self.ETH_TYPE, host_ips)
 
 
 class FaucetIPv6TupleTest(FaucetIPv4TupleTest):
-
     MAX_RULES = 1024
     ETH_TYPE = IPV6_ETH
-    NET_BASE = ipaddress.IPv6Network('fc00::00/64')
+    NET_BASE = ipaddress.IPv6Network("fc00::00/64")
     START_ACL_CONFIG = """
 acls:
   1:
     exact_match: True
     rules:
     - rule:
         actions: {allow: 1}
@@ -3195,15 +3726,14 @@
         ipv6_dst: ::1
         ipv6_src: ::1
         tcp_dst: 65535
         tcp_src: 65535
 """
 
 
-
 class FaucetConfigReloadTestBase(FaucetTest):
     """Test handling HUP signal with config change."""
 
     N_UNTAGGED = 4
     N_TAGGED = 0
     LINKS_PER_HOST = 1
     CONFIG_GLOBAL = """
@@ -3316,29 +3846,35 @@
                 allow: 1
 """
     ACL_COOKIE = None
 
     def setUp(self):
         super().setUp()
         self.ACL_COOKIE = random.randint(1, 2**16 - 1)
-        self.ACL = self.ACL.replace('COOKIE', str(self.ACL_COOKIE))
-        self.acl_config_file = '%s/acl.yaml' % self.tmpdir
-        with open(self.acl_config_file, 'w', encoding='utf-8') as config_file:
+        self.ACL = self.ACL.replace("COOKIE", str(self.ACL_COOKIE))
+        self.acl_config_file = "%s/acl.yaml" % self.tmpdir
+        with open(self.acl_config_file, "w", encoding="utf-8") as config_file:
             config_file.write(self.ACL)
-        self.CONFIG = '\n'.join(
-            (self.CONFIG, 'include:\n     - %s' % self.acl_config_file))
+        self.CONFIG = "\n".join(
+            (self.CONFIG, "include:\n     - %s" % self.acl_config_file)
+        )
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
-            links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=self.N_TAGGED,
+            n_untagged=self.N_UNTAGGED,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+        )
         self.start_net()
 
 
 class FaucetDelPortTest(FaucetConfigReloadTestBase):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
     200:
         description: "untagged"
 """
@@ -3354,163 +3890,209 @@
             %(port_4)d:
                 native_vlan: 200
 """
 
     def test_port_down_flow_gone(self):
         last_host = self.hosts_name_ordered()[-1]
         self.require_host_learned(last_host)
-        second_host_dst_match = {'eth_dst': last_host.MAC()}
+        second_host_dst_match = {"eth_dst": last_host.MAC()}
         self.wait_until_matching_flow(
-            second_host_dst_match, table_id=self._ETH_DST_TABLE)
+            second_host_dst_match, table_id=self._ETH_DST_TABLE
+        )
         self.change_port_config(
-            self.port_map['port_4'], None, None,
-            restart=True, cold_start=None)
+            self.port_map["port_4"], None, None, restart=True, cold_start=None
+        )
         self.wait_until_no_matching_flow(
-            second_host_dst_match, table_id=self._ETH_DST_TABLE)
+            second_host_dst_match, table_id=self._ETH_DST_TABLE
+        )
 
 
 class FaucetConfigReloadTest(FaucetConfigReloadTestBase):
-
     def test_add_unknown_dp(self):
         conf = self._get_faucet_conf()
-        conf['dps']['unknown'] = {
-            'dp_id': int(self.rand_dpid()),
-            'hardware': 'Open vSwitch',
+        conf["dps"]["unknown"] = {
+            "dp_id": int(self.rand_dpid()),
+            "hardware": "Open vSwitch",
         }
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=False, change_expected=False)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=False,
+        )
 
     def test_tabs_are_bad(self):
         self._enable_event_log()
 
         self.ping_all_when_learned()
-        self.assertEqual(0, self.scrape_prometheus_var('faucet_config_load_error', dpid=False))
-        event = self._wait_until_matching_event(lambda event: event['CONFIG_CHANGE']['success'])
-        good_config_hash_info = event['CONFIG_CHANGE']['config_hash_info']
-        self.assertNotEqual('', good_config_hash_info['hashes'])
+        self.assertEqual(
+            0, self.scrape_prometheus_var("faucet_config_load_error", dpid=False)
+        )
+        event = self._wait_until_matching_event(
+            lambda event: event["CONFIG_CHANGE"]["success"]
+        )
+        good_config_hash_info = event["CONFIG_CHANGE"]["config_hash_info"]
+        self.assertNotEqual("", good_config_hash_info["hashes"])
 
         orig_conf = self._get_faucet_conf()
-        self.force_faucet_reload(
-            '\t'.join(('tabs', 'are', 'bad')))
-        self.assertEqual(1, self.scrape_prometheus_var('faucet_config_load_error', dpid=False))
-        event = self._wait_until_matching_event(lambda event: not event['CONFIG_CHANGE']['success'])
-        self.assertEqual('', event['CONFIG_CHANGE']['config_hash_info']['hashes'])
+        self.force_faucet_reload("\t".join(("tabs", "are", "bad")))
+        self.assertEqual(
+            1, self.scrape_prometheus_var("faucet_config_load_error", dpid=False)
+        )
+        event = self._wait_until_matching_event(
+            lambda event: not event["CONFIG_CHANGE"]["success"]
+        )
+        self.assertEqual("", event["CONFIG_CHANGE"]["config_hash_info"]["hashes"])
 
         self.ping_all_when_learned()
         self.reload_conf(
-            orig_conf, self.faucet_config_path,
-            restart=True, cold_start=False, change_expected=False)
-        self.assertEqual(0, self.scrape_prometheus_var('faucet_config_load_error', dpid=False))
-        event = self._wait_until_matching_event(lambda event: event['CONFIG_CHANGE']['success'])
-        self.assertEqual(good_config_hash_info, event['CONFIG_CHANGE']['config_hash_info'])
+            orig_conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=False,
+        )
+        self.assertEqual(
+            0, self.scrape_prometheus_var("faucet_config_load_error", dpid=False)
+        )
+        event = self._wait_until_matching_event(
+            lambda event: event["CONFIG_CHANGE"]["success"]
+        )
+        self.assertEqual(
+            good_config_hash_info, event["CONFIG_CHANGE"]["config_hash_info"]
+        )
 
     def test_port_change_vlan(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         third_host, fourth_host = self.hosts_name_ordered()[2:]
         self.ping_all_when_learned()
         self.change_port_config(
-            self.port_map['port_1'], 'native_vlan', 200,
-            restart=False, cold_start=False)
+            self.port_map["port_1"], "native_vlan", 200, restart=False, cold_start=False
+        )
         self.wait_until_matching_flow(
-            {'vlan_vid': 200}, table_id=self._ETH_SRC_TABLE,
-            actions=['OUTPUT:CONTROLLER', 'GOTO_TABLE:%u' % self._ETH_DST_TABLE])
+            {"vlan_vid": 200},
+            table_id=self._ETH_SRC_TABLE,
+            actions=["OUTPUT:CONTROLLER", "GOTO_TABLE:%u" % self._ETH_DST_TABLE],
+        )
         self.change_port_config(
-            self.port_map['port_2'], 'native_vlan', 200,
-            restart=True, cold_start=False)
-        for port_name in ('port_1', 'port_2'):
+            self.port_map["port_2"], "native_vlan", 200, restart=True, cold_start=False
+        )
+        for port_name in ("port_1", "port_2"):
             self.wait_until_matching_flow(
-                {'in_port': int(self.port_map[port_name])},
+                {"in_port": int(self.port_map[port_name])},
                 table_id=self._VLAN_TABLE,
-                actions=['SET_FIELD: {vlan_vid:4296}'])
+                actions=["SET_FIELD: {vlan_vid:4296}"],
+            )
         self.one_ipv4_ping(first_host, second_host.IP(), require_host_learned=False)
         # hosts 1 and 2 now in VLAN 200, so they shouldn't see floods for 3 and 4.
-        self.verify_vlan_flood_limited(
-            third_host, fourth_host, first_host)
+        self.verify_vlan_flood_limited(third_host, fourth_host, first_host)
 
     def test_port_change_acl(self):
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         orig_conf = self._get_faucet_conf()
-        self.change_port_config(
-            self.port_map['port_1'], 'acl_in', 1,
-            cold_start=False)
+        self.change_port_config(self.port_map["port_1"], "acl_in", 1, cold_start=False)
         self.wait_until_matching_flow(
-            {'in_port': int(self.port_map['port_1']),
-             'eth_type': IPV4_ETH, 'tcp_dst': 5001, 'ip_proto': 6},
-            table_id=self._PORT_ACL_TABLE, cookie=self.ACL_COOKIE)
+            {
+                "in_port": int(self.port_map["port_1"]),
+                "eth_type": IPV4_ETH,
+                "tcp_dst": 5001,
+                "ip_proto": 6,
+            },
+            table_id=self._PORT_ACL_TABLE,
+            cookie=self.ACL_COOKIE,
+        )
         self.wait_until_matching_flow(
-            {'vlan_vid': 100}, table_id=self._ETH_SRC_TABLE,
-            actions=['OUTPUT:CONTROLLER', 'GOTO_TABLE:%u' % self._ETH_DST_TABLE])
+            {"vlan_vid": 100},
+            table_id=self._ETH_SRC_TABLE,
+            actions=["OUTPUT:CONTROLLER", "GOTO_TABLE:%u" % self._ETH_DST_TABLE],
+        )
         self.verify_tp_dst_blocked(5001, first_host, second_host)
         self.verify_tp_dst_notblocked(5002, first_host, second_host)
         self.reload_conf(
-            orig_conf, self.faucet_config_path,
-            restart=True, cold_start=False, host_cache=100)
-        self.verify_tp_dst_notblocked(
-            5001, first_host, second_host, table_id=None)
-        self.verify_tp_dst_notblocked(
-            5002, first_host, second_host, table_id=None)
+            orig_conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            host_cache=100,
+        )
+        self.verify_tp_dst_notblocked(5001, first_host, second_host, table_id=None)
+        self.verify_tp_dst_notblocked(5002, first_host, second_host, table_id=None)
 
     def test_port_change_perm_learn(self):
         first_host, second_host, third_host = self.hosts_name_ordered()[0:3]
         self.change_port_config(
-            self.port_map['port_1'], 'permanent_learn', True,
-            restart=True, cold_start=False)
+            self.port_map["port_1"],
+            "permanent_learn",
+            True,
+            restart=True,
+            cold_start=False,
+        )
         self.ping_all_when_learned(hard_timeout=0)
         original_third_host_mac = third_host.MAC()
         third_host.setMAC(first_host.MAC())
         self.assertEqual(100.0, self.ping((second_host, third_host)))
         self.retry_net_ping(hosts=(first_host, second_host))
         third_host.setMAC(original_third_host_mac)
         self.ping_all_when_learned(hard_timeout=0)
         self.change_port_config(
-            self.port_map['port_1'], 'acl_in', 1,
-            restart=True, cold_start=False)
+            self.port_map["port_1"], "acl_in", 1, restart=True, cold_start=False
+        )
         self.wait_until_matching_flow(
-            {'in_port': int(self.port_map['port_1']),
-             'eth_type': IPV4_ETH, 'tcp_dst': 5001, 'ip_proto': 6},
-            table_id=self._PORT_ACL_TABLE)
+            {
+                "in_port": int(self.port_map["port_1"]),
+                "eth_type": IPV4_ETH,
+                "tcp_dst": 5001,
+                "ip_proto": 6,
+            },
+            table_id=self._PORT_ACL_TABLE,
+        )
         self.verify_tp_dst_blocked(5001, first_host, second_host)
         self.verify_tp_dst_notblocked(5002, first_host, second_host)
 
 
 class FaucetDeleteConfigReloadTest(FaucetConfigReloadTestBase):
-
     def test_delete_interface(self):
         # With all ports changed, we should cold start.
         conf = self._get_faucet_conf()
-        del conf['dps'][self.DP_NAME]['interfaces']
-        conf['dps'][self.DP_NAME]['interfaces'] = {
-            int(self.port_map['port_1']): {
-                'native_vlan': 100,
-                'tagged_vlans': [200],
+        del conf["dps"][self.DP_NAME]["interfaces"]
+        conf["dps"][self.DP_NAME]["interfaces"] = {
+            int(self.port_map["port_1"]): {
+                "native_vlan": 100,
+                "tagged_vlans": [200],
             }
         }
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=True, change_expected=True)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=True,
+            change_expected=True,
+        )
 
 
 class FaucetRouterConfigReloadTest(FaucetConfigReloadTestBase):
-
     def test_router_config_reload(self):
         conf = self._get_faucet_conf()
-        conf['routers'] = {
-            'router-1': {
-                'vlans': [100, 200],
+        conf["routers"] = {
+            "router-1": {
+                "vlans": [100, 200],
             }
         }
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=True, change_expected=True)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=True,
+            change_expected=True,
+        )
 
 
 class FaucetConfigReloadAclTest(FaucetConfigReloadTestBase):
-
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acls_in: [allow]
             %(port_2)d:
                 native_vlan: 100
@@ -3523,57 +4105,70 @@
                 acl_in: deny
 """
 
     def _verify_hosts_learned(self, hosts):
         self.ping_all()
         for host in hosts:
             self.require_host_learned(host)
-        self.assertEqual(len(hosts), self.scrape_prometheus_var(
-            'vlan_hosts_learned', {'vlan': '100'}))
+        self.assertEqual(
+            len(hosts),
+            self.scrape_prometheus_var("vlan_hosts_learned", {"vlan": "100"}),
+        )
 
     def test_port_acls(self):
         hup = not self.STAT_RELOAD
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
         self._verify_hosts_learned((first_host, second_host))
         self.change_port_config(
-            self.port_map['port_3'], 'acl_in', 'allow',
-            restart=True, cold_start=False, hup=hup)
+            self.port_map["port_3"],
+            "acl_in",
+            "allow",
+            restart=True,
+            cold_start=False,
+            hup=hup,
+        )
         self.change_port_config(
-            self.port_map['port_1'], 'acls_in', [3, 4, 'allow'],
-            restart=True, cold_start=False, hup=hup)
+            self.port_map["port_1"],
+            "acls_in",
+            [3, 4, "allow"],
+            restart=True,
+            cold_start=False,
+            hup=hup,
+        )
         self.coldstart_conf(hup=hup)
         self._verify_hosts_learned((first_host, second_host, third_host))
         self.verify_tp_dst_blocked(5001, first_host, second_host)
         self.verify_tp_dst_notblocked(5002, first_host, second_host)
         self.verify_tp_dst_blocked(5003, first_host, second_host)
 
 
 class FaucetConfigReloadMACFlushTest(FaucetConfigReloadTestBase):
-
     def test_port_change_vlan(self):
         self.ping_all_when_learned()
-        self.assertEqual(4, len(self.scrape_prometheus(var='learned_l2_port')))
+        self.assertEqual(4, len(self.scrape_prometheus(var="learned_l2_port")))
         self.change_port_config(
-            self.port_map['port_1'], 'native_vlan', 200,
-            restart=False, cold_start=False)
+            self.port_map["port_1"], "native_vlan", 200, restart=False, cold_start=False
+        )
         self.wait_until_matching_flow(
-            {'vlan_vid': 200}, table_id=self._ETH_SRC_TABLE,
-            actions=['OUTPUT:CONTROLLER', 'GOTO_TABLE:%u' % self._ETH_DST_TABLE])
+            {"vlan_vid": 200},
+            table_id=self._ETH_SRC_TABLE,
+            actions=["OUTPUT:CONTROLLER", "GOTO_TABLE:%u" % self._ETH_DST_TABLE],
+        )
         self.change_port_config(
-            self.port_map['port_2'], 'native_vlan', 200,
-            restart=True, cold_start=False)
+            self.port_map["port_2"], "native_vlan", 200, restart=True, cold_start=False
+        )
         self.wait_until_matching_flow(
-            {'in_port': int(self.port_map['port_2'])},
+            {"in_port": int(self.port_map["port_2"])},
             table_id=self._VLAN_TABLE,
-            actions=['SET_FIELD: {vlan_vid:4296}'])
-        self.assertLess(len(self.scrape_prometheus(var='learned_l2_port')), 4)
+            actions=["SET_FIELD: {vlan_vid:4296}"],
+        )
+        self.assertLess(len(self.scrape_prometheus(var="learned_l2_port")), 4)
 
 
 class FaucetConfigReloadEmptyAclTest(FaucetConfigReloadTestBase):
-
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
             %(port_2)d:
                 native_vlan: 100
             %(port_3)d:
@@ -3588,170 +4183,204 @@
     200:
         description: "untagged"
     300:
         description: "untagged"
         acls_in: [1]
 """
 
-    STAT_RELOAD = '1'
+    STAT_RELOAD = "1"
 
     def test_port_acls(self):
         hup = not self.STAT_RELOAD
         self.change_port_config(
-            self.port_map['port_3'], 'acls_in', [],
-            restart=True, cold_start=False, hup=hup, change_expected=False)
+            self.port_map["port_3"],
+            "acls_in",
+            [],
+            restart=True,
+            cold_start=False,
+            hup=hup,
+            change_expected=False,
+        )
         self.change_port_config(
-            self.port_map['port_1'], 'acls_in', [],
-            restart=True, cold_start=False, hup=hup, change_expected=False)
+            self.port_map["port_1"],
+            "acls_in",
+            [],
+            restart=True,
+            cold_start=False,
+            hup=hup,
+            change_expected=False,
+        )
 
 
 class FaucetConfigStatReloadAclTest(FaucetConfigReloadAclTest):
-
     # Use the stat-based reload method.
-    STAT_RELOAD = '1'
+    STAT_RELOAD = "1"
 
 
 class FaucetUntaggedBGPDualstackDefaultRouteTest(FaucetUntaggedTest):
     """Test IPv4 routing and import default route from BGP."""
 
     NUM_FAUCET_CONTROLLERS = 1
 
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["10.0.0.254/24", "fc00::1:254/112"]
 routers:
     router1:
         bgp:
             as: 1
             connect_mode: "passive"
             port: %(bgp_port)d
             routerid: "1.1.1.1"
             server_addresses: ["127.0.0.1", "::1"]
             neighbor_addresses: ["127.0.0.1", "::1"]
             vlan: 100
-""" + """
+"""
+        + """
             neighbor_as: %u
-""" % PEER_BGP_AS
+"""
+        % PEER_BGP_AS
+    )
 
-    CONFIG = """
+    CONFIG = (
+        """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     exabgp_peer_conf = """
     static {
       route 0.0.0.0/0 next-hop 10.0.0.1 local-preference 100;
     }
 """
     exabgp_log = None
     exabgp_err = None
-    config_ports = {'bgp_port': None}
+    config_ports = {"bgp_port": None}
 
     def post_start_net(self):
         exabgp_conf = self.get_exabgp_conf(
-            mininet_test_util.LOCALHOST, self.exabgp_peer_conf)
+            mininet_test_util.LOCALHOST, self.exabgp_peer_conf
+        )
         self.exabgp_log, self.exabgp_err = self.start_exabgp(exabgp_conf)
 
     def test_untagged(self):
         """Test IPv4 routing, and BGP routes received."""
         self.assertEqual(self.NUM_FAUCET_CONTROLLERS, 1)
         first_host, second_host = self.hosts_name_ordered()[:2]
-        first_host_alias_ip = ipaddress.ip_interface('10.99.99.99/24')
+        first_host_alias_ip = ipaddress.ip_interface("10.99.99.99/24")
         first_host_alias_host_ip = ipaddress.ip_interface(
-            ipaddress.ip_network(first_host_alias_ip.ip))
+            ipaddress.ip_network(first_host_alias_ip.ip)
+        )
         self.host_ipv4_alias(first_host, first_host_alias_ip)
         self.wait_bgp_up(
-            mininet_test_util.LOCALHOST, 100, self.exabgp_log, self.exabgp_err)
+            mininet_test_util.LOCALHOST, 100, self.exabgp_log, self.exabgp_err
+        )
         self.assertGreater(
             self.scrape_prometheus_var(
-                'bgp_neighbor_routes', {'ipv': '4', 'vlan': '100'}, default=0),
-            0)
+                "bgp_neighbor_routes", {"ipv": "4", "vlan": "100"}, default=0
+            ),
+            0,
+        )
         self.wait_exabgp_sent_updates(self.exabgp_log)
-        self.add_host_route(
-            second_host, first_host_alias_host_ip, self.FAUCET_VIPV4.ip)
+        self.add_host_route(second_host, first_host_alias_host_ip, self.FAUCET_VIPV4.ip)
         for _ in range(2):
             self.one_ipv4_ping(second_host, first_host_alias_ip.ip)
             self.one_ipv4_controller_ping(first_host)
             self.coldstart_conf()
 
 
 class FaucetUntaggedBGPIPv4DefaultRouteTest(FaucetUntaggedTest):
     """Test IPv4 routing and import default route from BGP."""
 
     NUM_FAUCET_CONTROLLERS = 1
 
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["10.0.0.254/24"]
 routers:
     router1:
         bgp:
             as: 1
             connect_mode: "passive"
             port: %(bgp_port)d
             routerid: "1.1.1.1"
             server_addresses: ["127.0.0.1"]
             neighbor_addresses: ["127.0.0.1"]
             vlan: 100
-""" + """
+"""
+        + """
             neighbor_as: %u
-""" % PEER_BGP_AS
+"""
+        % PEER_BGP_AS
+    )
 
-    CONFIG = """
+    CONFIG = (
+        """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     exabgp_peer_conf = """
     static {
       route 0.0.0.0/0 next-hop 10.0.0.1 local-preference 100;
     }
 """
     exabgp_log = None
     exabgp_err = None
-    config_ports = {'bgp_port': None}
+    config_ports = {"bgp_port": None}
 
     def post_start_net(self):
         exabgp_conf = self.get_exabgp_conf(
-            mininet_test_util.LOCALHOST, self.exabgp_peer_conf)
+            mininet_test_util.LOCALHOST, self.exabgp_peer_conf
+        )
         self.exabgp_log, self.exabgp_err = self.start_exabgp(exabgp_conf)
 
     def test_untagged(self):
         """Test IPv4 routing, and BGP routes received."""
         self.assertEqual(self.NUM_FAUCET_CONTROLLERS, 1)
         first_host, second_host = self.hosts_name_ordered()[:2]
-        first_host_alias_ip = ipaddress.ip_interface('10.99.99.99/24')
+        first_host_alias_ip = ipaddress.ip_interface("10.99.99.99/24")
         first_host_alias_host_ip = ipaddress.ip_interface(
-            ipaddress.ip_network(first_host_alias_ip.ip))
+            ipaddress.ip_network(first_host_alias_ip.ip)
+        )
         self.host_ipv4_alias(first_host, first_host_alias_ip)
         self.wait_bgp_up(
-            mininet_test_util.LOCALHOST, 100, self.exabgp_log, self.exabgp_err)
+            mininet_test_util.LOCALHOST, 100, self.exabgp_log, self.exabgp_err
+        )
         self.assertGreater(
             self.scrape_prometheus_var(
-                'bgp_neighbor_routes', {'ipv': '4', 'vlan': '100'}, default=0),
-            0)
+                "bgp_neighbor_routes", {"ipv": "4", "vlan": "100"}, default=0
+            ),
+            0,
+        )
         self.wait_exabgp_sent_updates(self.exabgp_log)
-        self.add_host_route(
-            second_host, first_host_alias_host_ip, self.FAUCET_VIPV4.ip)
+        self.add_host_route(second_host, first_host_alias_host_ip, self.FAUCET_VIPV4.ip)
         self.one_ipv4_ping(second_host, first_host_alias_ip.ip)
         self.one_ipv4_controller_ping(first_host)
         self.coldstart_conf()
 
 
 class FaucetUntaggedBGPIPv4RouteTest(FaucetUntaggedTest):
     """Test IPv4 routing and import from BGP."""
 
     NUM_FAUCET_CONTROLLERS = 1
     NUM_GAUGE_CONTROLLERS = 1
 
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["10.0.0.254/24"]
         routes:
             - route:
                 ip_dst: 10.99.99.0/24
@@ -3762,73 +4391,86 @@
             as: 1
             connect_mode: "passive"
             port: %(bgp_port)d
             routerid: "1.1.1.1"
             server_addresses: ["127.0.0.1"]
             neighbor_addresses: ["127.0.0.1"]
             vlan: 100
-""" + """
+"""
+        + """
             neighbor_as: %u
-""" % PEER_BGP_AS
+"""
+        % PEER_BGP_AS
+    )
 
-    CONFIG = """
+    CONFIG = (
+        """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     exabgp_peer_conf = """
     static {
       route 10.0.1.0/24 next-hop 10.0.0.1 local-preference 100;
       route 10.0.2.0/24 next-hop 10.0.0.2 local-preference 100;
       route 10.0.3.0/24 next-hop 10.0.0.2 local-preference 100;
       route 10.0.4.0/24 next-hop 10.0.0.254;
       route 10.0.5.0/24 next-hop 10.10.0.1;
    }
 """
     exabgp_log = None
     exabgp_err = None
-    config_ports = {'bgp_port': None}
+    config_ports = {"bgp_port": None}
 
     def post_start_net(self):
         exabgp_conf = self.get_exabgp_conf(
-            mininet_test_util.LOCALHOST, self.exabgp_peer_conf)
+            mininet_test_util.LOCALHOST, self.exabgp_peer_conf
+        )
         self.exabgp_log, self.exabgp_err = self.start_exabgp(exabgp_conf)
 
     def test_untagged(self):
         """Test IPv4 routing, and BGP routes received."""
         self.assertEqual(self.NUM_FAUCET_CONTROLLERS, 1)
         first_host, second_host = self.hosts_name_ordered()[:2]
         # wait until 10.0.0.1 has been resolved
         self.wait_for_route_as_flow(
-            first_host.MAC(), ipaddress.IPv4Network('10.99.99.0/24'))
+            first_host.MAC(), ipaddress.IPv4Network("10.99.99.0/24")
+        )
         self.wait_bgp_up(
-            mininet_test_util.LOCALHOST, 100, self.exabgp_log, self.exabgp_err)
+            mininet_test_util.LOCALHOST, 100, self.exabgp_log, self.exabgp_err
+        )
         self.assertGreater(
             self.scrape_prometheus_var(
-                'bgp_neighbor_routes', {'ipv': '4', 'vlan': '100'}),
-            0)
+                "bgp_neighbor_routes", {"ipv": "4", "vlan": "100"}
+            ),
+            0,
+        )
         self.wait_exabgp_sent_updates(self.exabgp_log)
-        self.verify_invalid_bgp_route(r'.+10.0.4.0\/24.+cannot be us$')
-        self.verify_invalid_bgp_route(r'.+10.0.5.0\/24.+because nexthop not in VLAN.+')
+        self.verify_invalid_bgp_route(r".+10.0.4.0\/24.+cannot be us$")
+        self.verify_invalid_bgp_route(r".+10.0.5.0\/24.+because nexthop not in VLAN.+")
         self.wait_for_route_as_flow(
-            second_host.MAC(), ipaddress.IPv4Network('10.0.3.0/24'))
+            second_host.MAC(), ipaddress.IPv4Network("10.0.3.0/24")
+        )
         self.verify_ipv4_routing_mesh()
         self.flap_all_switch_ports()
         self.verify_ipv4_routing_mesh()
         for host in first_host, second_host:
             self.one_ipv4_controller_ping(host)
         self.verify_traveling_dhcp_mac()
 
 
 class FaucetUntaggedIPv4RouteTest(FaucetUntaggedTest):
     """Test IPv4 routing and export to BGP."""
 
     NUM_FAUCET_CONTROLLERS = 1
 
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["10.0.0.254/24"]
         routes:
             - route:
                 ip_dst: "10.0.1.0/24"
@@ -3845,71 +4487,80 @@
             as: 1
             connect_mode: "passive"
             port: %(bgp_port)d
             routerid: "1.1.1.1"
             server_addresses: ["127.0.0.1"]
             neighbor_addresses: ["127.0.0.1"]
             vlan: 100
-""" + """
+"""
+        + """
             neighbor_as: %u
-""" % PEER_BGP_AS
+"""
+        % PEER_BGP_AS
+    )
 
-    CONFIG = """
+    CONFIG = (
+        """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     exabgp_log = None
     exabgp_err = None
-    config_ports = {'bgp_port': None}
+    config_ports = {"bgp_port": None}
 
     def post_start_net(self):
         exabgp_conf = self.get_exabgp_conf(mininet_test_util.LOCALHOST)
         self.exabgp_log, self.exabgp_err = self.start_exabgp(exabgp_conf)
 
     def test_untagged(self):
         """Test IPv4 routing, and BGP routes sent."""
         self.verify_ipv4_routing_mesh()
         self.flap_all_switch_ports()
         self.verify_ipv4_routing_mesh()
         self.wait_bgp_up(
-            mininet_test_util.LOCALHOST, 100, self.exabgp_log, self.exabgp_err)
+            mininet_test_util.LOCALHOST, 100, self.exabgp_log, self.exabgp_err
+        )
         self.assertGreater(
             self.scrape_prometheus_var(
-                'bgp_neighbor_routes', {'ipv': '4', 'vlan': '100'}),
-            0)
+                "bgp_neighbor_routes", {"ipv": "4", "vlan": "100"}
+            ),
+            0,
+        )
         # exabgp should have received our BGP updates
         updates = self.exabgp_updates(self.exabgp_log)
         for route_string in (
-                '10.0.0.0/24 next-hop 10.0.0.254',
-                '10.0.1.0/24 next-hop 10.0.0.1',
-                '10.0.2.0/24 next-hop 10.0.0.2',
-                '10.0.3.0/24 next-hop 10.0.0.2'):
+            "10.0.0.0/24 next-hop 10.0.0.254",
+            "10.0.1.0/24 next-hop 10.0.0.1",
+            "10.0.2.0/24 next-hop 10.0.0.2",
+            "10.0.3.0/24 next-hop 10.0.0.2",
+        ):
             self.assertTrue(re.search(route_string, updates), msg=updates)
         # test nexthop expired when port goes down
         first_host = self.hosts_name_ordered()[0]
-        match, table = self.match_table(ipaddress.IPv4Network('10.0.0.1/32'))
+        match, table = self.match_table(ipaddress.IPv4Network("10.0.0.1/32"))
         ofmsg = None
         for _ in range(5):
             self.one_ipv4_controller_ping(first_host)
             ofmsg = self.get_matching_flow(match, table_id=table)
             if ofmsg:
                 break
             time.sleep(1)
         self.assertTrue(ofmsg, msg=match)
-        self.set_port_down(self.port_map['port_1'])
+        self.set_port_down(self.port_map["port_1"])
         for _ in range(5):
             if not self.get_matching_flow(match, table_id=table):
                 return
             time.sleep(1)
-        self.fail('host route %s still present' % match)
+        self.fail("host route %s still present" % match)
 
 
 class FaucetUntaggedRestBcastIPv4RouteTest(FaucetUntaggedIPv4RouteTest):
-
     CONFIG = """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 restricted_bcast_arpnd: true
@@ -3922,15 +4573,14 @@
             %(port_4)d:
                 native_vlan: 100
                 restricted_bcast_arpnd: true
 """
 
 
 class FaucetUntaggedVLanUnicastFloodTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: True
 """
 
@@ -3938,30 +4588,28 @@
 
     def test_untagged(self):
         self.ping_all_when_learned()
         self.assertTrue(self.bogus_mac_flooded_to_port1())
 
 
 class FaucetUntaggedNoVLanUnicastFloodTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 """
 
     CONFIG = CONFIG_BOILER_UNTAGGED
 
     def test_untagged(self):
         self.assertFalse(self.bogus_mac_flooded_to_port1())
 
 
 class FaucetUntaggedPortUnicastFloodTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 """
 
@@ -3981,15 +4629,14 @@
     def test_untagged(self):
         # VLAN level config to disable flooding takes precedence,
         # cannot enable port-only flooding.
         self.assertFalse(self.bogus_mac_flooded_to_port1())
 
 
 class FaucetUntaggedNoPortUnicastFloodTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: True
 """
 
@@ -4007,33 +4654,33 @@
 """
 
     def test_untagged(self):
         self.assertFalse(self.bogus_mac_flooded_to_port1())
 
 
 class FaucetUntaggedHostMoveTest(FaucetUntaggedTest):
-
     def test_untagged(self):
         self._enable_event_log()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         for _ in range(2):
             self.retry_net_ping(hosts=(first_host, second_host))
             self.ping((first_host, second_host))
             for host, in_port in (
-                    (first_host, self.port_map['port_1']),
-                    (second_host, self.port_map['port_2'])):
+                (first_host, self.port_map["port_1"]),
+                (second_host, self.port_map["port_2"]),
+            ):
                 self.require_host_learned(host, in_port=in_port)
             self.swap_host_macs(first_host, second_host)
-        for port in (self.port_map['port_1'], self.port_map['port_2']):
+        for port in (self.port_map["port_1"], self.port_map["port_2"]):
             self.wait_until_matching_lines_from_file(
-                r'.+L2_LEARN.+"previous_port_no": %u.+' % port, self.event_log)
+                r'.+L2_LEARN.+"previous_port_no": %u.+' % port, self.event_log
+            )
 
 
 class FaucetUntaggedHostPermanentLearnTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 """
 
     CONFIG = """
@@ -4048,31 +4695,36 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         self.ping_all_when_learned(hard_timeout=0)
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
-        self.assertTrue(self.prom_mac_learned(first_host.MAC(), port=self.port_map['port_1']))
+        self.assertTrue(
+            self.prom_mac_learned(first_host.MAC(), port=self.port_map["port_1"])
+        )
 
         # 3rd host impersonates 1st but 1st host still OK
         original_third_host_mac = third_host.MAC()
         third_host.setMAC(first_host.MAC())
         self.assertEqual(100.0, self.ping((second_host, third_host)))
-        self.assertTrue(self.prom_mac_learned(first_host.MAC(), port=self.port_map['port_1']))
-        self.assertFalse(self.prom_mac_learned(first_host.MAC(), port=self.port_map['port_3']))
+        self.assertTrue(
+            self.prom_mac_learned(first_host.MAC(), port=self.port_map["port_1"])
+        )
+        self.assertFalse(
+            self.prom_mac_learned(first_host.MAC(), port=self.port_map["port_3"])
+        )
         self.retry_net_ping(hosts=(first_host, second_host))
 
         # 3rd host stops impersonating, now everything fine again.
         third_host.setMAC(original_third_host_mac)
         self.ping_all_when_learned(hard_timeout=0)
 
 
 class FaucetCoprocessorTest(FaucetUntaggedTest):
-
     N_UNTAGGED = 3
     N_TAGGED = 1
 
     CONFIG = """
         interfaces:
             %(port_1)d:
                 coprocessor: {strategy: vlan_vid}
@@ -4085,31 +4737,36 @@
                 native_vlan: 100
 """
 
     def test_untagged(self):
         # Inject packet into pipeline using coprocessor.
         coprocessor_host, first_host, second_host, _ = self.hosts_name_ordered()
         self.one_ipv4_ping(first_host, second_host.IP())
-        tcpdump_filter = ' and '.join((
-            'ether dst %s' % first_host.MAC(),
-            'ether src %s' % coprocessor_host.MAC(),
-            'icmp'))
+        tcpdump_filter = " and ".join(
+            (
+                "ether dst %s" % first_host.MAC(),
+                "ether src %s" % coprocessor_host.MAC(),
+                "icmp",
+            )
+        )
         cmds = [
             lambda: coprocessor_host.cmd(
-                'arp -s %s %s' % (first_host.IP(), first_host.MAC())),
+                "arp -s %s %s" % (first_host.IP(), first_host.MAC())
+            ),
             lambda: coprocessor_host.cmd(
-                'fping %s -c3 %s' % (self.FPING_ARGS_SHORT, first_host.IP())),
+                "fping %s -c3 %s" % (self.FPING_ARGS_SHORT, first_host.IP())
+            ),
         ]
         tcpdump_txt = self.tcpdump_helper(
-            first_host, tcpdump_filter, cmds, timeout=5, vflags='-vv', packets=1)
+            first_host, tcpdump_filter, cmds, timeout=5, vflags="-vv", packets=1
+        )
         self.assertFalse(self.tcpdump_rx_packets(tcpdump_txt, packets=0))
 
 
 class FaucetUntaggedLoopTest(FaucetTest):
-
     NUM_DPS = 1
     N_TAGGED = 0
     N_UNTAGGED = 2
     LINKS_PER_HOST = 2
 
     CONFIG_GLOBAL = """
 vlans:
@@ -4130,74 +4787,88 @@
                 native_vlan: 100
                 loop_protect: True
 """
 
     def setUp(self):
         super().setUp()
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
-            links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=self.N_TAGGED,
+            n_untagged=self.N_UNTAGGED,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+        )
         self.start_net()
 
     def total_port_bans(self):
         total_bans = 0
         for i in range(self.LINKS_PER_HOST * self.N_UNTAGGED):
-            port_labels = self.port_labels(self.port_map['port_%u' % (i + 1)])
+            port_labels = self.port_labels(self.port_map["port_%u" % (i + 1)])
             total_bans += self.scrape_prometheus_var(
-                'port_learn_bans', port_labels, dpid=True, default=0)
+                "port_learn_bans", port_labels, dpid=True, default=0
+            )
         return total_bans
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()
         # Normal learning works
         self.one_ipv4_ping(first_host, second_host.IP())
 
         start_bans = self.total_port_bans()
         # Create a loop between interfaces on second host - a veth pair,
         # with two bridges, each connecting one leg of the pair to a host
         # interface.
-        self.quiet_commands(second_host, (
-            'ip link add name veth-loop1 type veth peer name veth-loop2',
-            'ip link set veth-loop1 up',
-            'ip link set veth-loop2 up',
-            # TODO: tune for loop mitigation performance.
-            'tc qdisc add dev veth-loop1 root tbf rate 1000kbps latency 10ms burst 1000',
-            'tc qdisc add dev veth-loop2 root tbf rate 1000kbps latency 10ms burst 1000',
-            # Connect one leg of veth pair to first host interface.
-            'brctl addbr br-loop1',
-            'brctl setfd br-loop1 0',
-            'ip link set br-loop1 up',
-            'brctl addif br-loop1 veth-loop1',
-            'brctl addif br-loop1 %s-eth0' % second_host.name,
-            # Connect other leg of veth pair.
-            'brctl addbr br-loop2',
-            'brctl setfd br-loop2 0',
-            'ip link set br-loop2 up',
-            'brctl addif br-loop2 veth-loop2',
-            'brctl addif br-loop2 %s-eth1' % second_host.name))
+        self.quiet_commands(
+            second_host,
+            (
+                "ip link add name veth-loop1 type veth peer name veth-loop2",
+                "ip link set veth-loop1 up",
+                "ip link set veth-loop2 up",
+                # TODO: tune for loop mitigation performance.
+                "tc qdisc add dev veth-loop1 root tbf rate 1000kbps latency 10ms burst 1000",
+                "tc qdisc add dev veth-loop2 root tbf rate 1000kbps latency 10ms burst 1000",
+                # Connect one leg of veth pair to first host interface.
+                "brctl addbr br-loop1",
+                "brctl setfd br-loop1 0",
+                "ip link set br-loop1 up",
+                "brctl addif br-loop1 veth-loop1",
+                "brctl addif br-loop1 %s-eth0" % second_host.name,
+                # Connect other leg of veth pair.
+                "brctl addbr br-loop2",
+                "brctl setfd br-loop2 0",
+                "ip link set br-loop2 up",
+                "brctl addif br-loop2 veth-loop2",
+                "brctl addif br-loop2 %s-eth1" % second_host.name,
+            ),
+        )
 
         # Flood some traffic into the loop
         for _ in range(3):
-            first_host.cmd('fping %s -c3 10.0.0.254' % self.FPING_ARGS_SHORT)
+            first_host.cmd("fping %s -c3 10.0.0.254" % self.FPING_ARGS_SHORT)
             end_bans = self.total_port_bans()
             if end_bans > start_bans:
                 return
             time.sleep(1)
         self.assertGreater(end_bans, start_bans)
 
         # Break the loop, and learning should work again
-        self.quiet_commands(second_host, (
-            'ip link set veth-loop1 down',
-            'ip link set veth-loop2 down',))
+        self.quiet_commands(
+            second_host,
+            (
+                "ip link set veth-loop1 down",
+                "ip link set veth-loop2 down",
+            ),
+        )
         self.one_ipv4_ping(first_host, second_host.IP())
 
 
 class FaucetUntaggedIPv4LACPTest(FaucetTest):
-
     NUM_DPS = 1
     N_TAGGED = 0
     N_UNTAGGED = 2
     LINKS_PER_HOST = 2
 
     CONFIG_GLOBAL = """
 vlans:
@@ -4225,28 +4896,36 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def setUp(self):
         super().setUp()
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=self.N_TAGGED, n_untagged=self.N_UNTAGGED,
-            links_per_host=self.LINKS_PER_HOST, hw_dpid=self.hw_dpid)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=self.N_TAGGED,
+            n_untagged=self.N_UNTAGGED,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+        )
         self.start_net()
 
     def test_untagged(self):
         first_host = self.hosts_name_ordered()[0]
 
         def get_lacp_port_id(port):
             port_labels = self.port_labels(port)
-            lacp_port_id = self.scrape_prometheus_var('lacp_port_id', port_labels, default=0)
+            lacp_port_id = self.scrape_prometheus_var(
+                "lacp_port_id", port_labels, default=0
+            )
             return lacp_port_id
 
-        bond = 'bond0'
+        bond = "bond0"
         # Linux driver should have this state (0x3f/63)
         #
         #    Actor State: 0x3f, LACP Activity, LACP Timeout, Aggregation, Synchronization, Collecting, Distributing
         #        .... ...1 = LACP Activity: Active
         #        .... ..1. = LACP Timeout: Short Timeout
         #        .... .1.. = Aggregation: Aggregatable
         #        .... 1... = Synchronization: In Sync
@@ -4318,349 +4997,445 @@
 details partner lacp pdu:
     system priority: 65535
     system mac address: 0e:00:00:00:00:01
     oper key: 1
     port priority: 2
     port number: %d
     port state: 62
-""".strip() % tuple(get_lacp_port_id(self.port_map['port_%u' % i]) for i in lag_ports)
+""".strip() % tuple(
+            get_lacp_port_id(self.port_map["port_%u" % i]) for i in lag_ports
+        )
 
         lacp_timeout = 5
 
         def prom_lacp_up_ports():
             lacp_up_ports = 0
             for lacp_port in lag_ports:
-                port_labels = self.port_labels(self.port_map['port_%u' % lacp_port])
-                lacp_state = self.scrape_prometheus_var('port_lacp_state', port_labels, default=0)
+                port_labels = self.port_labels(self.port_map["port_%u" % lacp_port])
+                lacp_state = self.scrape_prometheus_var(
+                    "port_lacp_state", port_labels, default=0
+                )
                 lacp_up_ports += 1 if lacp_state == 3 else 0
             return lacp_up_ports
 
         def require_lag_up_ports(expected_up_ports):
             for _ in range(lacp_timeout * 10):
                 if prom_lacp_up_ports() == expected_up_ports:
                     break
                 time.sleep(1)
             self.assertEqual(prom_lacp_up_ports(), expected_up_ports)
 
         def require_linux_bond_up():
             for _retries in range(lacp_timeout * 2):
-                result = first_host.cmd('cat /proc/net/bonding/%s|sed "s/[ \t]*$//g"' % bond)
-                result = '\n'.join([line.rstrip() for line in result.splitlines()])
-                with open(os.path.join(self.tmpdir, 'bonding-state.txt'), 'w', encoding='utf-8') as state_file:
+                result = first_host.cmd(
+                    'cat /proc/net/bonding/%s|sed "s/[ \t]*$//g"' % bond
+                )
+                result = "\n".join([line.rstrip() for line in result.splitlines()])
+                with open(
+                    os.path.join(self.tmpdir, "bonding-state.txt"),
+                    "w",
+                    encoding="utf-8",
+                ) as state_file:
                     state_file.write(result)
                 if re.search(synced_state_txt, result):
                     break
                 time.sleep(1)
             self.assertTrue(
                 re.search(synced_state_txt, result),
-                msg='LACP did not synchronize: %s\n\nexpected:\n\n%s' % (
-                    result, synced_state_txt))
+                msg="LACP did not synchronize: %s\n\nexpected:\n\n%s"
+                % (result, synced_state_txt),
+            )
 
         # Start with ports down.
         for port in lag_ports:
-            self.set_port_down(self.port_map['port_%u' % port])
+            self.set_port_down(self.port_map["port_%u" % port])
         require_lag_up_ports(0)
         orig_ip = first_host.IP()
         switch = self.first_switch()
         bond_members = [pair[0].name for pair in first_host.connectionsTo(switch)]
         # Deconfigure bond members
         for bond_member in bond_members:
-            self.quiet_commands(first_host, (
-                'ip link set %s down' % bond_member,
-                'ip address flush dev %s' % bond_member))
+            self.quiet_commands(
+                first_host,
+                (
+                    "ip link set %s down" % bond_member,
+                    "ip address flush dev %s" % bond_member,
+                ),
+            )
         # Configure bond interface
-        self.quiet_commands(first_host, (
-            ('ip link add %s address 0e:00:00:00:00:99 '
-             'type bond mode 802.3ad lacp_rate fast miimon 100') % bond,
-            'ip add add %s/24 dev %s' % (orig_ip, bond),
-            'ip link set %s up' % bond))
+        self.quiet_commands(
+            first_host,
+            (
+                (
+                    "ip link add %s address 0e:00:00:00:00:99 "
+                    "type bond mode 802.3ad lacp_rate fast miimon 100"
+                )
+                % bond,
+                "ip add add %s/24 dev %s" % (orig_ip, bond),
+                "ip link set %s up" % bond,
+            ),
+        )
         # Add bond members
         for bond_member in bond_members:
-            self.quiet_commands(first_host, (
-                'ip link set dev %s master %s' % (bond_member, bond),))
+            self.quiet_commands(
+                first_host, ("ip link set dev %s master %s" % (bond_member, bond),)
+            )
 
         for _flaps in range(2):
             # All ports down.
             for port in lag_ports:
-                self.set_port_down(self.port_map['port_%u' % port])
+                self.set_port_down(self.port_map["port_%u" % port])
             require_lag_up_ports(0)
             # Pick a random port to come up.
             up_port = random.choice(lag_ports)
-            self.set_port_up(self.port_map['port_%u' % up_port])
+            self.set_port_up(self.port_map["port_%u" % up_port])
             require_lag_up_ports(1)
             # We have connectivity with only one port.
             self.one_ipv4_ping(
-                first_host, self.FAUCET_VIPV4.ip, require_host_learned=False, intf=bond, retries=5)
+                first_host,
+                self.FAUCET_VIPV4.ip,
+                require_host_learned=False,
+                intf=bond,
+                retries=5,
+            )
             for port in lag_ports:
-                self.set_port_up(self.port_map['port_%u' % port])
+                self.set_port_up(self.port_map["port_%u" % port])
             # We have connectivity with two ports.
             require_lag_up_ports(2)
             require_linux_bond_up()
             self.one_ipv4_ping(
-                first_host, self.FAUCET_VIPV4.ip, require_host_learned=False, intf=bond, retries=5)
+                first_host,
+                self.FAUCET_VIPV4.ip,
+                require_host_learned=False,
+                intf=bond,
+                retries=5,
+            )
             # We have connectivity if that random port goes down.
-            self.set_port_down(self.port_map['port_%u' % up_port])
+            self.set_port_down(self.port_map["port_%u" % up_port])
             require_lag_up_ports(1)
             self.one_ipv4_ping(
-                first_host, self.FAUCET_VIPV4.ip, require_host_learned=False, intf=bond, retries=5)
+                first_host,
+                self.FAUCET_VIPV4.ip,
+                require_host_learned=False,
+                intf=bond,
+                retries=5,
+            )
             for port in lag_ports:
-                self.set_port_up(self.port_map['port_%u' % port])
+                self.set_port_up(self.port_map["port_%u" % port])
 
 
 class FaucetUntaggedIPv4LACPMismatchTest(FaucetUntaggedIPv4LACPTest):
     """Ensure remote LACP system ID mismatch is logged."""
 
     def test_untagged(self):
         first_host = self.hosts_name_ordered()[0]
         orig_ip = first_host.IP()
         switch = self.first_switch()
         bond_members = [pair[0].name for pair in first_host.connectionsTo(switch)]
         for i, bond_member in enumerate(bond_members):
-            bond = 'bond%u' % i
-            self.quiet_commands(first_host, (
-                'ip link set %s down' % bond_member,
-                'ip address flush dev %s' % bond_member,
-                ('ip link add %s address 0e:00:00:00:00:%2.2x '
-                 'type bond mode 802.3ad lacp_rate fast miimon 100') % (bond, i * 2 + i),
-                'ip add add %s/24 dev %s' % (orig_ip, bond),
-                'ip link set %s up' % bond,
-                'ip link set dev %s master %s' % (bond_member, bond)))
-        self.wait_until_matching_lines_from_faucet_log_files(r'.+actor system mismatch.+')
+            bond = "bond%u" % i
+            self.quiet_commands(
+                first_host,
+                (
+                    "ip link set %s down" % bond_member,
+                    "ip address flush dev %s" % bond_member,
+                    (
+                        "ip link add %s address 0e:00:00:00:00:%2.2x "
+                        "type bond mode 802.3ad lacp_rate fast miimon 100"
+                    )
+                    % (bond, i * 2 + i),
+                    "ip add add %s/24 dev %s" % (orig_ip, bond),
+                    "ip link set %s up" % bond,
+                    "ip link set dev %s master %s" % (bond_member, bond),
+                ),
+            )
+        self.wait_until_matching_lines_from_faucet_log_files(
+            r".+actor system mismatch.+"
+        )
 
 
 class FaucetUntaggedIPv4ControlPlaneFuzzTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["10.0.0.254/24"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_ping_fragment_controller(self):
         first_host = self.hosts_name_ordered()[0]
-        first_host.cmd('ping -s 1476 -c 3 %s' % self.FAUCET_VIPV4.ip)
+        first_host.cmd("ping -s 1476 -c 3 %s" % self.FAUCET_VIPV4.ip)
         self.one_ipv4_controller_ping(first_host)
 
     def test_fuzz_controller(self):
         first_host = self.hosts_name_ordered()[0]
         self.one_ipv4_controller_ping(first_host)
         packets = 1000
-        fuzz_template = 'python3 -c \"from scapy.all import * ; scapy.all.send(%s, count=%u)\"'
+        fuzz_template = (
+            'python3 -c "from scapy.all import * ; scapy.all.send(%s, count=%u)"'
+        )
         for fuzz_cmd in (
-                fuzz_template % ('IP(dst=\'%s\')/fuzz(%s(type=0))' % (self.FAUCET_VIPV4.ip, 'ICMP'), packets),
-                fuzz_template % ('IP(dst=\'%s\')/fuzz(%s(type=8))' % (self.FAUCET_VIPV4.ip, 'ICMP'), packets),
-                fuzz_template % ('fuzz(%s(pdst=\'%s\'))' % ('ARP', self.FAUCET_VIPV4.ip), packets)):
+            fuzz_template
+            % (
+                "IP(dst='%s')/fuzz(%s(type=0))" % (self.FAUCET_VIPV4.ip, "ICMP"),
+                packets,
+            ),
+            fuzz_template
+            % (
+                "IP(dst='%s')/fuzz(%s(type=8))" % (self.FAUCET_VIPV4.ip, "ICMP"),
+                packets,
+            ),
+            fuzz_template
+            % ("fuzz(%s(pdst='%s'))" % ("ARP", self.FAUCET_VIPV4.ip), packets),
+        ):
             fuzz_out = first_host.cmd(mininet_test_util.timeout_cmd(fuzz_cmd, 180))
             self.assertTrue(
-                re.search('Sent %u packets' % packets, fuzz_out), msg='%s: %s' % (
-                    fuzz_cmd, fuzz_out))
+                re.search("Sent %u packets" % packets, fuzz_out),
+                msg="%s: %s" % (fuzz_cmd, fuzz_out),
+            )
         self.one_ipv4_controller_ping(first_host)
 
     def test_flap_ping_controller(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         for _ in range(5):
             self.one_ipv4_ping(first_host, second_host.IP())
             for host in first_host, second_host:
                 self.one_ipv4_controller_ping(host)
             self.flap_all_switch_ports()
 
 
 class FaucetUntaggedIPv4ControlPlaneTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["10.0.0.254/24"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_fping_controller(self):
         first_host = self.hosts_name_ordered()[0]
         self.one_ipv4_controller_ping(first_host)
         # Try 64 byte icmp packets
         self.verify_controller_fping(first_host, self.FAUCET_VIPV4)
         # Try 128 byte icmp packets
         self.verify_controller_fping(first_host, self.FAUCET_VIPV4, size=128)
 
 
 class FaucetUntaggedIPv6RATest(FaucetUntaggedTest):
-
     FAUCET_MAC = "0e:00:00:00:00:99"
 
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["fe80::1:254/64", "fc00::1:254/112", "fc00::2:254/112", "10.0.0.254/24"]
         faucet_mac: "%s"
-""" % FAUCET_MAC
+"""
+        % FAUCET_MAC
+    )
 
-    CONFIG = """
+    CONFIG = (
+        """
         advertise_interval: 5
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_ndisc6(self):
         first_host = self.hosts_name_ordered()[0]
-        for vip in ('fe80::1:254', 'fc00::1:254', 'fc00::2:254'):
+        for vip in ("fe80::1:254", "fc00::1:254", "fc00::2:254"):
             self.assertEqual(
                 self.FAUCET_MAC.upper(),
-                first_host.cmd('ndisc6 -q %s %s' % (vip, first_host.defaultIntf())).strip())
+                first_host.cmd(
+                    "ndisc6 -q %s %s" % (vip, first_host.defaultIntf())
+                ).strip(),
+            )
 
     def test_rdisc6(self):
         first_host = self.hosts_name_ordered()[0]
-        rdisc6_results = sorted(list(set(first_host.cmd(
-            'rdisc6 -q %s' % first_host.defaultIntf()).splitlines())))
-        self.assertEqual(
-            ['fc00::1:0/112', 'fc00::2:0/112'],
-            rdisc6_results)
+        rdisc6_results = sorted(
+            list(
+                set(
+                    first_host.cmd(
+                        "rdisc6 -q %s" % first_host.defaultIntf()
+                    ).splitlines()
+                )
+            )
+        )
+        self.assertEqual(["fc00::1:0/112", "fc00::2:0/112"], rdisc6_results)
 
     def test_ra_advertise(self):
         first_host = self.hosts_name_ordered()[0]
-        tcpdump_filter = ' and '.join((
-            'ether dst 33:33:00:00:00:01',
-            'ether src %s' % self.FAUCET_MAC,
-            'icmp6',
-            'ip6[40] == 134',
-            'ip6 host fe80::1:254'))
+        tcpdump_filter = " and ".join(
+            (
+                "ether dst 33:33:00:00:00:01",
+                "ether src %s" % self.FAUCET_MAC,
+                "icmp6",
+                "ip6[40] == 134",
+                "ip6 host fe80::1:254",
+            )
+        )
         tcpdump_txt = self.tcpdump_helper(
-            first_host, tcpdump_filter, [], timeout=30, vflags='-vv', packets=1)
+            first_host, tcpdump_filter, [], timeout=30, vflags="-vv", packets=1
+        )
         for ra_required in (
-                r'ethertype IPv6 \(0x86dd\), length 142',
-                r'fe80::1:254 > ff02::1:.+ICMP6, router advertisement',
-                r'fc00::1:0/112, Flags \[onlink, auto\]',
-                r'fc00::2:0/112, Flags \[onlink, auto\]',
-                r'source link-address option \(1\), length 8 \(1\): %s' % self.FAUCET_MAC):
+            r"ethertype IPv6 \(0x86dd\), length 142",
+            r"fe80::1:254 > ff02::1:.+ICMP6, router advertisement",
+            r"fc00::1:0/112, Flags \[onlink, auto\]",
+            r"fc00::2:0/112, Flags \[onlink, auto\]",
+            r"source link-address option \(1\), length 8 \(1\): %s" % self.FAUCET_MAC,
+        ):
             self.assertTrue(
                 re.search(ra_required, tcpdump_txt),
-                msg='%s: %s' % (ra_required, tcpdump_txt))
+                msg="%s: %s" % (ra_required, tcpdump_txt),
+            )
 
     def test_rs_reply(self):
         first_host = self.hosts_name_ordered()[0]
-        tcpdump_filter = ' and '.join((
-            'ether src %s' % self.FAUCET_MAC,
-            'ether dst %s' % first_host.MAC(),
-            'icmp6',
-            'ip6[40] == 134',
-            'ip6 host fe80::1:254'))
+        tcpdump_filter = " and ".join(
+            (
+                "ether src %s" % self.FAUCET_MAC,
+                "ether dst %s" % first_host.MAC(),
+                "icmp6",
+                "ip6[40] == 134",
+                "ip6 host fe80::1:254",
+            )
+        )
         tcpdump_txt = self.tcpdump_helper(
-            first_host, tcpdump_filter, [
-                lambda: first_host.cmd(
-                    'rdisc6 -1 %s' % first_host.defaultIntf())],
-            timeout=30, vflags='-vv', packets=1)
+            first_host,
+            tcpdump_filter,
+            [lambda: first_host.cmd("rdisc6 -1 %s" % first_host.defaultIntf())],
+            timeout=30,
+            vflags="-vv",
+            packets=1,
+        )
         for ra_required in (
-                r'fe80::1:254 > fe80::.+ICMP6, router advertisement',
-                r'fc00::1:0/112, Flags \[onlink, auto\]',
-                r'fc00::2:0/112, Flags \[onlink, auto\]',
-                r'source link-address option \(1\), length 8 \(1\): %s' % self.FAUCET_MAC):
+            r"fe80::1:254 > fe80::.+ICMP6, router advertisement",
+            r"fc00::1:0/112, Flags \[onlink, auto\]",
+            r"fc00::2:0/112, Flags \[onlink, auto\]",
+            r"source link-address option \(1\), length 8 \(1\): %s" % self.FAUCET_MAC,
+        ):
             self.assertTrue(
                 re.search(ra_required, tcpdump_txt),
-                msg='%s: %s (%s)' % (ra_required, tcpdump_txt, tcpdump_filter))
+                msg="%s: %s (%s)" % (ra_required, tcpdump_txt, tcpdump_filter),
+            )
 
 
 class FaucetUntaggedIPv6ControlPlaneFuzzTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["fc00::1:254/112"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_flap_ping_controller(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        self.add_host_ipv6_address(first_host, 'fc00::1:1/112')
-        self.add_host_ipv6_address(second_host, 'fc00::1:2/112')
+        self.add_host_ipv6_address(first_host, "fc00::1:1/112")
+        self.add_host_ipv6_address(second_host, "fc00::1:2/112")
         for _ in range(5):
-            self.one_ipv6_ping(first_host, 'fc00::1:2')
+            self.one_ipv6_ping(first_host, "fc00::1:2")
             for host in first_host, second_host:
                 self.one_ipv6_controller_ping(host)
             self.flap_all_switch_ports()
 
     def test_fuzz_controller(self):
         first_host = self.hosts_name_ordered()[0]
-        self.add_host_ipv6_address(first_host, 'fc00::1:1/112')
+        self.add_host_ipv6_address(first_host, "fc00::1:1/112")
         self.one_ipv6_controller_ping(first_host)
         fuzz_success = False
         packets = 1000
         count = 0
         abort = False
 
         def note(*args):
-            error('%s:' % self._test_name(), *args + tuple('\n'))
+            error("%s:" % self._test_name(), *args + tuple("\n"))
 
         # Some of these tests have been slowing down and timing out,
         # So this code is intended to allow some debugging and analysis
         for fuzz_class in dir(scapy.all):
-            if fuzz_class.startswith('ICMPv6'):
-                fuzz_cmd = ("from scapy.all import * ;"
-                            "scapy.all.send(IPv6(dst='%s')/fuzz(%s()),count=%u)" %
-                            (self.FAUCET_VIPV6.ip, fuzz_class, packets))
-                out, start, too_long = '', time.time(), 30  # seconds
-                popen = first_host.popen('python3', '-c', fuzz_cmd)
+            if fuzz_class.startswith("ICMPv6"):
+                fuzz_cmd = (
+                    "from scapy.all import * ;"
+                    "scapy.all.send(IPv6(dst='%s')/fuzz(%s()),count=%u)"
+                    % (self.FAUCET_VIPV6.ip, fuzz_class, packets)
+                )
+                out, start, too_long = "", time.time(), 30  # seconds
+                popen = first_host.popen("python3", "-c", fuzz_cmd)
                 for _, line in pmonitor({first_host: popen}):
                     out += line
                     if time.time() - start > too_long:
-                        note('stopping', fuzz_class, 'after >', too_long, 'seconds')
-                        note('output was:', out)
+                        note("stopping", fuzz_class, "after >", too_long, "seconds")
+                        note("output was:", out)
                         popen.terminate()
                         abort = True
                         break
                 popen.wait()
-                if 'Sent %u packets' % packets in out:
+                if "Sent %u packets" % packets in out:
                     count += packets
                     elapsed = time.time() - start
-                    note('sent', packets, fuzz_class, 'packets in %.2fs' % elapsed)
+                    note("sent", packets, fuzz_class, "packets in %.2fs" % elapsed)
                     fuzz_success = True
                 if abort:
                     break
-        note('successfully sent', count, 'packets')
+        note("successfully sent", count, "packets")
         self.assertTrue(fuzz_success)
-        note('pinging', first_host)
+        note("pinging", first_host)
         self.one_ipv6_controller_ping(first_host)
-        note('test_fuzz_controller() complete')
+        note("test_fuzz_controller() complete")
 
 
 class FaucetUntaggedIPv6ControlPlaneTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["fc00::1:254/112"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_fping_controller(self):
         first_host = self.hosts_name_ordered()[0]
-        self.add_host_ipv6_address(first_host, 'fc00::1:1/112')
+        self.add_host_ipv6_address(first_host, "fc00::1:1/112")
         self.one_ipv6_controller_ping(first_host)
         # Try 64 byte icmp6 packets
         self.verify_controller_fping(first_host, self.FAUCET_VIPV6)
         # Try 128 byte icmp6 packets
         self.verify_controller_fping(first_host, self.FAUCET_VIPV6, size=128)
 
 
 class FaucetTaggedAndUntaggedDiffVlanTest(FaucetTest):
-
     N_TAGGED = 2
     N_UNTAGGED = 4
     LINKS_PER_HOST = 1
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
@@ -4679,36 +5454,42 @@
             %(port_4)d:
                 native_vlan: 101
 """
 
     def setUp(self):
         super().setUp()
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=2, n_untagged=2, links_per_host=self.LINKS_PER_HOST,
-            hw_dpid=self.hw_dpid)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=2,
+            n_untagged=2,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+        )
         self.start_net()
 
     def test_separate_untagged_tagged(self):
         tagged_host_pair = self.hosts_name_ordered()[:2]
         untagged_host_pair = self.hosts_name_ordered()[2:]
         self.verify_vlan_flood_limited(
-            tagged_host_pair[0], tagged_host_pair[1], untagged_host_pair[0])
+            tagged_host_pair[0], tagged_host_pair[1], untagged_host_pair[0]
+        )
         self.verify_vlan_flood_limited(
-            untagged_host_pair[0], untagged_host_pair[1], tagged_host_pair[0])
+            untagged_host_pair[0], untagged_host_pair[1], tagged_host_pair[0]
+        )
         # hosts within VLANs can ping each other
         self.retry_net_ping(hosts=tagged_host_pair)
         self.retry_net_ping(hosts=untagged_host_pair)
         # hosts cannot ping hosts in other VLANs
-        self.assertEqual(
-            100, self.ping([tagged_host_pair[0], untagged_host_pair[0]]))
+        self.assertEqual(100, self.ping([tagged_host_pair[0], untagged_host_pair[0]]))
 
 
 class FaucetUntaggedACLTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 acls:
     1:
         - rule:
@@ -4748,15 +5529,14 @@
     def test_port5002_notblocked(self):
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_notblocked(5002, first_host, second_host)
 
 
 class FaucetUntaggedEgressACLTest(FaucetUntaggedTest):
-
     REQUIRES_METADATA = True
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         acl_out: 1
 acls:
@@ -4787,37 +5567,37 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_port5001_blocked(self):
         egress_acl_table = self.scrape_prometheus_var(
-            'faucet_config_table_names',
-            labels={'table_name': 'egress_acl'}
+            "faucet_config_table_names", labels={"table_name": "egress_acl"}
         )
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_blocked(
-            5001, first_host, second_host, table_id=egress_acl_table)
+            5001, first_host, second_host, table_id=egress_acl_table
+        )
         self.ping_all_when_learned()
         self.verify_tp_dst_blocked(
-            5001, first_host, second_host, table_id=egress_acl_table)
+            5001, first_host, second_host, table_id=egress_acl_table
+        )
 
     def test_port5002_notblocked(self):
         egress_acl_table = self.scrape_prometheus_var(
-            'faucet_config_table_names',
-            labels={'table_name': 'egress_acl'}
+            "faucet_config_table_names", labels={"table_name": "egress_acl"}
         )
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_notblocked(
-            5002, first_host, second_host, table_id=egress_acl_table)
+            5002, first_host, second_host, table_id=egress_acl_table
+        )
 
 
 class FaucetUntaggedDPACLTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 acls:
     1:
         - rule:
@@ -4832,31 +5612,33 @@
             tcp_dst: 5001
             actions:
                 allow: 0
         - rule:
             actions:
                 allow: 1
 """
-    CONFIG = """
+    CONFIG = (
+        """
         dp_acls: [1]
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_port5001_blocked(self):
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_blocked(5001, first_host, second_host)
 
     def test_port5002_notblocked(self):
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_notblocked(5002, first_host, second_host)
 
 
 class FaucetUntaggedNoReconfACLTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 acls:
     1:
         - rule:
@@ -4881,35 +5663,38 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         matches = {
-            'in_port': int(self.port_map['port_1']),
-            'tcp_dst': 5001,
-            'eth_type': IPV4_ETH,
-            'ip_proto': 6}
+            "in_port": int(self.port_map["port_1"]),
+            "tcp_dst": 5001,
+            "eth_type": IPV4_ETH,
+            "ip_proto": 6,
+        }
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_blocked(5001, first_host, second_host)
         self.wait_until_matching_flow(
-            matches, table_id=self._PORT_ACL_TABLE, actions=[])
-        self.set_port_down(self.port_map['port_1'])
+            matches, table_id=self._PORT_ACL_TABLE, actions=[]
+        )
+        self.set_port_down(self.port_map["port_1"])
         self.wait_until_matching_flow(
-            matches, table_id=self._PORT_ACL_TABLE, actions=[])
-        self.set_port_up(self.port_map['port_1'])
+            matches, table_id=self._PORT_ACL_TABLE, actions=[]
+        )
+        self.set_port_up(self.port_map["port_1"])
         self.ping_all_when_learned()
         self.verify_tp_dst_blocked(5001, first_host, second_host)
         self.wait_until_matching_flow(
-            matches, table_id=self._PORT_ACL_TABLE, actions=[])
+            matches, table_id=self._PORT_ACL_TABLE, actions=[]
+        )
 
 
 class FaucetUntaggedACLTcpMaskTest(FaucetUntaggedACLTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 acls:
     1:
         - rule:
@@ -4940,15 +5725,14 @@
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_blocked(1024, first_host, second_host, mask=1024)
         self.verify_tp_dst_notblocked(1023, first_host, second_host, table_id=None)
 
 
 class FaucetUntaggedVLANACLTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 acls:
     1:
         - rule:
             dl_type: 0x800
             ip_proto: 6
             tcp_dst: 5001
@@ -4970,49 +5754,50 @@
 """
     CONFIG = CONFIG_BOILER_UNTAGGED
 
     def test_port5001_blocked(self):
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_blocked(
-            5001, first_host, second_host, table_id=self._VLAN_ACL_TABLE)
+            5001, first_host, second_host, table_id=self._VLAN_ACL_TABLE
+        )
 
     def test_port5002_notblocked(self):
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.verify_tp_dst_notblocked(
-            5002, first_host, second_host, table_id=self._VLAN_ACL_TABLE)
+            5002, first_host, second_host, table_id=self._VLAN_ACL_TABLE
+        )
 
 
 class FaucetUntaggedOutputOnlyTest(FaucetUntaggedTest):
-
     CONFIG = """
         interfaces:
             %(port_1)d:
                 output_only: True
             %(port_2)d:
                 native_vlan: 100
             %(port_3)d:
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         self.wait_until_matching_flow(
-            {'in_port': int(self.port_map['port_1'])},
+            {"in_port": int(self.port_map["port_1"])},
             table_id=self._VLAN_TABLE,
-            actions=[])
+            actions=[],
+        )
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
         self.assertEqual(100.0, self.ping((first_host, second_host)))
         self.assertEqual(0, self.ping((third_host, second_host)))
 
 
 class FaucetUntaggedACLMirrorTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5042,15 +5827,14 @@
 
     def test_eapol_mirrored(self):
         first_host, second_host, mirror_host = self.hosts_name_ordered()[0:3]
         self.verify_eapol_mirrored(first_host, second_host, mirror_host)
 
 
 class FaucetUntaggedACLOutputMirrorTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5077,15 +5861,14 @@
 
     def test_untagged(self):
         first_host, second_host, mirror_host = self.hosts_name_ordered()[0:3]
         self.verify_ping_mirrored(first_host, second_host, mirror_host)
 
 
 class FaucetUntaggedOrderedACLOutputMirrorTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5112,15 +5895,14 @@
 
     def test_untagged(self):
         first_host, second_host, mirror_host = self.hosts_name_ordered()[0:3]
         self.verify_ping_mirrored(first_host, second_host, mirror_host)
 
 
 class FaucetUntaggedACLMirrorDefaultAllowTest(FaucetUntaggedACLMirrorTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5141,15 +5923,14 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
 
 class FaucetMultiOutputTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
     200:
 acls:
     multi_out:
         - rule:
@@ -5168,37 +5949,60 @@
             %(port_3)d:
                 native_vlan: 200
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
-        first_host, second_host, third_host, fourth_host = self.hosts_name_ordered()[0:4]
-        tcpdump_filter = ('icmp')
+        first_host, second_host, third_host, fourth_host = self.hosts_name_ordered()[
+            0:4
+        ]
+        tcpdump_filter = "icmp"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
+            second_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                )
+            ],
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
         tcpdump_txt = self.tcpdump_helper(
-            third_host, tcpdump_filter, [
+            third_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    "arp -s %s %s" % (third_host.IP(), "01:02:03:04:05:06")
+                ),
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (third_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, third_host.IP())))])
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % third_host.IP(), tcpdump_txt))
+                    " ".join((self.FPINGS_ARGS_ONE, third_host.IP()))
+                ),
+            ],
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % third_host.IP(), tcpdump_txt)
+        )
         tcpdump_txt = self.tcpdump_helper(
-            fourth_host, tcpdump_filter, [
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, fourth_host.IP())))])
-        self.assertFalse(re.search(
-            '%s: ICMP echo request' % fourth_host.IP(), tcpdump_txt))
+            fourth_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    " ".join((self.FPINGS_ARGS_ONE, fourth_host.IP()))
+                )
+            ],
+        )
+        self.assertFalse(
+            re.search("%s: ICMP echo request" % fourth_host.IP(), tcpdump_txt)
+        )
 
 
 class FaucetMultiOrderedOutputTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
     200:
 acls:
     multi_out:
         - rule:
@@ -5217,37 +6021,60 @@
             %(port_3)d:
                 native_vlan: 200
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
-        first_host, second_host, third_host, fourth_host = self.hosts_name_ordered()[0:4]
-        tcpdump_filter = ('icmp')
+        first_host, second_host, third_host, fourth_host = self.hosts_name_ordered()[
+            0:4
+        ]
+        tcpdump_filter = "icmp"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
+            second_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                )
+            ],
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
         tcpdump_txt = self.tcpdump_helper(
-            third_host, tcpdump_filter, [
+            third_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    "arp -s %s %s" % (third_host.IP(), "01:02:03:04:05:06")
+                ),
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (third_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, third_host.IP())))])
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % third_host.IP(), tcpdump_txt))
+                    " ".join((self.FPINGS_ARGS_ONE, third_host.IP()))
+                ),
+            ],
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % third_host.IP(), tcpdump_txt)
+        )
         tcpdump_txt = self.tcpdump_helper(
-            fourth_host, tcpdump_filter, [
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, fourth_host.IP())))])
-        self.assertFalse(re.search(
-            '%s: ICMP echo request' % fourth_host.IP(), tcpdump_txt))
+            fourth_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    " ".join((self.FPINGS_ARGS_ONE, fourth_host.IP()))
+                )
+            ],
+        )
+        self.assertFalse(
+            re.search("%s: ICMP echo request" % fourth_host.IP(), tcpdump_txt)
+        )
 
 
 class FaucetUntaggedOutputTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5273,28 +6100,34 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
-        tcpdump_filter = ('icmp and ether dst 06:06:06:06:06:06')
+        tcpdump_filter = "icmp and ether dst 06:06:06:06:06:06"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
-        self.assertTrue(re.search(
-            'vlan 123', tcpdump_txt))
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
+                lambda: first_host.cmd(
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
+        self.assertTrue(re.search("vlan 123", tcpdump_txt))
 
 
 class FaucetUntaggedOrderedOutputTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5320,28 +6153,34 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
-        tcpdump_filter = ('icmp and ether dst 06:06:06:06:06:06')
+        tcpdump_filter = "icmp and ether dst 06:06:06:06:06:06"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
-        self.assertTrue(re.search(
-            'vlan 123', tcpdump_txt))
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
+        self.assertTrue(re.search("vlan 123", tcpdump_txt))
 
 
 class FaucetUntaggedMultiVlansOutputTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5367,28 +6206,34 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
-        tcpdump_filter = 'vlan'
+        tcpdump_filter = "vlan"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
-        self.assertTrue(re.search(
-            'vlan 456.+vlan 123', tcpdump_txt))
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
+        self.assertTrue(re.search("vlan 456.+vlan 123", tcpdump_txt))
 
 
 class FaucetUntaggedMultiVlansOrderedOutputTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5414,28 +6259,34 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
-        tcpdump_filter = 'vlan'
+        tcpdump_filter = "vlan"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))])
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
-        self.assertTrue(re.search(
-            'vlan 456.+vlan 123', tcpdump_txt))
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
+        self.assertTrue(re.search("vlan 456.+vlan 123", tcpdump_txt))
 
 
 class FaucetUntaggedMultiConfVlansOutputTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5461,29 +6312,41 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
-        tcpdump_filter = 'ether proto 0x88a8'
+        tcpdump_filter = "ether proto 0x88a8"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-            packets=1)
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt), msg=tcpdump_txt)
-        self.assertTrue(re.search(
-            r'vlan 456.+ethertype 802\.1Q-QinQ \(0x88a8\), vlan 123', tcpdump_txt), msg=tcpdump_txt)
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+            packets=1,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt),
+            msg=tcpdump_txt,
+        )
+        self.assertTrue(
+            re.search(
+                r"vlan 456.+ethertype 802\.1Q-QinQ \(0x88a8\), vlan 123", tcpdump_txt
+            ),
+            msg=tcpdump_txt,
+        )
 
 
 class FaucetUntaggedMultiConfVlansOrderedOutputTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 acls:
     1:
@@ -5509,29 +6372,41 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the rewritten address and VLAN
-        tcpdump_filter = 'ether proto 0x88a8'
+        tcpdump_filter = "ether proto 0x88a8"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-            packets=1)
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt), msg=tcpdump_txt)
-        self.assertTrue(re.search(
-            r'vlan 456.+ethertype 802\.1Q-QinQ \(0x88a8\), vlan 123', tcpdump_txt), msg=tcpdump_txt)
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
+                lambda: first_host.cmd(
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+            packets=1,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt),
+            msg=tcpdump_txt,
+        )
+        self.assertTrue(
+            re.search(
+                r"vlan 456.+ethertype 802\.1Q-QinQ \(0x88a8\), vlan 123", tcpdump_txt
+            ),
+            msg=tcpdump_txt,
+        )
 
 
 class FaucetUntaggedMirrorTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 """
 
     CONFIG = """
@@ -5549,36 +6424,49 @@
     def test_untagged(self):
         first_host, second_host, mirror_host = self.hosts_name_ordered()[0:3]
         first_host_ip = ipaddress.ip_address(first_host.IP())
         second_host_ip = ipaddress.ip_address(second_host.IP())
         self.flap_all_switch_ports()
         # Add mirror, test performance.
         self.change_port_config(
-            self.port_map['port_3'], 'mirror', self.port_map['port_1'],
-            restart=True, cold_start=False)
+            self.port_map["port_3"],
+            "mirror",
+            self.port_map["port_1"],
+            restart=True,
+            cold_start=False,
+        )
         self.verify_ping_mirrored(first_host, second_host, mirror_host)
         self.verify_bcast_ping_mirrored(first_host, second_host, mirror_host)
         self.verify_iperf_min(
-            ((first_host, self.port_map['port_1']),
-             (second_host, self.port_map['port_2'])),
-            MIN_MBPS, first_host_ip, second_host_ip,
-            sync_counters_func=lambda: self.one_ipv4_ping(first_host, second_host_ip))
+            (
+                (first_host, self.port_map["port_1"]),
+                (second_host, self.port_map["port_2"]),
+            ),
+            MIN_MBPS,
+            first_host_ip,
+            second_host_ip,
+            sync_counters_func=lambda: self.one_ipv4_ping(first_host, second_host_ip),
+        )
         # Remove mirror, test performance.
         self.change_port_config(
-            self.port_map['port_3'], 'mirror', [],
-            restart=True, cold_start=False)
+            self.port_map["port_3"], "mirror", [], restart=True, cold_start=False
+        )
         self.verify_iperf_min(
-            ((first_host, self.port_map['port_1']),
-             (second_host, self.port_map['port_2'])),
-            MIN_MBPS, first_host_ip, second_host_ip,
-            sync_counters_func=lambda: self.one_ipv4_ping(first_host, second_host_ip))
+            (
+                (first_host, self.port_map["port_1"]),
+                (second_host, self.port_map["port_2"]),
+            ),
+            MIN_MBPS,
+            first_host_ip,
+            second_host_ip,
+            sync_counters_func=lambda: self.one_ipv4_ping(first_host, second_host_ip),
+        )
 
 
 class FaucetUntaggedMultiMirrorTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 """
 
     CONFIG = """
@@ -5591,28 +6479,28 @@
                 output_only: True
             %(port_4)d:
                 output_only: True
 """
 
     def test_untagged(self):
         first_host, second_host, mirror_host = self.hosts_name_ordered()[:3]
-        ping_pairs = (
-            (first_host, second_host),
-            (second_host, first_host))
+        ping_pairs = ((first_host, second_host), (second_host, first_host))
         self.flap_all_switch_ports()
         self.change_port_config(
-            self.port_map['port_3'], 'mirror',
-            [self.port_map['port_1'], self.port_map['port_2']],
-            restart=True, cold_start=False, hup=True)
-        self.verify_ping_mirrored_multi(
-            ping_pairs, mirror_host, both_mirrored=True)
+            self.port_map["port_3"],
+            "mirror",
+            [self.port_map["port_1"], self.port_map["port_2"]],
+            restart=True,
+            cold_start=False,
+            hup=True,
+        )
+        self.verify_ping_mirrored_multi(ping_pairs, mirror_host, both_mirrored=True)
 
 
 class FaucetUntaggedMultiMirrorSepTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         unicast_flood: False
 """
 
@@ -5636,57 +6524,64 @@
         mirror_host = self.hosts_name_ordered()[2]
         self.verify_ping_mirrored(first_host, second_host, mirror_host)
         mirror_host = self.hosts_name_ordered()[3]
         self.verify_ping_mirrored(first_host, second_host, mirror_host)
 
 
 class FaucetTaggedTest(FaucetTest):
-
     N_UNTAGGED = 0
     N_TAGGED = 4
     LINKS_PER_HOST = 1
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
 """
 
     CONFIG = CONFIG_TAGGED_BOILER
 
     def setUp(self):
         super().setUp()
         self.topo = self.topo_class(
-            self.OVS_TYPE, self.ports_sock, self._test_name(), [self.dpid],
-            n_tagged=4, links_per_host=self.LINKS_PER_HOST,
-            hw_dpid=self.hw_dpid)
+            self.OVS_TYPE,
+            self.ports_sock,
+            self._test_name(),
+            [self.dpid],
+            n_tagged=4,
+            links_per_host=self.LINKS_PER_HOST,
+            hw_dpid=self.hw_dpid,
+        )
         self.start_net()
 
     def test_tagged(self):
         # Untagged traffic specifically dropped.
         for host in self.hosts_name_ordered():
             host.cmd(self.scapy_dhcp(host.MAC(), host.intf_root_name, count=3))
         for port in self.port_map.values():
             self.wait_nonzero_packet_count_flow(
-                {'in_port': port, 'vlan_tci': '0x0000/0x1fff'}, table_id=self._VLAN_TABLE)
+                {"in_port": port, "vlan_tci": "0x0000/0x1fff"},
+                table_id=self._VLAN_TABLE,
+            )
         self.ping_all_when_learned()
 
 
 class FaucetTaggedDTPTest(FaucetTaggedTest):
-
     def test_tagged(self):
         for host in self.hosts_name_ordered():
             scapy_txt = host.cmd(
-                ('python3 -c \"import sys ; from scapy.contrib.dtp import * ;'
-                 'negotiate_trunk(iface=\'%s\')\"' % host.intf_root_name))
-            self.assertTrue(re.search('Sent 1 packets', scapy_txt), msg=scapy_txt)
+                (
+                    'python3 -c "import sys ; from scapy.contrib.dtp import * ;'
+                    "negotiate_trunk(iface='%s')\"" % host.intf_root_name
+                )
+            )
+            self.assertTrue(re.search("Sent 1 packets", scapy_txt), msg=scapy_txt)
         super().test_tagged()
 
 
 class FaucetTaggedMirrorTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
 """
 
     CONFIG = """
@@ -5706,36 +6601,46 @@
         first_host, second_host, mirror_host = self.hosts_name_ordered()[0:3]
         self.flap_all_switch_ports()
         self.verify_ping_mirrored(first_host, second_host, mirror_host)
         self.verify_bcast_ping_mirrored(first_host, second_host, mirror_host)
         first_host_ip = ipaddress.ip_address(first_host.IP())
         second_host_ip = ipaddress.ip_address(second_host.IP())
         self.verify_iperf_min(
-            ((first_host, self.port_map['port_1']),
-             (second_host, self.port_map['port_2'])),
-            MIN_MBPS, first_host_ip, second_host_ip,
-            sync_counters_func=lambda: self.one_ipv4_ping(first_host, second_host_ip))
-        tagged_ports = (self.port_map['port_1'], self.port_map['port_2'], self.port_map['port_4'])
+            (
+                (first_host, self.port_map["port_1"]),
+                (second_host, self.port_map["port_2"]),
+            ),
+            MIN_MBPS,
+            first_host_ip,
+            second_host_ip,
+            sync_counters_func=lambda: self.one_ipv4_ping(first_host, second_host_ip),
+        )
+        tagged_ports = (
+            self.port_map["port_1"],
+            self.port_map["port_2"],
+            self.port_map["port_4"],
+        )
         for port in tagged_ports:
             self.wait_until_matching_flow(
-                {'vlan_vid': 100, 'in_port': port},
+                {"vlan_vid": 100, "in_port": port},
                 table_id=self._VLAN_TABLE,
-                actions=['GOTO_TABLE:%u' % self._ETH_SRC_TABLE])
+                actions=["GOTO_TABLE:%u" % self._ETH_SRC_TABLE],
+            )
         self.change_port_config(
-            self.port_map['port_3'], 'mirror', None,
-            restart=True, cold_start=False)
+            self.port_map["port_3"], "mirror", None, restart=True, cold_start=False
+        )
         for port in tagged_ports:
             self.wait_until_matching_flow(
-                {'vlan_vid': 100, 'in_port': port},
+                {"vlan_vid": 100, "in_port": port},
                 table_id=self._VLAN_TABLE,
-                actions=['GOTO_TABLE:%u' % self._ETH_SRC_TABLE])
+                actions=["GOTO_TABLE:%u" % self._ETH_SRC_TABLE],
+            )
 
 
 class FaucetTaggedVLANPCPTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
 acls:
     1:
         - rule:
@@ -5763,29 +6668,35 @@
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         self.quiet_commands(
             first_host,
-            ['ip link set %s type vlan egress %u:1' % (
-                first_host.defaultIntf(), i) for i in range(0, 8)])
+            [
+                "ip link set %s type vlan egress %u:1" % (first_host.defaultIntf(), i)
+                for i in range(0, 8)
+            ],
+        )
         self.one_ipv4_ping(first_host, second_host.IP())
         self.wait_nonzero_packet_count_flow(
-            {'vlan_vid': 100, 'vlan_pcp': 1}, table_id=self._PORT_ACL_TABLE)
-        tcpdump_filter = 'ether dst %s' % second_host.MAC()
+            {"vlan_vid": 100, "vlan_pcp": 1}, table_id=self._PORT_ACL_TABLE
+        )
+        tcpdump_filter = "ether dst %s" % second_host.MAC()
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
-                lambda: first_host.cmd(
-                    'ping -c3 %s' % second_host.IP())], root_intf=True, packets=1)
-        self.assertTrue(re.search('vlan 100, p 2,', tcpdump_txt))
+            second_host,
+            tcpdump_filter,
+            [lambda: first_host.cmd("ping -c3 %s" % second_host.IP())],
+            root_intf=True,
+            packets=1,
+        )
+        self.assertTrue(re.search("vlan 100, p 2,", tcpdump_txt))
 
 
 class FaucetTaggedVLANPCPOrderedTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
 acls:
     1:
         - rule:
@@ -5813,29 +6724,35 @@
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
         self.quiet_commands(
             first_host,
-            ['ip link set %s type vlan egress %u:1' % (
-                first_host.defaultIntf(), i) for i in range(0, 8)])
+            [
+                "ip link set %s type vlan egress %u:1" % (first_host.defaultIntf(), i)
+                for i in range(0, 8)
+            ],
+        )
         self.one_ipv4_ping(first_host, second_host.IP())
         self.wait_nonzero_packet_count_flow(
-            {'vlan_vid': 100, 'vlan_pcp': 1}, table_id=self._PORT_ACL_TABLE)
-        tcpdump_filter = 'ether dst %s' % second_host.MAC()
+            {"vlan_vid": 100, "vlan_pcp": 1}, table_id=self._PORT_ACL_TABLE
+        )
+        tcpdump_filter = "ether dst %s" % second_host.MAC()
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
-                lambda: first_host.cmd(
-                    'ping -c3 %s' % second_host.IP())], root_intf=True, packets=1)
-        self.assertTrue(re.search('vlan 100, p 2,', tcpdump_txt))
+            second_host,
+            tcpdump_filter,
+            [lambda: first_host.cmd("ping -c3 %s" % second_host.IP())],
+            root_intf=True,
+            packets=1,
+        )
+        self.assertTrue(re.search("vlan 100, p 2,", tcpdump_txt))
 
 
 class FaucetTaggedGlobalIPv4RouteTest(FaucetTaggedTest):
-
     def _vids():  # pylint: disable=no-method-argument
         return list(range(100, 148))
 
     def global_vid():  # pylint: disable=no-method-argument
         return 2047
 
     IPV = 4
@@ -5845,41 +6762,54 @@
     VIDS = _vids()
     GLOBAL_VID = global_vid()
     STR_VIDS = [str(i) for i in _vids()]
     NEW_VIDS = VIDS[1:]
 
     @staticmethod
     def netbase(vid, host):
-        return ipaddress.ip_interface('192.168.%u.%u' % (vid, host))
+        return ipaddress.ip_interface("192.168.%u.%u" % (vid, host))
 
     def fping(self, macvlan_int, ipg):
-        return 'fping %s -c1 -t1 -I%s %s > /dev/null 2> /dev/null' % (
-            self.FPING_ARGS_SHORT, macvlan_int, ipg)
+        return "fping %s -c1 -t1 -I%s %s > /dev/null 2> /dev/null" % (
+            self.FPING_ARGS_SHORT,
+            macvlan_int,
+            ipg,
+        )
 
     def fib_table(self):
         return self._IPV4_FIB_TABLE
 
     def macvlan_ping(self, host, ipa, macvlan_int):
         return self.one_ipv4_ping(host, ipa, intf=macvlan_int)
 
     def run_ip(self, args):
-        return 'ip -%u %s' % (self.IPV, args)
+        return "ip -%u %s" % (self.IPV, args)
 
     CONFIG_GLOBAL = """
 routers:
     global:
         vlans: [%s]
 vlans:
 %s
 """ % (
-        ','.join(STR_VIDS),
-        '\n'.join(['\n'.join(
-            ('    %u:',
-             '        description: "tagged"',
-             '        faucet_vips: ["192.168.%u.254/24"]')) % (i, i) for i in VIDS]))
+        ",".join(STR_VIDS),
+        "\n".join(
+            [
+                "\n".join(
+                    (
+                        "    %u:",
+                        '        description: "tagged"',
+                        '        faucet_vips: ["192.168.%u.254/24"]',
+                    )
+                )
+                % (i, i)
+                for i in VIDS
+            ]
+        ),
+    )
     CONFIG = """
         global_vlan: %u
         proactive_learn_v4: True
         max_wildcard_table_size: 1024
         table_sizes:
             vlan: %u
             vip: %u
@@ -5891,118 +6821,158 @@
                 native_vlan: 99
                 tagged_vlans: [%s]
                 hairpin_unicast: True
             %s:
                 native_vlan: 99
                 tagged_vlans: [%s]
                 hairpin_unicast: True
-""" % (global_vid(),
-       len(STR_VIDS) * 3,   # VLAN
-       len(STR_VIDS) * 2,   # VIP
-       len(STR_VIDS) * 12,  # Flood
-       '%(port_3)d', '%(port_1)d', '%(port_1)d',
-       ','.join(STR_VIDS), '%(port_2)d', ','.join(STR_VIDS))
+""" % (
+        global_vid(),
+        len(STR_VIDS) * 3,  # VLAN
+        len(STR_VIDS) * 2,  # VIP
+        len(STR_VIDS) * 12,  # Flood
+        "%(port_3)d",
+        "%(port_1)d",
+        "%(port_1)d",
+        ",".join(STR_VIDS),
+        "%(port_2)d",
+        ",".join(STR_VIDS),
+    )
 
     def configure_mesh(self, first_host, second_host):
         hosts = (first_host, second_host)
         required_ipds = set()
         ipd_to_macvlan = {}
 
         for i, host in enumerate(hosts, start=1):
             setup_commands = []
             for vid in self.NEW_VIDS:
-                vlan_int = '%s.%u' % (host.intf_root_name, vid)
-                macvlan_int = 'macvlan%u' % vid
+                vlan_int = "%s.%u" % (host.intf_root_name, vid)
+                macvlan_int = "macvlan%u" % vid
                 ipa = self.netbase(vid, i)
                 ipg = self.netbase(vid, 254)
                 ipd = self.netbase(vid, 253)
                 required_ipds.add(str(ipd.ip))
                 ipd_to_macvlan[str(ipd.ip)] = (macvlan_int, host)
-                setup_commands.extend([
-                    self.run_ip('link add link %s name %s type vlan id %u' % (
-                        host.intf_root_name, vlan_int, vid)),
-                    self.run_ip('link set dev %s up' % vlan_int),
-                    self.run_ip('link add %s link %s type macvlan mode vepa' % (macvlan_int, vlan_int)),
-                    self.run_ip('link set dev %s up' % macvlan_int),
-                    self.run_ip('address add %s/%u dev %s' % (ipa.ip, self.NETPREFIX, macvlan_int)),
-                    self.run_ip('route add default via %s table %u' % (ipg.ip, vid)),
-                    self.run_ip('rule add from %s table %u priority 100' % (ipa, vid)),
-                    # stimulate learning attempts for down host.
-                    self.run_ip('neigh add %s lladdr %s dev %s' % (ipd.ip, self.FAUCET_MAC, macvlan_int))])
+                setup_commands.extend(
+                    [
+                        self.run_ip(
+                            "link add link %s name %s type vlan id %u"
+                            % (host.intf_root_name, vlan_int, vid)
+                        ),
+                        self.run_ip("link set dev %s up" % vlan_int),
+                        self.run_ip(
+                            "link add %s link %s type macvlan mode vepa"
+                            % (macvlan_int, vlan_int)
+                        ),
+                        self.run_ip("link set dev %s up" % macvlan_int),
+                        self.run_ip(
+                            "address add %s/%u dev %s"
+                            % (ipa.ip, self.NETPREFIX, macvlan_int)
+                        ),
+                        self.run_ip(
+                            "route add default via %s table %u" % (ipg.ip, vid)
+                        ),
+                        self.run_ip(
+                            "rule add from %s table %u priority 100" % (ipa, vid)
+                        ),
+                        # stimulate learning attempts for down host.
+                        self.run_ip(
+                            "neigh add %s lladdr %s dev %s"
+                            % (ipd.ip, self.FAUCET_MAC, macvlan_int)
+                        ),
+                    ]
+                )
                 # next host routes via FAUCET for other host in same connected subnet
                 # to cause routing to be exercised.
                 for j, _ in enumerate(hosts, start=1):
                     if j != i:
                         other_ip = self.netbase(vid, j)
                         setup_commands.append(
-                            self.run_ip('route add %s via %s table %u' % (other_ip, ipg.ip, vid)))
+                            self.run_ip(
+                                "route add %s via %s table %u" % (other_ip, ipg.ip, vid)
+                            )
+                        )
                 for ipa in (ipg.ip, ipd.ip):
                     setup_commands.append(self.fping(macvlan_int, ipa))
 
             self.quiet_commands(host, setup_commands)
         return required_ipds, ipd_to_macvlan
 
     def verify_drop_rules(self, required_ipds, ipd_to_macvlan):
         for _ in range(10):
             if not required_ipds:
                 break
             drop_rules = self.get_matching_flows_on_dpid(
-                self.dpid, {'dl_type': self.ETH_TYPE, 'dl_vlan': str(self.GLOBAL_VID)},
-                table_id=self.fib_table(), actions=[])
+                self.dpid,
+                {"dl_type": self.ETH_TYPE, "dl_vlan": str(self.GLOBAL_VID)},
+                table_id=self.fib_table(),
+                actions=[],
+            )
             if drop_rules:
                 for drop_rule in drop_rules:
-                    match = drop_rule['match']
-                    del match['dl_type']
-                    del match['dl_vlan']
+                    match = drop_rule["match"]
+                    del match["dl_type"]
+                    del match["dl_vlan"]
                     self.assertEqual(1, len(match))
-                    ipd = list(match.values())[0].split('/')[0]
+                    ipd = list(match.values())[0].split("/")[0]
                     if ipd in required_ipds:
                         required_ipds.remove(ipd)
             for ipd in required_ipds:
                 macvlan_int, host = ipd_to_macvlan[ipd]
                 host.cmd(self.fping(macvlan_int, ipd))
             time.sleep(1)
-        self.assertFalse(required_ipds, msg='no drop rules for %s' % required_ipds)
+        self.assertFalse(required_ipds, msg="no drop rules for %s" % required_ipds)
 
     def verify_routing_performance(self, first_host, second_host):
         for first_host_ip, second_host_ip in (
-                (self.netbase(self.NEW_VIDS[0], 1), self.netbase(self.NEW_VIDS[0], 2)),
-                (self.netbase(self.NEW_VIDS[0], 1), self.netbase(self.NEW_VIDS[-1], 2)),
-                (self.netbase(self.NEW_VIDS[-1], 1), self.netbase(self.NEW_VIDS[0], 2))):
+            (self.netbase(self.NEW_VIDS[0], 1), self.netbase(self.NEW_VIDS[0], 2)),
+            (self.netbase(self.NEW_VIDS[0], 1), self.netbase(self.NEW_VIDS[-1], 2)),
+            (self.netbase(self.NEW_VIDS[-1], 1), self.netbase(self.NEW_VIDS[0], 2)),
+        ):
             self.verify_iperf_min(
-                ((first_host, self.port_map['port_1']),
-                 (second_host, self.port_map['port_2'])),
-                MIN_MBPS, first_host_ip.ip, second_host_ip.ip,
-                sync_counters_func=lambda: self.scapy_bcast(first_host))
+                (
+                    (first_host, self.port_map["port_1"]),
+                    (second_host, self.port_map["port_2"]),
+                ),
+                MIN_MBPS,
+                first_host_ip.ip,
+                second_host_ip.ip,
+                sync_counters_func=lambda: self.scapy_bcast(first_host),
+            )
 
     def verify_l3_mesh(self, first_host, second_host):
         for vid in self.NEW_VIDS:
-            macvlan_int = 'macvlan%u' % vid
+            macvlan_int = "macvlan%u" % vid
             first_host_ip = self.netbase(vid, 1)
             second_host_ip = self.netbase(vid, 2)
             self.macvlan_ping(first_host, second_host_ip.ip, macvlan_int)
             self.macvlan_ping(second_host, first_host_ip.ip, macvlan_int)
 
     def verify_l3_hairpin(self, first_host):
-        macvlan1_int = 'macvlan%u' % self.NEW_VIDS[0]
-        macvlan2_int = 'macvlan%u' % self.NEW_VIDS[1]
+        macvlan1_int = "macvlan%u" % self.NEW_VIDS[0]
+        macvlan2_int = "macvlan%u" % self.NEW_VIDS[1]
         macvlan2_ip = self.netbase(self.NEW_VIDS[1], 1)
         macvlan1_gw = self.netbase(self.NEW_VIDS[0], 254)
         macvlan2_gw = self.netbase(self.NEW_VIDS[1], 254)
         netns = self.hostns(first_host)
         setup_cmds = []
-        setup_cmds.extend(
-            [self.run_ip('link set %s netns %s' % (macvlan2_int, netns))])
+        setup_cmds.extend([self.run_ip("link set %s netns %s" % (macvlan2_int, netns))])
         for exec_cmd in (
-                (self.run_ip('address add %s/%u dev %s' % (macvlan2_ip.ip, self.NETPREFIX, macvlan2_int)),
-                 self.run_ip('link set %s up' % macvlan2_int),
-                 self.run_ip('route add default via %s' % macvlan2_gw.ip))):
-            setup_cmds.append('ip netns exec %s %s' % (netns, exec_cmd))
+            self.run_ip(
+                "address add %s/%u dev %s"
+                % (macvlan2_ip.ip, self.NETPREFIX, macvlan2_int)
+            ),
+            self.run_ip("link set %s up" % macvlan2_int),
+            self.run_ip("route add default via %s" % macvlan2_gw.ip),
+        ):
+            setup_cmds.append("ip netns exec %s %s" % (netns, exec_cmd))
         setup_cmds.append(
-            self.run_ip('route add %s via %s' % (macvlan2_ip, macvlan1_gw.ip)))
+            self.run_ip("route add %s via %s" % (macvlan2_ip, macvlan1_gw.ip))
+        )
         self.quiet_commands(first_host, setup_cmds)
         self.macvlan_ping(first_host, macvlan2_ip.ip, macvlan1_int)
 
     def test_tagged(self):
         first_host, second_host, mirror_host = self.hosts_name_ordered()[:3]
         required_ipds, ipd_to_macvlan = self.configure_mesh(first_host, second_host)
         self.verify_drop_rules(required_ipds, ipd_to_macvlan)
@@ -6010,15 +6980,14 @@
         self.verify_l3_mesh(first_host, second_host)
         self.verify_l3_hairpin(first_host)
         self.verify_ping_mirrored(first_host, second_host, mirror_host)
         self.verify_bcast_ping_mirrored(first_host, second_host, mirror_host)
 
 
 class FaucetTaggedGlobalIPv6RouteTest(FaucetTaggedGlobalIPv4RouteTest):
-
     IPV = 6
     NETPREFIX = 112
     ETH_TYPE = IPV6_ETH
 
     def _vids():  # pylint: disable=no-method-argument
         return list(range(100, 103))
 
@@ -6027,41 +6996,54 @@
 
     VIDS = _vids()
     GLOBAL_VID = global_vid()
     STR_VIDS = [str(i) for i in _vids()]
     NEW_VIDS = VIDS[1:]
 
     def netbase(self, vid, host):
-        return ipaddress.ip_interface('fc00::%u:%u' % (vid, host))
+        return ipaddress.ip_interface("fc00::%u:%u" % (vid, host))
 
     def fib_table(self):
         return self._IPV6_FIB_TABLE
 
     def fping(self, macvlan_int, ipg):
-        return 'fping6 %s -c1 -t1 -I%s %s > /dev/null 2> /dev/null' % (
-            self.FPING_ARGS_SHORT, macvlan_int, ipg)
+        return "fping6 %s -c1 -t1 -I%s %s > /dev/null 2> /dev/null" % (
+            self.FPING_ARGS_SHORT,
+            macvlan_int,
+            ipg,
+        )
 
     def macvlan_ping(self, host, ipa, macvlan_int):
         return self.one_ipv6_ping(host, ipa, intf=macvlan_int)
 
     def run_ip(self, args):
-        return 'ip -%u %s' % (self.IPV, args)
+        return "ip -%u %s" % (self.IPV, args)
 
     CONFIG_GLOBAL = """
 routers:
     global:
         vlans: [%s]
 vlans:
 %s
 """ % (
-        ','.join(STR_VIDS),
-        '\n'.join(['\n'.join(
-            ('    %u:',
-             '        description: "tagged"',
-             '        faucet_vips: ["fc00::%u:254/112"]')) % (i, i) for i in VIDS]))
+        ",".join(STR_VIDS),
+        "\n".join(
+            [
+                "\n".join(
+                    (
+                        "    %u:",
+                        '        description: "tagged"',
+                        '        faucet_vips: ["fc00::%u:254/112"]',
+                    )
+                )
+                % (i, i)
+                for i in VIDS
+            ]
+        ),
+    )
     CONFIG = """
         global_vlan: %u
         proactive_learn_v6: True
         max_wildcard_table_size: 512
         table_sizes:
             vlan: 256
             vip: 128
@@ -6073,92 +7055,107 @@
                 native_vlan: 99
                 tagged_vlans: [%s]
                 hairpin_unicast: True
             %s:
                 native_vlan: 99
                 tagged_vlans: [%s]
                 hairpin_unicast: True
-""" % (global_vid(), '%(port_3)d', '%(port_1)d', '%(port_1)d',
-       ','.join(STR_VIDS), '%(port_2)d', ','.join(STR_VIDS))
+""" % (
+        global_vid(),
+        "%(port_3)d",
+        "%(port_1)d",
+        "%(port_1)d",
+        ",".join(STR_VIDS),
+        "%(port_2)d",
+        ",".join(STR_VIDS),
+    )
 
 
 class FaucetTaggedScaleTest(FaucetTaggedTest):
-
     def _vids():  # pylint: disable=no-method-argument
         return list(range(100, 148))
 
     VIDS = _vids()
     STR_VIDS = [str(i) for i in _vids()]
     NEW_VIDS = VIDS[1:]
 
     CONFIG_GLOBAL = """
 vlans:
-""" + '\n'.join(['\n'.join(
-        ('    %u:',
-         '        description: "tagged"')) % i for i in VIDS])
+""" + "\n".join(
+        ["\n".join(("    %u:", '        description: "tagged"')) % i for i in VIDS]
+    )
     CONFIG = """
         interfaces:
             %s:
                 tagged_vlans: [%s]
             %s:
                 tagged_vlans: [%s]
             %s:
                 tagged_vlans: [%s]
             %s:
                 tagged_vlans: [%s]
-""" % ('%(port_1)d', ','.join(STR_VIDS),
-       '%(port_2)d', ','.join(STR_VIDS),
-       '%(port_3)d', ','.join(STR_VIDS),
-       '%(port_4)d', ','.join(STR_VIDS))
+""" % (
+        "%(port_1)d",
+        ",".join(STR_VIDS),
+        "%(port_2)d",
+        ",".join(STR_VIDS),
+        "%(port_3)d",
+        ",".join(STR_VIDS),
+        "%(port_4)d",
+        ",".join(STR_VIDS),
+    )
 
     def test_tagged(self):
         self.ping_all_when_learned()
         for host in self.hosts_name_ordered():
             setup_commands = []
             for vid in self.NEW_VIDS:
-                vlan_int = '%s.%u' % (host.intf_root_name, vid)
-                setup_commands.extend([
-                    'ip link add link %s name %s type vlan id %u' % (
-                        host.intf_root_name, vlan_int, vid),
-                    'ip link set dev %s up' % vlan_int])
+                vlan_int = "%s.%u" % (host.intf_root_name, vid)
+                setup_commands.extend(
+                    [
+                        "ip link add link %s name %s type vlan id %u"
+                        % (host.intf_root_name, vlan_int, vid),
+                        "ip link set dev %s up" % vlan_int,
+                    ]
+                )
             self.quiet_commands(host, setup_commands)
         for host in self.hosts_name_ordered():
             rdisc6_commands = []
             for vid in self.NEW_VIDS:
-                vlan_int = '%s.%u' % (host.intf_root_name, vid)
-                rdisc6_commands.append(
-                    'rdisc6 -r2 -w1 -q %s 2> /dev/null' % vlan_int)
+                vlan_int = "%s.%u" % (host.intf_root_name, vid)
+                rdisc6_commands.append("rdisc6 -r2 -w1 -q %s 2> /dev/null" % vlan_int)
             self.quiet_commands(host, rdisc6_commands)
         for vlan in self.NEW_VIDS:
-            vlan_int = '%s.%u' % (host.intf_root_name, vid)
+            vlan_int = "%s.%u" % (host.intf_root_name, vid)
             for _ in range(3):
                 for host in self.hosts_name_ordered():
                     self.quiet_commands(
-                        host,
-                        ['rdisc6 -r2 -w1 -q %s 2> /dev/null' % vlan_int])
+                        host, ["rdisc6 -r2 -w1 -q %s 2> /dev/null" % vlan_int]
+                    )
                 vlan_hosts_learned = self.scrape_prometheus_var(
-                    'vlan_hosts_learned', {'vlan': str(vlan)})
+                    "vlan_hosts_learned", {"vlan": str(vlan)}
+                )
                 if vlan_hosts_learned == len(self.hosts_name_ordered()):
                     break
                 time.sleep(1)
             self.assertGreater(
-                vlan_hosts_learned, 1,
-                msg='not all VLAN %u hosts learned (%u)' % (vlan, vlan_hosts_learned))
+                vlan_hosts_learned,
+                1,
+                msg="not all VLAN %u hosts learned (%u)" % (vlan, vlan_hosts_learned),
+            )
 
 
 class FaucetTaggedBroadcastTest(FaucetTaggedTest):
-
     def test_tagged(self):
         super().test_tagged()
         self.verify_broadcast()
         self.verify_no_bcast_to_self()
 
 
 class FaucetTaggedExtLoopProtectTest(FaucetTaggedTest):
-
     CONFIG = """
         interfaces:
             %(port_1)d:
                 tagged_vlans: [100]
                 loop_protect_external: True
             %(port_2)d:
                 tagged_vlans: [100]
@@ -6173,15 +7170,14 @@
         ext_port1, ext_port2, int_port1, int_port2 = self.hosts_name_ordered()
         self.verify_broadcast((ext_port1, ext_port2), False)
         self.verify_broadcast((int_port1, int_port2), True)
         self.verify_unicast((int_port1, int_port2), True)
 
 
 class FaucetTaggedWithUntaggedTest(FaucetTaggedTest):
-
     N_UNTAGGED = 0
     N_TAGGED = 4
     LINKS_PER_HOST = 1
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
@@ -6204,25 +7200,26 @@
                 native_vlan: 200
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         self.ping_all_when_learned()
         native_ips = [
-            ipaddress.ip_interface('10.99.99.%u/24' % (i + 1)) for i in range(len(self.hosts_name_ordered()))]
+            ipaddress.ip_interface("10.99.99.%u/24" % (i + 1))
+            for i in range(len(self.hosts_name_ordered()))
+        ]
         for native_ip, host in zip(native_ips, self.hosts_name_ordered()):
             self.host_ipv4_alias(host, native_ip, intf=host.intf_root_name)
         for own_native_ip, host in zip(native_ips, self.hosts_name_ordered()):
             for native_ip in native_ips:
                 if native_ip != own_native_ip:
                     self.one_ipv4_ping(host, native_ip.ip, intf=host.intf_root_name)
 
 
 class FaucetTaggedSwapVidMirrorTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
     101:
         description: "tagged"
 acls:
@@ -6251,32 +7248,38 @@
     """
 
     def test_tagged(self):
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
 
         def test_acl(tcpdump_host, tcpdump_filter):
             tcpdump_txt = self.tcpdump_helper(
-                tcpdump_host, tcpdump_filter, [
+                tcpdump_host,
+                tcpdump_filter,
+                [
+                    lambda: first_host.cmd(
+                        "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                    ),
                     lambda: first_host.cmd(
-                        'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                    lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-                root_intf=True)
-            self.assertTrue(re.search(
-                '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
-            self.assertTrue(re.search(
-                tcpdump_filter, tcpdump_txt))
+                        " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                    ),
+                ],
+                root_intf=True,
+            )
+            self.assertTrue(
+                re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+            )
+            self.assertTrue(re.search(tcpdump_filter, tcpdump_txt))
 
         # Saw swapped VID on second host
-        test_acl(second_host, 'vlan 101')
+        test_acl(second_host, "vlan 101")
         # Saw original VID on mirror host
-        test_acl(third_host, 'vlan 100')
+        test_acl(third_host, "vlan 100")
 
 
 class FaucetTaggedOrderedSwapVidMirrorTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
     101:
         description: "tagged"
 acls:
@@ -6305,32 +7308,38 @@
     """
 
     def test_tagged(self):
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
 
         def test_acl(tcpdump_host, tcpdump_filter):
             tcpdump_txt = self.tcpdump_helper(
-                tcpdump_host, tcpdump_filter, [
+                tcpdump_host,
+                tcpdump_filter,
+                [
+                    lambda: first_host.cmd(
+                        "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                    ),
                     lambda: first_host.cmd(
-                        'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                    lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-                root_intf=True)
-            self.assertTrue(re.search(
-                '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
-            self.assertTrue(re.search(
-                tcpdump_filter, tcpdump_txt))
+                        " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                    ),
+                ],
+                root_intf=True,
+            )
+            self.assertTrue(
+                re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+            )
+            self.assertTrue(re.search(tcpdump_filter, tcpdump_txt))
 
         # Saw swapped VID on second host
-        test_acl(second_host, 'vlan 101')
+        test_acl(second_host, "vlan 101")
         # Saw original VID on mirror host
-        test_acl(third_host, 'vlan 100')
+        test_acl(third_host, "vlan 100")
 
 
 class FaucetTaggedSwapVidOutputTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         unicast_flood: False
     101:
         description: "tagged"
@@ -6357,29 +7366,35 @@
             %(port_4)d:
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the swapped VLAN VID
-        tcpdump_filter = 'vlan 101'
+        tcpdump_filter = "vlan 101"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-            root_intf=True)
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
-        self.assertTrue(re.search(
-            'vlan 101', tcpdump_txt))
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
+                lambda: first_host.cmd(
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+            root_intf=True,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
+        self.assertTrue(re.search("vlan 101", tcpdump_txt))
 
 
 class FaucetTaggedSwapVidOrderedOutputTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         unicast_flood: False
     101:
         description: "tagged"
@@ -6406,29 +7421,35 @@
             %(port_4)d:
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expected to see the swapped VLAN VID
-        tcpdump_filter = 'vlan 101'
+        tcpdump_filter = "vlan 101"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-            root_intf=True)
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
-        self.assertTrue(re.search(
-            'vlan 101', tcpdump_txt))
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
+                lambda: first_host.cmd(
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+            root_intf=True,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
+        self.assertTrue(re.search("vlan 101", tcpdump_txt))
 
 
 class FaucetTaggedPopVlansOutputTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         unicast_flood: False
 acls:
     1:
@@ -6454,28 +7475,35 @@
                 tagged_vlans: [100]
             %(port_4)d:
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        tcpdump_filter = 'icmp and ether dst 06:06:06:06:06:06 and not vlan'
+        tcpdump_filter = "icmp and ether dst 06:06:06:06:06:06 and not vlan"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
                 lambda: first_host.cmd(
-                    ' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-            packets=10, root_intf=True)
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+            packets=10,
+            root_intf=True,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
 
 
 class FaucetTaggedPopVlansOrderedOutputTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         unicast_flood: False
 acls:
     1:
@@ -6501,70 +7529,81 @@
                 tagged_vlans: [100]
             %(port_4)d:
                 tagged_vlans: [100]
 """
 
     def test_tagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        tcpdump_filter = 'icmp and ether dst 06:06:06:06:06:06 and not vlan'
+        tcpdump_filter = "icmp and ether dst 06:06:06:06:06:06 and not vlan"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), '01:02:03:04:05:06')),
+                    "arp -s %s %s" % (second_host.IP(), "01:02:03:04:05:06")
+                ),
                 lambda: first_host.cmd(
-                    ' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-            packets=10, root_intf=True)
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+            packets=10,
+            root_intf=True,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
 
 
 class FaucetTaggedIPv4ControlPlaneTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         faucet_vips: ["10.0.0.254/24"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         max_resolve_backoff_time: 1
-""" + CONFIG_TAGGED_BOILER
+"""
+        + CONFIG_TAGGED_BOILER
+    )
 
     def test_ping_controller(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         self.one_ipv4_ping(first_host, second_host.IP())
         for host in first_host, second_host:
             self.one_ipv4_controller_ping(host)
 
 
 class FaucetTaggedIPv6ControlPlaneTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         faucet_vips: ["fc00::1:254/112"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         max_resolve_backoff_time: 1
-""" + CONFIG_TAGGED_BOILER
+"""
+        + CONFIG_TAGGED_BOILER
+    )
 
     def test_ping_controller(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        self.add_host_ipv6_address(first_host, 'fc00::1:1/112')
-        self.add_host_ipv6_address(second_host, 'fc00::1:2/112')
-        self.one_ipv6_ping(first_host, 'fc00::1:2')
+        self.add_host_ipv6_address(first_host, "fc00::1:1/112")
+        self.add_host_ipv6_address(second_host, "fc00::1:2/112")
+        self.one_ipv6_ping(first_host, "fc00::1:2")
         for host in first_host, second_host:
             self.one_ipv6_controller_ping(host)
 
 
 class FaucetTaggedICMPv6ACLTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 acls:
     1:
         - rule:
             dl_type: %u
             vlan_vid: 100
             ip_proto: 58
@@ -6576,15 +7615,18 @@
         - rule:
             actions:
                 allow: 1
 vlans:
     100:
         description: "tagged"
         faucet_vips: ["fc00::1:254/112"]
-""" % (IPV6_ETH, '%(port_2)d')
+""" % (
+        IPV6_ETH,
+        "%(port_2)d",
+    )
 
     CONFIG = """
         max_resolve_backoff_time: 1
         interfaces:
             %(port_1)d:
                 tagged_vlans: [100]
                 acl_in: 1
@@ -6594,24 +7636,29 @@
                 tagged_vlans: [100]
             %(port_4)d:
                 tagged_vlans: [100]
 """
 
     def test_icmpv6_acl_match(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        self.add_host_ipv6_address(first_host, 'fc00::1:1/112')
-        self.add_host_ipv6_address(second_host, 'fc00::1:2/112')
-        self.one_ipv6_ping(first_host, 'fc00::1:2')
+        self.add_host_ipv6_address(first_host, "fc00::1:1/112")
+        self.add_host_ipv6_address(second_host, "fc00::1:2/112")
+        self.one_ipv6_ping(first_host, "fc00::1:2")
         self.wait_nonzero_packet_count_flow(
-            {'dl_type': IPV6_ETH, 'ip_proto': 58, 'icmpv6_type': 135,
-             'ipv6_nd_target': 'fc00::1:2'}, table_id=self._PORT_ACL_TABLE)
+            {
+                "dl_type": IPV6_ETH,
+                "ip_proto": 58,
+                "icmpv6_type": 135,
+                "ipv6_nd_target": "fc00::1:2",
+            },
+            table_id=self._PORT_ACL_TABLE,
+        )
 
 
 class FaucetTaggedICMPv6OrderedACLTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 acls:
     1:
         - rule:
             dl_type: %u
             vlan_vid: 100
             ip_proto: 58
@@ -6623,15 +7670,18 @@
         - rule:
             actions:
                 allow: 1
 vlans:
     100:
         description: "tagged"
         faucet_vips: ["fc00::1:254/112"]
-""" % (IPV6_ETH, '%(port_2)d')
+""" % (
+        IPV6_ETH,
+        "%(port_2)d",
+    )
 
     CONFIG = """
         max_resolve_backoff_time: 1
         interfaces:
             %(port_1)d:
                 tagged_vlans: [100]
                 acl_in: 1
@@ -6641,24 +7691,29 @@
                 tagged_vlans: [100]
             %(port_4)d:
                 tagged_vlans: [100]
 """
 
     def test_icmpv6_acl_match(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        self.add_host_ipv6_address(first_host, 'fc00::1:1/112')
-        self.add_host_ipv6_address(second_host, 'fc00::1:2/112')
-        self.one_ipv6_ping(first_host, 'fc00::1:2')
+        self.add_host_ipv6_address(first_host, "fc00::1:1/112")
+        self.add_host_ipv6_address(second_host, "fc00::1:2/112")
+        self.one_ipv6_ping(first_host, "fc00::1:2")
         self.wait_nonzero_packet_count_flow(
-            {'dl_type': IPV6_ETH, 'ip_proto': 58, 'icmpv6_type': 135,
-             'ipv6_nd_target': 'fc00::1:2'}, table_id=self._PORT_ACL_TABLE)
+            {
+                "dl_type": IPV6_ETH,
+                "ip_proto": 58,
+                "icmpv6_type": 135,
+                "ipv6_nd_target": "fc00::1:2",
+            },
+            table_id=self._PORT_ACL_TABLE,
+        )
 
 
 class FaucetTaggedIPv4RouteTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         faucet_vips: ["10.0.0.254/24"]
         routes:
             - route:
@@ -6690,34 +7745,38 @@
                 native_vlan: 200
 """
 
     def test_tagged(self):
         self._enable_event_log()
         host_pair = self.hosts_name_ordered()[:2]
         first_host, second_host = host_pair
-        first_host_routed_ip = ipaddress.ip_interface('10.0.1.1/24')
-        second_host_routed_ip = ipaddress.ip_interface('10.0.2.1/24')
+        first_host_routed_ip = ipaddress.ip_interface("10.0.1.1/24")
+        second_host_routed_ip = ipaddress.ip_interface("10.0.2.1/24")
         for _coldstart in range(2):
             for _swaps in range(3):
                 self.verify_ipv4_routing(
-                    first_host, first_host_routed_ip,
-                    second_host, second_host_routed_ip)
+                    first_host, first_host_routed_ip, second_host, second_host_routed_ip
+                )
                 self.swap_host_macs(first_host, second_host)
             self.coldstart_conf()
         # change of a VLAN/ports not involved in routing, should be a warm start.
         for vid in (300, 200):
             self.change_port_config(
-                self.port_map['port_4'], 'native_vlan', vid,
-                restart=True, cold_start=False)
+                self.port_map["port_4"],
+                "native_vlan",
+                vid,
+                restart=True,
+                cold_start=False,
+            )
         self.wait_until_matching_lines_from_file(
-            r'.+L3_LEARN.+10.0.0.[12].+', self.event_log)
+            r".+L3_LEARN.+10.0.0.[12].+", self.event_log
+        )
 
 
 class FaucetTaggedTargetedResolutionIPv4RouteTest(FaucetTaggedIPv4RouteTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         faucet_vips: ["10.0.0.254/24"]
         targeted_gw_resolution: True
         routes:
@@ -6730,106 +7789,115 @@
             - route:
                 ip_dst: "10.0.3.0/24"
                 ip_gw: "10.0.0.2"
 """
 
 
 class FaucetTaggedProactiveNeighborIPv4RouteTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         faucet_vips: ["10.0.0.254/24"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
         proactive_learn_v4: True
-""" + CONFIG_TAGGED_BOILER
+"""
+        + CONFIG_TAGGED_BOILER
+    )
 
     def test_tagged(self):
         host_pair = self.hosts_name_ordered()[:2]
         first_host, second_host = host_pair
-        first_host_alias_ip = ipaddress.ip_interface('10.0.0.99/24')
+        first_host_alias_ip = ipaddress.ip_interface("10.0.0.99/24")
         first_host_alias_host_ip = ipaddress.ip_interface(
-            ipaddress.ip_network(first_host_alias_ip.ip))
+            ipaddress.ip_network(first_host_alias_ip.ip)
+        )
         self.host_ipv4_alias(first_host, first_host_alias_ip)
         self.add_host_route(second_host, first_host_alias_host_ip, self.FAUCET_VIPV4.ip)
         self.one_ipv4_ping(second_host, first_host_alias_ip.ip)
         self.assertGreater(
-            self.scrape_prometheus_var(
-                'vlan_neighbors', {'ipv': '4', 'vlan': '100'}),
-            1)
+            self.scrape_prometheus_var("vlan_neighbors", {"ipv": "4", "vlan": "100"}), 1
+        )
 
 
 class FaucetTaggedProactiveNeighborIPv6RouteTest(FaucetTaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "tagged"
         faucet_vips: ["fc00::1:3/64"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
         proactive_learn_v6: True
-""" + CONFIG_TAGGED_BOILER
+"""
+        + CONFIG_TAGGED_BOILER
+    )
 
     def test_tagged(self):
         host_pair = self.hosts_name_ordered()[:2]
         first_host, second_host = host_pair
-        first_host_alias_ip = ipaddress.ip_interface('fc00::1:99/64')
-        faucet_vip_ip = ipaddress.ip_interface('fc00::1:3/126')
+        first_host_alias_ip = ipaddress.ip_interface("fc00::1:99/64")
+        faucet_vip_ip = ipaddress.ip_interface("fc00::1:3/126")
         first_host_alias_host_ip = ipaddress.ip_interface(
-            ipaddress.ip_network(first_host_alias_ip.ip))
-        self.add_host_ipv6_address(first_host, ipaddress.ip_interface('fc00::1:1/64'))
+            ipaddress.ip_network(first_host_alias_ip.ip)
+        )
+        self.add_host_ipv6_address(first_host, ipaddress.ip_interface("fc00::1:1/64"))
         # We use a narrower mask to force second_host to use the /128 route,
         # since otherwise it would realize :99 is directly connected via ND and send direct.
-        self.add_host_ipv6_address(second_host, ipaddress.ip_interface('fc00::1:2/126'))
+        self.add_host_ipv6_address(second_host, ipaddress.ip_interface("fc00::1:2/126"))
         self.add_host_ipv6_address(first_host, first_host_alias_ip)
         self.add_host_route(second_host, first_host_alias_host_ip, faucet_vip_ip.ip)
         self.one_ipv6_ping(second_host, first_host_alias_ip.ip)
         self.assertGreater(
-            self.scrape_prometheus_var(
-                'vlan_neighbors', {'ipv': '6', 'vlan': '100'}),
-            1)
+            self.scrape_prometheus_var("vlan_neighbors", {"ipv": "6", "vlan": "100"}), 1
+        )
 
 
 class FaucetUntaggedIPv4GlobalInterVLANRouteTest(FaucetUntaggedTest):
-
     NUM_FAUCET_CONTROLLERS = 1
 
-    FAUCET_MAC2 = '0e:00:00:00:00:02'
+    FAUCET_MAC2 = "0e:00:00:00:00:02"
 
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         faucet_vips: ["10.100.0.254/24"]
     200:
         faucet_vips: ["10.200.0.254/24"]
         faucet_mac: "%s"
-""" % FAUCET_MAC2 + """
+"""
+        % FAUCET_MAC2
+        + """
 routers:
     global:
         vlans: [100, 200]
         bgp:
             as: 1
             connect_mode: "passive"
             port: %(bgp_port)d
             routerid: "1.1.1.1"
             server_addresses: ["127.0.0.1", "::1"]
             neighbor_addresses: ["127.0.0.1", "::1"]
             vlan: 100
-""" + """
+"""
+        + """
             neighbor_as: %u
-""" % PEER_BGP_AS
+"""
+        % PEER_BGP_AS
+    )
 
     CONFIG = """
         global_vlan: 300
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
         proactive_learn_v4: True
         interfaces:
@@ -6847,58 +7915,64 @@
     static {
       route 10.99.99.0/24 next-hop 10.200.0.1 local-preference 100;
       route 10.0.5.0/24 next-hop 127.0.0.1;
     }
 """
     exabgp_log = None
     exabgp_err = None
-    config_ports = {'bgp_port': None}
+    config_ports = {"bgp_port": None}
 
     def post_start_net(self):
         exabgp_conf = self.get_exabgp_conf(
-            mininet_test_util.LOCALHOST, self.exabgp_peer_conf)
+            mininet_test_util.LOCALHOST, self.exabgp_peer_conf
+        )
         self.exabgp_log, self.exabgp_err = self.start_exabgp(exabgp_conf)
 
     def test_untagged(self):
-        first_host_ip = ipaddress.ip_interface('10.100.0.1/24')
-        first_faucet_vip = ipaddress.ip_interface('10.100.0.254/24')
-        second_host_ip = ipaddress.ip_interface('10.200.0.1/24')
-        second_faucet_vip = ipaddress.ip_interface('10.200.0.254/24')
+        first_host_ip = ipaddress.ip_interface("10.100.0.1/24")
+        first_faucet_vip = ipaddress.ip_interface("10.100.0.254/24")
+        second_host_ip = ipaddress.ip_interface("10.200.0.1/24")
+        second_faucet_vip = ipaddress.ip_interface("10.200.0.254/24")
         first_host, second_host = self.hosts_name_ordered()[:2]
         first_host.setIP(str(first_host_ip.ip), prefixLen=24)
         second_host.setIP(str(second_host_ip.ip), prefixLen=24)
         self.add_host_route(first_host, second_host_ip, first_faucet_vip.ip)
         self.add_host_route(second_host, first_host_ip, second_faucet_vip.ip)
         self.one_ipv4_ping(first_host, second_host_ip.ip)
         self.one_ipv4_ping(second_host, first_host_ip.ip)
         self.assertEqual(
-            self._ip_neigh(first_host, first_faucet_vip.ip, 4), self.FAUCET_MAC)
+            self._ip_neigh(first_host, first_faucet_vip.ip, 4), self.FAUCET_MAC
+        )
         self.assertEqual(
-            self._ip_neigh(second_host, second_faucet_vip.ip, 4), self.FAUCET_MAC2)
+            self._ip_neigh(second_host, second_faucet_vip.ip, 4), self.FAUCET_MAC2
+        )
         self.wait_for_route_as_flow(
-            second_host.MAC(), ipaddress.IPv4Network('10.99.99.0/24'), vlan_vid=300)
-        self.verify_invalid_bgp_route(r'.+10.0.5.0\/24.+because nexthop not in VLAN.+')
+            second_host.MAC(), ipaddress.IPv4Network("10.99.99.0/24"), vlan_vid=300
+        )
+        self.verify_invalid_bgp_route(r".+10.0.5.0\/24.+because nexthop not in VLAN.+")
 
 
 class FaucetUntaggedIPv4InterVLANRouteTest(FaucetUntaggedTest):
+    FAUCET_MAC2 = "0e:00:00:00:00:02"
 
-    FAUCET_MAC2 = '0e:00:00:00:00:02'
-
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         faucet_vips: ["10.100.0.254/24", "169.254.1.1/24"]
     vlanb:
         vid: 200
         faucet_vips: ["10.200.0.254/24", "169.254.2.1/24"]
         faucet_mac: "%s"
 routers:
     router-1:
         vlans: [100, vlanb]
-""" % FAUCET_MAC2
+"""
+        % FAUCET_MAC2
+    )
 
     CONFIG = """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
         proactive_learn_v4: True
         interfaces:
             %(port_1)d:
@@ -6908,112 +7982,124 @@
             %(port_3)d:
                 native_vlan: vlanb
             %(port_4)d:
                 native_vlan: vlanb
 """
 
     def test_untagged(self):
-        first_host_ip = ipaddress.ip_interface('10.100.0.1/24')
-        first_faucet_vip = ipaddress.ip_interface('10.100.0.254/24')
-        second_host_ip = ipaddress.ip_interface('10.200.0.1/24')
-        second_faucet_vip = ipaddress.ip_interface('10.200.0.254/24')
+        first_host_ip = ipaddress.ip_interface("10.100.0.1/24")
+        first_faucet_vip = ipaddress.ip_interface("10.100.0.254/24")
+        second_host_ip = ipaddress.ip_interface("10.200.0.1/24")
+        second_faucet_vip = ipaddress.ip_interface("10.200.0.254/24")
         first_host, second_host = self.hosts_name_ordered()[:2]
         first_host.setIP(str(first_host_ip.ip), prefixLen=24)
         second_host.setIP(str(second_host_ip.ip), prefixLen=24)
         self.add_host_route(first_host, second_host_ip, first_faucet_vip.ip)
         self.add_host_route(second_host, first_host_ip, second_faucet_vip.ip)
 
         for vlanb_vid in (300, 200):
             self.one_ipv4_ping(first_host, second_host_ip.ip)
             self.one_ipv4_ping(second_host, first_host_ip.ip)
             self.assertEqual(
-                self._ip_neigh(first_host, first_faucet_vip.ip, 4), self.FAUCET_MAC)
+                self._ip_neigh(first_host, first_faucet_vip.ip, 4), self.FAUCET_MAC
+            )
             self.assertEqual(
-                self._ip_neigh(second_host, second_faucet_vip.ip, 4), self.FAUCET_MAC2)
+                self._ip_neigh(second_host, second_faucet_vip.ip, 4), self.FAUCET_MAC2
+            )
             self.change_vlan_config(
-                'vlanb', 'vid', vlanb_vid, restart=True, cold_start=True)
+                "vlanb", "vid", vlanb_vid, restart=True, cold_start=True
+            )
 
 
 class FaucetUntaggedPortSwapIPv4InterVLANRouteTest(FaucetUntaggedTest):
+    FAUCET_MAC2 = "0e:00:00:00:00:02"
 
-    FAUCET_MAC2 = '0e:00:00:00:00:02'
-
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     vlana:
         vid: 100
         faucet_vips: ["10.100.0.254/24", "169.254.1.1/24"]
     vlanb:
         vid: 200
         faucet_vips: ["10.200.0.254/24", "169.254.2.1/24"]
         faucet_mac: "%s"
 routers:
     router-1:
         vlans: [vlana, vlanb]
-""" % FAUCET_MAC2
+"""
+        % FAUCET_MAC2
+    )
 
     CONFIG = """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
         proactive_learn_v4: True
         interfaces:
             %(port_1)d:
                 native_vlan: vlana
             %(port_2)d:
                 native_vlan: vlanb
 """
 
     def test_untagged(self):
-        first_host_ip = ipaddress.ip_interface('10.100.0.1/24')
-        first_faucet_vip = ipaddress.ip_interface('10.100.0.254/24')
-        second_host_ip = ipaddress.ip_interface('10.200.0.1/24')
-        second_faucet_vip = ipaddress.ip_interface('10.200.0.254/24')
+        first_host_ip = ipaddress.ip_interface("10.100.0.1/24")
+        first_faucet_vip = ipaddress.ip_interface("10.100.0.254/24")
+        second_host_ip = ipaddress.ip_interface("10.200.0.1/24")
+        second_faucet_vip = ipaddress.ip_interface("10.200.0.254/24")
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
 
         def test_connectivity(host_a, host_b):
             host_a.setIP(str(first_host_ip.ip), prefixLen=24)
             host_b.setIP(str(second_host_ip.ip), prefixLen=24)
             self.add_host_route(host_a, second_host_ip, first_faucet_vip.ip)
             self.add_host_route(host_b, first_host_ip, second_faucet_vip.ip)
             self.one_ipv4_ping(host_a, second_host_ip.ip)
             self.one_ipv4_ping(host_b, first_host_ip.ip)
             self.assertEqual(
-                self._ip_neigh(host_a, first_faucet_vip.ip, 4), self.FAUCET_MAC)
+                self._ip_neigh(host_a, first_faucet_vip.ip, 4), self.FAUCET_MAC
+            )
             self.assertEqual(
-                self._ip_neigh(host_b, second_faucet_vip.ip, 4), self.FAUCET_MAC2)
+                self._ip_neigh(host_b, second_faucet_vip.ip, 4), self.FAUCET_MAC2
+            )
 
         test_connectivity(first_host, second_host)
 
         # Delete port 1, add port 3
         self.change_port_config(
-            self.port_map['port_1'], None, None,
-            restart=False, cold_start=False)
+            self.port_map["port_1"], None, None, restart=False, cold_start=False
+        )
         self.add_port_config(
-            self.port_map['port_3'], {'native_vlan': 'vlana'},
-            restart=True, cold_start=True)
+            self.port_map["port_3"],
+            {"native_vlan": "vlana"},
+            restart=True,
+            cold_start=True,
+        )
 
         test_connectivity(third_host, second_host)
 
 
 class FaucetUntaggedExpireIPv4InterVLANRouteTest(FaucetUntaggedTest):
+    FAUCET_MAC2 = "0e:00:00:00:00:02"
 
-    FAUCET_MAC2 = '0e:00:00:00:00:02'
-
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         faucet_vips: ["10.100.0.254/24"]
     vlanb:
         vid: 200
         faucet_vips: ["10.200.0.254/24"]
         faucet_mac: "%s"
 routers:
     router-1:
         vlans: [100, vlanb]
-""" % FAUCET_MAC2
+"""
+        % FAUCET_MAC2
+    )
 
     CONFIG = """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
         max_host_fib_retry_count: 2
         proactive_learn_v4: True
         interfaces:
@@ -7024,50 +8110,52 @@
             %(port_3)d:
                 native_vlan: vlanb
             %(port_4)d:
                 native_vlan: vlanb
 """
 
     def test_untagged(self):
-        first_host_ip = ipaddress.ip_interface('10.100.0.1/24')
-        first_faucet_vip = ipaddress.ip_interface('10.100.0.254/24')
-        second_host_ip = ipaddress.ip_interface('10.200.0.1/24')
-        second_faucet_vip = ipaddress.ip_interface('10.200.0.254/24')
+        first_host_ip = ipaddress.ip_interface("10.100.0.1/24")
+        first_faucet_vip = ipaddress.ip_interface("10.100.0.254/24")
+        second_host_ip = ipaddress.ip_interface("10.200.0.1/24")
+        second_faucet_vip = ipaddress.ip_interface("10.200.0.254/24")
         first_host, second_host = self.hosts_name_ordered()[:2]
         first_host.setIP(str(first_host_ip.ip), prefixLen=24)
         second_host.setIP(str(second_host_ip.ip), prefixLen=24)
         self.add_host_route(first_host, second_host_ip, first_faucet_vip.ip)
         self.add_host_route(second_host, first_host_ip, second_faucet_vip.ip)
         self.one_ipv4_ping(first_host, second_host_ip.ip)
         self.one_ipv4_ping(second_host, first_host_ip.ip)
-        second_host.cmd('ifconfig %s down' % second_host.defaultIntf().name)
-        expired_re = r'.+expiring dead route %s.+' % second_host_ip.ip
+        second_host.cmd("ifconfig %s down" % second_host.defaultIntf().name)
+        expired_re = r".+expiring dead route %s.+" % second_host_ip.ip
         self.wait_until_matching_lines_from_faucet_log_files(expired_re)
-        second_host.cmd('ifconfig %s up' % second_host.defaultIntf().name)
+        second_host.cmd("ifconfig %s up" % second_host.defaultIntf().name)
         self.add_host_route(second_host, first_host_ip, second_faucet_vip.ip)
         self.one_ipv4_ping(second_host, first_host_ip.ip)
         self.one_ipv4_ping(first_host, second_host_ip.ip)
 
 
 class FaucetUntaggedIPv6InterVLANRouteTest(FaucetUntaggedTest):
+    FAUCET_MAC2 = "0e:00:00:00:00:02"
 
-    FAUCET_MAC2 = '0e:00:00:00:00:02'
-
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         faucet_vips: ["fc00::1:254/112", "fe80::1:254/112"]
     vlanb:
         vid: 200
         faucet_vips: ["fc01::1:254/112", "fe80::2:254/112"]
         faucet_mac: "%s"
 routers:
     router-1:
         vlans: [100, vlanb]
-""" % FAUCET_MAC2
+"""
+        % FAUCET_MAC2
+    )
 
     CONFIG = """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
         proactive_learn_v6: True
         interfaces:
             %(port_1)d:
@@ -7079,28 +8167,25 @@
             %(port_4)d:
                 native_vlan: vlanb
 """
 
     def test_untagged(self):
         host_pair = self.hosts_name_ordered()[:2]
         first_host, second_host = host_pair
-        first_host_net = ipaddress.ip_interface('fc00::1:1/64')
-        second_host_net = ipaddress.ip_interface('fc01::1:1/64')
+        first_host_net = ipaddress.ip_interface("fc00::1:1/64")
+        second_host_net = ipaddress.ip_interface("fc01::1:1/64")
         self.add_host_ipv6_address(first_host, first_host_net)
         self.add_host_ipv6_address(second_host, second_host_net)
-        self.add_host_route(
-            first_host, second_host_net, self.FAUCET_VIPV6.ip)
-        self.add_host_route(
-            second_host, first_host_net, self.FAUCET_VIPV6_2.ip)
+        self.add_host_route(first_host, second_host_net, self.FAUCET_VIPV6.ip)
+        self.add_host_route(second_host, first_host_net, self.FAUCET_VIPV6_2.ip)
         self.one_ipv6_ping(first_host, second_host_net.ip)
         self.one_ipv6_ping(second_host, first_host_net.ip)
 
 
 class FaucetUntaggedIPv4PolicyRouteTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "100"
         faucet_vips: ["10.0.0.254/24"]
         acl_in: pbr
     200:
@@ -7158,45 +8243,46 @@
                 native_vlan: 100
 """
 
     def test_untagged(self):
         # 10.99.0.1 is on b2, and 10.99.0.2 is on b3
         # we want to route 10.99.0.0/24 to b2, but we want
         # want to PBR 10.99.0.2/32 to b3.
-        first_host_ip = ipaddress.ip_interface('10.0.0.1/24')
-        first_faucet_vip = ipaddress.ip_interface('10.0.0.254/24')
-        second_host_ip = ipaddress.ip_interface('10.20.0.2/24')
-        second_faucet_vip = ipaddress.ip_interface('10.20.0.254/24')
-        third_host_ip = ipaddress.ip_interface('10.30.0.3/24')
-        third_faucet_vip = ipaddress.ip_interface('10.30.0.254/24')
+        first_host_ip = ipaddress.ip_interface("10.0.0.1/24")
+        first_faucet_vip = ipaddress.ip_interface("10.0.0.254/24")
+        second_host_ip = ipaddress.ip_interface("10.20.0.2/24")
+        second_faucet_vip = ipaddress.ip_interface("10.20.0.254/24")
+        third_host_ip = ipaddress.ip_interface("10.30.0.3/24")
+        third_faucet_vip = ipaddress.ip_interface("10.30.0.254/24")
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
-        remote_ip = ipaddress.ip_interface('10.99.0.1/24')
-        remote_ip2 = ipaddress.ip_interface('10.99.0.2/24')
+        remote_ip = ipaddress.ip_interface("10.99.0.1/24")
+        remote_ip2 = ipaddress.ip_interface("10.99.0.2/24")
         second_host.setIP(str(second_host_ip.ip), prefixLen=24)
         third_host.setIP(str(third_host_ip.ip), prefixLen=24)
         self.host_ipv4_alias(second_host, remote_ip)
         self.host_ipv4_alias(third_host, remote_ip2)
         self.add_host_route(first_host, remote_ip, first_faucet_vip.ip)
         self.add_host_route(second_host, first_host_ip, second_faucet_vip.ip)
         self.add_host_route(third_host, first_host_ip, third_faucet_vip.ip)
         # ensure all nexthops resolved.
         self.one_ipv4_ping(first_host, first_faucet_vip.ip)
         self.one_ipv4_ping(second_host, second_faucet_vip.ip)
         self.one_ipv4_ping(third_host, third_faucet_vip.ip)
         self.wait_for_route_as_flow(
-            second_host.MAC(), ipaddress.IPv4Network('10.99.0.0/24'), vlan_vid=200)
+            second_host.MAC(), ipaddress.IPv4Network("10.99.0.0/24"), vlan_vid=200
+        )
         self.wait_for_route_as_flow(
-            third_host.MAC(), ipaddress.IPv4Network('10.99.0.0/24'), vlan_vid=300)
+            third_host.MAC(), ipaddress.IPv4Network("10.99.0.0/24"), vlan_vid=300
+        )
         # verify b1 can reach 10.99.0.1 and .2 on b2 and b3 respectively.
         self.one_ipv4_ping(first_host, remote_ip.ip)
         self.one_ipv4_ping(first_host, remote_ip2.ip)
 
 
 class FaucetUntaggedIPv4PolicyRouteOrdereredTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "100"
         faucet_vips: ["10.0.0.254/24"]
         acl_in: pbr
     200:
@@ -7254,279 +8340,295 @@
                 native_vlan: 100
 """
 
     def test_untagged(self):
         # 10.99.0.1 is on b2, and 10.99.0.2 is on b3
         # we want to route 10.99.0.0/24 to b2, but we want
         # want to PBR 10.99.0.2/32 to b3.
-        first_host_ip = ipaddress.ip_interface('10.0.0.1/24')
-        first_faucet_vip = ipaddress.ip_interface('10.0.0.254/24')
-        second_host_ip = ipaddress.ip_interface('10.20.0.2/24')
-        second_faucet_vip = ipaddress.ip_interface('10.20.0.254/24')
-        third_host_ip = ipaddress.ip_interface('10.30.0.3/24')
-        third_faucet_vip = ipaddress.ip_interface('10.30.0.254/24')
+        first_host_ip = ipaddress.ip_interface("10.0.0.1/24")
+        first_faucet_vip = ipaddress.ip_interface("10.0.0.254/24")
+        second_host_ip = ipaddress.ip_interface("10.20.0.2/24")
+        second_faucet_vip = ipaddress.ip_interface("10.20.0.254/24")
+        third_host_ip = ipaddress.ip_interface("10.30.0.3/24")
+        third_faucet_vip = ipaddress.ip_interface("10.30.0.254/24")
         first_host, second_host, third_host = self.hosts_name_ordered()[:3]
-        remote_ip = ipaddress.ip_interface('10.99.0.1/24')
-        remote_ip2 = ipaddress.ip_interface('10.99.0.2/24')
+        remote_ip = ipaddress.ip_interface("10.99.0.1/24")
+        remote_ip2 = ipaddress.ip_interface("10.99.0.2/24")
         second_host.setIP(str(second_host_ip.ip), prefixLen=24)
         third_host.setIP(str(third_host_ip.ip), prefixLen=24)
         self.host_ipv4_alias(second_host, remote_ip)
         self.host_ipv4_alias(third_host, remote_ip2)
         self.add_host_route(first_host, remote_ip, first_faucet_vip.ip)
         self.add_host_route(second_host, first_host_ip, second_faucet_vip.ip)
         self.add_host_route(third_host, first_host_ip, third_faucet_vip.ip)
         # ensure all nexthops resolved.
         self.one_ipv4_ping(first_host, first_faucet_vip.ip)
         self.one_ipv4_ping(second_host, second_faucet_vip.ip)
         self.one_ipv4_ping(third_host, third_faucet_vip.ip)
         self.wait_for_route_as_flow(
-            second_host.MAC(), ipaddress.IPv4Network('10.99.0.0/24'), vlan_vid=200)
+            second_host.MAC(), ipaddress.IPv4Network("10.99.0.0/24"), vlan_vid=200
+        )
         self.wait_for_route_as_flow(
-            third_host.MAC(), ipaddress.IPv4Network('10.99.0.0/24'), vlan_vid=300)
+            third_host.MAC(), ipaddress.IPv4Network("10.99.0.0/24"), vlan_vid=300
+        )
         # verify b1 can reach 10.99.0.1 and .2 on b2 and b3 respectively.
         self.one_ipv4_ping(first_host, remote_ip.ip)
         self.one_ipv4_ping(first_host, remote_ip2.ip)
 
 
 class FaucetUntaggedMixedIPv4RouteTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["172.16.0.254/24", "10.0.0.254/24"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         arp_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_untagged(self):
         host_pair = self.hosts_name_ordered()[:2]
         first_host, second_host = host_pair
-        first_host_net = ipaddress.ip_interface('10.0.0.1/24')
-        second_host_net = ipaddress.ip_interface('172.16.0.1/24')
+        first_host_net = ipaddress.ip_interface("10.0.0.1/24")
+        second_host_net = ipaddress.ip_interface("172.16.0.1/24")
         second_host.setIP(str(second_host_net.ip), prefixLen=24)
         self.one_ipv4_ping(first_host, self.FAUCET_VIPV4.ip)
         self.one_ipv4_ping(second_host, self.FAUCET_VIPV4_2.ip)
-        self.add_host_route(
-            first_host, second_host_net, self.FAUCET_VIPV4.ip)
-        self.add_host_route(
-            second_host, first_host_net, self.FAUCET_VIPV4_2.ip)
+        self.add_host_route(first_host, second_host_net, self.FAUCET_VIPV4.ip)
+        self.add_host_route(second_host, first_host_net, self.FAUCET_VIPV4_2.ip)
         self.one_ipv4_ping(first_host, second_host_net.ip)
         self.one_ipv4_ping(second_host, first_host_net.ip)
 
 
 class FaucetUntaggedMixedIPv6RouteTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["fc00::1:254/112", "fc01::1:254/112"]
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_untagged(self):
         host_pair = self.hosts_name_ordered()[:2]
         first_host, second_host = host_pair
-        first_host_net = ipaddress.ip_interface('fc00::1:1/64')
-        second_host_net = ipaddress.ip_interface('fc01::1:1/64')
+        first_host_net = ipaddress.ip_interface("fc00::1:1/64")
+        second_host_net = ipaddress.ip_interface("fc01::1:1/64")
         self.add_host_ipv6_address(first_host, first_host_net)
         self.one_ipv6_ping(first_host, self.FAUCET_VIPV6.ip)
         self.add_host_ipv6_address(second_host, second_host_net)
         self.one_ipv6_ping(second_host, self.FAUCET_VIPV6_2.ip)
-        self.add_host_route(
-            first_host, second_host_net, self.FAUCET_VIPV6.ip)
-        self.add_host_route(
-            second_host, first_host_net, self.FAUCET_VIPV6_2.ip)
+        self.add_host_route(first_host, second_host_net, self.FAUCET_VIPV6.ip)
+        self.add_host_route(second_host, first_host_net, self.FAUCET_VIPV6_2.ip)
         self.one_ipv6_ping(first_host, second_host_net.ip)
         self.one_ipv6_ping(second_host, first_host_net.ip)
 
 
 class FaucetUntaggedBGPIPv6DefaultRouteTest(FaucetUntaggedTest):
-
     NUM_FAUCET_CONTROLLERS = 1
 
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["fc00::1:254/112"]
 routers:
     router1:
         bgp:
             as: 1
             connect_mode: "passive"
             port: %(bgp_port)d
             routerid: "1.1.1.1"
             server_addresses: ["::1"]
             neighbor_addresses: ["::1"]
             vlan: 100
-""" + """
+"""
+        + """
             neighbor_as: %u
-""" % PEER_BGP_AS
+"""
+        % PEER_BGP_AS
+    )
 
-    CONFIG = """
+    CONFIG = (
+        """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     exabgp_peer_conf = """
     static {
       route ::/0 next-hop fc00::1:1 local-preference 100;
     }
 """
 
     exabgp_log = None
     exabgp_err = None
-    config_ports = {'bgp_port': None}
+    config_ports = {"bgp_port": None}
 
     def post_start_net(self):
-        exabgp_conf = self.get_exabgp_conf('::1', self.exabgp_peer_conf)
+        exabgp_conf = self.get_exabgp_conf("::1", self.exabgp_peer_conf)
         self.exabgp_log, self.exabgp_err = self.start_exabgp(exabgp_conf)
 
     def test_untagged(self):
         self.assertEqual(self.NUM_FAUCET_CONTROLLERS, 1)
         first_host, second_host = self.hosts_name_ordered()[:2]
-        self.add_host_ipv6_address(first_host, 'fc00::1:1/112')
-        self.add_host_ipv6_address(second_host, 'fc00::1:2/112')
-        first_host_alias_ip = ipaddress.ip_interface('fc00::50:1/112')
+        self.add_host_ipv6_address(first_host, "fc00::1:1/112")
+        self.add_host_ipv6_address(second_host, "fc00::1:2/112")
+        first_host_alias_ip = ipaddress.ip_interface("fc00::50:1/112")
         first_host_alias_host_ip = ipaddress.ip_interface(
-            ipaddress.ip_network(first_host_alias_ip.ip))
+            ipaddress.ip_network(first_host_alias_ip.ip)
+        )
         self.add_host_ipv6_address(first_host, first_host_alias_ip)
-        self.wait_bgp_up('::1', 100, self.exabgp_log, self.exabgp_err)
+        self.wait_bgp_up("::1", 100, self.exabgp_log, self.exabgp_err)
         self.assertGreater(
             self.scrape_prometheus_var(
-                'bgp_neighbor_routes', {'ipv': '6', 'vlan': '100'}),
-            0)
+                "bgp_neighbor_routes", {"ipv": "6", "vlan": "100"}
+            ),
+            0,
+        )
         self.wait_exabgp_sent_updates(self.exabgp_log)
-        self.add_host_route(
-            second_host, first_host_alias_host_ip, self.FAUCET_VIPV6.ip)
+        self.add_host_route(second_host, first_host_alias_host_ip, self.FAUCET_VIPV6.ip)
         self.one_ipv6_ping(second_host, first_host_alias_ip.ip)
         self.one_ipv6_controller_ping(first_host)
         self.coldstart_conf()
 
 
 class FaucetUntaggedBGPIPv6RouteTest(FaucetUntaggedTest):
-
     NUM_FAUCET_CONTROLLERS = 1
 
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["fc00::1:254/112"]
 routers:
     router1:
         bgp:
             as: 1
             connect_mode: "passive"
             port: %(bgp_port)d
             routerid: "1.1.1.1"
             server_addresses: ["::1"]
             neighbor_addresses: ["::1"]
             vlan: 100
-""" + """
+"""
+        + """
             neighbor_as: %u
-""" % PEER_BGP_AS
+"""
+        % PEER_BGP_AS
+    )
 
-    CONFIG = """
+    CONFIG = (
+        """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     exabgp_peer_conf = """
     static {
       route fc00::10:0/112 next-hop fc00::1:1 local-preference 100;
       route fc00::20:0/112 next-hop fc00::1:2 local-preference 100;
       route fc00::30:0/112 next-hop fc00::1:2 local-preference 100;
       route fc00::40:0/112 next-hop fc00::1:254;
       route fc00::50:0/112 next-hop fc00::2:2;
     }
 """
     exabgp_log = None
     exabgp_err = None
-    config_ports = {'bgp_port': None}
+    config_ports = {"bgp_port": None}
 
     def post_start_net(self):
-        exabgp_conf = self.get_exabgp_conf('::1', self.exabgp_peer_conf)
+        exabgp_conf = self.get_exabgp_conf("::1", self.exabgp_peer_conf)
         self.exabgp_log, self.exabgp_err = self.start_exabgp(exabgp_conf)
 
     def test_untagged(self):
         self.assertEqual(self.NUM_FAUCET_CONTROLLERS, 1)
         first_host, second_host = self.hosts_name_ordered()[:2]
-        self.wait_bgp_up('::1', 100, self.exabgp_log, self.exabgp_err)
+        self.wait_bgp_up("::1", 100, self.exabgp_log, self.exabgp_err)
         self.assertGreater(
             self.scrape_prometheus_var(
-                'bgp_neighbor_routes', {'ipv': '6', 'vlan': '100'}),
-            0)
+                "bgp_neighbor_routes", {"ipv": "6", "vlan": "100"}
+            ),
+            0,
+        )
         self.wait_exabgp_sent_updates(self.exabgp_log)
-        self.verify_invalid_bgp_route(r'.+fc00::40:0\/112.+cannot be us$')
+        self.verify_invalid_bgp_route(r".+fc00::40:0\/112.+cannot be us$")
         self.verify_ipv6_routing_mesh()
         self.flap_all_switch_ports()
         self.verify_ipv6_routing_mesh()
         for host in first_host, second_host:
             self.one_ipv6_controller_ping(host)
         self.verify_traveling_dhcp_mac()
 
 
 class FaucetUntaggedSameVlanIPv6RouteTest(FaucetUntaggedTest):
-
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["fc00::10:1/112", "fc00::20:1/112"]
         routes:
             - route:
                 ip_dst: "fc00::10:0/112"
                 ip_gw: "fc00::10:2"
             - route:
                 ip_dst: "fc00::20:0/112"
                 ip_gw: "fc00::20:2"
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[:2]
-        first_host_ip = ipaddress.ip_interface('fc00::10:2/112')
-        first_host_ctrl_ip = ipaddress.ip_address('fc00::10:1')
-        second_host_ip = ipaddress.ip_interface('fc00::20:2/112')
-        second_host_ctrl_ip = ipaddress.ip_address('fc00::20:1')
+        first_host_ip = ipaddress.ip_interface("fc00::10:2/112")
+        first_host_ctrl_ip = ipaddress.ip_address("fc00::10:1")
+        second_host_ip = ipaddress.ip_interface("fc00::20:2/112")
+        second_host_ctrl_ip = ipaddress.ip_address("fc00::20:1")
         self.add_host_ipv6_address(first_host, first_host_ip)
         self.add_host_ipv6_address(second_host, second_host_ip)
-        self.add_host_route(
-            first_host, second_host_ip, first_host_ctrl_ip)
-        self.add_host_route(
-            second_host, first_host_ip, second_host_ctrl_ip)
-        self.wait_for_route_as_flow(
-            first_host.MAC(), first_host_ip.network)
-        self.wait_for_route_as_flow(
-            second_host.MAC(), second_host_ip.network)
+        self.add_host_route(first_host, second_host_ip, first_host_ctrl_ip)
+        self.add_host_route(second_host, first_host_ip, second_host_ctrl_ip)
+        self.wait_for_route_as_flow(first_host.MAC(), first_host_ip.network)
+        self.wait_for_route_as_flow(second_host.MAC(), second_host_ip.network)
         self.one_ipv6_ping(first_host, second_host_ip.ip)
         self.one_ipv6_ping(first_host, second_host_ctrl_ip)
         self.one_ipv6_ping(second_host, first_host_ip.ip)
         self.one_ipv6_ping(second_host, first_host_ctrl_ip)
 
 
 class FaucetUntaggedIPv6RouteTest(FaucetUntaggedTest):
-
     NUM_FAUCET_CONTROLLERS = 1
 
-    CONFIG_GLOBAL = """
+    CONFIG_GLOBAL = (
+        """
 vlans:
     100:
         description: "untagged"
         faucet_vips: ["fc00::1:254/112"]
         routes:
             - route:
                 ip_dst: "fc00::10:0/112"
@@ -7543,54 +8645,63 @@
             as: 1
             connect_mode: "passive"
             port: %(bgp_port)d
             routerid: "1.1.1.1"
             server_addresses: ["::1"]
             neighbor_addresses: ["::1"]
             vlan: 100
-""" + """
+"""
+        + """
             neighbor_as: %u
-""" % PEER_BGP_AS
+"""
+        % PEER_BGP_AS
+    )
 
-    CONFIG = """
+    CONFIG = (
+        """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     exabgp_log = None
     exabgp_err = None
-    config_ports = {'bgp_port': None}
+    config_ports = {"bgp_port": None}
 
     def post_start_net(self):
-        exabgp_conf = self.get_exabgp_conf('::1')
+        exabgp_conf = self.get_exabgp_conf("::1")
         self.exabgp_log, self.exabgp_err = self.start_exabgp(exabgp_conf)
 
     def test_untagged(self):
         self.verify_ipv6_routing_mesh()
         second_host = self.hosts_name_ordered()[1]
         self.flap_all_switch_ports()
         self.wait_for_route_as_flow(
-            second_host.MAC(), ipaddress.IPv6Network('fc00::30:0/112'))
+            second_host.MAC(), ipaddress.IPv6Network("fc00::30:0/112")
+        )
         self.verify_ipv6_routing_mesh()
-        self.wait_bgp_up('::1', 100, self.exabgp_log, self.exabgp_err)
+        self.wait_bgp_up("::1", 100, self.exabgp_log, self.exabgp_err)
         self.assertGreater(
             self.scrape_prometheus_var(
-                'bgp_neighbor_routes', {'ipv': '6', 'vlan': '100'}, default=0),
-            0)
+                "bgp_neighbor_routes", {"ipv": "6", "vlan": "100"}, default=0
+            ),
+            0,
+        )
         updates = self.exabgp_updates(self.exabgp_log)
         for route_string in (
-                'fc00::1:0/112 next-hop fc00::1:254',
-                'fc00::10:0/112 next-hop fc00::1:1',
-                'fc00::20:0/112 next-hop fc00::1:2',
-                'fc00::30:0/112 next-hop fc00::1:2'):
+            "fc00::1:0/112 next-hop fc00::1:254",
+            "fc00::10:0/112 next-hop fc00::1:1",
+            "fc00::20:0/112 next-hop fc00::1:2",
+            "fc00::30:0/112 next-hop fc00::1:2",
+        ):
             self.assertTrue(re.search(route_string, updates), msg=updates)
 
 
 class FaucetUntaggedRestBcastIPv6RouteTest(FaucetUntaggedIPv6RouteTest):
-
     CONFIG = """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 restricted_bcast_arpnd: true
@@ -7619,62 +8730,78 @@
                 ip_dst: "fc00::10:0/112"
                 ip_gw: "fc00::1:1"
             - route:
                 ip_dst: "fc00::20:0/112"
                 ip_gw: "fc00::1:2"
 """
 
-    CONFIG = """
+    CONFIG = (
+        """
         nd_neighbor_timeout: 2
         max_resolve_backoff_time: 1
-""" + CONFIG_TAGGED_BOILER
+"""
+        + CONFIG_TAGGED_BOILER
+    )
 
     def test_tagged(self):
         """Test IPv6 routing works."""
         host_pair = self.hosts_name_ordered()[:2]
         first_host, second_host = host_pair
-        first_host_ip = ipaddress.ip_interface('fc00::1:1/112')
-        second_host_ip = ipaddress.ip_interface('fc00::1:2/112')
-        first_host_routed_ip = ipaddress.ip_interface('fc00::10:1/112')
-        second_host_routed_ip = ipaddress.ip_interface('fc00::20:1/112')
+        first_host_ip = ipaddress.ip_interface("fc00::1:1/112")
+        second_host_ip = ipaddress.ip_interface("fc00::1:2/112")
+        first_host_routed_ip = ipaddress.ip_interface("fc00::10:1/112")
+        second_host_routed_ip = ipaddress.ip_interface("fc00::20:1/112")
         for _coldstart in range(2):
             for _swaps in range(5):
                 self.verify_ipv6_routing_pair(
-                    first_host, first_host_ip, first_host_routed_ip,
-                    second_host, second_host_ip, second_host_routed_ip)
+                    first_host,
+                    first_host_ip,
+                    first_host_routed_ip,
+                    second_host,
+                    second_host_ip,
+                    second_host_routed_ip,
+                )
                 self.swap_host_macs(first_host, second_host)
             self.coldstart_conf()
 
 
 class FaucetGroupTableTest(FaucetUntaggedTest):
-
-    CONFIG = """
+    CONFIG = (
+        """
         group_table: True
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def test_group_exist(self):
         self.assertEqual(
             100,
             self.get_group_id_for_matching_flow(
-                {'dl_vlan': '100', 'dl_dst': 'ff:ff:ff:ff:ff:ff'},
-                table_id=self._FLOOD_TABLE))
+                {"dl_vlan": "100", "dl_dst": "ff:ff:ff:ff:ff:ff"},
+                table_id=self._FLOOD_TABLE,
+            ),
+        )
 
 
 class FaucetTaggedGroupTableTest(FaucetTaggedTest):
-
-    CONFIG = """
+    CONFIG = (
+        """
         group_table: True
-""" + CONFIG_TAGGED_BOILER
+"""
+        + CONFIG_TAGGED_BOILER
+    )
 
     def test_group_exist(self):
         self.assertEqual(
             100,
             self.get_group_id_for_matching_flow(
-                {'dl_vlan': '100', 'dl_dst': 'ff:ff:ff:ff:ff:ff'},
-                table_id=self._FLOOD_TABLE))
+                {"dl_vlan": "100", "dl_dst": "ff:ff:ff:ff:ff:ff"},
+                table_id=self._FLOOD_TABLE,
+            ),
+        )
 
 
 class FaucetEthSrcMaskTest(FaucetUntaggedTest):
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
@@ -7700,30 +8827,30 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        first_host.setMAC('0e:0d:00:00:00:99')
+        first_host.setMAC("0e:0d:00:00:00:99")
         self.retry_net_ping(hosts=(first_host, second_host))
         self.wait_nonzero_packet_count_flow(
-            {'dl_src': '0e:0d:00:00:00:00/ff:ff:00:00:00:00'},
-            table_id=self._PORT_ACL_TABLE)
+            {"dl_src": "0e:0d:00:00:00:00/ff:ff:00:00:00:00"},
+            table_id=self._PORT_ACL_TABLE,
+        )
 
 
 class FaucetDestRewriteTest(FaucetUntaggedTest):
-
     def override_mac():  # pylint: disable=no-method-argument
-        return '0e:00:00:00:00:02'
+        return "0e:00:00:00:00:02"
 
     OVERRIDE_MAC = override_mac()
 
     def rewrite_mac():  # pylint: disable=no-method-argument
-        return '0e:00:00:00:00:03'
+        return "0e:00:00:00:00:03"
 
     REWRITE_MAC = rewrite_mac()
 
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
@@ -7736,15 +8863,18 @@
                 allow: 1
                 output:
                     set_fields:
                         - eth_dst: "%s"
         - rule:
             actions:
                 allow: 1
-""" % (override_mac(), rewrite_mac())
+""" % (
+        override_mac(),
+        rewrite_mac(),
+    )
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acl_in: 1
             %(port_2)d:
                 native_vlan: 100
@@ -7753,67 +8883,89 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expect to see the rewritten mac address.
-        tcpdump_filter = ('icmp and ether dst %s' % self.REWRITE_MAC)
+        tcpdump_filter = "icmp and ether dst %s" % self.REWRITE_MAC
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    "arp -s %s %s" % (second_host.IP(), self.OVERRIDE_MAC)
+                ),
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), self.OVERRIDE_MAC)),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-            timeout=5, packets=1)
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+            timeout=5,
+            packets=1,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
 
-    def verify_dest_rewrite(self, source_host, overridden_host, rewrite_host, tcpdump_host):
+    def verify_dest_rewrite(
+        self, source_host, overridden_host, rewrite_host, tcpdump_host
+    ):
         overridden_host.setMAC(self.OVERRIDE_MAC)
         rewrite_host.setMAC(self.REWRITE_MAC)
-        rewrite_host.cmd('arp -s %s %s' % (overridden_host.IP(), overridden_host.MAC()))
-        rewrite_host.cmd(' '.join((self.FPINGS_ARGS_ONE, overridden_host.IP())))
+        rewrite_host.cmd("arp -s %s %s" % (overridden_host.IP(), overridden_host.MAC()))
+        rewrite_host.cmd(" ".join((self.FPINGS_ARGS_ONE, overridden_host.IP())))
         self.wait_until_matching_flow(
-            {'dl_dst': self.REWRITE_MAC},
+            {"dl_dst": self.REWRITE_MAC},
             table_id=self._ETH_DST_TABLE,
-            actions=['OUTPUT:%u' % self.port_map['port_3']])
-        tcpdump_filter = ('icmp and ether src %s and ether dst %s' % (
-            source_host.MAC(), rewrite_host.MAC()))
+            actions=["OUTPUT:%u" % self.port_map["port_3"]],
+        )
+        tcpdump_filter = "icmp and ether src %s and ether dst %s" % (
+            source_host.MAC(),
+            rewrite_host.MAC(),
+        )
         tcpdump_txt = self.tcpdump_helper(
-            tcpdump_host, tcpdump_filter, [
+            tcpdump_host,
+            tcpdump_filter,
+            [
                 lambda: source_host.cmd(
-                    'arp -s %s %s' % (rewrite_host.IP(), overridden_host.MAC())),
+                    "arp -s %s %s" % (rewrite_host.IP(), overridden_host.MAC())
+                ),
                 # this will fail if no reply
                 lambda: self.one_ipv4_ping(
-                    source_host, rewrite_host.IP(), require_host_learned=False)],
-            timeout=3, packets=1)
+                    source_host, rewrite_host.IP(), require_host_learned=False
+                ),
+            ],
+            timeout=3,
+            packets=1,
+        )
         # ping from h1 to h2.mac should appear in third host, and not second host, as
         # the acl should rewrite the dst mac.
-        self.assertFalse(re.search(
-            '%s: ICMP echo request' % rewrite_host.IP(), tcpdump_txt))
+        self.assertFalse(
+            re.search("%s: ICMP echo request" % rewrite_host.IP(), tcpdump_txt)
+        )
 
     def test_switching(self):
         """Tests that a acl can rewrite the destination mac address,
-           and the packet will only go out the port of the new mac.
-           (Continues through faucet pipeline)
+        and the packet will only go out the port of the new mac.
+        (Continues through faucet pipeline)
         """
         source_host, overridden_host, rewrite_host = self.hosts_name_ordered()[0:3]
         self.verify_dest_rewrite(
-            source_host, overridden_host, rewrite_host, overridden_host)
+            source_host, overridden_host, rewrite_host, overridden_host
+        )
 
 
 class FaucetDestRewriteOrderedTest(FaucetUntaggedTest):
-
     def override_mac():  # pylint: disable=no-method-argument
-        return '0e:00:00:00:00:02'
+        return "0e:00:00:00:00:02"
 
     OVERRIDE_MAC = override_mac()
 
     def rewrite_mac():  # pylint: disable=no-method-argument
-        return '0e:00:00:00:00:03'
+        return "0e:00:00:00:00:03"
 
     REWRITE_MAC = rewrite_mac()
 
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
@@ -7826,15 +8978,18 @@
                 allow: 1
                 output:
                     - set_fields:
                         - eth_dst: "%s"
         - rule:
             actions:
                 allow: 1
-""" % (override_mac(), rewrite_mac())
+""" % (
+        override_mac(),
+        rewrite_mac(),
+    )
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acl_in: 1
             %(port_2)d:
                 native_vlan: 100
@@ -7843,63 +8998,86 @@
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untagged(self):
         first_host, second_host = self.hosts_name_ordered()[0:2]
         # we expect to see the rewritten mac address.
-        tcpdump_filter = ('icmp and ether dst %s' % self.REWRITE_MAC)
+        tcpdump_filter = "icmp and ether dst %s" % self.REWRITE_MAC
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
+            second_host,
+            tcpdump_filter,
+            [
+                lambda: first_host.cmd(
+                    "arp -s %s %s" % (second_host.IP(), self.OVERRIDE_MAC)
+                ),
                 lambda: first_host.cmd(
-                    'arp -s %s %s' % (second_host.IP(), self.OVERRIDE_MAC)),
-                lambda: first_host.cmd(' '.join((self.FPINGS_ARGS_ONE, second_host.IP())))],
-            timeout=5, packets=1)
-        self.assertTrue(re.search(
-            '%s: ICMP echo request' % second_host.IP(), tcpdump_txt))
+                    " ".join((self.FPINGS_ARGS_ONE, second_host.IP()))
+                ),
+            ],
+            timeout=5,
+            packets=1,
+        )
+        self.assertTrue(
+            re.search("%s: ICMP echo request" % second_host.IP(), tcpdump_txt)
+        )
 
-    def verify_dest_rewrite(self, source_host, overridden_host, rewrite_host, tcpdump_host):
+    def verify_dest_rewrite(
+        self, source_host, overridden_host, rewrite_host, tcpdump_host
+    ):
         overridden_host.setMAC(self.OVERRIDE_MAC)
         rewrite_host.setMAC(self.REWRITE_MAC)
-        rewrite_host.cmd('arp -s %s %s' % (overridden_host.IP(), overridden_host.MAC()))
-        rewrite_host.cmd(' '.join((self.FPINGS_ARGS_ONE, overridden_host.IP())))
+        rewrite_host.cmd("arp -s %s %s" % (overridden_host.IP(), overridden_host.MAC()))
+        rewrite_host.cmd(" ".join((self.FPINGS_ARGS_ONE, overridden_host.IP())))
         self.wait_until_matching_flow(
-            {'dl_dst': self.REWRITE_MAC},
+            {"dl_dst": self.REWRITE_MAC},
             table_id=self._ETH_DST_TABLE,
-            actions=['OUTPUT:%u' % self.port_map['port_3']])
-        tcpdump_filter = ('icmp and ether src %s and ether dst %s' % (
-            source_host.MAC(), rewrite_host.MAC()))
+            actions=["OUTPUT:%u" % self.port_map["port_3"]],
+        )
+        tcpdump_filter = "icmp and ether src %s and ether dst %s" % (
+            source_host.MAC(),
+            rewrite_host.MAC(),
+        )
         tcpdump_txt = self.tcpdump_helper(
-            tcpdump_host, tcpdump_filter, [
+            tcpdump_host,
+            tcpdump_filter,
+            [
                 lambda: source_host.cmd(
-                    'arp -s %s %s' % (rewrite_host.IP(), overridden_host.MAC())),
+                    "arp -s %s %s" % (rewrite_host.IP(), overridden_host.MAC())
+                ),
                 # this will fail if no reply
                 lambda: self.one_ipv4_ping(
-                    source_host, rewrite_host.IP(), require_host_learned=False)],
-            timeout=3, packets=1)
+                    source_host, rewrite_host.IP(), require_host_learned=False
+                ),
+            ],
+            timeout=3,
+            packets=1,
+        )
         # ping from h1 to h2.mac should appear in third host, and not second host, as
         # the acl should rewrite the dst mac.
-        self.assertFalse(re.search(
-            '%s: ICMP echo request' % rewrite_host.IP(), tcpdump_txt))
+        self.assertFalse(
+            re.search("%s: ICMP echo request" % rewrite_host.IP(), tcpdump_txt)
+        )
 
     def test_switching(self):
         """Tests that a acl can rewrite the destination mac address,
-           and the packet will only go out the port of the new mac.
-           (Continues through faucet pipeline)
+        and the packet will only go out the port of the new mac.
+        (Continues through faucet pipeline)
         """
         source_host, overridden_host, rewrite_host = self.hosts_name_ordered()[0:3]
         self.verify_dest_rewrite(
-            source_host, overridden_host, rewrite_host, overridden_host)
+            source_host, overridden_host, rewrite_host, overridden_host
+        )
 
 
 class FaucetSetFieldsTest(FaucetUntaggedTest):
     # A generic test to verify that a flow will set fields specified for
     # matching packets
-    OUTPUT_MAC = '0f:00:12:23:48:03'
-    SRC_MAC = '0f:12:00:00:00:ff'
+    OUTPUT_MAC = "0f:00:12:23:48:03"
+    SRC_MAC = "0f:12:00:00:00:ff"
 
     IP_DSCP_VAL = 46
     # this is the converted DSCP value that is displayed
     NW_TOS_VAL = 184
     IPV4_SRC_VAL = "192.0.2.0"
     IPV4_DST_VAL = "198.51.100.0"
     # ICMP echo request
@@ -7927,15 +9105,20 @@
             eth_type: 0x0800
             ip_proto: 1
             actions:
                 allow: 1
                 output:
                     set_fields:
                         - icmpv4_type: %d
-""" % (IPV4_SRC_VAL, IPV4_DST_VAL, IP_DSCP_VAL, ICMPV4_TYPE_VAL)
+""" % (
+        IPV4_SRC_VAL,
+        IPV4_DST_VAL,
+        IP_DSCP_VAL,
+        ICMPV4_TYPE_VAL,
+    )
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acl_in: 1
             %(port_2)d:
                 native_vlan: 100
@@ -7949,61 +9132,86 @@
         # Send a basic UDP packet through the faucet pipeline and verify that
         # the expected fields were updated via tcpdump output
         source_host, dest_host = self.hosts_name_ordered()[0:2]
         dest_host.setMAC(self.OUTPUT_MAC)
 
         # scapy command to create and send a UDP packet
         scapy_pkt = self.scapy_base_udp(
-            self.SRC_MAC, source_host.defaultIntf(), source_host.IP(),
-            dest_host.IP(), self.UDP_DST_PORT, self.UDP_SRC_PORT,
-            dst=self.OUTPUT_MAC)
+            self.SRC_MAC,
+            source_host.defaultIntf(),
+            source_host.IP(),
+            dest_host.IP(),
+            self.UDP_DST_PORT,
+            self.UDP_SRC_PORT,
+            dst=self.OUTPUT_MAC,
+        )
 
         tcpdump_filter = "ether dst %s" % self.OUTPUT_MAC
         tcpdump_txt = self.tcpdump_helper(
-            dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
-            root_intf=True, packets=1)
+            dest_host,
+            tcpdump_filter,
+            [lambda: source_host.cmd(scapy_pkt)],
+            root_intf=True,
+            packets=1,
+        )
 
         # verify that the packet we've received on the dest_host has the
         # overwritten values
         self.assertTrue(
-            re.search("%s.%s > %s.%s" % (self.IPV4_SRC_VAL, self.UDP_SRC_PORT,
-                                         self.IPV4_DST_VAL, self.UDP_DST_PORT),
-                      tcpdump_txt))
+            re.search(
+                "%s.%s > %s.%s"
+                % (
+                    self.IPV4_SRC_VAL,
+                    self.UDP_SRC_PORT,
+                    self.IPV4_DST_VAL,
+                    self.UDP_DST_PORT,
+                ),
+                tcpdump_txt,
+            )
+        )
         # check the packet's converted dscp value
         self.assertTrue(re.search("tos %s" % hex(self.NW_TOS_VAL), tcpdump_txt))
 
     def test_set_fields_icmp(self):
         # Send a basic ICMP packet through the faucet pipeline and verify that
         # the expected fields were updated via tcpdump output
         source_host, dest_host = self.hosts_name_ordered()[0:2]
         dest_host.setMAC(self.OUTPUT_MAC)
 
         # scapy command to create and send an ICMP packet
         scapy_pkt = self.scapy_icmp(
-            self.SRC_MAC, source_host.defaultIntf(), source_host.IP(),
-            dest_host.IP(), dst=self.OUTPUT_MAC)
+            self.SRC_MAC,
+            source_host.defaultIntf(),
+            source_host.IP(),
+            dest_host.IP(),
+            dst=self.OUTPUT_MAC,
+        )
 
         tcpdump_filter = "ether dst %s" % self.OUTPUT_MAC
         tcpdump_txt = self.tcpdump_helper(
-            dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
-            root_intf=True, packets=1)
+            dest_host,
+            tcpdump_filter,
+            [lambda: source_host.cmd(scapy_pkt)],
+            root_intf=True,
+            packets=1,
+        )
 
         # verify that the packet we've received on the dest_host has been
         # overwritten to be an ICMP echo request
         self.assertTrue(re.search("ICMP echo request", tcpdump_txt))
 
     def test_untagged(self):
         pass
 
 
 class FaucetOrderedSetFieldsTest(FaucetUntaggedTest):
     # A generic test to verify that a flow will set fields specified for
     # matching packets
-    OUTPUT_MAC = '0f:00:12:23:48:03'
-    SRC_MAC = '0f:12:00:00:00:ff'
+    OUTPUT_MAC = "0f:00:12:23:48:03"
+    SRC_MAC = "0f:12:00:00:00:ff"
 
     IP_DSCP_VAL = 46
     # this is the converted DSCP value that is displayed
     NW_TOS_VAL = 184
     IPV4_SRC_VAL = "192.0.2.0"
     IPV4_DST_VAL = "198.51.100.0"
     # ICMP echo request
@@ -8031,15 +9239,20 @@
             eth_type: 0x0800
             ip_proto: 1
             actions:
                 allow: 1
                 output:
                     - set_fields:
                         - icmpv4_type: %d
-""" % (IPV4_SRC_VAL, IPV4_DST_VAL, IP_DSCP_VAL, ICMPV4_TYPE_VAL)
+""" % (
+        IPV4_SRC_VAL,
+        IPV4_DST_VAL,
+        IP_DSCP_VAL,
+        ICMPV4_TYPE_VAL,
+    )
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acl_in: 1
             %(port_2)d:
                 native_vlan: 100
@@ -8053,47 +9266,72 @@
         # Send a basic UDP packet through the faucet pipeline and verify that
         # the expected fields were updated via tcpdump output
         source_host, dest_host = self.hosts_name_ordered()[0:2]
         dest_host.setMAC(self.OUTPUT_MAC)
 
         # scapy command to create and send a UDP packet
         scapy_pkt = self.scapy_base_udp(
-            self.SRC_MAC, source_host.defaultIntf(), source_host.IP(),
-            dest_host.IP(), self.UDP_DST_PORT, self.UDP_SRC_PORT,
-            dst=self.OUTPUT_MAC)
+            self.SRC_MAC,
+            source_host.defaultIntf(),
+            source_host.IP(),
+            dest_host.IP(),
+            self.UDP_DST_PORT,
+            self.UDP_SRC_PORT,
+            dst=self.OUTPUT_MAC,
+        )
 
         tcpdump_filter = "ether dst %s" % self.OUTPUT_MAC
         tcpdump_txt = self.tcpdump_helper(
-            dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
-            root_intf=True, packets=1)
+            dest_host,
+            tcpdump_filter,
+            [lambda: source_host.cmd(scapy_pkt)],
+            root_intf=True,
+            packets=1,
+        )
 
         # verify that the packet we've received on the dest_host has the
         # overwritten values
         self.assertTrue(
-            re.search("%s.%s > %s.%s" % (self.IPV4_SRC_VAL, self.UDP_SRC_PORT,
-                                         self.IPV4_DST_VAL, self.UDP_DST_PORT),
-                      tcpdump_txt))
+            re.search(
+                "%s.%s > %s.%s"
+                % (
+                    self.IPV4_SRC_VAL,
+                    self.UDP_SRC_PORT,
+                    self.IPV4_DST_VAL,
+                    self.UDP_DST_PORT,
+                ),
+                tcpdump_txt,
+            )
+        )
         # check the packet's converted dscp value
         self.assertTrue(re.search("tos %s" % hex(self.NW_TOS_VAL), tcpdump_txt))
 
     def test_set_fields_icmp(self):
         # Send a basic ICMP packet through the faucet pipeline and verify that
         # the expected fields were updated via tcpdump output
         source_host, dest_host = self.hosts_name_ordered()[0:2]
         dest_host.setMAC(self.OUTPUT_MAC)
 
         # scapy command to create and send an ICMP packet
         scapy_pkt = self.scapy_icmp(
-            self.SRC_MAC, source_host.defaultIntf(), source_host.IP(),
-            dest_host.IP(), dst=self.OUTPUT_MAC)
+            self.SRC_MAC,
+            source_host.defaultIntf(),
+            source_host.IP(),
+            dest_host.IP(),
+            dst=self.OUTPUT_MAC,
+        )
 
         tcpdump_filter = "ether dst %s" % self.OUTPUT_MAC
         tcpdump_txt = self.tcpdump_helper(
-            dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
-            root_intf=True, packets=1)
+            dest_host,
+            tcpdump_filter,
+            [lambda: source_host.cmd(scapy_pkt)],
+            root_intf=True,
+            packets=1,
+        )
 
         # verify that the packet we've received on the dest_host has been
         # overwritten to be an ICMP echo request
         self.assertTrue(re.search("ICMP echo request", tcpdump_txt))
 
     def test_untagged(self):
         pass
@@ -8131,33 +9369,33 @@
                 native_vlan: 100
             %(port_4)d:
                 native_vlan: 100
 """
 
     def test_untracked_tcp_blocked(self):
         self.wait_until_matching_flow(
-            {'ct_state': '0/0x20',
-             'eth_type': 0x0800,
-             'ip_proto': 6},
-            table_id=self._PORT_ACL_TABLE)
+            {"ct_state": "0/0x20", "eth_type": 0x0800, "ip_proto": 6},
+            table_id=self._PORT_ACL_TABLE,
+        )
 
         self.ping_all_when_learned()
 
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        tcpdump_filter = ('tcp and port 1024')
+        tcpdump_filter = "tcp and port 1024"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
-                lambda: first_host.cmd(f"nc -w 1 {second_host.IP()} 1024")])
+            second_host,
+            tcpdump_filter,
+            [lambda: first_host.cmd(f"nc -w 1 {second_host.IP()} 1024")],
+        )
         self.assertNotIn(f"{second_host.IP()}.1024: Flags [S]", tcpdump_txt)
 
         self.wait_nonzero_packet_count_flow(
-            {'ct_state': '0/0x20',
-             'eth_type': 0x0800,
-             'ip_proto': 6},
-            table_id=self._PORT_ACL_TABLE)
+            {"ct_state": "0/0x20", "eth_type": 0x0800, "ip_proto": 6},
+            table_id=self._PORT_ACL_TABLE,
+        )
 
 
 class FaucetConntrackCommitTest(FaucetUntaggedTest):
     """Test that new TCP flows can be matched/tracked and commited to conntrack"""
 
     SOFTWARE_ONLY = True
 
@@ -8201,31 +9439,31 @@
                 native_vlan: 100
 """
 
     def test_commit_tracked_tcp(self):
         self.ping_all_when_learned()
 
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        tcpdump_filter = ('tcp and port 1024')
+        tcpdump_filter = "tcp and port 1024"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
-                lambda: first_host.cmd(f"nc -w 1 {second_host.IP()} 1024")])
+            second_host,
+            tcpdump_filter,
+            [lambda: first_host.cmd(f"nc -w 1 {second_host.IP()} 1024")],
+        )
 
         self.wait_nonzero_packet_count_flow(
-            {'ct_state': '0/0x20',
-             'eth_type': 0x0800,
-             'ip_proto': 6},
-            actions=['NX_CT: {flags: 0, zone: [1..17], table: 0, alg: 0, actions: []}'],
-            table_id=self._PORT_ACL_TABLE)
+            {"ct_state": "0/0x20", "eth_type": 0x0800, "ip_proto": 6},
+            actions=["NX_CT: {flags: 0, zone: [1..17], table: 0, alg: 0, actions: []}"],
+            table_id=self._PORT_ACL_TABLE,
+        )
         self.wait_nonzero_packet_count_flow(
-            {'ct_state': '0x21/0x21',
-             'eth_type': 0x0800,
-             'ip_proto': 6},
-            actions=['NX_CT: {flags: 1, zone: [1..17], table: 1, alg: 0, actions: []}'],
-            table_id=self._PORT_ACL_TABLE)
+            {"ct_state": "0x21/0x21", "eth_type": 0x0800, "ip_proto": 6},
+            actions=["NX_CT: {flags: 1, zone: [1..17], table: 1, alg: 0, actions: []}"],
+            table_id=self._PORT_ACL_TABLE,
+        )
 
         self.assertIn(f"{second_host.IP()}.1024: Flags [S]", tcpdump_txt)
 
 
 class FaucetConntrackClearTest(FaucetUntaggedTest):
     """Verify clear flag can be set in CT action"""
 
@@ -8325,36 +9563,41 @@
                 native_vlan: 100
 """
 
     def test_nat_tcp(self):
         self.ping_all_when_learned()
 
         first_host, second_host = self.hosts_name_ordered()[0:2]
-        tcpdump_filter = ('tcp and port 1024')
+        tcpdump_filter = "tcp and port 1024"
         tcpdump_txt = self.tcpdump_helper(
-            second_host, tcpdump_filter, [
-                lambda: first_host.cmd(f"nc -w 1 {second_host.IP()} 1024")])
+            second_host,
+            tcpdump_filter,
+            [lambda: first_host.cmd(f"nc -w 1 {second_host.IP()} 1024")],
+        )
 
-        tcpdump_regex = re.escape("192.0.2.250.") + r"\d+" + \
-            re.escape(f" > {second_host.IP()}.1024: Flags [S]")
+        tcpdump_regex = (
+            re.escape("192.0.2.250.")
+            + r"\d+"
+            + re.escape(f" > {second_host.IP()}.1024: Flags [S]")
+        )
         self.assertRegex(tcpdump_txt, tcpdump_regex)
 
 
 class FaucetDscpMatchTest(FaucetUntaggedTest):
     # Match all packets with this IP_DSP and eth_type, based on the ryu API def
     # e.g {"ip_dscp": 3, "eth_type": 2048}
     # Note: the ip_dscp field is translated to nw_tos in OpenFlow 1.0:
     # see https://tools.ietf.org/html/rfc2474#section-3
     IP_DSCP_MATCH = 46
     ETH_TYPE = 2048
 
-    SRC_MAC = '0e:00:00:00:00:ff'
-    DST_MAC = '0e:00:00:00:00:02'
+    SRC_MAC = "0e:00:00:00:00:ff"
+    DST_MAC = "0e:00:00:00:00:02"
 
-    REWRITE_MAC = '0f:00:12:23:48:03'
+    REWRITE_MAC = "0f:00:12:23:48:03"
 
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 
 acls:
@@ -8366,15 +9609,18 @@
                 allow: 1
                 output:
                     set_fields:
                         - eth_dst: "%s"
         - rule:
             actions:
                 allow: 1
-""" % (IP_DSCP_MATCH, REWRITE_MAC)
+""" % (
+        IP_DSCP_MATCH,
+        REWRITE_MAC,
+    )
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acl_in: 1
             %(port_2)d:
                 native_vlan: 100
@@ -8387,44 +9633,50 @@
     def test_untagged(self):
         # Tests that a packet with an ip_dscp field will be appropriately
         # matched and proceeds through the faucet pipeline. This test verifies
         # that packets with the dscp field can have their eth_dst field modified
         source_host, dest_host = self.hosts_name_ordered()[0:2]
         dest_host.setMAC(self.REWRITE_MAC)
         self.wait_until_matching_flow(
-            {'ip_dscp': self.IP_DSCP_MATCH,
-             'eth_type': self.ETH_TYPE},
-            table_id=self._PORT_ACL_TABLE)
+            {"ip_dscp": self.IP_DSCP_MATCH, "eth_type": self.ETH_TYPE},
+            table_id=self._PORT_ACL_TABLE,
+        )
 
         # scapy command to create and send a packet with the specified fields
-        scapy_pkt = self.scapy_dscp(self.SRC_MAC, self.DST_MAC, 184,
-                                    source_host.defaultIntf())
+        scapy_pkt = self.scapy_dscp(
+            self.SRC_MAC, self.DST_MAC, 184, source_host.defaultIntf()
+        )
 
         tcpdump_filter = "ether dst %s" % self.REWRITE_MAC
         tcpdump_txt = self.tcpdump_helper(
-            dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
-            root_intf=True, packets=1)
+            dest_host,
+            tcpdump_filter,
+            [lambda: source_host.cmd(scapy_pkt)],
+            root_intf=True,
+            packets=1,
+        )
         # verify that the packet we've received on the dest_host is from the
         # source MAC address
-        self.assertTrue(re.search("%s > %s" % (self.SRC_MAC, self.REWRITE_MAC),
-                                  tcpdump_txt))
+        self.assertTrue(
+            re.search("%s > %s" % (self.SRC_MAC, self.REWRITE_MAC), tcpdump_txt)
+        )
 
 
 class FaucetOrderedDscpMatchTest(FaucetUntaggedTest):
     # Match all packets with this IP_DSP and eth_type, based on the ryu API def
     # e.g {"ip_dscp": 3, "eth_type": 2048}
     # Note: the ip_dscp field is translated to nw_tos in OpenFlow 1.0:
     # see https://tools.ietf.org/html/rfc2474#section-3
     IP_DSCP_MATCH = 46
     ETH_TYPE = 2048
 
-    SRC_MAC = '0e:00:00:00:00:ff'
-    DST_MAC = '0e:00:00:00:00:02'
+    SRC_MAC = "0e:00:00:00:00:ff"
+    DST_MAC = "0e:00:00:00:00:02"
 
-    REWRITE_MAC = '0f:00:12:23:48:03'
+    REWRITE_MAC = "0f:00:12:23:48:03"
 
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 
 acls:
@@ -8436,15 +9688,18 @@
                 allow: 1
                 output:
                     - set_fields:
                         - eth_dst: "%s"
         - rule:
             actions:
                 allow: 1
-""" % (IP_DSCP_MATCH, REWRITE_MAC)
+""" % (
+        IP_DSCP_MATCH,
+        REWRITE_MAC,
+    )
     CONFIG = """
         interfaces:
             %(port_1)d:
                 native_vlan: 100
                 acl_in: 1
             %(port_2)d:
                 native_vlan: 100
@@ -8457,282 +9712,303 @@
     def test_untagged(self):
         # Tests that a packet with an ip_dscp field will be appropriately
         # matched and proceeds through the faucet pipeline. This test verifies
         # that packets with the dscp field can have their eth_dst field modified
         source_host, dest_host = self.hosts_name_ordered()[0:2]
         dest_host.setMAC(self.REWRITE_MAC)
         self.wait_until_matching_flow(
-            {'ip_dscp': self.IP_DSCP_MATCH,
-             'eth_type': self.ETH_TYPE},
-            table_id=self._PORT_ACL_TABLE)
+            {"ip_dscp": self.IP_DSCP_MATCH, "eth_type": self.ETH_TYPE},
+            table_id=self._PORT_ACL_TABLE,
+        )
 
         # scapy command to create and send a packet with the specified fields
-        scapy_pkt = self.scapy_dscp(self.SRC_MAC, self.DST_MAC, 184,
-                                    source_host.defaultIntf())
+        scapy_pkt = self.scapy_dscp(
+            self.SRC_MAC, self.DST_MAC, 184, source_host.defaultIntf()
+        )
 
         tcpdump_filter = "ether dst %s" % self.REWRITE_MAC
         tcpdump_txt = self.tcpdump_helper(
-            dest_host, tcpdump_filter, [lambda: source_host.cmd(scapy_pkt)],
-            root_intf=True, packets=1)
+            dest_host,
+            tcpdump_filter,
+            [lambda: source_host.cmd(scapy_pkt)],
+            root_intf=True,
+            packets=1,
+        )
         # verify that the packet we've received on the dest_host is from the
         # source MAC address
-        self.assertTrue(re.search("%s > %s" % (self.SRC_MAC, self.REWRITE_MAC),
-                                  tcpdump_txt))
+        self.assertTrue(
+            re.search("%s > %s" % (self.SRC_MAC, self.REWRITE_MAC), tcpdump_txt)
+        )
 
 
-@unittest.skip('use_idle_timeout unreliable')
+@unittest.skip("use_idle_timeout unreliable")
 class FaucetWithUseIdleTimeoutTest(FaucetUntaggedTest):
     CONFIG_GLOBAL = """
 vlans:
     100:
         description: "untagged"
 """
-    CONFIG = """
+    CONFIG = (
+        """
         timeout: 1
         use_idle_timeout: True
-""" + CONFIG_BOILER_UNTAGGED
+"""
+        + CONFIG_BOILER_UNTAGGED
+    )
 
     def wait_for_host_removed(self, host, in_port, timeout=5):
         for _ in range(timeout):
             if not self.host_learned(host, in_port=in_port, timeout=1):
                 return
-        self.fail('host %s still learned' % host)
+        self.fail("host %s still learned" % host)
 
     def wait_for_flowremoved_msg(self, src_mac=None, dst_mac=None, timeout=30):
         pattern = "OFPFlowRemoved"
         mac = None
         if src_mac:
             pattern = "OFPFlowRemoved(.*)'eth_src': '%s'" % src_mac
             mac = src_mac
         if dst_mac:
             pattern = "OFPFlowRemoved(.*)'eth_dst': '%s'" % dst_mac
             mac = dst_mac
         for _ in range(timeout):
             for _, debug_log_name in self._get_ofchannel_logs():
-                with open(debug_log_name, encoding='utf-8') as debug_log:
+                with open(debug_log_name, encoding="utf-8") as debug_log:
                     debug = debug_log.read()
                 if re.search(pattern, debug):
                     return
             time.sleep(1)
-        self.fail('Not received OFPFlowRemoved for host %s' % mac)
+        self.fail("Not received OFPFlowRemoved for host %s" % mac)
 
     def wait_for_host_log_msg(self, host_mac, msg):
-        host_log_re = r'.*%s %s.*' % (msg, host_mac)
+        host_log_re = r".*%s %s.*" % (msg, host_mac)
         self.wait_until_matching_lines_from_faucet_log_files(host_log_re)
 
     def test_untagged(self):
         self.ping_all_when_learned()
         first_host, second_host = self.hosts_name_ordered()[:2]
         self.swap_host_macs(first_host, second_host)
         for host, port in (
-                (first_host, self.port_map['port_1']),
-                (second_host, self.port_map['port_2'])):
+            (first_host, self.port_map["port_1"]),
+            (second_host, self.port_map["port_2"]),
+        ):
             self.wait_for_flowremoved_msg(src_mac=host.MAC())
             self.require_host_learned(host, in_port=int(port))
 
 
-@unittest.skip('use_idle_timeout unreliable')
+@unittest.skip("use_idle_timeout unreliable")
 class FaucetWithUseIdleTimeoutRuleExpiredTest(FaucetWithUseIdleTimeoutTest):
-
     def test_untagged(self):
         """Host that is actively sending should have its dst rule renewed as the
         rule expires. Host that is not sending expires as usual.
         """
         self.ping_all_when_learned()
         first_host, second_host, third_host, fourth_host = self.hosts_name_ordered()
-        self.host_ipv4_alias(first_host, ipaddress.ip_interface('10.99.99.1/24'))
-        first_host.cmd('arp -s %s %s' % (second_host.IP(), second_host.MAC()))
-        first_host.cmd('timeout 120s ping -I 10.99.99.1 %s &' % second_host.IP())
+        self.host_ipv4_alias(first_host, ipaddress.ip_interface("10.99.99.1/24"))
+        first_host.cmd("arp -s %s %s" % (second_host.IP(), second_host.MAC()))
+        first_host.cmd("timeout 120s ping -I 10.99.99.1 %s &" % second_host.IP())
         for host in (second_host, third_host, fourth_host):
             self.host_drop_all_ips(host)
-        self.wait_for_host_log_msg(first_host.MAC(), 'refreshing host')
-        self.assertTrue(self.host_learned(
-            first_host, in_port=int(self.port_map['port_1'])))
+        self.wait_for_host_log_msg(first_host.MAC(), "refreshing host")
+        self.assertTrue(
+            self.host_learned(first_host, in_port=int(self.port_map["port_1"]))
+        )
         for host, port in (
-                (second_host, self.port_map['port_2']),
-                (third_host, self.port_map['port_3']),
-                (fourth_host, self.port_map['port_4'])):
+            (second_host, self.port_map["port_2"]),
+            (third_host, self.port_map["port_3"]),
+            (fourth_host, self.port_map["port_4"]),
+        ):
             self.wait_for_flowremoved_msg(src_mac=host.MAC())
-            self.wait_for_host_log_msg(host.MAC(), 'expiring host')
+            self.wait_for_host_log_msg(host.MAC(), "expiring host")
             self.wait_for_host_removed(host, in_port=int(port))
 
 
 class FaucetDisconnectTest(FaucetUntaggedTest):
     """Test that switch works properly after repeated disconnections
-       caused by DPID mismatch"""
+    caused by DPID mismatch"""
 
     def update_config(self, dpid):
         """Update config with good/bad DPID"""
         conf = self._get_faucet_conf()
-        conf['dps'][self.DP_NAME]['dp_id'] = int(dpid)
+        conf["dps"][self.DP_NAME]["dp_id"] = int(dpid)
         self.reload_conf(
-            conf, self.faucet_config_path,
-            restart=True, cold_start=False, change_expected=False)
+            conf,
+            self.faucet_config_path,
+            restart=True,
+            cold_start=False,
+            change_expected=False,
+        )
 
     def test_untagged(self):
         """Run untagged test after disconnects and config update"""
         # We update the config with a bad DPID and then wait for
         # 'unknown datapath' messages, indicating switch connections that
         # FAUCET has rejected. The switch should see them as
         # 'connection reset by peer'.
-        mask = int(16 * 'f', 16)
-        bad_dpid = (int(self.dpid) + 0xdeadbeef) & mask
+        mask = int(16 * "f", 16)
+        bad_dpid = (int(self.dpid) + 0xDEADBEEF) & mask
         self.update_config(dpid=bad_dpid)
         self.wait_until_matching_lines_from_faucet_log_files(
-            r'.*ERROR.*unknown datapath', timeout=60, count=4)
+            r".*ERROR.*unknown datapath", timeout=60, count=4
+        )
         self.update_config(dpid=self.dpid)
         super().test_untagged()
 
 
 class FaucetBadFlowModTest(FaucetUntaggedTest):
     """Test that switch and FAUCET still work after we send some bad flow_mods"""
 
     def base_flow_mod(self):
         """Return a base flow mod that we mess with"""
-        return {'dpid': self.dpid,
-                'cookie': 0,
-                'cookie_mask': 0,
-                'table_id': 0,
-                'idle_timeout': 29,
-                'hard_timeout': 91,
-                'flags': 1,
-                'priority': 1,
-                'match': {'in_port': 1},
-                'actions': [{
-                    'type': 'OUTPUT',
-                    'port': 2}]}
+        return {
+            "dpid": self.dpid,
+            "cookie": 0,
+            "cookie_mask": 0,
+            "table_id": 0,
+            "idle_timeout": 29,
+            "hard_timeout": 91,
+            "flags": 1,
+            "priority": 1,
+            "match": {"in_port": 1},
+            "actions": [{"type": "OUTPUT", "port": 2}],
+        }
 
     # For now, the flow_mods are reasonably well-formed but with
     # parameters that are incorrect for the switch and for FAUCET
 
     def bad_dpid(self):
         """Return a random, bad dpid parameter"""
-        mask = int(16 * 'f', 16)
+        mask = int(16 * "f", 16)
         dpid = (int(self.dpid) + random.randint(0, 1 << 63)) & mask
-        return {'dpid': dpid}
+        return {"dpid": dpid}
 
     @staticmethod
     def bad_table():
         """Return a bad table ID parameter"""
         # This should be higher than FAUCET's max table ID
         bad_table_start = 32
-        return {'table_id': random.randint(bad_table_start, 100)}
+        return {"table_id": random.randint(bad_table_start, 100)}
 
     def bad_port(self):
         """Return a (hopefully very) bad port number"""
         max_port = max(self.port_map.values())
         offset = random.randint(0x1000, 0xE0000000)
         mask = 0xEFFFFFFF
         return (max_port + offset) & mask
 
     def bad_match(self):
         """Return a bad match field"""
         matches = (
             # Bad input port
-            {'in_port': self.bad_port()},
+            {"in_port": self.bad_port()},
             # IPv4 (broadcast) src with bad ('reserved') ethertype
-            {'nw_src': '255.255.255.255', 'dl_type': 0xFFFF},
+            {"nw_src": "255.255.255.255", "dl_type": 0xFFFF},
             # IPv4 with IPv6 ethertype:
-            {'nw_src': '1.2.3.4', 'dl_type': 0x86DD},
+            {"nw_src": "1.2.3.4", "dl_type": 0x86DD},
             # IPv4 address as IPv6 dst
-            {'ipv6_dst': '1.2.3.4', 'dl_type': 0x86DD},
+            {"ipv6_dst": "1.2.3.4", "dl_type": 0x86DD},
             # IPv6 dst with Bad/reserved ip_proto
-            {'ipv6_dst': '2001::aaaa:bbbb:cccc:1111', 'ip_proto': 255},
+            {"ipv6_dst": "2001::aaaa:bbbb:cccc:1111", "ip_proto": 255},
             # Destination port but no transport protocol
-            {'tp_dst': 80},
+            {"tp_dst": 80},
             # ARP opcode on non-ARP packetx
-            {'arp_op': 0x3, 'dl_type': 0x1234})
+            {"arp_op": 0x3, "dl_type": 0x1234},
+        )
         match = random.sample(matches, 1)[0]
-        return {'match': match}
+        return {"match": match}
 
     def bad_actions(self, count=1):
         """Return a questionable actions parameter"""
         actions = (
-            {'type': 'OUTPUT', 'port': self.bad_port()},
-            {'type': 'PUSH_MPLS', 'ethertype': 0x8BAD},
-            {'type': 'SET_QUEUE', 'queue_id':
-             random.randint(0x8000, 0xFFFFFFFF)})
-        return {'actions': random.sample(actions, count)}
+            {"type": "OUTPUT", "port": self.bad_port()},
+            {"type": "PUSH_MPLS", "ethertype": 0x8BAD},
+            {"type": "SET_QUEUE", "queue_id": random.randint(0x8000, 0xFFFFFFFF)},
+        )
+        return {"actions": random.sample(actions, count)}
 
     # Possible options for bad parameters
-    bad_options = ('dpid', 'table', 'match', 'actions')
+    bad_options = ("dpid", "table", "match", "actions")
 
     def bad_flow_mod(self):
         """Return a flow mod with some bad parameters"""
         flow_mod = self.base_flow_mod()
         # Add two or more bad options
-        options = random.sample(self.bad_options,
-                                random.randint(2, len(self.bad_options)))
+        options = random.sample(
+            self.bad_options, random.randint(2, len(self.bad_options))
+        )
         for option in options:
-            param = getattr(self, 'bad_%s' % option)()
+            param = getattr(self, "bad_%s" % option)()
             flow_mod.update(param)
         return flow_mod
 
     def send_flow_mod(self, flow_mod, timeout=5):
         """Send flow_mod to switch via ofctl"""
         int_dpid = mininet_test_util.str_int_dpid(self.dpid)
-        return self._ofctl_post(int_dpid, 'stats/flowentry/modify',
-                                timeout=timeout, params=flow_mod)
+        return self._ofctl_post(
+            int_dpid, "stats/flowentry/modify", timeout=timeout, params=flow_mod
+        )
 
     def tearDown(self, ignore_oferrors=True):
         """Ignore OF errors on teardown"""
         oferrors = super().tearDown(ignore_oferrors)
-        oferrors = re.findall(r'type: (\w+)', oferrors)
+        oferrors = re.findall(r"type: (\w+)", oferrors)
         counter = collections.Counter(oferrors)
-        error('Ignored OF error count: %s\n' % dict(counter))
+        error("Ignored OF error count: %s\n" % dict(counter))
         # TODO: ensure at least one error is always generated.
 
     # pylint: disable=arguments-differ
     def test_untagged(self, count=10):
         """Send a bunch of bad flow mods, then verify connectivity"""
         for _ in range(count):
             flow_mod = self.bad_flow_mod()
-            error('sending bad flow_mod', flow_mod, '\n')
+            error("sending bad flow_mod", flow_mod, "\n")
             self.send_flow_mod(flow_mod)
         self.ping_all_when_learned()
 
 
 class FaucetUntaggedMorePortsBase(FaucetUntaggedTest):
     """Base class for untagged test with more ports"""
 
     # pylint: disable=invalid-name
     N_UNTAGGED = 16  # Maximum number of ports to test
     EVENT_LOGGER_TIMEOUT = 180  # Timeout for event logger process
 
     # Config lines for additional ports
-    CONFIG_EXTRA_PORT = """
+    CONFIG_EXTRA_PORT = (
+        """
             {port}:
-                native_vlan: 100""" + "\n"
+                native_vlan: 100"""
+        + "\n"
+    )
 
     def pre_start_net(self):
         """Extend config with more ports if needed"""
         self.assertTrue(self.CONFIG.endswith(CONFIG_BOILER_UNTAGGED))
         # We know how to extend the config for more ports
-        base_port_count = len(re.findall('port', CONFIG_BOILER_UNTAGGED))
+        base_port_count = len(re.findall("port", CONFIG_BOILER_UNTAGGED))
         ports = self.topo.dpid_ports(self.dpid)
         for port in ports[base_port_count:]:
             self.CONFIG += self.CONFIG_EXTRA_PORT.format(port=port)
         super()._init_faucet_config()
 
     def setUp(self):
         """Make sure N_UNTAGGED doesn't exceed hw port count"""
-        if self.config and self.config.get('hw_switch', False):
-            self.N_UNTAGGED = min(len(self.config['dp_ports']),
-                                  self.N_UNTAGGED)
-        error('(%d ports) ' % self.N_UNTAGGED)
+        if self.config and self.config.get("hw_switch", False):
+            self.N_UNTAGGED = min(len(self.config["dp_ports"]), self.N_UNTAGGED)
+        error("(%d ports) " % self.N_UNTAGGED)
         super().setUp()
 
 
 class FaucetSingleUntagged32PortTest(FaucetUntaggedMorePortsBase):
     """Untagged test with up to 32 ports"""
 
     # pylint: disable=invalid-name
     N_UNTAGGED = 32  # Maximum number of ports to test
 
 
-@unittest.skip('slow and potentially unreliable on GitHub workflows')
+@unittest.skip("slow and potentially unreliable on GitHub workflows")
 class FaucetSingleUntagged48PortTest(FaucetUntaggedMorePortsBase):
     """Untagged test with up to 48 ports"""
 
     # pylint: disable=invalid-name
     N_UNTAGGED = 48  # Maximum number of ports to test
     EVENT_LOGGER_TIMEOUT = 360  # Timeout for event logger process
```

### Comparing `c65faucet-1.0.49/tests/run_unit_tests.sh` & `c65faucet-1.0.50/tests/run_unit_tests.sh`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/clib/test_topo.py` & `c65faucet-1.0.50/tests/unit/clib/test_topo.py`

 * *Files 26% similar despite different names*

```diff
@@ -15,38 +15,48 @@
     START_PORT = 5
     PORT_ORDER = [0, 1, 2, 3]
 
     class FakeExtendedHost:
         """Fake class for a mininet extended host"""
 
     def get_serialno(self, *_args, **_kwargs):
-        """"Return mock serial number"""
+        """Return mock serial number"""
         self.serial += 1
         return self.serial
 
     def test_port_order(self):
         """Test port order extension & port order option"""
         port_order = [3, 2, 1, 0]
         extended = FaucetFakeOFTopoGenerator.extend_port_order(port_order, max_length=8)
         self.assertEqual(extended, [3, 2, 1, 0, 7, 6, 5, 4])
         port_order = [1, 2, 3, 4, 0]
-        extended = FaucetFakeOFTopoGenerator.extend_port_order(port_order, max_length=10)
+        extended = FaucetFakeOFTopoGenerator.extend_port_order(
+            port_order, max_length=10
+        )
         self.assertEqual(extended, [1, 2, 3, 4, 0, 6, 7, 8, 9, 5])
         host_links = {0: [0], 1: [1]}
         host_vlans = {0: 0, 1: 0}
         switch_links = [(0, 1)]
         link_vlans = {(0, 1): [0]}
         port_order = [3, 2, 1, 0]
         expected_ports = [self.START_PORT + port for port in port_order]
         topo = FaucetFakeOFTopoGenerator(
-            '', '', '',
-            2, False,
-            host_links, host_vlans, switch_links, link_vlans,
-            start_port=self.START_PORT, port_order=port_order,
-            get_serialno=self.get_serialno)
+            "",
+            "",
+            "",
+            2,
+            False,
+            host_links,
+            host_vlans,
+            switch_links,
+            link_vlans,
+            start_port=self.START_PORT,
+            port_order=port_order,
+            get_serialno=self.get_serialno,
+        )
         s1_name = topo.switches_by_id[0]
         s1_ports = list(topo.ports[s1_name].keys())
         self.assertEqual(s1_ports, expected_ports[:2])
         s2_name = topo.switches_by_id[1]
         s2_ports = list(topo.ports[s2_name].keys())
         self.assertEqual(s2_ports, expected_ports[:2])
 
@@ -56,75 +66,108 @@
         host_links = {0: [0], 1: [1]}
         host_vlans = {0: 0, 1: 0}
         switch_links = [(0, 1)]
         link_vlans = {(0, 1): [0]}
         port_order = [3, 2, 1, 0]
         expected_ports = [start_port + port for port in port_order]
         topo = FaucetFakeOFTopoGenerator(
-            '', '', '',
-            2, False,
-            host_links, host_vlans, switch_links, link_vlans,
-            start_port=start_port, port_order=port_order,
-            get_serialno=self.get_serialno)
+            "",
+            "",
+            "",
+            2,
+            False,
+            host_links,
+            host_vlans,
+            switch_links,
+            link_vlans,
+            start_port=start_port,
+            port_order=port_order,
+            get_serialno=self.get_serialno,
+        )
         s1_name, s2_name = topo.switches_by_id.values()
         h1_name, h2_name = topo.hosts_by_id.values()
         self.assertEqual(topo.ports[s1_name][expected_ports[0]][0], s2_name)
         self.assertEqual(topo.ports[s2_name][expected_ports[0]][0], s1_name)
         self.assertEqual(topo.ports[s1_name][expected_ports[1]][0], h1_name)
         self.assertEqual(topo.ports[s2_name][expected_ports[1]][0], h2_name)
 
     def test_hw_build(self):
         """Test the topology is built with hardware requirements"""
         host_links = {0: [0], 1: [1]}
         host_vlans = {0: 0, 1: 0}
         switch_links = [(0, 1)]
         link_vlans = {(0, 1): [0]}
         hw_dpid = 0x123
-        hw_ports = {1: 'p1', 2: 'p2', 3: 'p3', 4: 'p4', 5: 'p5', 6: 'p6'}
+        hw_ports = {1: "p1", 2: "p2", 3: "p3", 4: "p4", 5: "p5", 6: "p6"}
         topo = FaucetFakeOFTopoGenerator(
-            '', '', '',
-            2, False,
-            host_links, host_vlans, switch_links, link_vlans,
-            hw_dpid=hw_dpid, hw_ports=hw_ports,
-            start_port=self.START_PORT, port_order=self.PORT_ORDER,
-            get_serialno=self.get_serialno)
+            "",
+            "",
+            "",
+            2,
+            False,
+            host_links,
+            host_vlans,
+            switch_links,
+            link_vlans,
+            hw_dpid=hw_dpid,
+            hw_ports=hw_ports,
+            start_port=self.START_PORT,
+            port_order=self.PORT_ORDER,
+            get_serialno=self.get_serialno,
+        )
         self.assertEqual(topo.dpids_by_id[0], hw_dpid)
         self.assertEqual(list(topo.ports[topo.switches_by_id[0]].keys()), [1, 2])
 
     def test_no_links(self):
         """Test single switch topology"""
         host_links = {0: [0]}
         host_vlans = {0: 0}
         switch_links = {}
         link_vlans = {}
         topo = FaucetFakeOFTopoGenerator(
-            '', '', '',
-            2, False,
-            host_links, host_vlans, switch_links, link_vlans,
-            start_port=self.START_PORT, port_order=self.PORT_ORDER,
-            get_serialno=self.get_serialno)
+            "",
+            "",
+            "",
+            2,
+            False,
+            host_links,
+            host_vlans,
+            switch_links,
+            link_vlans,
+            start_port=self.START_PORT,
+            port_order=self.PORT_ORDER,
+            get_serialno=self.get_serialno,
+        )
         self.assertEqual(len(topo.hosts()), 1)
         self.assertEqual(len(topo.switches()), 1)
         self.assertEqual(len(topo.links()), 1)
         host_name = topo.hosts_by_id[0]
         switch_name = topo.switches_by_id[0]
         self.assertEqual((switch_name, host_name), topo.links()[0])
 
     def test_build(self):
         """Test the topology is built correctly"""
         host_links = {0: [0], 1: [1]}
         host_vlans = {0: 0, 1: [0, 1]}
         switch_links = [(0, 1), (0, 1), (0, 1)]
         link_vlans = {(0, 1): [0, 1]}
         topo = FaucetFakeOFTopoGenerator(
-            '', '', '',
-            2, False,
-            host_links, host_vlans, switch_links, link_vlans,
-            start_port=self.START_PORT, port_order=self.PORT_ORDER,
-            get_serialno=self.get_serialno)
+            "",
+            "",
+            "",
+            2,
+            False,
+            host_links,
+            host_vlans,
+            switch_links,
+            link_vlans,
+            start_port=self.START_PORT,
+            port_order=self.PORT_ORDER,
+            get_serialno=self.get_serialno,
+        )
         self.assertEqual(len(topo.dpids_by_id), 2)
         self.assertEqual(len(topo.hosts_by_id), 2)
         self.assertEqual(len(topo.switches_by_id), 2)
         _, host_port_maps, link_port_maps = topo.create_port_maps()
         self.assertEqual(len(link_port_maps[(0, 1)]), 3)
         self.assertEqual(len(host_port_maps[0]), 1)
         self.assertEqual(len(host_port_maps[1]), 1)
@@ -135,63 +178,90 @@
         self.assertIn((dp1, host1), links)
         self.assertIn((dp0, dp1), links)
         self.assertEqual(links.count((dp0, dp1)), 3)
 
     def test_host_options(self):
         """Test the topology correctly provides mininet host options"""
         host_options = {
-            0: {'inNamespace': True, 'ip': '127.0.0.1'},
-            1: {'cls': self.FakeExtendedHost}}
+            0: {"inNamespace": True, "ip": "127.0.0.1"},
+            1: {"cls": self.FakeExtendedHost},
+        }
         host_links = {0: [0], 1: [0]}
         host_vlans = {0: 0, 1: None}
         switch_links = []
         link_vlans = {}
         topo = FaucetFakeOFTopoGenerator(
-            '', '', '',
-            2, False,
-            host_links, host_vlans, switch_links, link_vlans,
+            "",
+            "",
+            "",
+            2,
+            False,
+            host_links,
+            host_vlans,
+            switch_links,
+            link_vlans,
             host_options=host_options,
-            start_port=self.START_PORT, port_order=self.PORT_ORDER,
-            get_serialno=self.get_serialno)
+            start_port=self.START_PORT,
+            port_order=self.PORT_ORDER,
+            get_serialno=self.get_serialno,
+        )
         for host_id, opts in host_options.items():
             info = topo.nodeInfo(topo.hosts_by_id[host_id])
             for key, value in opts.items():
                 self.assertIn(key, info)
                 self.assertEqual(value, info[key])
 
     def test_link_port_map(self):
         """Test correctly generated link port map"""
         host_links = {0: [0], 1: [1]}
         host_vlans = {0: 0, 1: 0}
         switch_links = [(0, 1), (0, 1), (1, 2)]
         link_vlans = {edge: None for edge in switch_links}
         topo = FaucetFakeOFTopoGenerator(
-            '', '', '',
-            2, False,
-            host_links, host_vlans, switch_links, link_vlans,
-            start_port=self.START_PORT, port_order=self.PORT_ORDER,
-            get_serialno=self.get_serialno)
-        link_port_maps = topo._create_link_port_map()  # pylint: disable=protected-access
+            "",
+            "",
+            "",
+            2,
+            False,
+            host_links,
+            host_vlans,
+            switch_links,
+            link_vlans,
+            start_port=self.START_PORT,
+            port_order=self.PORT_ORDER,
+            get_serialno=self.get_serialno,
+        )
+        link_port_maps = (
+            topo._create_link_port_map()
+        )  # pylint: disable=protected-access
         self.assertEqual(
-            link_port_maps,
-            {(0, 1): [5, 6], (1, 0): [5, 6], (1, 2): [7], (2, 1): [5]})
+            link_port_maps, {(0, 1): [5, 6], (1, 0): [5, 6], (1, 2): [7], (2, 1): [5]}
+        )
 
     def test_host_port_map(self):
         """Test correctly generated host port map"""
         host_links = {0: [0, 2], 1: [1]}
         host_vlans = {0: 0, 1: 0}
         switch_links = [(0, 1), (0, 1), (1, 2)]
         link_vlans = {edge: None for edge in switch_links}
         topo = FaucetFakeOFTopoGenerator(
-            '', '', '',
-            2, False,
-            host_links, host_vlans, switch_links, link_vlans,
-            start_port=self.START_PORT, port_order=self.PORT_ORDER,
-            get_serialno=self.get_serialno)
-        host_port_maps = topo._create_host_port_map()  # pylint: disable=protected-access
-        self.assertEqual(
-            host_port_maps,
-            {0: {0: [7], 2: [6]}, 1: {1: [8]}})
+            "",
+            "",
+            "",
+            2,
+            False,
+            host_links,
+            host_vlans,
+            switch_links,
+            link_vlans,
+            start_port=self.START_PORT,
+            port_order=self.PORT_ORDER,
+            get_serialno=self.get_serialno,
+        )
+        host_port_maps = (
+            topo._create_host_port_map()
+        )  # pylint: disable=protected-access
+        self.assertEqual(host_port_maps, {0: {0: [7], 2: [6]}, 1: {1: [8]}})
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_check_config.py` & `c65faucet-1.0.50/tests/unit/faucet/test_check_config.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_config.py` & `c65faucet-1.0.50/tests/unit/faucet/test_config.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_fctl.py` & `c65faucet-1.0.50/tests/unit/faucet/test_fctl.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_main.py` & `c65faucet-1.0.50/tests/unit/faucet/test_main.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_port.py` & `c65faucet-1.0.50/tests/unit/faucet/test_port.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_valve.py` & `c65faucet-1.0.50/tests/unit/faucet/test_valve.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_valve_config.py` & `c65faucet-1.0.50/tests/unit/faucet/test_valve_config.py`

 * *Files 0% similar despite different names*

```diff
@@ -1305,15 +1305,15 @@
     )
 
     def setUp(self):
         """Setup basic port and vlan config"""
         self.setup_valves(self.CONFIG)
 
     def _get_info(self, metric, name):
-        """ "Return (single) info dict for metric"""
+        """Return (single) info dict for metric"""
         # There doesn't seem to be a nice API for this,
         # so we use the prometheus client internal API
         metrics = list(metric.collect())
         self.assertEqual(len(metrics), 1)
         samples = metrics[0].samples
         self.assertEqual(len(samples), 1)
         sample = samples[0]
```

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_valve_dot1x.py` & `c65faucet-1.0.50/tests/unit/faucet/test_valve_dot1x.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_valve_egress.py` & `c65faucet-1.0.50/tests/unit/faucet/test_valve_egress.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_valve_of.py` & `c65faucet-1.0.50/tests/unit/faucet/test_valve_of.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_valve_stack.py` & `c65faucet-1.0.50/tests/unit/faucet/test_valve_stack.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_valveapp_smoke.py` & `c65faucet-1.0.50/tests/unit/faucet/test_valveapp_smoke.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/faucet/test_vlan.py` & `c65faucet-1.0.50/tests/unit/faucet/test_vlan.py`

 * *Files identical despite different names*

### Comparing `c65faucet-1.0.49/tests/unit/gauge/test_config_gauge.py` & `c65faucet-1.0.50/tests/unit/gauge/test_config_gauge.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 import os
 import shutil
 import tempfile
 import unittest
 from faucet import config_parser as cp
 from faucet.conf import InvalidConfigError
 
-LOGNAME = '/dev/null'
+LOGNAME = "/dev/null"
 
 
 # pylint: disable=invalid-name
 class TestGaugeConfig(unittest.TestCase):  # pytype: disable=module-attr
     """Test gauge.yaml config parsing."""
 
     DEFAULT_FAUCET_CONFIG = """
@@ -56,24 +56,24 @@
         except InvalidConfigError:
             return False
         return True
 
     def conf_file_name(self, faucet=False):
         """Return path for configuration file."""
         if faucet:
-            return os.path.join(self.tmpdir, 'faucet.yaml')
-        return os.path.join(self.tmpdir, 'gauge.yaml')
+            return os.path.join(self.tmpdir, "faucet.yaml")
+        return os.path.join(self.tmpdir, "gauge.yaml")
 
     def create_config_files(self, config, faucet_config=None):
         """Returns file path to file containing the config parameter."""
         gauge_file_name = self.conf_file_name()
         faucet_file_name = self.conf_file_name(faucet=True)
-        with open(gauge_file_name, 'w', encoding='utf-8') as conf_file:
+        with open(gauge_file_name, "w", encoding="utf-8") as conf_file:
             conf_file.write(config.format(faucet_file_name))
-        with open(faucet_file_name, 'w', encoding='utf-8') as conf_file:
+        with open(faucet_file_name, "w", encoding="utf-8") as conf_file:
             if faucet_config:
                 conf_file.write(faucet_config)
             else:
                 conf_file.write(self.DEFAULT_FAUCET_CONFIG)
         return (gauge_file_name, faucet_file_name)
 
     def get_config(self, conf_suffix):
@@ -91,21 +91,23 @@
         db: 'prometheus'
 dbs:
     prometheus:
         type: 'prometheus'
 """
         conf = self.get_config(GAUGE_CONF)
         gauge_file, _ = self.create_config_files(conf)
-        _, _, _, watcher_confs = cp.watcher_parser(gauge_file, 'gauge_config_test', None)
-        self.assertEqual(len(watcher_confs), 2, 'failed to create config for each dp')
+        _, _, _, watcher_confs = cp.watcher_parser(
+            gauge_file, "gauge_config_test", None
+        )
+        self.assertEqual(len(watcher_confs), 2, "failed to create config for each dp")
         for watcher_conf in watcher_confs:
-            msg = 'all_dps config not applied to each dp'
-            self.assertEqual(watcher_conf.type, 'port_stats', msg)
+            msg = "all_dps config not applied to each dp"
+            self.assertEqual(watcher_conf.type, "port_stats", msg)
             self.assertEqual(watcher_conf.interval, 10, msg)
-            self.assertEqual(watcher_conf.db_type, 'prometheus', msg)
+            self.assertEqual(watcher_conf.db_type, "prometheus", msg)
 
     def test_no_all_dps(self):
         """Test setting all_dps and dps together."""
         GAUGE_CONF = """
 watchers:
     port_stats_poller:
         type: 'port_stats'
@@ -115,15 +117,15 @@
         db: 'prometheus'
 dbs:
     prometheus:
         type: 'prometheus'
 """
         conf = self.get_config(GAUGE_CONF)
         gauge_file, _ = self.create_config_files(conf)
-        self.assertFalse(self.parse_conf_result(gauge_file, 'gauge_config_test'))
+        self.assertFalse(self.parse_conf_result(gauge_file, "gauge_config_test"))
 
     def test_invalid_watcher_type(self):
         """Test setting invalid watcher type."""
         GAUGE_CONF = """
 watchers:
     port_stats_poller:
         type: 'not known'
@@ -132,15 +134,15 @@
         db: 'prometheus'
 dbs:
     prometheus:
         type: 'prometheus'
 """
         conf = self.get_config(GAUGE_CONF)
         gauge_file, _ = self.create_config_files(conf)
-        self.assertFalse(self.parse_conf_result(gauge_file, 'gauge_config_test'))
+        self.assertFalse(self.parse_conf_result(gauge_file, "gauge_config_test"))
 
     def test_file_not_writable(self):
         """Test file arg is not writable."""
         GAUGE_CONF = """
 watchers:
     ft_10:
         interval: 600
@@ -151,15 +153,15 @@
     text:
         file: '/not/writable/ft.yml.gz'
         type: 'text'
         compress: True
 """
         conf = self.get_config(GAUGE_CONF)
         gauge_file, _ = self.create_config_files(conf)
-        self.assertFalse(self.parse_conf_result(gauge_file, 'gauge_config_test'))
+        self.assertFalse(self.parse_conf_result(gauge_file, "gauge_config_test"))
 
     def test_no_faucet_config_file(self):
         """Test missing FAUCET config."""
         GAUGE_CONF = """
 faucet:
     dps:
         dp1:
@@ -175,19 +177,20 @@
         type: 'port_stats'
         dps: ['dp1']
         db: 'prometheus'
 dbs:
     prometheus:
         type: 'prometheus'
 """
-        gauge_file, _ = self.create_config_files(GAUGE_CONF, '')
+        gauge_file, _ = self.create_config_files(GAUGE_CONF, "")
         _, _, _, watcher_confs = cp.watcher_parser(
-            gauge_file, 'gauge_config_test', None)
+            gauge_file, "gauge_config_test", None
+        )
         watcher_conf = watcher_confs[0]
-        msg = 'failed to create watcher correctly when dps configured in gauge.yaml'
-        self.assertEqual(watcher_conf.dps[0], 'dp1', msg)
-        self.assertEqual(watcher_conf.type, 'port_stats', msg)
-        self.assertEqual(watcher_conf.db_type, 'prometheus', msg)
+        msg = "failed to create watcher correctly when dps configured in gauge.yaml"
+        self.assertEqual(watcher_conf.dps[0], "dp1", msg)
+        self.assertEqual(watcher_conf.type, "port_stats", msg)
+        self.assertEqual(watcher_conf.db_type, "prometheus", msg)
 
 
 if __name__ == "__main__":
     unittest.main()  # pytype: disable=module-attr
```

### Comparing `c65faucet-1.0.49/tests/unit/gauge/test_gauge.py` & `c65faucet-1.0.50/tests/unit/gauge/test_gauge.py`

 * *Files 10% similar despite different names*

```diff
@@ -38,162 +38,173 @@
         pass
 
 
 def create_mock_datapath(num_ports):
     """Mock a datapath by creating mocked datapath ports."""
 
     dp_id = random.randint(1, 5000)
-    dp_name = mock.PropertyMock(return_value='datapath')
+    dp_name = mock.PropertyMock(return_value="datapath")
 
     def table_by_id(i):
         """Mock a table by id"""
 
         table = mock.Mock()
-        table_name = mock.PropertyMock(return_value='table' + str(i))
+        table_name = mock.PropertyMock(return_value="table" + str(i))
         type(table).name = table_name
         return table
 
     def port_labels(port_no):
         """Provide labels for a port"""
 
         return {
-            'port': 'port%u' % port_no, 'port_description': 'port%u' % port_no,
-            'dp_id': hex(dp_id), 'dp_name': dp_name}
+            "port": "port%u" % port_no,
+            "port_description": "port%u" % port_no,
+            "dp_id": hex(dp_id),
+            "dp_name": dp_name,
+        }
 
     ports = {}
     for i in range(1, num_ports + 1):
         port = mock.Mock()
-        port_name = mock.PropertyMock(return_value='port' + str(i))
+        port_name = mock.PropertyMock(return_value="port" + str(i))
         type(port).name = port_name
         ports[i] = port
 
-    datapath = mock.Mock(ports=ports, dp_id=dp_id, port_labels=port_labels, table_by_id=table_by_id)
+    datapath = mock.Mock(
+        ports=ports, dp_id=dp_id, port_labels=port_labels, table_by_id=table_by_id
+    )
     type(datapath).name = dp_name
     return datapath
 
 
 def start_server(handler):
-    """ Starts a HTTPServer and runs it as a daemon thread """
+    """Starts a HTTPServer and runs it as a daemon thread"""
 
-    server = HTTPServer(('', 0), handler)
+    server = HTTPServer(("", 0), handler)
     server_thread = threading.Thread(target=server.serve_forever)
     server_thread.daemon = True
     server_thread.start()
     return server
 
 
 def port_state_msg(datapath, port_num, reason, status=0):
-    """ Create an OFPPortStatus message with random values. """
+    """Create an OFPPortStatus message with random values."""
 
-    port = parser.OFPPort(port_num,
-                          '00:00:00:d0:00:0' + str(port_num),
-                          datapath.ports[port_num].name,
-                          0,
-                          status,
-                          random.randint(1, 10000),
-                          random.randint(1, 10000),
-                          random.randint(1, 10000),
-                          random.randint(1, 10000),
-                          random.randint(1, 10000),
-                          random.randint(1, 10000)
-                          )
+    port = parser.OFPPort(
+        port_num,
+        "00:00:00:d0:00:0" + str(port_num),
+        datapath.ports[port_num].name,
+        0,
+        status,
+        random.randint(1, 10000),
+        random.randint(1, 10000),
+        random.randint(1, 10000),
+        random.randint(1, 10000),
+        random.randint(1, 10000),
+        random.randint(1, 10000),
+    )
 
     return parser.OFPPortStatus(datapath, reason, port)
 
 
 def port_stats_msg(datapath):
-    """ Create an OFPPortStatsReply with random values. """
+    """Create an OFPPortStatsReply with random values."""
 
     stats = []
     sec = random.randint(1, 10000)
     nsec = random.randint(0, 10000)
     for port_num in datapath.ports:
-        port_stats = parser.OFPPortStats(port_num,
-                                         random.randint(1, 10000),
-                                         random.randint(1, 10000),
-                                         random.randint(1, 10000),
-                                         random.randint(1, 10000),
-                                         random.randint(0, 10000),
-                                         random.randint(0, 10000),
-                                         random.randint(0, 10000),
-                                         random.randint(0, 10000),
-                                         random.randint(0, 10000),
-                                         random.randint(0, 10000),
-                                         random.randint(0, 10000),
-                                         random.randint(0, 10000),
-                                         sec,
-                                         nsec
-                                         )
+        port_stats = parser.OFPPortStats(
+            port_num,
+            random.randint(1, 10000),
+            random.randint(1, 10000),
+            random.randint(1, 10000),
+            random.randint(1, 10000),
+            random.randint(0, 10000),
+            random.randint(0, 10000),
+            random.randint(0, 10000),
+            random.randint(0, 10000),
+            random.randint(0, 10000),
+            random.randint(0, 10000),
+            random.randint(0, 10000),
+            random.randint(0, 10000),
+            sec,
+            nsec,
+        )
         stats.append(port_stats)
     return parser.OFPPortStatsReply(datapath, body=stats)
 
 
 def flow_stats_msg(datapath, instructions):
-    """ Create an OFPFlowStatsReply with random values. """
+    """Create an OFPFlowStatsReply with random values."""
 
     matches = generate_all_matches()
-    flow_stats = parser.OFPFlowStats(random.randint(0, 9),
-                                     random.randint(1, 10000),
-                                     random.randint(0, 10000),
-                                     random.randint(1, 10000),
-                                     random.randint(1, 10000),
-                                     random.randint(1, 10000),
-                                     0,
-                                     random.randint(1, 10000),
-                                     random.randint(1, 10000),
-                                     random.randint(1, 10000),
-                                     matches,
-                                     instructions
-                                     )
+    flow_stats = parser.OFPFlowStats(
+        random.randint(0, 9),
+        random.randint(1, 10000),
+        random.randint(0, 10000),
+        random.randint(1, 10000),
+        random.randint(1, 10000),
+        random.randint(1, 10000),
+        0,
+        random.randint(1, 10000),
+        random.randint(1, 10000),
+        random.randint(1, 10000),
+        matches,
+        instructions,
+    )
 
     return parser.OFPFlowStatsReply(datapath, body=[flow_stats])
 
 
 def generate_all_matches():
     """
     Generate all OpenFlow Extensible Matches (oxm) and return
     a single OFPMatch with all of these oxms. The value for each
     oxm is the largest value possible for the data type. For
     example, the largest number for a 4 bit int is 15.
     """
     matches = dict()
     for oxm_type in ofproto.oxm_types:
         if oxm_type.type == type_desc.MacAddr:
-            value = 'ff:ff:ff:ff:ff:ff'
+            value = "ff:ff:ff:ff:ff:ff"
         elif oxm_type.type == type_desc.IPv4Addr:
-            value = '255.255.255.255'
+            value = "255.255.255.255"
         elif oxm_type.type == type_desc.IPv6Addr:
-            value = 'ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff'
+            value = "ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff"
         elif isinstance(oxm_type.type, type_desc.IntDescr):
             value = 2**oxm_type.type.size - 1
         else:
             continue
 
         matches[oxm_type.name] = value
 
     return parser.OFPMatch(**matches)
 
 
 def logger_to_ofp(port_stats):
-    """ Translates between the logger stat name and the OpenFlow stat name"""
+    """Translates between the logger stat name and the OpenFlow stat name"""
 
-    return {'packets_out': port_stats.tx_packets,
-            'packets_in': port_stats.rx_packets,
-            'bytes_out': port_stats.tx_bytes,
-            'bytes_in': port_stats.rx_bytes,
-            'dropped_out': port_stats.tx_dropped,
-            'dropped_in': port_stats.rx_dropped,
-            'errors_out': port_stats.tx_errors,
-            'errors_in': port_stats.rx_errors
-            }
+    return {
+        "packets_out": port_stats.tx_packets,
+        "packets_in": port_stats.rx_packets,
+        "bytes_out": port_stats.tx_bytes,
+        "bytes_in": port_stats.rx_bytes,
+        "dropped_out": port_stats.tx_dropped,
+        "dropped_in": port_stats.rx_dropped,
+        "errors_out": port_stats.tx_errors,
+        "errors_in": port_stats.rx_errors,
+    }
 
 
 def get_matches(match_dict):
     """Create a set of match name and value tuples"""
-    return {(entry['OXMTlv']['field'], entry['OXMTlv']['value']) for entry in match_dict}
+    return {
+        (entry["OXMTlv"]["field"], entry["OXMTlv"]["value"]) for entry in match_dict
+    }
 
 
 def check_instructions(original_inst, logger_inst, test):
     """
     Check that the original instructions matches the
     instructions from the logger
     """
@@ -206,35 +217,35 @@
 
 def compare_flow_msg(flow_msg, flow_dict, test):
     """
     Compare the body section of an OFPFlowStatsReply
     message to a dict representation of it
     """
     for stat_name, stat_val in flow_dict.items():
-        if stat_name == 'match':
-            match_set = get_matches(stat_val['OFPMatch']['oxm_fields'])
+        if stat_name == "match":
+            match_set = get_matches(stat_val["OFPMatch"]["oxm_fields"])
             test.assertEqual(match_set, set(flow_msg.body[0].match.items()))
-        elif stat_name == 'instructions':
+        elif stat_name == "instructions":
             check_instructions(flow_msg.body[0].instructions, stat_val, test)
         else:
             test.assertEqual(getattr(flow_msg.body[0], stat_name), stat_val)
 
 
 class PretendInflux(QuietHandler):
     """An HTTP Handler that receives InfluxDB messages."""
 
     def do_POST(self):  # pylint: disable=invalid-name
-        """ Write request contents to the HTTP server,
-        if there is an output file to write to. """
+        """Write request contents to the HTTP server,
+        if there is an output file to write to."""
 
-        if hasattr(self.server, 'output_file'):
-            content_length = int(self.headers['content-length'])
+        if hasattr(self.server, "output_file"):
+            content_length = int(self.headers["content-length"])
             data = self.rfile.read(content_length)
-            data = data.decode('utf-8')
-            with open(self.server.output_file, 'w', encoding='utf-8') as log:
+            data = data.decode("utf-8")
+            with open(self.server.output_file, "w", encoding="utf-8") as log:
                 log.write(data)
 
         self.send_response(204)
         self.end_headers()
 
 
 class GaugePrometheusTests(unittest.TestCase):  # pytype: disable=module-attr
@@ -243,73 +254,79 @@
     prom_client = gauge_prom.GaugePrometheusClient(reg=CollectorRegistry())
 
     @staticmethod
     def parse_prom_output(output):
         """Parses the port stats from prometheus into a dictionary"""
 
         parsed_output = {}
-        for line in output.split('\n'):
+        for line in output.split("\n"):
             # discard comments and stats not related to port stats
-            if line.startswith('#') or not line.startswith(gauge_prom.PROM_PORT_PREFIX):
+            if line.startswith("#") or not line.startswith(gauge_prom.PROM_PORT_PREFIX):
                 continue
 
-            index = line.find('{')
+            index = line.find("{")
             # get the stat name e.g. of_port_rx_bytes and strip 'of_port_'
             prefix = gauge_prom.PROM_PORT_PREFIX + gauge_prom.PROM_PREFIX_DELIM
-            stat_name = line[0:index].replace(prefix, '')
+            stat_name = line[0:index].replace(prefix, "")
             # get the labels within {}
-            labels = line[index + 1:line.find('}')].split(',')
+            x, y = index + 1, line.find("}")
+            labels = line[x:y].split(",")
 
             for label in labels:
-                lab_name, lab_val = label.split('=', 1)
-                lab_val = lab_val.replace('"', '')
-                if lab_name == 'dp_id':
+                lab_name, lab_val = label.split("=", 1)
+                lab_val = lab_val.replace('"', "")
+                if lab_name == "dp_id":
                     dp_id = int(lab_val, 16)
-                elif lab_name == 'port':
+                elif lab_name == "port":
                     port_name = lab_val
 
             key = (dp_id, port_name)
-            stat_val = line.split(' ')[-1]
+            stat_val = line.split(" ")[-1]
             if key not in parsed_output:
                 parsed_output[key] = []
 
             parsed_output[key].append((stat_name, float(stat_val)))
 
         return parsed_output
 
     @staticmethod
     def get_prometheus_stats(addr, port):
         """Attempts to contact the prometheus server
         at the address to grab port stats."""
 
-        url = 'http://{}:{}'.format(addr, port)
+        url = "http://{}:{}".format(addr, port)
         session = requests.Session()
         adapter = requests.adapters.HTTPAdapter(max_retries=10)
-        session.mount('http://', adapter)
+        session.mount("http://", adapter)
         return session.get(url).text
 
     def test_poller(self):
         """Test the update method to see if it pushes port stats"""
 
         datapath = create_mock_datapath(2)
 
-        conf = mock.Mock(dp=datapath,
-                         type='',
-                         interval=1,
-                         prometheus_port=9303,
-                         prometheus_addr='localhost',
-                         use_test_thread=True
-                         )
+        conf = mock.Mock(
+            dp=datapath,
+            type="",
+            interval=1,
+            prometheus_port=9303,
+            prometheus_addr="localhost",
+            use_test_thread=True,
+        )
 
-        prom_poller = gauge_prom.GaugePortStatsPrometheusPoller(conf, '__name__', self.prom_client)
+        prom_poller = gauge_prom.GaugePortStatsPrometheusPoller(
+            conf, "__name__", self.prom_client
+        )
         prom_poller._running = True
         msg = port_stats_msg(datapath)
         prom_poller.update(time.time(), msg)
 
-        prom_lines = self.get_prometheus_stats(conf.prometheus_addr, conf.prometheus_port)
+        prom_lines = self.get_prometheus_stats(
+            conf.prometheus_addr, conf.prometheus_port
+        )
         prom_lines = self.parse_prom_output(prom_lines)
 
         for port_num, port in datapath.ports.items():
             port_stats = msg.body[int(port_num) - 1]
             stats = prom_lines[(datapath.dp_id, port.name)]
             stats_found = set()
 
@@ -320,79 +337,87 @@
             self.assertEqual(stats_found, set(gauge_prom.PROM_PORT_VARS))
 
     def test_port_state(self):
         """Test the update method to see if it pushes port state"""
 
         datapath = create_mock_datapath(2)
 
-        conf = mock.Mock(dp=datapath,
-                         type='',
-                         interval=1,
-                         prometheus_port=9303,
-                         prometheus_addr='localhost',
-                         use_test_thread=True
-                         )
+        conf = mock.Mock(
+            dp=datapath,
+            type="",
+            interval=1,
+            prometheus_port=9303,
+            prometheus_addr="localhost",
+            use_test_thread=True,
+        )
 
-        prom_poller = gauge_prom.GaugePortStatePrometheusPoller(conf, '__name__', self.prom_client)
+        prom_poller = gauge_prom.GaugePortStatePrometheusPoller(
+            conf, "__name__", self.prom_client
+        )
         prom_poller._running = True
         reasons = [ofproto.OFPPR_ADD, ofproto.OFPPR_DELETE, ofproto.OFPPR_MODIFY]
         for i in range(1, len(conf.dp.ports) + 1):
-
             msg = port_state_msg(conf.dp, i, reasons[i - 1])
             port_name = conf.dp.ports[i].name
             rcv_time = int(time.time())
             prom_poller.update(rcv_time, msg)
 
-            prom_lines = self.get_prometheus_stats(conf.prometheus_addr, conf.prometheus_port)
+            prom_lines = self.get_prometheus_stats(
+                conf.prometheus_addr, conf.prometheus_port
+            )
             prom_lines = self.parse_prom_output(prom_lines)
 
             stats = prom_lines[(datapath.dp_id, port_name)]
             stats_found = set()
 
             for stat_name, stat_val in stats:
-                msg_data = msg if stat_name == 'reason' else msg.desc
+                msg_data = msg if stat_name == "reason" else msg.desc
                 self.assertAlmostEqual(stat_val, getattr(msg_data, stat_name))
                 stats_found.add(stat_name)
 
             self.assertEqual(stats_found, set(gauge_prom.PROM_PORT_STATE_VARS))
 
     def test_flow_stats(self):
         """Check the update method of the GaugeFlowTablePrometheusPoller class"""
 
         datapath = create_mock_datapath(2)
 
-        conf = mock.Mock(dp=datapath,
-                         type='',
-                         interval=1,
-                         prometheus_port=9303,
-                         prometheus_addr='localhost',
-                         use_test_thread=True
-                         )
+        conf = mock.Mock(
+            dp=datapath,
+            type="",
+            interval=1,
+            prometheus_port=9303,
+            prometheus_addr="localhost",
+            use_test_thread=True,
+        )
 
-        prom_poller = gauge_prom.GaugeFlowTablePrometheusPoller(conf, '__name__', self.prom_client)
+        prom_poller = gauge_prom.GaugeFlowTablePrometheusPoller(
+            conf, "__name__", self.prom_client
+        )
         rcv_time = int(time.time())
         instructions = [parser.OFPInstructionGotoTable(1)]
         msg = flow_stats_msg(conf.dp, instructions)
         prom_poller.update(rcv_time, msg)
 
 
 class GaugeInfluxShipperTest(unittest.TestCase):  # pytype: disable=module-attr
     """Tests the InfluxShipper"""
 
     @staticmethod
     def create_config_obj(port=12345):
         """Create a mock config object that contains the necessary InfluxDB config"""
 
-        conf = mock.Mock(influx_host='localhost',
-                         influx_port=port,
-                         influx_user='gauge',
-                         influx_pwd='',
-                         influx_db='gauge',
-                         influx_timeout=10
-                         )
+        conf = mock.Mock(
+            influx_host="localhost",
+            influx_port=port,
+            influx_user="gauge",
+            influx_pwd="",
+            influx_db="gauge",
+            influx_timeout=10,
+        )
         return conf
 
     def get_values(self, dict_to_unpack):
         """Get all the values from a nested dictionary"""
 
         values = []
         for value in dict_to_unpack.values():
@@ -408,15 +433,17 @@
 
         server = None
 
         try:
             server = start_server(PretendInflux)
             shipper = gauge_influx.InfluxShipper()
             shipper.conf = self.create_config_obj(server.server_port)
-            points = [{'measurement': 'test_stat_name', 'fields': {'value': 1}}, ]
+            points = [
+                {"measurement": "test_stat_name", "fields": {"value": 1}},
+            ]
             shipper.ship_points(points)
         except (ConnectionError, ReadTimeout) as err:
             self.fail("Code threw an exception: {}".format(err))
         finally:
             if server:
                 server.socket.close()
                 server.shutdown()
@@ -425,100 +452,107 @@
         """Checks that even when there is a connection error,
         there is no exception thrown"""
 
         try:
             shipper = gauge_influx.InfluxShipper()
             shipper.conf = self.create_config_obj()
             shipper.logger = mock.Mock()
-            points = [{'measurement': 'test_stat_name', 'fields': {'value': 1}}, ]
+            points = [
+                {"measurement": "test_stat_name", "fields": {"value": 1}},
+            ]
             shipper.ship_points(points)
         except (ConnectionError, ReadTimeout) as err:
             self.fail("Code threw an exception: {}".format(err))
 
     def test_ship_no_config(self):
         """Check that no exceptions are thrown when
         there is no config"""
 
         try:
             shipper = gauge_influx.InfluxShipper()
-            points = [{'measurement': 'test_stat_name', 'fields': {'value': 1}}, ]
+            points = [
+                {"measurement": "test_stat_name", "fields": {"value": 1}},
+            ]
             shipper.ship_points(points)
         except (ConnectionError, ReadTimeout) as err:
             self.fail("Code threw an exception: {}".format(err))
 
     def test_point(self):
         """Checks that the points produced still have the variables given to it"""
 
         shipper = gauge_influx.InfluxShipper()
-        dp_name = 'faucet-1'
-        port_name = 'port1.0.1'
+        dp_name = "faucet-1"
+        port_name = "port1.0.1"
         rcv_time = int(time.time())
-        stat_name = 'test_stat_name'
+        stat_name = "test_stat_name"
         # max uint64 number
         stat_val = 2**64 - 1
 
-        port_point = shipper.make_port_point(dp_name, port_name, rcv_time, stat_name, stat_val)
+        port_point = shipper.make_port_point(
+            dp_name, port_name, rcv_time, stat_name, stat_val
+        )
         values = {dp_name, port_name, rcv_time, stat_name}
         port_vals = set(self.get_values(port_point))
         port_vals_stat = port_vals.difference(values)
         self.assertEqual(len(port_vals_stat), 1)
         self.assertAlmostEqual(port_vals_stat.pop(), stat_val)
 
-        tags = {'dp_name': dp_name, 'port_name': port_name}
+        tags = {"dp_name": dp_name, "port_name": port_name}
         point = shipper.make_point(tags, rcv_time, stat_name, stat_val)
         point_vals = set(self.get_values(point))
         point_vals_stat = point_vals.difference(values)
         self.assertEqual(len(point_vals_stat), 1)
         self.assertAlmostEqual(point_vals_stat.pop(), stat_val)
 
 
 class GaugeInfluxUpdateTest(unittest.TestCase):  # pytype: disable=module-attr
     """Test the Influx loggers update methods"""
 
     server = None
 
     def setUp(self):
-        """ Starts up an HTTP server to mock InfluxDB.
-        Also opens a new temp file for the server to write to """
+        """Starts up an HTTP server to mock InfluxDB.
+        Also opens a new temp file for the server to write to"""
 
         self.server = start_server(PretendInflux)
         self.temp_fd, self.server.output_file = tempfile.mkstemp()
 
     def tearDown(self):
-        """ Close the temp file (which should delete it)
-        and stop the HTTP server """
+        """Close the temp file (which should delete it)
+        and stop the HTTP server"""
         os.close(self.temp_fd)
         os.remove(self.server.output_file)
         self.server.socket.close()
         self.server.shutdown()
 
     def create_config_obj(self, datapath):
         """Create a mock config object that contains the necessary InfluxDB config"""
 
-        conf = mock.Mock(influx_host='localhost',
-                         influx_port=self.server.server_port,
-                         influx_user='gauge',
-                         influx_pwd='',
-                         influx_db='gauge',
-                         influx_timeout=10,
-                         interval=5,
-                         dp=datapath
-                         )
+        conf = mock.Mock(
+            influx_host="localhost",
+            influx_port=self.server.server_port,
+            influx_user="gauge",
+            influx_pwd="",
+            influx_db="gauge",
+            influx_timeout=10,
+            interval=5,
+            dp=datapath,
+        )
         return conf
 
     @staticmethod
     def parse_key_value(dictionary, kv_list):
         """
         When given a list consisting of strings such as: 'key1=val1',
         add to the dictionary as dictionary['key1'] = 'val1'.
         Ignore entries in the list which do not contain '='
         """
         for key_val in kv_list:
-            if '=' in key_val:
-                key, val = key_val.split('=')
+            if "=" in key_val:
+                key, val = key_val.split("=")
 
                 try:
                     val = float(val)
                     val = int(val)
                 except ValueError:
                     pass
 
@@ -532,158 +566,167 @@
         The tags are separated with a comma and the fields
         are separated with a space. The measurement always
         appears first, and the timestamp is always last
 
         """
         influx_data = dict()
 
-        tags = output_to_parse.split(',')
-        fields = tags[-1].split(' ')
+        tags = output_to_parse.split(",")
+        fields = tags[-1].split(" ")
         tags[-1] = fields[0]
-        influx_data['timestamp'] = int(fields[-1])
+        influx_data["timestamp"] = int(fields[-1])
         fields = fields[1:-1]
 
         self.parse_key_value(influx_data, tags)
         self.parse_key_value(influx_data, fields)
 
         return (tags[0], influx_data)
 
     def test_port_state(self):
-        """ Check the update method of the GaugePortStateInfluxDBLogger class"""
+        """Check the update method of the GaugePortStateInfluxDBLogger class"""
 
         conf = self.create_config_obj(create_mock_datapath(3))
-        db_logger = gauge_influx.GaugePortStateInfluxDBLogger(conf, '__name__', mock.Mock())
+        db_logger = gauge_influx.GaugePortStateInfluxDBLogger(
+            conf, "__name__", mock.Mock()
+        )
         db_logger._running = True
 
         reasons = [ofproto.OFPPR_ADD, ofproto.OFPPR_DELETE, ofproto.OFPPR_MODIFY]
         for i in range(1, len(conf.dp.ports) + 1):
-
             msg = port_state_msg(conf.dp, i, reasons[i - 1])
             rcv_time = int(time.time())
             db_logger.update(rcv_time, msg)
 
-            with open(self.server.output_file, 'r', encoding='utf-8') as log:
+            with open(self.server.output_file, "r", encoding="utf-8") as log:
                 output = log.read()
 
             influx_data = self.parse_influx_output(output)[1]
             data = {conf.dp.name, conf.dp.ports[i].name, rcv_time, reasons[i - 1]}
             self.assertEqual(data, set(influx_data.values()))
 
     def test_port_stats(self):
         """Check the update method of the GaugePortStatsInfluxDBLogger class"""
         conf = self.create_config_obj(create_mock_datapath(2))
-        db_logger = gauge_influx.GaugePortStatsInfluxDBLogger(conf, '__name__', mock.Mock())
+        db_logger = gauge_influx.GaugePortStatsInfluxDBLogger(
+            conf, "__name__", mock.Mock()
+        )
         db_logger._running = True
 
         msg = port_stats_msg(conf.dp)
         rcv_time = int(time.time())
 
         db_logger.update(rcv_time, msg)
-        with open(self.server.output_file, 'r', encoding='utf-8') as log:
+        with open(self.server.output_file, "r", encoding="utf-8") as log:
             output = log.readlines()
 
         for line in output:
             measurement, influx_data = self.parse_influx_output(line)
 
             # get the number at the end of the port_name
-            port_num = influx_data['port_name']  # pytype: disable=unsupported-operands
+            port_num = influx_data["port_name"]  # pytype: disable=unsupported-operands
             # get the original port stat value
-            port_stat_val = logger_to_ofp(
-                msg.body[port_num - 1])[measurement]  # pytype: disable=unsupported-operands
-
-            self.assertEqual(port_stat_val, influx_data['value'])
-            self.assertEqual(conf.dp.name, influx_data['dp_name'])
-            self.assertEqual(rcv_time, influx_data['timestamp'])
+            port_stat_val = logger_to_ofp(msg.body[port_num - 1])[
+                measurement
+            ]  # pytype: disable=unsupported-operands
+
+            self.assertEqual(port_stat_val, influx_data["value"])
+            self.assertEqual(conf.dp.name, influx_data["dp_name"])
+            self.assertEqual(rcv_time, influx_data["timestamp"])
 
     def test_flow_stats(self):
         """Check the update method of the GaugeFlowTableInfluxDBLogger class"""
 
         conf = self.create_config_obj(create_mock_datapath(0))
-        db_logger = gauge_influx.GaugeFlowTableInfluxDBLogger(conf, '__name__', mock.Mock())
+        db_logger = gauge_influx.GaugeFlowTableInfluxDBLogger(
+            conf, "__name__", mock.Mock()
+        )
         db_logger._running = True
 
         rcv_time = int(time.time())
         instructions = [parser.OFPInstructionGotoTable(1)]
         msg = flow_stats_msg(conf.dp, instructions)
         db_logger.update(rcv_time, msg)
 
-        other_fields = {'dp_name': conf.dp.name,
-                        'dp_id': hex(conf.dp.dp_id),
-                        'timestamp': rcv_time,
-                        'priority': msg.body[0].priority,
-                        'table_id': msg.body[0].table_id,
-                        'inst_count': len(msg.body[0].instructions),
-                        'vlan': msg.body[0].match.get('vlan_vid') ^ ofproto.OFPVID_PRESENT,
-                        'cookie': msg.body[0].cookie,
-                        }
+        other_fields = {
+            "dp_name": conf.dp.name,
+            "dp_id": hex(conf.dp.dp_id),
+            "timestamp": rcv_time,
+            "priority": msg.body[0].priority,
+            "table_id": msg.body[0].table_id,
+            "inst_count": len(msg.body[0].instructions),
+            "vlan": msg.body[0].match.get("vlan_vid") ^ ofproto.OFPVID_PRESENT,
+            "cookie": msg.body[0].cookie,
+        }
 
-        with open(self.server.output_file, 'r', encoding='utf-8') as log:
+        with open(self.server.output_file, "r", encoding="utf-8") as log:
             output = log.readlines()
 
         for line in output:
             measurement, influx_data = self.parse_influx_output(line)
 
             for stat_name, stat_val in influx_data.items():
-                if stat_name == 'value':
-                    if measurement == 'flow_packet_count':
+                if stat_name == "value":
+                    if measurement == "flow_packet_count":
                         self.assertEqual(msg.body[0].packet_count, stat_val)
-                    elif measurement == 'flow_byte_count':
+                    elif measurement == "flow_byte_count":
                         self.assertEqual(msg.body[0].byte_count, stat_val)
                     else:
                         self.fail("Unknown measurement")
 
                 elif stat_name in other_fields:
                     self.assertEqual(other_fields[stat_name], stat_val)
 
                 elif stat_name in msg.body[0].match:
                     self.assertEqual(msg.body[0].match.get(stat_name), stat_val)
 
                 else:
-                    self.fail("Unknown key: {} and value: {}".format(stat_name, stat_val))
+                    self.fail(
+                        "Unknown key: {} and value: {}".format(stat_name, stat_val)
+                    )
 
 
 class GaugeThreadPollerTest(unittest.TestCase):  # pytype: disable=module-attr
     """Tests the methods in the GaugeThreadPoller class"""
 
     def setUp(self):
         """Creates a gauge poller and initialises class variables"""
         self.interval = 1
         conf = mock.Mock(interval=self.interval)
-        self.poller = gauge_pollers.GaugeThreadPoller(conf, '__name__', mock.Mock())
+        self.poller = gauge_pollers.GaugeThreadPoller(conf, "__name__", mock.Mock())
         self.send_called = False
 
     def fake_send_req(self):
         """This should be called instead of the send_req method in the
         GaugeThreadPoller class, which just throws an error"""
         self.send_called = True
 
     def test_start(self):
-        """ Checks if the poller is started """
+        """Checks if the poller is started"""
         self.poller.send_req = self.fake_send_req
 
         self.poller.start(mock.Mock(), active=True)
         poller_thread = self.poller.thread
         hub.sleep(self.interval + 1)
         self.assertTrue(self.send_called)
         self.assertFalse(poller_thread.dead)
 
     def test_stop(self):
-        """ Check if a poller can be stopped """
+        """Check if a poller can be stopped"""
         self.poller.send_req = self.fake_send_req
 
         self.poller.start(mock.Mock(), active=True)
         poller_thread = self.poller.thread
         self.poller.stop()
         hub.sleep(self.interval + 1)
 
         self.assertFalse(self.send_called)
         self.assertTrue(poller_thread.dead)
 
     def test_active(self):
-        """Check if active reflects the state of the poller """
+        """Check if active reflects the state of the poller"""
         self.assertFalse(self.poller.is_active())
         self.assertFalse(self.poller.running())
         self.poller.start(mock.Mock(), active=True)
         self.assertTrue(self.poller.is_active())
         self.assertTrue(self.poller.running())
         self.poller.stop()
         self.assertFalse(self.poller.is_active())
@@ -719,35 +762,39 @@
 
 class GaugePortStatsPollerTest(GaugePollerTest):
     """Checks the GaugePortStatsPoller class"""
 
     def test_send_req(self):
         """Check that the poller sends a port stats request"""
         conf = mock.Mock(interval=1)
-        poller = gauge_pollers.GaugePortStatsPoller(conf, '__name__', mock.Mock())
+        poller = gauge_pollers.GaugePortStatsPoller(conf, "__name__", mock.Mock())
         self.check_send_req(poller, parser.OFPPortStatsRequest)
 
     def test_no_response(self):
         """Check that the poller doesnt throw an exception"""
-        poller = gauge_pollers.GaugePortStatsPoller(mock.Mock(), '__name__', mock.Mock())
+        poller = gauge_pollers.GaugePortStatsPoller(
+            mock.Mock(), "__name__", mock.Mock()
+        )
         self.check_no_response(poller)
 
 
 class GaugeFlowTablePollerTest(GaugePollerTest):
     """Checks the GaugeFlowTablePoller class"""
 
     def test_send_req(self):
         """Check that the poller sends a flow stats request"""
         conf = mock.Mock(interval=1)
-        poller = gauge_pollers.GaugeFlowTablePoller(conf, '__name__', mock.Mock())
+        poller = gauge_pollers.GaugeFlowTablePoller(conf, "__name__", mock.Mock())
         self.check_send_req(poller, parser.OFPFlowStatsRequest)
 
     def test_no_response(self):
         """Check that the poller doesnt throw an exception"""
-        poller = gauge_pollers.GaugeFlowTablePoller(mock.Mock(), '__name__', mock.Mock())
+        poller = gauge_pollers.GaugeFlowTablePoller(
+            mock.Mock(), "__name__", mock.Mock()
+        )
         self.check_no_response(poller)
 
 
 class GaugeWatcherTest(unittest.TestCase):  # pytype: disable=module-attr
     """Checks the loggers in watcher.py."""
 
     conf = None
@@ -756,179 +803,183 @@
 
     def setUp(self):
         """Creates a temporary file and directory and a mocked conf object"""
         self.temp_path = tempfile.mkdtemp()
         self.conf = mock.Mock(
             file=os.path.join(self.temp_path, self.tmp_filename),
             path=self.temp_path,
-            compress=False
+            compress=False,
         )
 
     def tearDown(self):
         """Removes the temporary directory and its contents"""
         shutil.rmtree(self.temp_path)
 
     def get_file_contents(self, filename=tmp_filename):
         """Return the contents of the temporary file and clear it"""
         filename = os.path.join(self.temp_path, filename)
-        with open(filename, 'r+', encoding='utf-8') as file_:
+        with open(filename, "r+", encoding="utf-8") as file_:
             contents = file_.read()
             file_.seek(0, 0)
             file_.truncate()
         return contents
 
     def test_port_state(self):
         """Check the update method in the GaugePortStateLogger class"""
 
-        reasons = {'unknown': 5,
-                   'add': ofproto.OFPPR_ADD,
-                   'delete': ofproto.OFPPR_DELETE,
-                   'up': ofproto.OFPPR_MODIFY,
-                   'down': ofproto.OFPPR_MODIFY
-                   }
+        reasons = {
+            "unknown": 5,
+            "add": ofproto.OFPPR_ADD,
+            "delete": ofproto.OFPPR_DELETE,
+            "up": ofproto.OFPPR_MODIFY,
+            "down": ofproto.OFPPR_MODIFY,
+        }
 
         # add an ofproto attribute to the datapath
         datapath = create_mock_datapath(1)
-        ofp_attr = {'ofproto': ofproto}
+        ofp_attr = {"ofproto": ofproto}
         datapath.configure_mock(**ofp_attr)
         self.conf.dp = datapath
-        logger = watcher.GaugePortStateLogger(self.conf, '__name__', mock.Mock())
+        logger = watcher.GaugePortStateLogger(self.conf, "__name__", mock.Mock())
         logger._running = True
 
         for reason, ofppr in reasons.items():
             state = 0
-            if reason == 'down':
+            if reason == "down":
                 state = ofproto.OFPPS_LINK_DOWN
 
             msg = port_state_msg(datapath, 1, ofppr, state)
             logger.update(time.time(), msg)
 
             log_str = self.get_file_contents().lower()
             self.assertTrue(reason in log_str)
-            self.assertTrue(msg.desc.name in log_str or 'port ' + str(msg.desc.port_no) in log_str)
+            self.assertTrue(
+                msg.desc.name in log_str or "port " + str(msg.desc.port_no) in log_str
+            )
 
-            hexs = re.findall(r'0x[0-9A-Fa-f]+', log_str)
+            hexs = re.findall(r"0x[0-9A-Fa-f]+", log_str)
             hexs = [int(num, 16) for num in hexs]
             self.assertTrue(datapath.dp_id in hexs or str(datapath.dp_id) in log_str)
 
     def test_port_stats(self):
         """Check the update method in the GaugePortStatsLogger class"""
 
         # add an ofproto attribute to the datapath
         datapath = create_mock_datapath(2)
-        ofp_attr = {'ofproto': ofproto}
+        ofp_attr = {"ofproto": ofproto}
         datapath.configure_mock(**ofp_attr)
 
         # add the datapath as an attribute to the config
-        dp_attr = {'dp': datapath}
+        dp_attr = {"dp": datapath}
         self.conf.configure_mock(**dp_attr)
 
-        logger = watcher.GaugePortStatsLogger(self.conf, '__name__', mock.Mock())
+        logger = watcher.GaugePortStatsLogger(self.conf, "__name__", mock.Mock())
         logger._running = True
         msg = port_stats_msg(datapath)
 
         original_stats = []
         for body in msg.body:
             original_stats.append(logger_to_ofp(body))
 
         logger.update(time.time(), msg)
 
         log_str = self.get_file_contents()
         for stat_name in original_stats[0]:
             stat_name = stat_name.split("_")
             # grab any lines that mention the stat_name
-            pattern = r'^.*{}.{}.*$'.format(stat_name[0], stat_name[1])
+            pattern = r"^.*{}.{}.*$".format(stat_name[0], stat_name[1])
             stats_list = re.findall(pattern, log_str, re.MULTILINE)
 
             for line in stats_list:
                 self.assertTrue(datapath.name in line)
                 # grab the port number (only works for single digit port nums)
-                index = line.find('port')
+                index = line.find("port")
                 port_num = int(line[index + 4])
                 # grab the number at the end of the line
-                last_n = re.search(r'(\d+)$', line)
+                last_n = re.search(r"(\d+)$", line)
                 assert last_n
                 val = int(last_n.group())
-                logger_stat_name = '_'.join((stat_name[0], stat_name[1]))
+                logger_stat_name = "_".join((stat_name[0], stat_name[1]))
                 original_val = original_stats[port_num - 1][logger_stat_name]
                 self.assertEqual(original_val, val)
 
     def test_flow_stats(self):
         """Check the update method in the GaugeFlowStatsLogger class"""
 
         # add an ofproto attribute to the datapath
         datapath = create_mock_datapath(0)
-        ofp_attr = {'ofproto': ofproto}
+        ofp_attr = {"ofproto": ofproto}
         datapath.configure_mock(**ofp_attr)
 
         # add the datapath as an attribute to the config
-        dp_attr = {'dp': datapath}
+        dp_attr = {"dp": datapath}
         self.conf.configure_mock(**dp_attr)
 
-        logger = watcher.GaugeFlowTableLogger(self.conf, '__name__', mock.Mock())
+        logger = watcher.GaugeFlowTableLogger(self.conf, "__name__", mock.Mock())
         logger._running = True
         instructions = [parser.OFPInstructionGotoTable(1)]
 
         msg = flow_stats_msg(datapath, instructions)
         rcv_time = time.time()
         rcv_time_str = logger._rcv_time(rcv_time)
         logger.update(rcv_time, msg)
         log_str = self.get_file_contents(
             "{}--flowtable--{}.json".format(datapath.name, rcv_time_str)
         )
 
-        yaml_dict = yaml_load(log_str)['OFPFlowStatsReply']['body'][0]['OFPFlowStats']
+        yaml_dict = yaml_load(log_str)["OFPFlowStatsReply"]["body"][0]["OFPFlowStats"]
 
         compare_flow_msg(msg, yaml_dict, self)
 
 
 class OSKenAppSmokeTest(unittest.TestCase):  # pytype: disable=module-attr
     """Test Gauge Ryu app."""
 
     def setUp(self):
         self.tmpdir = tempfile.mkdtemp()
-        os.environ['GAUGE_LOG'] = os.path.join(self.tmpdir, 'gauge.log')
-        os.environ['GAUGE_EXCEPTION_LOG'] = os.path.join(self.tmpdir, 'gauge-exception.log')
+        os.environ["GAUGE_LOG"] = os.path.join(self.tmpdir, "gauge.log")
+        os.environ["GAUGE_EXCEPTION_LOG"] = os.path.join(
+            self.tmpdir, "gauge-exception.log"
+        )
         self.os_ken_app = None
 
     def tearDown(self):
         valve_util.close_logger(self.os_ken_app.logger)
         valve_util.close_logger(self.os_ken_app.exc_logger)
         shutil.rmtree(self.tmpdir)
 
     @staticmethod
     def _fake_dp():
-        datapath = namedtuple('datapath', ['id', 'close'])(0, lambda: None)
+        datapath = namedtuple("datapath", ["id", "close"])(0, lambda: None)
         return datapath
 
     def _fake_event(self):
         datapath = self._fake_dp()
-        msg = namedtuple('msg', ['datapath'])(datapath)
+        msg = namedtuple("msg", ["datapath"])(datapath)
         event = EventOFPMsgBase(msg=msg)
         event.dp = msg.datapath
         return event
 
     @staticmethod
     def _write_config(config_file_name, config):
-        with open(config_file_name, 'w', encoding='utf-8') as config_file:
+        with open(config_file_name, "w", encoding="utf-8") as config_file:
             config_file.write(config)
 
     def test_gauge(self):
         """Test Gauge can be initialized."""
-        os.environ['GAUGE_CONFIG'] = '/dev/null'
-        self.os_ken_app = gauge.Gauge(
-            dpset={},
-            reg=CollectorRegistry())
+        os.environ["GAUGE_CONFIG"] = "/dev/null"
+        self.os_ken_app = gauge.Gauge(dpset={}, reg=CollectorRegistry())
         self.os_ken_app.reload_config(None)
         self.assertFalse(self.os_ken_app._config_files_changed())
         self.os_ken_app._update_watcher(None, self._fake_event())
         self.os_ken_app._start_watchers(self._fake_dp(), {}, time.time())
         for event_handler in (
-                self.os_ken_app._datapath_connect,
-                self.os_ken_app._datapath_disconnect):
+            self.os_ken_app._datapath_connect,
+            self.os_ken_app._datapath_disconnect,
+        ):
             event_handler(self._fake_event())
 
     def test_gauge_config(self):
         """Test Gauge minimal config."""
         faucet_conf1 = """
 vlans:
    100:
@@ -949,18 +1000,19 @@
    dp1:
        dp_id: 0x1
        interfaces:
            2:
                description: "2"
                native_vlan: 100
 """
-        os.environ['FAUCET_CONFIG'] = os.path.join(self.tmpdir, 'faucet.yaml')
-        self._write_config(os.environ['FAUCET_CONFIG'], faucet_conf1)
-        os.environ['GAUGE_CONFIG'] = os.path.join(self.tmpdir, 'gauge.yaml')
-        gauge_conf = """
+        os.environ["FAUCET_CONFIG"] = os.path.join(self.tmpdir, "faucet.yaml")
+        self._write_config(os.environ["FAUCET_CONFIG"], faucet_conf1)
+        os.environ["GAUGE_CONFIG"] = os.path.join(self.tmpdir, "gauge.yaml")
+        gauge_conf = (
+            """
 faucet_configs:
    - '%s'
 watchers:
     port_status_poller:
         type: 'port_state'
         all_dps: True
         db: 'prometheus'
@@ -975,40 +1027,40 @@
         interval: 60
         db: 'prometheus'
 dbs:
     prometheus:
         type: 'prometheus'
         prometheus_addr: '0.0.0.0'
         prometheus_port: 0
-""" % os.environ['FAUCET_CONFIG']
-        self._write_config(os.environ['GAUGE_CONFIG'], gauge_conf)
-        self.os_ken_app = gauge.Gauge(
-            dpset={},
-            reg=CollectorRegistry())
+"""
+            % os.environ["FAUCET_CONFIG"]
+        )
+        self._write_config(os.environ["GAUGE_CONFIG"], gauge_conf)
+        self.os_ken_app = gauge.Gauge(dpset={}, reg=CollectorRegistry())
         self.os_ken_app.reload_config(None)
         self.assertFalse(self.os_ken_app._config_files_changed())
         self.assertTrue(self.os_ken_app.watchers)
         self.os_ken_app.reload_config(None)
         self.assertTrue(self.os_ken_app.watchers)
         self.assertFalse(self.os_ken_app._config_files_changed())
         # Load a new FAUCET config.
-        self._write_config(os.environ['FAUCET_CONFIG'], faucet_conf2)
+        self._write_config(os.environ["FAUCET_CONFIG"], faucet_conf2)
         self.assertTrue(self.os_ken_app._config_files_changed())
         self.os_ken_app.reload_config(None)
         self.assertTrue(self.os_ken_app.watchers)
         self.assertFalse(self.os_ken_app._config_files_changed())
         # Load an invalid Gauge config
-        self._write_config(os.environ['GAUGE_CONFIG'], 'invalid')
+        self._write_config(os.environ["GAUGE_CONFIG"], "invalid")
         self.assertTrue(self.os_ken_app._config_files_changed())
         self.os_ken_app.reload_config(None)
         self.assertTrue(self.os_ken_app.watchers)
         # Keep trying to load a valid version.
         self.assertTrue(self.os_ken_app._config_files_changed())
         # Load good Gauge config back
-        self._write_config(os.environ['GAUGE_CONFIG'], gauge_conf)
+        self._write_config(os.environ["GAUGE_CONFIG"], gauge_conf)
         self.assertTrue(self.os_ken_app._config_files_changed())
         self.os_ken_app.reload_config(None)
         self.assertTrue(self.os_ken_app.watchers)
         self.assertFalse(self.os_ken_app._config_files_changed())
 
 
 if __name__ == "__main__":
```

### Comparing `c65faucet-1.0.49/tests/unit/gauge/test_main.py` & `c65faucet-1.0.50/tests/unit/gauge/test_main.py`

 * *Files 8% similar despite different names*

```diff
@@ -26,17 +26,19 @@
 
 class MainTestCase(unittest.TestCase):  # pytype: disable=module-attr
     """Test __main__ methods."""
 
     def test_parse_args(self):
         """Sanity check argument parsing."""
         self.assertFalse(parse_args([]).verbose)
-        self.assertTrue(parse_args(['--verbose']).verbose)
+        self.assertTrue(parse_args(["--verbose"]).verbose)
 
     def test_build_ryu_args(self):
         """Test build_ryu_args()."""
-        self.assertTrue(build_ryu_args(['gauge', '--use-stderr', '--use-syslog', '--verbose']))
-        self.assertFalse(build_ryu_args(['gauge', '--version']))
+        self.assertTrue(
+            build_ryu_args(["gauge", "--use-stderr", "--use-syslog", "--verbose"])
+        )
+        self.assertFalse(build_ryu_args(["gauge", "--version"]))
 
 
 if __name__ == "__main__":
     unittest.main()  # pytype: disable=module-attr
```

### Comparing `c65faucet-1.0.49/tests/unit/packaging/test_packaging.py` & `c65faucet-1.0.50/tests/unit/packaging/test_packaging.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,53 +27,55 @@
 import requirements
 
 
 class CheckDebianPackageTestCase(unittest.TestCase):  # pytype: disable=module-attr
     """Test debian packaging."""
 
     def _parse_deb_control(self, control_file):
-        with open(control_file, 'r', encoding='utf-8') as handle:
+        with open(control_file, "r", encoding="utf-8") as handle:
             control = handle.read()
 
         faucet_dpkg = str()
         for line in control.split("\n"):
             if line.startswith("Package: python3-faucet"):
                 faucet_dpkg += line
             elif faucet_dpkg:
                 if not line:
                     break
                 faucet_dpkg += "\n{}".format(line)
 
         faucet_dpkg = parse_control_fields(parse_deb822(faucet_dpkg))
         self.faucet_dpkg_deps = {}
-        for dep in faucet_dpkg['Depends']:
+        for dep in faucet_dpkg["Depends"]:
             if isinstance(dep, VersionedRelationship):
                 if dep.name not in self.faucet_dpkg_deps:
                     self.faucet_dpkg_deps[dep.name] = []
-                self.faucet_dpkg_deps[dep.name].append("{}{}".format(dep.operator, dep.version))
+                self.faucet_dpkg_deps[dep.name].append(
+                    "{}{}".format(dep.operator, dep.version)
+                )
 
     def _parse_pip_requirements(self, requirements_file):
         self.faucet_pip_reqs = {}
-        with open(requirements_file, 'r', encoding='utf-8') as handle:
+        with open(requirements_file, "r", encoding="utf-8") as handle:
             for pip_req in requirements.parse(handle):
                 self.faucet_pip_reqs[pip_req.name] = pip_req.specs
 
     def _pip_req_to_dpkg_name(self, pip_req):
         if pip_req in self.dpkg_name:
             return self.dpkg_name[pip_req]
         return "python3-" + pip_req
 
     def setUp(self):
-        src_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), '../../../')
-        control_file = os.path.join(src_dir, 'debian/control')
-        requirements_file = os.path.join(src_dir, 'requirements.txt')
+        src_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), "../../../")
+        control_file = os.path.join(src_dir, "debian/control")
+        requirements_file = os.path.join(src_dir, "requirements.txt")
 
         self.dpkg_name = {
-            'os_ken': 'python3-os-ken',
-            'prometheus_client': 'python3-prometheus-client'
+            "os_ken": "python3-os-ken",
+            "prometheus_client": "python3-prometheus-client",
         }
 
         self._parse_deb_control(control_file)
         self._parse_pip_requirements(requirements_file)
 
     def disabled_test_pip_reqs_in_deb_package(self):
         """Test pip requirements are listed as dependencies on debian package."""
@@ -89,35 +91,39 @@
             dpkg_name = self._pip_req_to_dpkg_name(pip_req)
 
             if pip_req_versions:
                 debian_package_dependencies = [
                     dpkg_name + x for x in self.faucet_dpkg_deps[dpkg_name]
                 ]
                 for pip_req_specifier, pip_req_version in pip_req_versions:
-                    if pip_req_specifier == '==':
+                    if pip_req_specifier == "==":
                         # debian/control is annoying about how it handles exact
                         # versions, calculate the debian equivalent of the
                         # pip requirements match and compare that
                         lower_version = pip_req_version
-                        lower_match = '>=' + lower_version
+                        lower_match = ">=" + lower_version
 
-                        upper_version = pip_req_version.split('.')
+                        upper_version = pip_req_version.split(".")
                         upper_version[-1] = str(int(upper_version[-1]) + 1)
-                        upper_version = '.'.join(upper_version)
-                        upper_match = '<<' + upper_version
+                        upper_version = ".".join(upper_version)
+                        upper_match = "<<" + upper_version
 
-                        self.assertIn(dpkg_name + lower_match, debian_package_dependencies)
-                        self.assertIn(dpkg_name + upper_match, debian_package_dependencies)
-                    elif pip_req_specifier == '<':
+                        self.assertIn(
+                            dpkg_name + lower_match, debian_package_dependencies
+                        )
+                        self.assertIn(
+                            dpkg_name + upper_match, debian_package_dependencies
+                        )
+                    elif pip_req_specifier == "<":
                         # debian/control uses << instead of <
-                        match = dpkg_name + '<<' + pip_req_version
+                        match = dpkg_name + "<<" + pip_req_version
                         self.assertIn(match, debian_package_dependencies)
-                    elif pip_req_specifier == '>':
+                    elif pip_req_specifier == ">":
                         # debian/control uses >> instead of >
-                        match = dpkg_name + '>>' + pip_req_version
+                        match = dpkg_name + ">>" + pip_req_version
                         self.assertIn(match, debian_package_dependencies)
                     else:
                         match = dpkg_name + pip_req_specifier + pip_req_version
                         self.assertIn(match, debian_package_dependencies)
 
 
 if __name__ == "__main__":
```

